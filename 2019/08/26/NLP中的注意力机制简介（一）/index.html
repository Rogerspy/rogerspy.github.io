<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  <title>NLP中的注意力机制简介（一） | Rogerspy&#39;s Home</title>
  
  <meta name="keywords" content="Machine Learning, Deep Learning, NLP">
  
  

  
  <link rel="alternate" href="/atom.xml" title="Rogerspy's Home">
  

  <meta name="HandheldFriendly" content="True">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <!-- meta -->
  
  
  <meta name="theme-color" content="#FFFFFF">
  <meta name="msapplication-TileColor" content="#1BC3FB">
  <meta name="msapplication-config" content="https://cdn.jsdelivr.net/gh/xaoxuu/assets@master/favicon/favicons/browserconfig.xml">
  

  <!-- link -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.css">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.10.1/css/all.min.css">
  
  
  <link rel="shortcut icon" type="image/x-icon" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@master/favicon/favicon.ico">
  <link rel="icon" type="image/x-icon" sizes="32x32" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@master/favicon/favicons/favicon-32x32.png">
  <link rel="apple-touch-icon" type="image/png" sizes="180x180" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@master/favicon/favicons/apple-touch-icon.png">
  <link rel="mask-icon" color="#1BC3FB" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@master/favicon/favicons/safari-pinned-tab.svg">
  <link rel="manifest" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@master/favicon/favicons/site.webmanifest">
  

  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.5/css/style.css">
  

  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>
  

  
  
  <!-- 时间线 -->
  <link rel="stylesheet" href="/css/timeline.css">
  <!-- 血小板-->
  <link rel="stylesheet" href="/live2d/css/live2d.css">
  <style>
	.article p .mjx-math {
	    font-family: Menlo,Monaco,courier,monospace,"Lucida Console",'Source Code Pro',"Microsoft YaHei",Helvetica,Arial,sans-serif,Ubuntu;
        background: none;
        padding: 2px;
        border-radius: 4px;
	}
  </style>
</head>

<body>
  
  
  <div class="cover-wrapper">
    <cover class='cover post half'>
      
        
  <h1 class='title'>Rogerspy's Home</h1>


  <div class="m_search">
    <form name="searchform" class="form u-search-form">
      <input type="text" class="input u-search-input" placeholder="" />
      <i class="icon fas fa-search fa-fw"></i>
    </form>
  </div>

<div class='menu navgation'>
  <ul class='h-list'>
    
      
        <li>
          <a class="nav home" href="/"
            
            
            id="home">
            <i class='fas fa-edit fa-fw'></i>&nbsp;博文
          </a>
        </li>
      
        <li>
          <a class="nav home" href="/video/"
            
            
            id="video">
            <i class='fas fa-film fa-fw'></i>&nbsp;视频
          </a>
        </li>
      
        <li>
          <a class="nav home" href="/material/"
            
              rel="nofollow"
            
            
            id="material">
            <i class='fas fa-briefcase fa-fw'></i>&nbsp;资料
          </a>
        </li>
      
        <li>
          <a class="nav home" href="/about/"
            
              rel="nofollow"
            
            
            id="about">
            <i class='fas fa-info-circle fa-fw'></i>&nbsp;关于
          </a>
        </li>
      
    
  </ul>
</div>

      
    </cover>
    <header class="l_header pure">
  <div id="loading-bar-wrapper">
    <div id="loading-bar" class="pure"></div>
  </div>

	<div class='wrapper'>
		<div class="nav-main container container--flex">
      <a class="logo flat-box" href='/' >
        
          Rogerspy's Home
        
      </a>
			<div class='menu navgation'>
				<ul class='h-list'>
          
  					
  						<li>
								<a class="nav flat-box" href="/blog/"
                  
                  
                  id="blog">
									<i class='fas fa-edit fa-fw'></i>&nbsp;博客
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/video/"
                  
                  
                  id="video">
									<i class='fas fa-film fa-fw'></i>&nbsp;视频小站
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/material/"
                  
                  
                  id="material">
									<i class='fas fa-briefcase fa-fw'></i>&nbsp;学习资料
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/diary/"
                  
                  
                  id="diary">
									<i class='fas fa-book fa-fw'></i>&nbsp;随心记
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/categories/"
                  
                    rel="nofollow"
                  
                  
                  id="categories">
									<i class='fas fa-folder-open fa-fw'></i>&nbsp;分类
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/tags/"
                  
                    rel="nofollow"
                  
                  
                  id="tags">
									<i class='fas fa-hashtag fa-fw'></i>&nbsp;标签
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/archives/"
                  
                    rel="nofollow"
                  
                  
                  id="archives">
									<i class='fas fa-archive fa-fw'></i>&nbsp;归档
								</a>
							</li>
      			
      		
				</ul>
			</div>

			
				<div class="m_search">
					<form name="searchform" class="form u-search-form">
						<input type="text" class="input u-search-input" placeholder="搜索" />
						<i class="icon fas fa-search fa-fw"></i>
					</form>
				</div>
			
			<ul class='switcher h-list'>
				
					<li class='s-search'><a class="fas fa-search fa-fw" href='javascript:void(0)'></a></li>
				
				<li class='s-menu'><a class="fas fa-bars fa-fw" href='javascript:void(0)'></a></li>
			</ul>
		</div>

		<div class='nav-sub container container--flex'>
			<a class="logo flat-box"></a>
			<ul class='switcher h-list'>
				<li class='s-comment'><a class="flat-btn fas fa-comments fa-fw" href='javascript:void(0)'></a></li>
        
          <li class='s-toc'><a class="flat-btn fas fa-list fa-fw" href='javascript:void(0)'></a></li>
        
			</ul>
		</div>
	</div>
</header>
	<aside class="menu-phone">
    <header>
		<nav class="menu navgation">
      <ul>
        
          
            <li>
							<a class="nav flat-box" href="/"
                
                
                id="home">
								<i class='fas fa-clock fa-fw'></i>&nbsp;近期文章
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/blog/archives/"
                
                  rel="nofollow"
                
                
                id="blogarchives">
								<i class='fas fa-archive fa-fw'></i>&nbsp;文章归档
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/blog/"
                
                
                id="blog">
								<i class='fas fa-edit fa-fw'></i>&nbsp;我的博客
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/video/"
                
                  rel="nofollow"
                
                
                id="video">
								<i class='fas fa-film fa-fw'></i>&nbsp;我的视频
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/material/"
                
                  rel="nofollow"
                
                
                id="material">
								<i class='fas fa-briefcase fa-fw'></i>&nbsp;学习资料
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/about/"
                
                  rel="nofollow"
                
                
                id="about">
								<i class='fas fa-info-circle fa-fw'></i>&nbsp;关于小站
							</a>
            </li>
          
       
      </ul>
		</nav>
    </header>
	</aside>
<script>setLoadingBarProgress(40);</script>

  </div>


  <div class="l_body">
    <div class='body-wrapper'>
      <div class='l_main'>
  

  
    <article id="post" class="post white-box article-type-post" itemscope itemprop="blogPost">
      


  <section class='meta'>
    
    
    <div class="meta" id="header-meta">
      
        
  
    <h1 class="title">
      <a href="/2019/08/26/NLP中的注意力机制简介（一）/">
        NLP中的注意力机制简介（一）
      </a>
    </h1>
  


      
      <div class='new-meta-box'>
        
          
        
          
            
  <div class='new-meta-item author'>
    <a href="https://rogerspy.gitee.io" rel="nofollow">
      
        <i class="fas fa-user" aria-hidden="true"></i>
      
      <p>Rogerspy</p>
    </a>
  </div>


          
        
          
            <div class="new-meta-item date">
  <a class='notlink'>
    <i class="fas fa-calendar-alt" aria-hidden="true"></i>
    <p>2019-08-26</p>
  </a>
</div>

          
        
          
            
  
  <div class='new-meta-item category'>
    <a href='/categories/nlp/' rel="nofollow">
      <i class="fas fa-folder-open" aria-hidden="true"></i>
      <p>NLP</p>
    </a>
  </div>


          
        
          
            
  
    <div class="new-meta-item browse busuanzi">
      <a class='notlink'>
        <i class="fas fa-eye" aria-hidden="true"></i>
        <p>
          <span id="busuanzi_value_page_pv">
            <i class="fas fa-spinner fa-spin fa-fw" aria-hidden="true"></i>
          </span>
        </p>
      </a>
    </div>
  


          
        
          
            

          
        
          
            
  
    <div style="margin-right: 10px;">
      <span class="post-time">
        <span class="post-meta-item-icon">
          <i class="fa fa-keyboard"></i>
          <span class="post-meta-item-text">  字数统计: </span>
          <span class="post-count">8k字</span>
        </span>
      </span>
      &nbsp; | &nbsp;
      <span class="post-time">
        <span class="post-meta-item-icon">
          <i class="fa fa-hourglass-half"></i>
          <span class="post-meta-item-text">  阅读时长≈</span>
          <span class="post-count">31分</span>
        </span>
      </span>
    </div>
  

          
        
      </div>
      
        <hr>
      
    </div>
  </section>


      <section class="article typo">
        <div class="article-entry" itemprop="articleBody">
          <h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>传统的注意力机制是与<em>encoder-decoder</em>架构相结合的，其中编码器和解码器都是<em>RNN</em>。首先是一段文本序列输入到编码器当中，然后将编码器的最后一个隐状态单元作为解码器的初始状态，然后一个接一个地产生目标序列，如下所示：</p>
<a id="more"></a>
<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/99e5fb3a-b8f1-4b14-8c27-c4cefef15695.gif" alt></p>
<p><em>encoder-decoder</em>结构在机器翻译任务中被广泛使用，尽管这种方法相较之前的统计翻译方法对翻译效果有很大的提升，但是这种基于<em>RNN</em>的结构也存在两个非常严重的问题：</p>
<ol>
<li><em>RNN</em>是可遗忘的（forgetful），这就意味着随着信息的传递，在经历了多个时间步长之后，旧的信息可能会丢失；（<strong>长程依赖问题</strong>）</li>
<li>在解码过程中没有利用词对齐信息，也就是说在产生目标序列的过程中每产生一个词是基于整个源序列的信息的，由于注意力分散，所以在产生目标序列的过程中也容易出错。（<strong>注意力分散问题</strong>）</li>
</ol>
<p>为了解决以上两个问题， <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener"><em>Bahdanau et al., 2014</em></a>和 <a href="https://arxiv.org/abs/1508.04025" target="_blank" rel="noopener"><em>Luong et al., 2015</em></a>提出了注意力机制。论文中仍然使用基于<em>RNN</em>的<em>encoder-decoder</em>结构，但是在解码过程中每个时间步长都会计算注意力得分，然后再生成该隐状态下对应的输出。下面我们以<em>seq2seq with attention</em>模型为例，介绍传统注意力机制过程：</p>
<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/48d0da24-2b86-49e7-a43b-beaab016bfa3.gif" alt></p>
<p>加入了注意力机制的<em>encoder-decoder</em>模型与标准的<em>encoder-encoder</em>有两点区别：</p>
<ol>
<li>传入<em>decoder</em>的数据不再是单纯的<em>encoder</em>的最后一个隐状态，而是<em>encoder</em>的所有隐状态；</li>
<li><em>decoder</em>在产生输出之前还会有额外的计算，用于确定当前<em>decoder</em>的隐状态应该对应到哪个<em>encoder</em>的隐状态，这样相当于集中注意力来产生与之对应的输出，这也是注意力机制得名的由来。</li>
</ol>
<p>注意力得分的计算过程如下所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/d97693da-0368-48e5-aa1f-a0e8f9a24e32.gif" alt></p>
<ol>
<li>准备好输入到<em>decoder</em>的所有<em>encoder</em>隐状态$\mathbf{h}_1,\mathbf{h}_2,\mathbf{h}_3$；</li>
<li>对每一个隐状态给定一个得分；</li>
<li>对得分使用<em>softmax</em>进行归一化，即为注意力得分；</li>
<li>使用注意力得分和每个<em>decoder</em>隐状态相乘，得到隐状态在当前<em>decoder</em>下的重要性；</li>
<li>最后对加权后的<em>encoder</em>隐状态进行求和，求和后的隐状态即相当于原始<em>encoder-decoder</em>中的单一隐状态。</li>
</ol>
<p>以上步骤在<em>decoder</em>中的每一个时间步长上都进行一次计算，仍然以翻译模型为例，对整个过程进行简单的介绍：</p>
<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5fe59340-18a3-49b8-89a7-47fe74dd2648.gif" alt></p>
<ol>
<li>首先是<em>encoder</em>中对输入的句子序列进行编码，得到$h_1, h_2, h_3$三个隐状态待用；</li>
<li>第一个<em>decoder</em>单元接收到源序列中的终止符<em><end></end></em>开始进行解码，<em>decoder</em>单元首先初始化一个隐状态权重，此时与接收到的源序列终止符<em><end></end></em>相互作用，但并不产生输出，而是产生一个新的隐状态$\mathbf{h}_4$；</li>
<li>利用上述计算注意力得分的方法对$\mathbf{h}_1,\mathbf{h}_2,\mathbf{h}_3$，进行注意力计算得到$C_4$；</li>
<li>将$C_4$和$\mathbf{h}_4$进行拼接得到一个新的向量；</li>
<li>将新向量传入到一个全连接层，全连接层的输出则是第一个<em>decoder</em>的输出，即目标序列的第一个元素；</li>
<li>将上述$\mathbf{h}_4$隐状态作为第二个<em>decoder</em>单元的初始隐状态，第一个<em>decoder</em>的输出向量（即全连接层的输出）作为第二个<em>decoder</em>的输入，再次重复上面的步骤$3-5$。</li>
</ol>
<p>从上面的过程可以看出，注意力机制的核心点在于注意力权重的计算，下面呢我们对这个计算过程进行公式化描述：</p>
<script type="math/tex; mode=display">
e_{ji} = a(\mathbf{h}_i^{in}, \mathbf{h}_j^{out})</script><script type="math/tex; mode=display">
\alpha_{ji} = \frac{exp(e_{ji})}{\sum_iexp(e_{ji})}</script><script type="math/tex; mode=display">
\mathbf{c}_j = \sum_i\alpha_{ji}\mathbf{h}_i^{in}</script><p>其中$\mathbf{h}_i^{in}$表示<em>decoder</em>单元的输入隐状态（即<em>encoder</em>第$i$个隐状态），$\mathbf{h}_j^{out}$表示当前<em>decoder</em>单元的输出，$e_{ji}$表示第$i$个<em>encoder</em>隐藏层对当前<em>decoder</em>单元输出的影响权重（$j$表示第$j$个<em>decoder</em>时间步长，为了方便这里指当前<em>step</em>）,此即为上述注意力全忠计算过程中的步骤$2$。公式$(2)$即使用<em>softmax</em>进行权重的归一化，$\alpha_{ji}$就是归一化后的注意力权重，即为注意力权重计算过程步骤$3$。公式$(3)$表示的就是注意力权重计算过程步骤$4-5$。</p>
<p>得到注意力权重以后，与当前的<em>decoder</em>隐状态$h_j$和输入$y_{j-1}$结合最终输出目标元素（<strong>注意：</strong>这里当前<em>decoder</em>的输入是上一个<em>decoder</em>的输出，可参考前面的<em>seq2seq</em>模型的整个过程的介绍。）</p>
<script type="math/tex; mode=display">
\mathbf{y}_j = f_y(\mathbf{h}_j^{out}, \mathbf{y}_{j-1},\mathbf{c}_j)</script><script type="math/tex; mode=display">
\mathbf{h}_{j+1}^{out} = f_h(\mathbf{h}_j^{out}, \mathbf{y}_j)</script><p>其中$f_y$和$f_h$表示<em>RNN</em>的输出层和隐状态层。</p>
<p>将上述计算过程对每一个<em>RNN</em>时间步长进行重复计算就能解决最开始我们提出的单纯基于<em>RNN</em>的<em>encoder-decoder</em>模型所带来的问题：</p>
<ol>
<li>对于长程依赖问题，由于计算注意力过程每次都会通过输入序列的隐状态来计算，因此$\mathbf{c}_j$不会受到源序列长度的影响；</li>
<li>对于注意力分散的问题，由于我们在每一次进行<em>decode</em>的时候都会进行注意力加权，注意力只会集中在对当前影响较大的部分序列上，而不是把注意力分散到整个序列中，因此有效的解决了注意力分散的问题。</li>
</ol>
<p>注意力机制自从被提出来以后，在机器翻译领域取得了非常打的成功，随后注意力机制被广泛应用于NLP各个领域中，并且为了解决不同领域中各种问题，注意力机制也出现了各种各样的变种，下图总结了最近几年关于注意力机制的一些比较重要的研究。</p>
<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/1565940458240.png" alt></p>
<p>下面我们就介绍一些比较重要的注意力模型。</p>
<h1 id="2-注意力模型的基本形式"><a href="#2-注意力模型的基本形式" class="headerlink" title="2. 注意力模型的基本形式"></a>2. 注意力模型的基本形式</h1><p>前面我们介绍了注意力机制在神经机器翻译中的应用，为了更一般化、形式化注意力机制，首先定义$V=\{\mathbf{v}_i\} \in \mathbb{R}^{n\times d_v}$，其中$\mathbf{v}_i$表示序列元素对应的向量（这里实际上应该是经过编码后的隐状态向量即前文所说的$\mathbf{h}_i$），重写之前的注意力模型：</p>
<script type="math/tex; mode=display">
e_i = a(\mathbf{v}_i, \mathbf{u})</script><script type="math/tex; mode=display">
\alpha_i = \frac{exp(e_i)}{\sum_i exp(e_i)}</script><script type="math/tex; mode=display">
c = \sum \alpha_i \mathbf{v}_i</script><p>其中$\mathbf{u} \in \mathbb{R}^{d_v}$表示序列$\{\mathbf{v}_i\}$对应的输出（<em>pattern vector</em>）， 而$e_i$则表示序列$\{\mathbf{v}_i\}$中第$i$个元素对输出的影响，多数情况下$d_u=d_v=d$，而$a(\cdot)$函数（又叫<em>Alignment score function</em>）通常有一下几种选择：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Alignment score function $a(\cdot )$</th>
<th style="text-align:center">Notes</th>
<th style="text-align:center">Citation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Content-base</td>
<td style="text-align:center">$a(\mathbf{v}_i, \mathbf{u})=cosine([\mathbf{v}_i, \mathbf{u}])$</td>
<td style="text-align:center">—</td>
<td style="text-align:center"><a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener">Graves2014</a></td>
</tr>
<tr>
<td style="text-align:center">Additive</td>
<td style="text-align:center">$a(\mathbf{v}_i, \mathbf{u}) = \mathbf{w}_2^{T}tanh(W_1[\mathbf{v}_i;\mathbf{u}])$</td>
<td style="text-align:center">—</td>
<td style="text-align:center"><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau2015</a></td>
</tr>
<tr>
<td style="text-align:center">Location-base</td>
<td style="text-align:center">$\alpha_i=softmax(W\mathbf{u})$</td>
<td style="text-align:center">只依赖目标输出，直接计算注意力权重</td>
<td style="text-align:center"><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a></td>
</tr>
<tr>
<td style="text-align:center">General</td>
<td style="text-align:center">$ a(\mathbf{v}_i, \mathbf{u}) = \mathbf{u}^TW\mathbf{v}_i$</td>
<td style="text-align:center">—</td>
<td style="text-align:center"><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a></td>
</tr>
<tr>
<td style="text-align:center">Dot-product</td>
<td style="text-align:center">$a(\mathbf{v}_i, \mathbf{u}) = \mathbf{u}^T\mathbf{v}_i$</td>
<td style="text-align:center">—</td>
<td style="text-align:center"><a href="https://arxiv.org/pdf/1508.4025.pdf" target="_blank" rel="noopener">Luong2015</a></td>
</tr>
<tr>
<td style="text-align:center">Scaled Dot-product</td>
<td style="text-align:center">$a(\mathbf{v}_i, \mathbf{u}) = \frac{\mathbf{u}^T\mathbf{v}_i}{\sqrt{n}}$</td>
<td style="text-align:center">其中$n$是$\mathbf{v}_i$的向量维度</td>
<td style="text-align:center"><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener">Vaswani2017</a></td>
</tr>
</tbody>
</table>
</div>
<p>以<em>Content-base</em>为例，计算出$e_i$在进行归一化，实际上可以认为这是一个相似性计算（类似$cos(\cdot)$）的操作，也就是说这里是在计算$\mathbf{v}_i$与输出之间的相似性。</p>
<p>为了避免混淆，我们先解释一下本文所采用的符号：</p>
<ul>
<li>小写符号表示标量，如$e_i$;</li>
<li>小写加粗符号表示向量， 如$\mathbf{v}_i$;</li>
<li>大写符号表示矩阵， 如$W$；</li>
<li>大写加粗符号表示张量，如$\mathbf{W}$；</li>
<li>$W, b$默认为待学习的权重矩阵和偏置</li>
</ul>
<h1 id="3-注意力机制的变种"><a href="#3-注意力机制的变种" class="headerlink" title="3. 注意力机制的变种"></a>3. 注意力机制的变种</h1><p>前面我们讨论了注意力机制的基本形式，由于其简单且有效使得注意力机制在NLP领域被广泛使用，但是通常在一些复杂性况下，这种简单形式的注意力机制仍然不够强大。随着注意力机制在不同场景下的应用，各种各样与之相关的注意力机制被提出来，总结起来可以概括为：<strong><em>基础注意力（Basic Attention）</em></strong>，<strong><em>多维注意力（Multi-dimensional Attention）</em></strong>，<strong><em>层级注意力（Hierarchical Attention）</em></strong>，<strong><em>自注意力（Self-Attention）</em></strong>，<strong><em>基于记忆的注意力（Memory-based Attention）</em></strong>，<strong><em>指针网络（Pointer Network）</em></strong>，<strong><em>特定任务下的注意力（Task-specific Attention）</em></strong>。</p>
<p>注意我们是根据不同的NLP任务应用场景对注意力机制进行分类的，在一些论文或者博客中通常见到的注意力模型分类将注意力分成<em>soft attention, hard attention, global attention, local attention, self-attention</em>等，与本文的分法不太相同，这是因为分类的依据不同。这里对这种分类方法的注意力机制做简单的介绍：</p>
<p><em>soft/hard attention</em>的区别类似于<em>word embedding</em>和<em>one-hot</em>的区别。<em>soft attention</em>的优势在于注意力平滑可微分，也就是说可以利用梯度下降进行权重更新，缺点就是如果输入较大（输入序列过长）比较消耗计算资源。<em>hard attention</em>的优势就是计算资源消耗少，但缺点就是不能微分，在进行权重更新的时候需要比较复杂的技术来处理这个问题。</p>
<p><em>global attention</em> 类似于<em>soft attention</em>将整个序列的信息融合进一个向量中，<em>local attention</em>类似于<em>hard attention</em>，它是在一个选定的窗口内进行<em>soft attention</em>，而在窗口之外仍然是<em>hard attention</em>。在特定任务下的注意力机制中我们会介绍一个<em>local attention</em>在机器翻译中的应用。</p>
<p>表1总结了不同注意力模型各自的特点：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">注意力机制类型</th>
<th style="text-align:center">特点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Basic Attention</td>
<td style="text-align:center">从一个序列中抽取出重要的元素</td>
</tr>
<tr>
<td style="text-align:center">Multi-dimensional Attention</td>
<td style="text-align:center">获得元素之间的多种操作类型</td>
</tr>
<tr>
<td style="text-align:center">Hierarchical Attention</td>
<td style="text-align:center">抽取全局和局部的重要信息</td>
</tr>
<tr>
<td style="text-align:center">Self-Attention</td>
<td style="text-align:center">抽取序列中隐含的上下文信息</td>
</tr>
<tr>
<td style="text-align:center">Memory-based Attention</td>
<td style="text-align:center">获取NLP任务中的隐藏的依赖关系</td>
</tr>
<tr>
<td style="text-align:center">Pointer Network</td>
<td style="text-align:center">对输入序列进行排序</td>
</tr>
<tr>
<td style="text-align:center">Task-specific Attention</td>
<td style="text-align:center">获取特定任务中的重要信息</td>
</tr>
</tbody>
</table>
</div>
<h2 id="3-1-多维注意力机制"><a href="#3-1-多维注意力机制" class="headerlink" title="3.1 多维注意力机制"></a>3.1 多维注意力机制</h2><p>前面我们介绍的基础注意力模型可以认为是一维的注意力机制（<em>1D-attention</em>），因为对于序列$V=\{\mathbf{v}_i\}$中的每一个元素计算出来的注意力权重$\alpha_i$都是一个标量，即$V=\{\mathbf{v}_i\}$对应的注意力权重为$\mathbf{\alpha}=\{\alpha_i\} \in \mathbb{R}^n$。但是在需要提取多种信息表示的情况下<em>1D-attention</em>就显得无能为力了，比如：</p>
<blockquote>
<p>Fish Burger is the best dish it tastes fresh.</p>
</blockquote>
<p>这样一句很简单的话，我们可以有不同的理解方式：</p>
<ol>
<li>晚餐之中哪道菜是最好的？</li>
<li>晚餐中<em>Fish Burger</em>这道菜和其他菜比起来怎么样？</li>
</ol>
<p>对于第一个理解方式，那么注意力应该在<em>Fish Burger</em>；而对于第二种理解方式，注意力应该在<em>the best</em>上面。也就是说针对不同语境（<em>representation space</em>），同一句话的注意力重点也是不同的。因此，我们需要多个注意力来处理这类情况，即所谓的<em>Multi-dimensional attention</em>。</p>
<p>最简单的方式就是多个<em>1D-attention</em>堆积成一个<em>Multi-dimensional attention</em>。例如<a href="http://www.aaai.org/Conferences/AAAI/2017/PreliminaryPapers/15-Wang-W-14441.pdf" target="_blank" rel="noopener">Wang et al., 2017</a>提出的<em>2D-attention</em>：给定一个输入序列$V=\{\mathbf{v}_i\}$，两个<em>representation space</em> $U = \{\mathbf{u}^a, \mathbf{u}^p\}$，权重张量$\mathbf{W}=\{W_a, W_p\}$，注意力权重的计算变为：</p>
<script type="math/tex; mode=display">
\mathbf{e}_i = tanh([\mathbf{v}_i^TW_a\mathbf{u}^a:\mathbf{v}_i^TW_p\mathbf{u}^p])</script><p>其中$[:]$表示两个向量的拼接，$E = \{\mathbf{e}_i\}$， 经过<em>softmax</em>归一化之后得到注意力矩阵$\Lambda = \{\alpha_i\}$。结果如图所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/1565763505669.png" alt></p>
<p><em>Aspect attention</em>和<em>Opinion attention</em>表示两个<em>representation space</em>，<em>Aspect attention</em>能把注意力集中在<em>Fish Burger</em>上，而<em>Opinion attention</em>能把注意力集中在<em>best</em>上，这样就解决了上面提到的问题。</p>
<p>然而这种多维注意力机制存在一个问题：</p>
<blockquote>
<p>如果每一维注意力都有相似的注意力，那么最后得到的信息会存在信息冗余的问题。</p>
</blockquote>
<p>针对这个问题<a href="https://openreview.net/pdf?id=BJC_jUqxe" target="_blank" rel="noopener">Lin et al. 2017</a>提出一种惩罚机制，即</p>
<script type="math/tex; mode=display">
P = \|(\Lambda \Lambda^T-I)\|_F^2</script><p>其中$\Lambda$ 表示注意力矩阵，$I$表示单位矩阵，$|\cdot|_F$表示<em>Frobenius</em>范数。类似于添加一个$L_2$正则项，这个惩罚项会乘以一个系数（超参数）然后和原始的损失函数一同计算最小化。</p>
<p>下面我们讨论一下这个惩罚项的有效性（原始论文中作者最先考虑的是<em>KL</em>散度，但是在实际测试过程中发现效果并不理想，所以才考虑添加这样一个惩罚项）。</p>
<p>考虑 $\Lambda = \{\alpha_a, \alpha_p\}$，由于$\alpha_a$和$\alpha_p$都经过了<em>softmax</em>，因此$\alpha_a$和$\alpha_p$可以被认为是离散概率分布中的概率质量（<em>probability mass</em>），对于$\Lambda \Lambda^T$矩阵中任意非对角的元素$\alpha_{ij}(i\neq j)$，对应于两个分布的元素级乘积求和：</p>
<script type="math/tex; mode=display">
0 \lt \alpha_{ij}=\sum_{k=1}^n\alpha_k^i\alpha_k^j \lt 1</script><p>其中$\alpha_k^i$和$\alpha_k^j$分别对应$\mathbf{\alpha}_a$和$\mathbf{\alpha}_p$的第$k$个元素。最极端情况下假设$\mathbf{\alpha}_a$和$\mathbf{\alpha}_p$没有任何交叉点，即$\alpha_{ij}=0$，否则$\alpha_{ij} \gt 0$；另一个极端情况是假设$\mathbf{\alpha}_a$和$\mathbf{\alpha}_p$完全相等，即两个注意力全部在同一个词上面，此时$\alpha_{ij}=1$。我们从$\Lambda \Lambda^T$中减去一个单位矩阵，相当于强迫$\Lambda \Lambda^T$的对角元素都约等于1，当$\alpha_{ij}=1(i=j)$时，$\alpha_{ij}=0(i \neq j) $，而我们的目的就是使每个维度上的注意力都不相同，所以我们需要$\alpha_{ij}=0(i \neq j)$，满足第一种极端假设。</p>
<p>实际上我们最小化这个惩罚项的时候是在将注意力矩阵$\Lambda $进行正交化，每一行都与其他行正交，即每一行都与其他行的注意力不同。</p>
<h2 id="3-2-层级注意力机制"><a href="#3-2-层级注意力机制" class="headerlink" title="3.2 层级注意力机制"></a>3.2 层级注意力机制</h2><h3 id="3-2-1-自下而上的层级注意力机制（Bottom-up-Hierarchical-Attention）"><a href="#3-2-1-自下而上的层级注意力机制（Bottom-up-Hierarchical-Attention）" class="headerlink" title="3.2.1 自下而上的层级注意力机制（Bottom-up Hierarchical Attention）"></a>3.2.1 自下而上的层级注意力机制（Bottom-up Hierarchical Attention）</h3><p>下面我们考虑文本分类任务。文本分类可以说是NLP中最基础的任务之一了。文本具有典型的层级结构：</p>
<blockquote>
<p>character =&gt; word =&gt; sentence =&gt; document</p>
</blockquote>
<p><em>document</em>的类别取决于构成它的<em>sentence</em>，而<em>sentence</em>的意思又取决于<em>word</em>以及词序，也就是说要想对文本很好的进行分类，需要抓住其中的<strong>关键词</strong>和<strong>关键句子</strong>（语序可以通过<em>RNN</em>等网络结构来解决，这里不讨论语序的问题）。</p>
<p>针对这个问题<a href="https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf" target="_blank" rel="noopener">Yang et al., 2016</a>提出了<em>Hierarchical Attention Networks (HAN)</em>，也就是层级注意力神经网络。<em>HAN</em>使用双向<em>GRU</em>网络对序列进行编码。给定一个序列$V_j=\{\mathbf{v}_i\}$表示文档中第$j$个句子，$\mathbf{v}_i$则表示句子中第$i$个词。给定一个序列集$\mathbf{D} = \{V_i\}$表示一篇文档。</p>
<ul>
<li><strong>Word Encoder</strong></li>
</ul>
<script type="math/tex; mode=display">
\mathbf{h}_i^{word} = BiGRU(\mathbf{v}_i)</script><script type="math/tex; mode=display">
\mathbf{u}_i^{word} = tanh(W_w\mathbf{h}_i^{word}+b_w)</script><p>这里<em>encoder</em>编码后的隐状态又经过了一层全连接层进行编码。</p>
<ul>
<li><strong>Word Attention</strong></li>
</ul>
<script type="math/tex; mode=display">
e_i^{word} = a(\mathbf{u}_i^{word}, \mathbf{u}_w) = \mathbf{u}_w{^T}\mathbf{u}_i{^{word}}</script><script type="math/tex; mode=display">
\alpha_i^{word} = softmax(e_i^{word})</script><script type="math/tex; mode=display">
\mathbf{s}_i = \sum_t \alpha_i^{word}\mathbf{h}_i^{word}</script><p>其中$\mathbf{s}_i$表示$V_i$经过编码后的句向量，$t$表示句子长度，$\mathbf{u}_w$是随机初始化的可学习参数。</p>
<ul>
<li><strong>Sentence Encoder</strong></li>
</ul>
<script type="math/tex; mode=display">
\mathbf{h}_i^{sent} = BiGRU(\mathbf{s}_i)</script><script type="math/tex; mode=display">
\mathbf{u}_i^{sent} = tanh(W_s\mathbf{h}_i^{sent}+b_s)</script><ul>
<li><strong>Sentence Attention</strong></li>
</ul>
<script type="math/tex; mode=display">
e_i^{sent} = a(\mathbf{u}_i^{sent}, \mathbf{u}_s) = \mathbf{u}_s{^T}\mathbf{u}_s^{sent}</script><script type="math/tex; mode=display">
\alpha_i^{sent} = softmax(e_i^{sent})</script><script type="math/tex; mode=display">
\mathbf{d} = \sum_i \alpha_i^{sent}\mathbf{h}_i^{sent}</script><p>其中$\mathbf{d}$就是<em>document</em>对应的向量。</p>
<h3 id="3-2-2-自上而下的层级注意力机制（Top-down-Hierarchical-Attention）"><a href="#3-2-2-自上而下的层级注意力机制（Top-down-Hierarchical-Attention）" class="headerlink" title="3.2.2 自上而下的层级注意力机制（Top-down Hierarchical Attention）"></a>3.2.2 自上而下的层级注意力机制（Top-down Hierarchical Attention）</h3><p><em>HAN</em>是一种<em>bottom-up</em>类型的层级注意力机制，除此之外还有<em>top-down</em>类型的层级注意力机制，比如<a href="https://pdfs.semanticscholar.org/6ed8/0c434270bfb09c37a0ac87e0b3f554becbe3.pdf" target="_blank" rel="noopener">Ji et al., 2017</a>提出使用层级注意力机制进行语法错误改正（<em>grammatical error correction</em>）。在这篇文章中，在编码阶段使用<em>GRU</em>进行编码，但是为了解决<em>OOV (out-of-vocabulary)</em>问题，作者提出了所谓的<em>Hybrid encoder</em>，即对于不在词表中的词（<em>UNK</em>），使用字母编码向量代替<em>UNK</em>，这个并非本文讨论的重点，因此下面我们集中讨论这篇文章中<em>top-down</em>层级注意力机制：<em>word-level attention</em>和<em>character-level attention</em>。</p>
<p>语法错误改正的特点正好和文本分类相反，文本分类是需要从词到句再到文档一层一层抓住文本所描述的语义，而语法错误改正是需要先理解文本的语义，然后根据语义对其中的词进行修改。词的修改也并不是修改全词，有可能只是其中几个字母的错误，因此需要用<em>top-down</em>层级注意力机制。</p>
<p>目标词的确定是通过以下两种方式：</p>
<ol>
<li>如果目标词在词表内，则使用<em>word-level attention</em></li>
<li>如果目标词不在词表内，则使用<em>character-level attention</em></li>
</ol>
<p>具体来说就是：</p>
<ol>
<li><p>以词为基础利用<em>GRU</em>进行句子编码（此处的编码指的是<em>Hybrid encoder</em>），得到隐状态$\mathbf{h}_1, \mathbf{h}_2, …, \mathbf{h}_T$；</p>
</li>
<li><p>对于源词和目标词都在词表中的情况，直接使用<em>word-level</em>注意力权重进行解码，生成目标序列；</p>
</li>
<li><p>对于目标词不在词表的情况（模型预测出来的目标词为<em>UNK</em>），$p(UNK|\mathbf{x}_c)\cdot p(char_seq|\mathbf{x}_c)$；</p>
</li>
<li><p>其中<em>character-sequence</em>的生成方法如下：</p>
<p>初始化<em>character-decoder</em>隐状态：$\mathbf{d}_s^\sim = Relu(W[\mathbf{c}_s;\mathbf{d}_s])$，其中$\mathbf{c}_s=\sum_T\alpha_j\mathbf{h}_j$，$\mathbf{d_s}$表示当前<em>word-decoder</em>的隐状态；</p>
<ul>
<li>若目标词对应的源词在词表中，直接使用<em>character-decoder</em>生成一个词；</li>
<li>若目标词对应的源词也不在此表中（即源词和目标词都为<em>UNK</em>），首先使用<em>character-decoder</em>生成一个词，然后利用编码时使用的<em>character-encoder</em>得到的<em>UNK</em>源词编码与生成的词进行加权求和即得到目标词</li>
</ul>
<script type="math/tex; mode=display">
\mathbf{d}_n^{cc} = Relu(W_c[\mathbf{c}_n^c;\mathbf{d}_n^c])</script></li>
</ol>
<p>另外，这种自上而下的注意力机制还被应用于为唱片挑选合适的海报<a href="https://arxiv.org/abs/1708.02977" target="_blank" rel="noopener">Yu et al., 2017</a>以及文本生成，这里就不详细介绍了。总的来说想这种需要先从全局信息来确定局部信息的场景都可以使用这种从上而下的层级注意力机制。</p>
<h2 id="3-3-自注意力机制"><a href="#3-3-自注意力机制" class="headerlink" title="3.3 自注意力机制"></a>3.3 自注意力机制</h2><p>回顾一下基础的注意力机制：给定一个序列$V=\{\mathbf{v}_i\}$和模式向量$\mathbf{u}$，对于每一个$\mathbf{v}_i$我们都可以计算一个注意力权重$\alpha_i=softmax(a(\mathbf{v}_i, \mathbf{u}))$。实际上这种注意力机制是一种外部注意力，正如前面我们讨论的，这种注意力实际上有点类似于计算$\mathbf{v}_i$与外部模式向量$ \mathbf{u}$的匹配度（或者相似度），注意力权重依赖于外部模式。</p>
<p>所谓自注意力机制指的是$\mathbf{u}$是输入序列自身的一部分，即：</p>
<script type="math/tex; mode=display">
e_{ij} = a(\mathbf{v}_i, \mathbf{v}_j)</script><script type="math/tex; mode=display">
\alpha_{ij} = softmax(e_{ij})</script><p>通常情况下，</p>
<script type="math/tex; mode=display">
\alpha_{ij} = softmax(tanh(\mathbf{w}^T[\mathbf{v}_i;\mathbf{v}_j]+b))</script><p>那么自注意力机制通常在什么场景下使用呢？</p>
<ul>
<li><p>获取序列内部元素之间的相互依赖关系。比如”<em>Volleyball match is in progress between ladies</em>“，这句话中其他词都是在围绕<em>match</em>进行描述的。从上面的形式描述也可以看出，自注意力机制是在计算序列内部元素之间的匹配度，因此当我们需要获取序列内部的依赖关系的时候自注意力机制就可发挥作用。</p>
</li>
<li><p>通过语义获取词义，类似于词义消歧。举个例子：</p>
<blockquote>
<p>I arrived at the bank after crossing the street.</p>
<p>I arrived at the bank after crossing the river.</p>
</blockquote>
<p>其中<em>bank</em>再第一句中最可能的意思应该是<em>银行</em>， 而第二句中的<em>bank</em>最可能的语义应该是<em>岸边</em>。因为自注意力能使第一句中的<em>bank</em>注意到<em>street</em>，而第二句中的<em>bank</em>注意到<em>river</em>。</p>
</li>
</ul>
<p>自注意力机制最成功的的案例应该属于最近大火的<em>Transformer</em>了，下面我们会对<em>Transformer</em>进行单独的介绍，这里就先跳过。</p>
<h2 id="3-4-基于记忆力的注意力机制"><a href="#3-4-基于记忆力的注意力机制" class="headerlink" title="3.4 基于记忆力的注意力机制"></a>3.4 基于记忆力的注意力机制</h2><p>为了介绍基于记忆力的注意力机制，我们重新构建旧的注意力机制。</p>
<p>假设有一组键值对$V =\{(\mathbf{k}_i, \mathbf{v}_i)\}$和一个检索向量（<em>query vector</em>）$\mathbf{q}$，重新定义注意力权重计算过程：</p>
<script type="math/tex; mode=display">
e_i = a(\mathbf{k}_i, \mathbf{q})</script><script type="math/tex; mode=display">
\alpha_i = softmax(e_i)</script><script type="math/tex; mode=display">
\mathbf{c} = \sum_i \alpha_i \mathbf{v}_i</script><p>这种注意力机制有点类似信息检索，给定一些文本和一个检索词，根据这些文本与检索词之间的匹配度（注意力权重$\{\alpha_i\}$）提取文本中的信息。当然信息检索是根据文本与检索词之间的匹配度进行文本排序，这里不做讨论。需要注意的是，如果所有的$\mathbf{k}_i=\mathbf{v}_i$的话，基于记忆力的注意力机制又转化成基础的注意力机制。更多关于$\mathbf{k}$和$\mathbf{v}$的内容我们会在<em>Transformer</em>专题里面进行讨论。</p>
<p>下面我们从两个方面详细讨论基于记忆力的注意力机制相对于基础记忆力机制的优势：</p>
<ul>
<li><p>可复用性（<em>Reusability</em>）</p>
<p>对于问答系统来说，一个基本的问题是答案与问题并不是直接相关的，举个例子：</p>
<blockquote>
<ol>
<li>Sam walks into the kitchen.</li>
<li>Sam picks up an apple.</li>
<li>Sam walks into bedroom.</li>
<li>Sam drops the apple. </li>
</ol>
<p>Q: Where is the apple?</p>
</blockquote>
<p>上面四个句子与<em>apple</em>匹配度比较高的应该是第2和第4句话，在用基础注意力机制的时候，注意力权重会集中在这两句话上，但是我们知道正确答案却在第3句话。因此，基础注意力机制没有办法解决这种间接相关（或者说需要进行一定程度上的推理）的任务。</p>
<p>但是如果我们通过迭代更新记忆信息来模拟时序推理的话，这个问题就可以得到解决。<a href="https://arxiv.org/pdf/1503.08895.pdf" target="_blank" rel="noopener">Sukhbaatar et al., 2015</a>通过结合记忆神经网络和注意力机制，为这类问题提出了一种解决方案。这里我们只讨论<em>Sukhbaatar et al., 2015</em>的论文中的注意力机制和记忆力神经网络结合的部分，其他的细节问题不做讨论，过程如下：</p>
<ol>
<li>初始化$\mathbf{q}_t=\phi_q(question)$；</li>
<li>计算句向量$\mathbf{k}_i=\phi_k(x_i )$；</li>
<li>$e_i=\mathbf{q}_t^T\mathbf{k}_i$；</li>
<li>$\alpha_i=softmax(e_i)$；</li>
<li>$\mathbf{v}_i=\phi_v(x_i)$；</li>
<li>$\mathbf{c}_i=\sum_i \alpha_i \mathbf{v}_i$；</li>
<li>更新问题向量$\mathbf{q}_{t+1}=\mathbf{q}_{t}+\mathbf{c}_i$；</li>
<li>重复$2-7$</li>
</ol>
<p>我们对上面的过程做一个解释：</p>
<p>第一步：初始化问题，将问题转化成向量，需要注意这里是一个一维向量，不是多个词向量序列构成的句矩阵，包括下面$x_i$指的是一个句子而不是词。$\phi_q, \phi_k, \phi_v$都是将句子转化成一维的向量，具体怎么转化的在文章中有具体的介绍，这里不是我们讨论的重点。</p>
<p>第二步：将第$i$个句子转化成句向量。</p>
<p>第三至第六步：就是标准的注意力加权求和。</p>
<p>第七步：更新问题向量。</p>
<p>整个推理过程如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/1565860574005.png" alt></p>
<p>另外有很多人在这方面也做了很多工作，比如<em><a href="https://arxiv.org/pdf/1506.07285.pdf" target="_blank" rel="noopener">Kumar et al. 2016</a>，<a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener">Graves et al., 2014</a></em>这里不详细介绍了。下面我们继续介绍第二点优势。</p>
</li>
<li><p>灵活性（<em>Flexibility</em>）</p>
<p>由于<em>键</em> 和<em>值</em> 有不同的向量表示，我们就可以自由的设计相应的嵌入层更好的获得相应的信息，比如分别设计针对问题和答案的向量表示，比如<a href>Miller et al., 2016</a>提出的<em>key-value memory network</em>：设计一个窗口结构，窗口中心的词为值向量，而窗口周围的词为键向量，用上面的例子则<em>apple</em>和<em>bedroom</em>为值向量，他们周围的词为键向量。</p>
<p>如果我们希望使用基础的注意力机制只需要令$\mathbf{k}=\mathbf{v}$即可，这也是其灵活性的表现。另外还有一些其他的键值向量的设计方式，这里我们不再过多介绍了。</p>
</li>
</ul>
<h2 id="3-5-指针网络"><a href="#3-5-指针网络" class="headerlink" title="3.5 指针网络"></a>3.5 指针网络</h2><p>对于一些排序的问题（<em>sorting</em>）或者“巡回推销员”（<em>travelling salesman</em>）问题输出并不是确定的，而是随着输入的变化输出也随之改变。面对这类问题前面提到的各种注意力模型都无能为力了，因此 <a href="https://arxiv.org/abs/1506.03134" target="_blank" rel="noopener">Vinyals, et al. 2015</a>提出了<strong>指针网络</strong>的方法。不同于其他注意力模型将上下文信息糅合进一个向量里面，指针网络是在解码阶段通过注意力机制一个一个从输入序列中选择元素进行输出。</p>
<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/1565925694004.png" alt></p>
<p>给定一个输入序列$V=\{\mathbf{v}_i\}$，指针网络输出一个序列$\mathbf{c}=\{c_i\}$，其中$c_i$是$V$中元素的索引。</p>
<script type="math/tex; mode=display">
\begin{align}
y_i &= p(c_i|c_1,...c_{i-1}, V) \nonumber\\ 
    &= \sigma(a(\mathbf{v}_i, \mathbf{u})) \nonumber\\
    &= \sigma(\mathbf{w}_2^{T}tanh(W_1[\mathbf{v}_i;\mathbf{u}])) \nonumber
\end{align}</script><h2 id="3-6-特定任务下的注意力机制"><a href="#3-6-特定任务下的注意力机制" class="headerlink" title="3.6 特定任务下的注意力机制"></a>3.6 特定任务下的注意力机制</h2><ul>
<li><strong>文本摘要</strong></li>
</ul>
<p><a href="https://www.aclweb.org/anthology/P17-1108" target="_blank" rel="noopener">Tan et al., 2017</a>在文本摘要任务中提出了一种类似于<em>PageRank</em>的<em>graph-based attention</em>。给定一篇文档$V=\{\mathbf{v}_i\}$，其中$\mathbf{v}_i$表示句子向量， 假设注意力分布为$\mathbf{\alpha}=\{\alpha_i\}$，则$\mathbf{\alpha}$满足一下条件：</p>
<script type="math/tex; mode=display">
\mathbf{\alpha}(t+1)=\lambda WD^{-1}\mathbf{\alpha}(t)+(1-\lambda)\mathbf{y}</script><p>其中$W$表示方阵$W(i,j)=\mathbf{v}_i^TM\mathbf{v}_j$，其中M是一个待学习的权重矩阵。$D$是一个对角矩阵，对角线上的元素$d_i=W(i,i)$，这是为了保证$WD^{-1}=1$，$\lambda$是一个阻尼系数，$\mathbf{y} \in \mathbb{R}^n$，并且$\mathbf{y} $中的所有元素都是$1/n$。</p>
<p>我们来考虑下这个注意力的物理意义是什么？忽略阻尼系数$\lambda$，我们来看下$WD^{-1}$表示什么：</p>
<script type="math/tex; mode=display">
WD^{-1} =(\mathbf{v}_i^TM\mathbf{v}_j)D^{-1}</script><p>其中的$W$与之前的$e_i$何其相似（公式$(9) $），而$D^{-1}$的存在是为了保证这一项能各元素相加为$1$，这又与$softmax(e_i)$何其相似。也就是说$WD^{-1}$我们也可以看成是一个注意力权重，而这个注意力权重是把注意力放在另一个注意力上。从公式$(32)$我们可以看出$\alpha_i(t)$注意力越大$\alpha_i(t+1)$也就会越大。也就是说，如果一个句子与其他重要的句子有更多的关联性的话，那么这个句子也会更重要。这个思想如同我们前面提到的和<em>PageRank</em>非常相似。</p>
<ul>
<li><strong>机器翻译</strong></li>
</ul>
<p><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong et al., 2015</a>为了解决机器翻译过程中，对长序列的所有元素都计算注意力太过消耗资源，因此提出一个叫做局部注意力机制（<em>local attention</em>）的方法。</p>
<p>所谓局部注意力机制其实很简单，就是从整个序列中设定一个窗口$D$，选取$[p_t-D, p_t+D]$范围内的序列计算注意力。其中$p_t$有两种确定方法：</p>
<ol>
<li>假设源序列和目标序列是一一对应的，则$p_t=t$，即以步长为1沿着序列依次向后滑动窗口取子序列；</li>
<li>假设源序列和目标序列并不是一一对应的，则$p_t=S\cdot sigmoid(\mathbf{v}_p^Ttanh(W_p\mathbf{h}_t))$，其中$S$表示序列的源总长度。然后计算注意力权重，最后视同高斯平滑得到输出。</li>
</ol>
<ul>
<li><strong>其他</strong></li>
</ul>
<p>除了以上介绍的各种各样的注意力机制以外，<a href="https://arxiv.org/pdf/1702.00887.pdf" target="_blank" rel="noopener">Kim et al., 2017</a>还提出一种叫做结构化注意力机制的方法。这种注意力机制可用于处理序列选择性问题，例如从一个序列中选出一个子序列，或者从语法树中选取一个子树。基本方法是将注意力模型看成是条件随机场<em>（CRF）</em>，引入一个隐变量$Z={\mathbf{z}_i}$，然后对序列进行编码-解码。这里不再详细介绍这种方法了，如有兴趣可以看论文原文。</p>
<h1 id="4-小结"><a href="#4-小结" class="headerlink" title="4. 小结"></a>4. 小结</h1><p>从上面的介绍我们可以看到，注意力机制的强大之处几乎在NLP领域的各个任务中都有相当出色的发挥：</p>
<ul>
<li>机器翻译，作为最先引入注意力机制的任务，注意力机制使得机器翻译的水平比之前有一个大的飞跃；</li>
<li>多模式理解，使用多维注意力机制使我们能从不同角度理解自然语言</li>
<li>文本分类，使用自下而上的层级注意力机制提升文本分类的能力</li>
<li>语法纠正，自上而下的注意力机制帮助我们完成这类人物</li>
<li>语言模型，自注意力机制，尤其是<em>Transformer</em>的横空出世，使得语言模型的水准达到了前所未有的高度</li>
<li>阅读理解，基于记忆力的注意力机制使得注意力机制具有了推理能力，帮助我们完成阅读理解的任务</li>
<li>文本摘要，在基于图的注意力机制下，完成了类似<em>PageRank</em>的抽取式文本摘要，虽然目前没有看到其在信息检索方面的研究，但是我相信把它应用于信息检索领域也会有不俗的表现</li>
<li>序列排序，指针网络的出现补足了注意力机制在排序方面的应用空白</li>
<li>另外对于分词，分块，语法解析等基础任务同样有相关的注意力机制的研究成果</li>
</ul>
<p>从上面的总结我们不难得出一个结论：<strong><em>Attention is all you need</em></strong> 此言非虚！</p>
<h1 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5. 参考资料"></a>5. 参考资料</h1><ol>
<li><a href="http://www.researchgate.net/publication/328953535_An_Introductory_Survey_on_Attention_Mechanisms_in_NLP_Problems" target="_blank" rel="noopener">An Introductory Survey on Attention Mechanisms in NLP Problems, </a><em>Dichao Hu</em>, 2018</li>
<li><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate, </a><em>Bahdanau et al.</em>, 2014</li>
<li><a href="https://arxiv.org/abs/1508.04025" target="_blank" rel="noopener">Effective Approaches to Attention-based Neural Machine Translation, </a><em>Luong et al.,</em> 2015</li>
<li><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank" rel="noopener">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention), </a><em>Jay Alammar</em></li>
<li><a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener">Neural Turing Machines, </a><em>Graves et al.,</em> 2014</li>
<li><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE, </a><em>Bahdanau et al.,</em> 2015</li>
<li><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener">Attention is all you need, </a><em>Vaswani et al.</em>, 2017</li>
<li><a href="http://www.aaai.org/Conferences/AAAI/2017/PreliminaryPapers/15-Wang-W-14441.pdf" target="_blank" rel="noopener">Coupled multi-layer attentions for co-extraction of aspect and opinion terms, </a><em>Wang et al.,</em> 2017</li>
<li><a href="https://openreview.net/pdf?id=BJC_jUqxe" target="_blank" rel="noopener">A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING,</a> <em>Lin et al.,</em> 2017</li>
<li><a href="https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf" target="_blank" rel="noopener">Hierarchical attention networks for document classification, </a><em>Yang et al.,</em> 2016</li>
<li><a href="https://pdfs.semanticscholar.org/6ed8/0c434270bfb09c37a0ac87e0b3f554becbe3.pdf" target="_blank" rel="noopener"> A nested attention neural hybrid model for grammatical error correction, </a><em>Ji et al.,</em> 2017</li>
<li><a href="https://arxiv.org/abs/1708.02977" target="_blank" rel="noopener"> Hierarchicallyattentive rnn for album summarization and storytelling, </a><em>Yu et al.,</em> 2017</li>
<li><a href="https://arxiv.org/pdf/1503.08895.pdf" target="_blank" rel="noopener">End-toend memory networks, </a><em>Sukhbaatar et al.,</em> 2015</li>
<li><a href="https://arxiv.org/pdf/1506.07285.pdf" target="_blank" rel="noopener">Ask me anything: Dynamic memory networks for natural language processing, </a><em>Kumar et al.,</em> 2016</li>
<li><a href="https://arxiv.org/abs/1606.03126" target="_blank" rel="noopener"> Key-value memory networks for directly reading documents, </a><em>Miller et al.,</em> 2016</li>
<li><a href="https://arxiv.org/abs/1506.03134" target="_blank" rel="noopener">Pointer Networks, </a><em>Vinyals, et al.,</em> 2015</li>
<li><a href="https://www.aclweb.org/anthology/P17-1108" target="_blank" rel="noopener"> Abstractive document summarization with a graph-based attentional neural model, </a><em>Tan et al.,</em> 2017</li>
<li><a href="https://arxiv.org/pdf/1702.00887.pdf" target="_blank" rel="noopener"> Structured attention networks, </a><em>Kim et al.,</em> 2017</li>
</ol>

        </div>
        
          


  <section class='meta' id="footer-meta">
    <hr>
    <div class='new-meta-box'>
      
        
          <div class="new-meta-item date" itemprop="dateUpdated" datetime="2022-01-12T16:42:29+08:00">
  <a class='notlink'>
    <i class="fas fa-clock" aria-hidden="true"></i>
    <p>最后更新于 2022年1月12日</p>
  </a>
</div>

        
      
        
          
  
  <div class="new-meta-item meta-tags"><a class="tag" href="/tags/attention/" rel="nofollow"><i class="fas fa-hashtag" aria-hidden="true"></i>&nbsp;<p>Attention</p></a></div>


        
      
        
          
  <div class="new-meta-item share -mob-share-list">
  <div class="-mob-share-list share-body">
    
      
        <a class="-mob-share-qq" title="QQ好友" rel="external nofollow noopener noreferrer"
          
          href="http://connect.qq.com/widget/shareqq/index.html?url=https://rogerspy.gitee.io/2019/08/26/NLP中的注意力机制简介（一）/&title=NLP中的注意力机制简介（一） | Rogerspy's Home&summary=1. 简介传统的注意力机制是与encoder-decoder架构相结合的，其中编码器和解码器都是RNN。首先是一段文本序列输入到编码器当中，然后将编码器的最后一个隐状态单元作为解码器的初始状态，然后一个接一个地产生目标序列，如下所示："
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/qq.png">
          
        </a>
      
    
      
        <a class="-mob-share-qzone" title="QQ空间" rel="external nofollow noopener noreferrer"
          
          href="https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=https://rogerspy.gitee.io/2019/08/26/NLP中的注意力机制简介（一）/&title=NLP中的注意力机制简介（一） | Rogerspy's Home&summary=1. 简介传统的注意力机制是与encoder-decoder架构相结合的，其中编码器和解码器都是RNN。首先是一段文本序列输入到编码器当中，然后将编码器的最后一个隐状态单元作为解码器的初始状态，然后一个接一个地产生目标序列，如下所示："
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/qzone.png">
          
        </a>
      
    
      
        <a class='qrcode' rel="external nofollow noopener noreferrer" href='data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPYAAAD2CAAAAADAeSUUAAADPElEQVR42u3aQW7bQAwFUN//0i7QVYFY8iepFDb1tApSdzRvvGCGn49H/Dz/Pj9/8+9z9Pujf03edfT5n595/MaDjY2N/SXs5+lzvnT+yvOjOdpPdYXEgo2Njb2VnWz0fHNvXjNeJ/lKCl8PNjY29i3ZSeHpff7aAoaNjY2NfV4YJuDqFpPDwsbGxsbOm0r5RaJaonq8/9RLw8bGxv54dq+5/5k//2K+jY2Njf2R7Of4ma9Z3Vt++TlcARsbG3sR+/yaMSls85GdXtuoUNiwsbGxV7DzSPW8tOSrTYZyrloHGxsbex/7vMAkl4RqQz+/flQvG+fFDBsbG3sfOxltqf7p3wtZy/en8dUFGxsbex87bwlVU9S86Z9cXZLQt3D02NjY2F/Ozq8HvfGdXpTbC5ULXx42Njb2OnY1xM2LR7X4VZtT58dx+DM2Njb2Ona+RD73Ut3opAQm//dFKoKNjY29gl1t3CQlqhrH5tFyr5kVxQPY2NjYK9iT0LTXDJpce3oNpubIDjY2NvYHs3vNmkkkkMe9zbw6uCxhY2Nj341dHazpXTPyoZz8ON6M7GBjY2OvYOeMavnJDzHfdLV0XZAqY2NjY388ewJLVshD3DyKqH5tL9bBxsbGXsSujtQkJarQ3IlXe8ZPYQAIGxsbex07vyrMX9x7y8XjO9jY2Njr2NWmUi8YyIOH3hHn8QY2Njb2DvakHV8NbnvFMmkhlcseNjY29iJ2Plg5D2Xz1ZKi2GtIYWNjY29lJxeGvHj0NtcLa3uXJWxsbOx97ElIkLTsq2Uv2Vv1a3tTxrCxsbFXsKvtm6tKzqQ51QsMsLGxse/ALvwpH/Sr8gg2yqXjPVQV2NjY2PvYvaGZ3txQfrFJqNVwGhsbG3srOy9UyUavioHzwlY+SmxsbOwvZD+LT7VlP29RzdtPL1bAxsbGXsTuhQTVLeYtpzlmcqzY2NjY38tOilbvFCcBQDVUKMcD2NjY2OvYvV7UvFDNC2SziGJjY2PfmJ0XiaSAzQtes6RhY2Nj35I9H5TMY4bq2E254GFjY2OvYydNpWobKNlK7+15II2NjY19B3Y16J0PX1bLVT5ClBwWNjY29iL2H4mA3nL6RMSnAAAAAElFTkSuQmCC'>
        
          <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/wechat.png">
        
        </a>
      
    
      
        <a class="-mob-share-weibo" title="微博" rel="external nofollow noopener noreferrer"
          
          href="http://service.weibo.com/share/share.php?url=https://rogerspy.gitee.io/2019/08/26/NLP中的注意力机制简介（一）/&title=NLP中的注意力机制简介（一） | Rogerspy's Home&summary=1. 简介传统的注意力机制是与encoder-decoder架构相结合的，其中编码器和解码器都是RNN。首先是一段文本序列输入到编码器当中，然后将编码器的最后一个隐状态单元作为解码器的初始状态，然后一个接一个地产生目标序列，如下所示："
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/weibo.png">
          
        </a>
      
    
  </div>
</div>



        
      
    </div>
  </section>


        
        
            <div class="prev-next">
                
                    <section class="prev">
                        <span class="art-item-left">
                            <h6><i class="fas fa-chevron-left" aria-hidden="true"></i>&nbsp;上一页</h6>
                            <h4>
                                <a href="/2019/08/27/NLP中的注意力机制简介（二）/" rel="prev" title="NLP中的注意力机制简介（二）">
                                  
                                      NLP中的注意力机制简介（二）
                                  
                                </a>
                            </h4>
                            
                                
                                <h6 class="tags">
                                    <a class="tag" href="/tags/attention/"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>Attention</a> <a class="tag" href="/tags/transformer/"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>Transformer</a>
                                </h6>
                            
                        </span>
                    </section>
                
                
            </div>
        
      </section>
    </article>
  

  
    <!-- 显示推荐文章和评论 -->



  <article class="post white-box comments">
    <section class="article typo">
      <h4><i class="fas fa-comments fa-fw" aria-hidden="true"></i>&nbsp;评论</h4>
      
      
      
        <section id="comments">
          <div id="gitalk-container"></div>
        </section>
      
      
    </section>
  </article>


  




<!-- 根据页面mathjax变量决定是否加载MathJax数学公式js -->

  <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": {
      preferredFont: "TeX",
      availableFonts: ["STIX","TeX"],
      linebreaks: { automatic:true },
      EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
      inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
      processEscapes: true,
      ignoreClass: "tex2jax_ignore|dno",
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: { autoNumber: "AMS" },
      noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
      Macros: { href: "{}" }
    },
    messageStyle: "none"
  });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += (all[i].SourceElement().parentNode.className ? ' ' : '') + 'has-jax';
    }
  });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>




  <script>
    window.subData = {
      title: 'NLP中的注意力机制简介（一）',
      tools: true
    }
  </script>


</div>
<aside class='l_side'>
  
    
    
      
        
          
          
            <section class='widget shake author'>
  <div class='content pure'>
    
      <div class='avatar'>
        <img class='avatar' src='https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/65-1Z31313530JC.jpeg'/>
      </div>
    
    
    
      <div class="social-wrapper">
        
          
            <a href="/atom.xml"
              class="social fas fa-rss flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
          
            <a href="mailto:rogerspy@163.com"
              class="social fas fa-envelope flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
          
            <a href="https://github.com/rogerspy"
              class="social fab fa-github flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
          
            <a href="https://music.163.com/#/user/home?id=1960721923"
              class="social fas fa-headphones-alt flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
      </div>
    
  </div>
</section>

          
        
      
        
          
          
            
  <section class='widget toc-wrapper'>
    
<header class='pure'>
  <div><i class="fas fa-list fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;本文目录</div>
  
    <div class='wrapper'><a class="s-toc rightBtn" rel="external nofollow noopener noreferrer" href="javascript:void(0)"><i class="fas fa-thumbtack fa-fw"></i></a></div>
  
</header>

    <div class='content pure'>
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-简介"><span class="toc-text">1. 简介</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-注意力模型的基本形式"><span class="toc-text">2. 注意力模型的基本形式</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-注意力机制的变种"><span class="toc-text">3. 注意力机制的变种</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-多维注意力机制"><span class="toc-text">3.1 多维注意力机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-层级注意力机制"><span class="toc-text">3.2 层级注意力机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-自下而上的层级注意力机制（Bottom-up-Hierarchical-Attention）"><span class="toc-text">3.2.1 自下而上的层级注意力机制（Bottom-up Hierarchical Attention）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-自上而下的层级注意力机制（Top-down-Hierarchical-Attention）"><span class="toc-text">3.2.2 自上而下的层级注意力机制（Top-down Hierarchical Attention）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-自注意力机制"><span class="toc-text">3.3 自注意力机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-基于记忆力的注意力机制"><span class="toc-text">3.4 基于记忆力的注意力机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5-指针网络"><span class="toc-text">3.5 指针网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-6-特定任务下的注意力机制"><span class="toc-text">3.6 特定任务下的注意力机制</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-小结"><span class="toc-text">4. 小结</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-参考资料"><span class="toc-text">5. 参考资料</span></a></li></ol>
    </div>
  </section>


          
        
      
        
          
          
            <section class='widget grid'>
  
<header class='pure'>
  <div><i class="fas fa-map-signs fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;站内导航</div>
  
</header>

  <div class='content pure'>
    <ul class="grid navgation">
      
        <li><a class="flat-box" " href="/"
          
          
          id="home">
          
            <i class="fas fa-clock fa-fw" aria-hidden="true"></i>
          
          近期文章
        </a></li>
      
        <li><a class="flat-box" " href="/blog/"
          
          
          id="blog">
          
            <i class="fas fa-edit fa-fw" aria-hidden="true"></i>
          
          我的博客
        </a></li>
      
        <li><a class="flat-box" " href="/paper_note/"
          
          
          id="paper_note">
          
            <i class="fas fa-book fa-fw" aria-hidden="true"></i>
          
          论文笔记
        </a></li>
      
        <li><a class="flat-box" " href="/algorithm/"
          
          
          id="algorithm">
          
            <i class="fas fa-cube fa-fw" aria-hidden="true"></i>
          
          算法基础
        </a></li>
      
        <li><a class="flat-box" " href="/leetcode/"
          
          
          id="leetcode">
          
            <i class="fas fa-code fa-fw" aria-hidden="true"></i>
          
          Leetcode
        </a></li>
      
        <li><a class="flat-box" " href="/video/"
          
          
          id="video">
          
            <i class="fas fa-film fa-fw" aria-hidden="true"></i>
          
          视频小站
        </a></li>
      
        <li><a class="flat-box" " href="/material/"
          
          
          id="material">
          
            <i class="fas fa-briefcase fa-fw" aria-hidden="true"></i>
          
          学习资料
        </a></li>
      
        <li><a class="flat-box" " href="/dataset/"
          
          
          id="dataset">
          
            <i class="fas fa-database fa-fw" aria-hidden="true"></i>
          
          数据集
        </a></li>
      
        <li><a class="flat-box" " href="/articles/"
          
          
          id="articles">
          
            <i class="fas fa-sticky-note fa-fw" aria-hidden="true"></i>
          
          杂文天地
        </a></li>
      
        <li><a class="flat-box" " href="/archives/"
          
            rel="nofollow"
          
          
          id="archives">
          
            <i class="fas fa-archive fa-fw" aria-hidden="true"></i>
          
          文章归档
        </a></li>
      
        <li><a class="flat-box" " href="/personal_center/"
          
          
          id="personal_center">
          
            <i class="fas fa-university fa-fw" aria-hidden="true"></i>
          
          个人中心
        </a></li>
      
        <li><a class="flat-box" " href="/about/"
          
            rel="nofollow"
          
          
          id="about">
          
            <i class="fas fa-info-circle fa-fw" aria-hidden="true"></i>
          
          关于小站
        </a></li>
      
    </ul>
  </div>
</section>

          
        
      
        
          
          
            <section class='widget list'>
  
<header class='pure'>
  <div><i class="fas fa-terminal fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;机器学习框架</div>
  
</header>

  <div class='content pure'>
    <ul class="entry">
      
        <li><a class="flat-box" href="https://rogerspy.gitee.io/pytorch-zh/"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-star fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;PyTorch 中文文档
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://keras-zh.readthedocs.io/"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-star fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Keras 中文文档
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://tensorflow.google.cn/"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-star fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Tensorflow 中文文档
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="http://scikitlearn.com.cn/"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-star fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Scikit Learn 中文文档
          </div>
          
        </a></li>
      
    </ul>
  </div>
</section>

          
        
      
        
          
          
            <section class='widget list'>
  
<header class='pure'>
  <div><i class="fas fa-wrench fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;百宝箱</div>
  
</header>

  <div class='content pure'>
    <ul class="entry">
      
        <li><a class="flat-box" href="https://rogerspy.github.io/excalidraw-claymate/"
          
          
            target="_blank"
          
          >
          <div class='name'>
            
              <i class="fas fa-magic fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Excalidraw-Claymate
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://rogerspy.github.io/jupyterlite/"
          
          
            target="_blank"
          
          >
          <div class='name'>
            
              <i class="fas fa-terminal fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;JupyterLite
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://rogerspy.github.io/kanban/"
          
          
            target="_blank"
          
          >
          <div class='name'>
            
              <i class="fas fa-table fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Kanban
          </div>
          
        </a></li>
      
    </ul>
  </div>
</section>

          
        
      
        
          
          
            <section class='widget list'>
  
<header class='pure'>
  <div><i class="fas fa-eye fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;睁眼看世界</div>
  
</header>

  <div class='content pure'>
    <ul class="entry">
      
        <li><a class="flat-box" href="https://deeplearn.org/"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-link fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Deep Learning Monitor
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://paperswithcode.com/sota"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-link fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Browse State-of-the-Art
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://huggingface.co/transformers/"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-link fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Transformers
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://huggingface.co/models"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-link fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Transformers-models
          </div>
          
        </a></li>
      
    </ul>
  </div>
</section>

          
        
      
        
          
          
            
  <section class='widget category'>
    
<header class='pure'>
  <div><i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;文章分类</div>
  
    <a class="rightBtn"
    
      rel="nofollow"
    
    
    href="/categories/"
    title="categories/">
    <i class="fas fa-expand-arrows-alt fa-fw"></i></a>
  
</header>

    <div class='content pure'>
      <ul class="entry">
        
          <li><a class="flat-box" " href="/categories/nl2sql/"><div class='name'>NL2SQL</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" " href="/categories/nlp/"><div class='name'>NLP</div><div class='badge'>(23)</div></a></li>
        
          <li><a class="flat-box" " href="/categories/博客转载/"><div class='name'>博客转载</div><div class='badge'>(7)</div></a></li>
        
          <li><a class="flat-box" " href="/categories/数据结构与算法/"><div class='name'>数据结构与算法</div><div class='badge'>(11)</div></a></li>
        
          <li><a class="flat-box" " href="/categories/知识图谱/"><div class='name'>知识图谱</div><div class='badge'>(4)</div></a></li>
        
          <li><a class="flat-box" " href="/categories/论文解读/"><div class='name'>论文解读</div><div class='badge'>(2)</div></a></li>
        
          <li><a class="flat-box" " href="/categories/语言模型/"><div class='name'>语言模型</div><div class='badge'>(14)</div></a></li>
        
      </ul>
    </div>
  </section>


          
        
      
        
          
          
            
  <section class='widget tagcloud'>
    
<header class='pure'>
  <div><i class="fas fa-fire fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;热门标签</div>
  
    <a class="rightBtn"
    
      rel="nofollow"
    
    
    href="/tags/"
    title="tags/">
    <i class="fas fa-expand-arrows-alt fa-fw"></i></a>
  
</header>

    <div class='content pure'>
      <a href="/tags/attention/" style="font-size: 16.5px; color: #888">Attention</a> <a href="/tags/cnnlm/" style="font-size: 14px; color: #999">CNNLM</a> <a href="/tags/cvt/" style="font-size: 14px; color: #999">CVT</a> <a href="/tags/data-structure/" style="font-size: 14px; color: #999">Data Structure</a> <a href="/tags/deep/" style="font-size: 14px; color: #999">Deep</a> <a href="/tags/dijkstra-s-algorithm/" style="font-size: 14px; color: #999">Dijkstra's Algorithm</a> <a href="/tags/ffnnlm/" style="font-size: 14px; color: #999">FFNNLM</a> <a href="/tags/gaussian/" style="font-size: 14px; color: #999">Gaussian</a> <a href="/tags/graph-algorithm/" style="font-size: 14px; color: #999">Graph Algorithm</a> <a href="/tags/initialization/" style="font-size: 14px; color: #999">Initialization</a> <a href="/tags/kg/" style="font-size: 17.75px; color: #808080">KG</a> <a href="/tags/lstm/" style="font-size: 14px; color: #999">LSTM</a> <a href="/tags/lstmlm/" style="font-size: 14px; color: #999">LSTMLM</a> <a href="/tags/language-model/" style="font-size: 20.25px; color: #6f6f6f">Language Model</a> <a href="/tags/log-linear-language-model/" style="font-size: 14px; color: #999">Log-Linear Language Model</a> <a href="/tags/mhsa/" style="font-size: 14px; color: #999">MHSA</a> <a href="/tags/nlp/" style="font-size: 20.25px; color: #6f6f6f">NLP</a> <a href="/tags/nmt/" style="font-size: 22.75px; color: #5e5e5e">NMT</a> <a href="/tags/norm/" style="font-size: 14px; color: #999">Norm</a> <a href="/tags/probabilistic-language-model/" style="font-size: 14px; color: #999">Probabilistic Language Model</a> <a href="/tags/rnnlm/" style="font-size: 14px; color: #999">RNNLM</a> <a href="/tags/roc-auc/" style="font-size: 14px; color: #999">ROC-AUC</a> <a href="/tags/transformer/" style="font-size: 24px; color: #555">Transformer</a> <a href="/tags/context2vec/" style="font-size: 14px; color: #999">context2vec</a> <a href="/tags/data-noising/" style="font-size: 14px; color: #999">data noising</a> <a href="/tags/divide-conquer/" style="font-size: 14px; color: #999">divide-conquer</a> <a href="/tags/einsum/" style="font-size: 14px; color: #999">einsum</a> <a href="/tags/insertion/" style="font-size: 16.5px; color: #888">insertion</a> <a href="/tags/insertion-deletion/" style="font-size: 15.25px; color: #919191">insertion-deletion</a> <a href="/tags/knowledge-modelling/" style="font-size: 15.25px; color: #919191">knowledge-modelling</a> <a href="/tags/nl2infographic/" style="font-size: 14px; color: #999">nl2infographic</a> <a href="/tags/nl2sql/" style="font-size: 14px; color: #999">nl2sql</a> <a href="/tags/ontology/" style="font-size: 14px; color: #999">ontology</a> <a href="/tags/parallel-recurrent/" style="font-size: 14px; color: #999">parallel-recurrent</a> <a href="/tags/pre-trained-seq2seq/" style="font-size: 14px; color: #999">pre-trained seq2seq</a> <a href="/tags/pytorch/" style="font-size: 14px; color: #999">pytorch</a> <a href="/tags/queue/" style="font-size: 19px; color: #777">queue</a> <a href="/tags/sparse/" style="font-size: 14px; color: #999">sparse</a> <a href="/tags/stack/" style="font-size: 14px; color: #999">stack</a> <a href="/tags/survey/" style="font-size: 14px; color: #999">survey</a> <a href="/tags/tensorflow/" style="font-size: 14px; color: #999">tensorflow</a> <a href="/tags/text2viz/" style="font-size: 14px; color: #999">text2viz</a> <a href="/tags/weighted-head/" style="font-size: 14px; color: #999">weighted-head</a> <a href="/tags/半监督语言模型/" style="font-size: 14px; color: #999">半监督语言模型</a> <a href="/tags/双数组前缀树/" style="font-size: 14px; color: #999">双数组前缀树</a> <a href="/tags/推荐系统/" style="font-size: 14px; color: #999">推荐系统</a> <a href="/tags/数据结构/" style="font-size: 21.5px; color: #666">数据结构</a> <a href="/tags/数组/" style="font-size: 14px; color: #999">数组</a> <a href="/tags/时间复杂度/" style="font-size: 14px; color: #999">时间复杂度</a> <a href="/tags/算法/" style="font-size: 14px; color: #999">算法</a> <a href="/tags/评估方法/" style="font-size: 14px; color: #999">评估方法</a> <a href="/tags/词向量/" style="font-size: 15.25px; color: #919191">词向量</a> <a href="/tags/隐式正则化/" style="font-size: 14px; color: #999">隐式正则化</a>
    </div>
  </section>


          
        
      
        
          
          
            


  <section class='widget music'>
    
<header class='pure'>
  <div><i class="fas fa-compact-disc fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;最近在听</div>
  
    <a class="rightBtn"
    
      rel="external nofollow noopener noreferrer"
    
    
      target="_blank"
    
    href="https://music.163.com/#/user/home?id=1960721923"
    title="https://music.163.com/#/user/home?id=1960721923">
    <i class="far fa-heart fa-fw"></i></a>
  
</header>

    <div class='content pure'>
      
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.css">
  <div class="aplayer"
    data-theme="#1BCDFC"
    
    
    data-mode="circulation"
    data-server="tencent"
    data-type="playlist"
    data-id="3351822215"
    data-volume="0.7">
  </div>
  <script src="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/meting@1.1.0/dist/Meting.min.js"></script>


    </div>
  </section>


          
        
      
    

  
</aside>

<footer id="footer" class="clearfix">
  <div id="sitetime"></div>
  
  
    <div class="social-wrapper">
      
        
          <a href="/atom.xml"
            class="social fas fa-rss flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
        
          <a href="mailto:rogerspy@163.com"
            class="social fas fa-envelope flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
        
          <a href="https://github.com/rogerspy"
            class="social fab fa-github flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
        
          <a href="https://music.163.com/#/user/home?id=1960721923"
            class="social fas fa-headphones-alt flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
    </div>
  
  <br>
  <div><p>博客内容遵循 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议</a></p>
</div>
  <div>
    本站使用
    <a href="https://xaoxuu.com/wiki/material-x/" target="_blank" class="codename">Material X</a>
    作为主题
    
      ，
      总访问量为
      <span id="busuanzi_value_site_pv"><i class="fas fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span>
      次
    
    。
  </div>
	</footer>

<script>setLoadingBarProgress(80);</script>
<!-- 点击特效，输入特效 运行时间 -->
<script type="text/javascript" src="/cool/cooltext.js"></script>
<script type="text/javascript" src="/cool/clicklove.js"></script>
<script type="text/javascript" src="/cool/sitetime.js"></script>



      <script>setLoadingBarProgress(60);</script>
    </div>
    <a class="s-top fas fa-arrow-up fa-fw" href='javascript:void(0)'></a>
  </div>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>

  <script>
    var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
    var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
    var ALGOLIA_API_KEY = "";
    var ALGOLIA_APP_ID = "";
    var ALGOLIA_INDEX_NAME = "";
    var AZURE_SERVICE_NAME = "";
    var AZURE_INDEX_NAME = "";
    var AZURE_QUERY_KEY = "";
    var BAIDU_API_ID = "";
    var SEARCH_SERVICE = "hexo" || "hexo";
    var ROOT = "/"||"/";
    if(!ROOT.endsWith('/'))ROOT += '/';
  </script>

<script src="//instant.page/1.2.2" type="module" integrity="sha384-2xV8M5griQmzyiY3CDqh1dn4z3llDVqZDqzjzcY+jCBCk/a5fXJmuZ/40JJAPeoU"></script>


  <script async src="https://cdn.jsdelivr.net/npm/scrollreveal@4.0.5/dist/scrollreveal.min.js"></script>
  <script type="text/javascript">
    $(function() {
      const $reveal = $('.reveal');
      if ($reveal.length === 0) return;
      const sr = ScrollReveal({ distance: 0 });
      sr.reveal('.reveal');
    });
  </script>


  <script src="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.js"></script>
  <script type="text/javascript">
    $(function() {
      Waves.attach('.flat-btn', ['waves-button']);
      Waves.attach('.float-btn', ['waves-button', 'waves-float']);
      Waves.attach('.float-btn-light', ['waves-button', 'waves-float', 'waves-light']);
      Waves.attach('.flat-box', ['waves-block']);
      Waves.attach('.float-box', ['waves-block', 'waves-float']);
      Waves.attach('.waves-image');
      Waves.init();
    });
  </script>


  <script async src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-busuanzi@2.3/js/busuanzi.pure.mini.js"></script>




  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
    <script type="text/javascript">
      $(function(){
        if ('.cover') {
          $('.cover').backstretch(
          ["https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/a0c9e6f9efad8b731cb7376504bd10d79d2053.jpg"],
          {
            duration: "6000",
            fade: "2500"
          });
        } else {
          $.backstretch(
          ["https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/a0c9e6f9efad8b731cb7376504bd10d79d2053.jpg"],
          {
            duration: "6000",
            fade: "2500"
          });
        }
      });
    </script>
  







  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script type="text/javascript">
    var gitalk = new Gitalk({
      clientID: "35a5e4dc744cc7d162af",
      clientSecret: "7b5a409e17ce0c1971f284eac9f8902eb4b8feba",
      repo: "rogerspy.github.io",
      owner: "Rogerspy",
      admin: "Rogerspy",
      
        id: "/wiki/material-x/",
      
      distractionFreeMode: false  // Facebook-like distraction free mode
    });
    gitalk.render('gitalk-container');
  </script>





  <script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.5/js/app.js"></script>


  <script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.5/js/search.js"></script>




<!-- 复制 -->
<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  let COPY_SUCCESS = "复制成功";
  let COPY_FAILURE = "复制失败";
  /*页面载入完成后，创建复制按钮*/
  !function (e, t, a) {
    /* code */
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '  <i class="fa fa-copy"></i><span>复制</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });

      clipboard.on('success', function(e) {
        //您可以加入成功提示
        console.info('Action:', e.action);
        console.info('Text:', e.text);
        console.info('Trigger:', e.trigger);
        success_prompt(COPY_SUCCESS);
        e.clearSelection();
      });
      clipboard.on('error', function(e) {
        //您可以加入失败提示
        console.error('Action:', e.action);
        console.error('Trigger:', e.trigger);
        fail_prompt(COPY_FAILURE);
      });
    }
    initCopyCode();

  }(window, document);

  /**
   * 弹出式提示框，默认1.5秒自动消失
   * @param message 提示信息
   * @param style 提示样式，有alert-success、alert-danger、alert-warning、alert-info
   * @param time 消失时间
   */
  var prompt = function (message, style, time)
  {
      style = (style === undefined) ? 'alert-success' : style;
      time = (time === undefined) ? 1500 : time*1000;
      $('<div>')
          .appendTo('body')
          .addClass('alert ' + style)
          .html(message)
          .show()
          .delay(time)
          .fadeOut();
  };

  // 成功提示
  var success_prompt = function(message, time)
  {
      prompt(message, 'alert-success', time);
  };

  // 失败提示
  var fail_prompt = function(message, time)
  {
      prompt(message, 'alert-danger', time);
  };

  // 提醒
  var warning_prompt = function(message, time)
  {
      prompt(message, 'alert-warning', time);
  };

  // 信息提示
  var info_prompt = function(message, time)
  {
      prompt(message, 'alert-info', time);
  };

</script>


<!-- fancybox -->
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
<script>
  let LAZY_LOAD_IMAGE = "";
  $(".article-entry").find("fancybox").find("img").each(function () {
      var element = document.createElement("a");
      $(element).attr("data-fancybox", "gallery");
      $(element).attr("href", $(this).attr("src"));
      /* 图片采用懒加载处理时,
       * 一般图片标签内会有个属性名来存放图片的真实地址，比如 data-original,
       * 那么此处将原本的属性名src替换为对应属性名data-original,
       * 修改如下
       */
       if (LAZY_LOAD_IMAGE) {
         $(element).attr("href", $(this).attr("data-original"));
       }
      $(this).wrap(element);
  });
</script>





  <script>setLoadingBarProgress(100);</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!--<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>
