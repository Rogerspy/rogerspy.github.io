<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  <title>Transformer家族之NA Trasnsformer | Rogerspy&#39;s Home</title>
  
  <meta name="keywords" content="Machine Learning, Deep Learning, NLP">
  
  

  
  <link rel="alternate" href="/atom.xml" title="Rogerspy's Home">
  

  <meta name="HandheldFriendly" content="True">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <!-- meta -->
  
  
  <meta name="theme-color" content="#FFFFFF">
  <meta name="msapplication-TileColor" content="#1BC3FB">
  <meta name="msapplication-config" content="https://cdn.jsdelivr.net/gh/xaoxuu/assets@master/favicon/favicons/browserconfig.xml">
  

  <!-- link -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.css">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.10.1/css/all.min.css">
  
  
  <link rel="shortcut icon" type="image/x-icon" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@master/favicon/favicon.ico">
  <link rel="icon" type="image/x-icon" sizes="32x32" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@master/favicon/favicons/favicon-32x32.png">
  <link rel="apple-touch-icon" type="image/png" sizes="180x180" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@master/favicon/favicons/apple-touch-icon.png">
  <link rel="mask-icon" color="#1BC3FB" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@master/favicon/favicons/safari-pinned-tab.svg">
  <link rel="manifest" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@master/favicon/favicons/site.webmanifest">
  

  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.5/css/style.css">
  

  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>
  

  
  
  <!-- 时间线 -->
  <link rel="stylesheet" href="/css/timeline.css">
  <!-- 血小板-->
  <link rel="stylesheet" href="/live2d/css/live2d.css">
  <style>
	.article p .mjx-math {
	    font-family: Menlo,Monaco,courier,monospace,"Lucida Console",'Source Code Pro',"Microsoft YaHei",Helvetica,Arial,sans-serif,Ubuntu;
        background: none;
        padding: 2px;
        border-radius: 4px;
	}
  </style>
</head>

<body>
  
  
  <div class="cover-wrapper">
    <cover class='cover post half'>
      
        
  <h1 class='title'>Rogerspy's Home</h1>


  <div class="m_search">
    <form name="searchform" class="form u-search-form">
      <input type="text" class="input u-search-input" placeholder="" />
      <i class="icon fas fa-search fa-fw"></i>
    </form>
  </div>

<div class='menu navgation'>
  <ul class='h-list'>
    
      
        <li>
          <a class="nav home" href="/"
            
            
            id="home">
            <i class='fas fa-edit fa-fw'></i>&nbsp;博文
          </a>
        </li>
      
        <li>
          <a class="nav home" href="/video/"
            
            
            id="video">
            <i class='fas fa-film fa-fw'></i>&nbsp;视频
          </a>
        </li>
      
        <li>
          <a class="nav home" href="/material/"
            
              rel="nofollow"
            
            
            id="material">
            <i class='fas fa-briefcase fa-fw'></i>&nbsp;资料
          </a>
        </li>
      
        <li>
          <a class="nav home" href="/about/"
            
              rel="nofollow"
            
            
            id="about">
            <i class='fas fa-info-circle fa-fw'></i>&nbsp;关于
          </a>
        </li>
      
    
  </ul>
</div>

      
    </cover>
    <header class="l_header pure">
  <div id="loading-bar-wrapper">
    <div id="loading-bar" class="pure"></div>
  </div>

	<div class='wrapper'>
		<div class="nav-main container container--flex">
      <a class="logo flat-box" href='/' >
        
          Rogerspy's Home
        
      </a>
			<div class='menu navgation'>
				<ul class='h-list'>
          
  					
  						<li>
								<a class="nav flat-box" href="/blog/"
                  
                  
                  id="blog">
									<i class='fas fa-edit fa-fw'></i>&nbsp;博客
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/video/"
                  
                  
                  id="video">
									<i class='fas fa-film fa-fw'></i>&nbsp;视频小站
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/material/"
                  
                  
                  id="material">
									<i class='fas fa-briefcase fa-fw'></i>&nbsp;学习资料
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/diary/"
                  
                  
                  id="diary">
									<i class='fas fa-book fa-fw'></i>&nbsp;随心记
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/categories/"
                  
                    rel="nofollow"
                  
                  
                  id="categories">
									<i class='fas fa-folder-open fa-fw'></i>&nbsp;分类
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/tags/"
                  
                    rel="nofollow"
                  
                  
                  id="tags">
									<i class='fas fa-hashtag fa-fw'></i>&nbsp;标签
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/blog/archives/"
                  
                    rel="nofollow"
                  
                  
                  id="blogarchives">
									<i class='fas fa-archive fa-fw'></i>&nbsp;归档
								</a>
							</li>
      			
      		
				</ul>
			</div>

			
				<div class="m_search">
					<form name="searchform" class="form u-search-form">
						<input type="text" class="input u-search-input" placeholder="搜索" />
						<i class="icon fas fa-search fa-fw"></i>
					</form>
				</div>
			
			<ul class='switcher h-list'>
				
					<li class='s-search'><a class="fas fa-search fa-fw" href='javascript:void(0)'></a></li>
				
				<li class='s-menu'><a class="fas fa-bars fa-fw" href='javascript:void(0)'></a></li>
			</ul>
		</div>

		<div class='nav-sub container container--flex'>
			<a class="logo flat-box"></a>
			<ul class='switcher h-list'>
				<li class='s-comment'><a class="flat-btn fas fa-comments fa-fw" href='javascript:void(0)'></a></li>
        
          <li class='s-toc'><a class="flat-btn fas fa-list fa-fw" href='javascript:void(0)'></a></li>
        
			</ul>
		</div>
	</div>
</header>
	<aside class="menu-phone">
    <header>
		<nav class="menu navgation">
      <ul>
        
          
            <li>
							<a class="nav flat-box" href="/"
                
                
                id="home">
								<i class='fas fa-clock fa-fw'></i>&nbsp;近期文章
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/blog/archives/"
                
                  rel="nofollow"
                
                
                id="blogarchives">
								<i class='fas fa-archive fa-fw'></i>&nbsp;文章归档
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/blog/"
                
                
                id="blog">
								<i class='fas fa-edit fa-fw'></i>&nbsp;我的博客
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/video/"
                
                  rel="nofollow"
                
                
                id="video">
								<i class='fas fa-film fa-fw'></i>&nbsp;我的视频
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/material/"
                
                  rel="nofollow"
                
                
                id="material">
								<i class='fas fa-briefcase fa-fw'></i>&nbsp;学习资料
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/about/"
                
                  rel="nofollow"
                
                
                id="about">
								<i class='fas fa-info-circle fa-fw'></i>&nbsp;关于小站
							</a>
            </li>
          
       
      </ul>
		</nav>
    </header>
	</aside>
<script>setLoadingBarProgress(40);</script>

  </div>


  <div class="l_body">
    <div class='body-wrapper'>
      <div class='l_main'>
  

  
    <article id="post" class="post white-box article-type-post" itemscope itemprop="blogPost">
      


  <section class='meta'>
    
    
    <div class="meta" id="header-meta">
      
        
  
    <h1 class="title">
      <a href="/2019/09/26/transformer家族-nat/">
        Transformer家族之NA Trasnsformer
      </a>
    </h1>
  


      
      <div class='new-meta-box'>
        
          
        
          
            
  <div class='new-meta-item author'>
    <a href="https://rogerspy.gitee.io" rel="nofollow">
      
        <i class="fas fa-user" aria-hidden="true"></i>
      
      <p>Rogerspy</p>
    </a>
  </div>


          
        
          
            <div class="new-meta-item date">
  <a class='notlink'>
    <i class="fas fa-calendar-alt" aria-hidden="true"></i>
    <p>2019-09-26</p>
  </a>
</div>

          
        
          
            
  
  <div class='new-meta-item category'>
    <a href='/categories/nlp/' rel="nofollow">
      <i class="fas fa-folder-open" aria-hidden="true"></i>
      <p>NLP</p>
    </a>
  </div>


          
        
          
            
  
    <div class="new-meta-item browse busuanzi">
      <a class='notlink'>
        <i class="fas fa-eye" aria-hidden="true"></i>
        <p>
          <span id="busuanzi_value_page_pv">
            <i class="fas fa-spinner fa-spin fa-fw" aria-hidden="true"></i>
          </span>
        </p>
      </a>
    </div>
  


          
        
          
            

          
        
          
            
  
    <div style="margin-right: 10px;">
      <span class="post-time">
        <span class="post-meta-item-icon">
          <i class="fa fa-keyboard"></i>
          <span class="post-meta-item-text">  字数统计: </span>
          <span class="post-count">3.3k字</span>
        </span>
      </span>
      &nbsp; | &nbsp;
      <span class="post-time">
        <span class="post-meta-item-icon">
          <i class="fa fa-hourglass-half"></i>
          <span class="post-meta-item-text">  阅读时长≈</span>
          <span class="post-count">13分</span>
        </span>
      </span>
    </div>
  

          
        
      </div>
      
        <hr>
      
    </div>
  </section>


      <section class="article typo">
        <div class="article-entry" itemprop="articleBody">
          <p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5396ee05ly1g5pqn3ch6zj20u092znph.jpg" alt></p>
<p>本文继续介绍关于<em>transformer</em>在<em>non-auto regression</em>方面的研究，今天要介绍的是<em>Gu et al. 2018</em> 发表在<em>ICLR 2018</em>上的文章<a href="http://arxiv.org/abs/1711.02281" target="_blank" rel="noopener">Non-autoregressive neural machine translation</a> 。</p>
<a id="more"></a>
<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>之前我们介绍了由于<em>transformer</em>的<em>auto-regression</em>机制，导致模型在做推理的时候会变得非常慢。针对这个问题很多研究者都做了探索，之前介绍的几篇论文都没有真正做到<em>non-auto regression</em>，而今天我们要介绍的这篇文章则从根本上做到了<em>non-auto regression</em>。</p>
<h2 id="1-1-Auto-Regressive-Decoding"><a href="#1-1-Auto-Regressive-Decoding" class="headerlink" title="1.1 Auto-Regressive Decoding"></a>1.1 Auto-Regressive Decoding</h2><p>给定一个源序列$X={x<em>1, x_2, …, x</em>{T’}}$，翻译模型通过链式条件概率的方式预测输出序列$Y = { y_1, y_2, …, y_T}$:</p>
<script type="math/tex; mode=display">
p_{AR}(Y|X;\theta) = \Pi_{t=1}^{T+1} p(y_t|y_{0:t-1}, x_{1:T'};\theta)</script><p>其中$y<em>0$表示句子开始符，比如$BOS$；$y</em>{T+1}$表示句子结束符，比如$EOS$ 。</p>
<h2 id="1-2-Maximum-Likelihood-training"><a href="#1-2-Maximum-Likelihood-training" class="headerlink" title="1.2  Maximum Likelihood training"></a>1.2  Maximum Likelihood training</h2><p>训练的时候直接使用<em>Maximum Likelihood training</em>方法对模型进行训练：</p>
<script type="math/tex; mode=display">
L_{ML} = \log P_{AR}\{Y|X;\theta\} = \sum_{t=1}^{T+1} \log p(y_t|y_{0:t-1},x_{1:T'};\theta)</script><h2 id="1-3-Non-Auto-regressive-Decoding"><a href="#1-3-Non-Auto-regressive-Decoding" class="headerlink" title="1.3 Non-Auto regressive Decoding"></a>1.3 Non-Auto regressive Decoding</h2><p>从上面的$p_{AR}(Y|X;\theta)$中可以看出，实际上要预测$Y={y_t}$需要两个条件：</p>
<ul>
<li>知道$Y$的长度$T$，虽然对于<em>auto-regression</em>来说，解码过程并不知道$T$显示的值，但是由于编码的开始和结束可以通过句子开始符（$&lt; BOS&gt;$）和句子结束符（$&lt; EOS&gt;$）来控制，编码过程是一个词一个词的生成 ，知道遇到结束符，则编码过程结束。因此，模型可以隐式的知道$T$的值。但是对于并行生成句子序列，我们必须提前知道句子的长度，才能一次性生成一个长度为$T$的序列，所以对于<em>Non-auto reregression</em>来说$T$必须是显式的；</li>
<li>第二点当然就是$Y$序列本身了，当知道了需要预测的序列长度，就可以根据输入预测输出了。</li>
</ul>
<p>因此，我们可以把预测$Y={y_t}$任务分为两部分，第一部分预测$T$的大小，第二部分生成长度为$T$的序列：</p>
<script type="math/tex; mode=display">
p_{NA}(Y|X;\theta) = p_L(T|x_{1:T'};\theta)\cdot \Pi_{t=1}^{T} p(y_t|x_{1:T'};\theta)</script><p>这样我们就可以分别独立的训练这两部分，而在推理的时候 能并行计算。</p>
<h2 id="1-4-多模态问题"><a href="#1-4-多模态问题" class="headerlink" title="1.4 多模态问题"></a>1.4 多模态问题</h2><p>这种简单的方法虽然看起来合理，但是实际上有一个很大的问题：多模态问题 。具体来说就是同一句话有多种翻译方式，比如<em>Thank you</em>可以翻译成<em>谢谢</em>、<em>感谢</em>等。由于$p(y_t)$只与$X$有关，所以无法获得训练数据中<em>谢谢</em>、<em>感谢</em> 等不同翻译方式的分布。</p>
<p>当<em>A B</em>既可以翻译成<em>1 2 3</em>，又可以翻译成<em>4 5 6</em>时，实际上相当于</p>
<script type="math/tex; mode=display">
\{A, B\} => \{1, 2, 3, 4, 5, 6\}</script><p>的一个映射，而最佳的映射组合是${1,2,3}$和${4,5,6}$，但是由于<em>non-auto regression</em>每个词都是独立的，所以无法获取到词与词之间的依赖关系，每种序列组合称为<em>mode</em>。</p>
<p>另外使用<em>Maxium Likelihood</em>进行训练的时候，模型倾向于使用在训练集中出现概率最大的<em>mode</em>覆盖掉其他小概率<em>mode</em>，这实际上是有一定问题的。</p>
<p>要解决多模态问题，一般有三种方法：</p>
<ul>
<li>增强模型处理多模态的能力；</li>
<li>在训练集中减少<em>mode</em>的数量；</li>
<li>改变学习目标</li>
</ul>
<p>实际上最有效的还是第一种方法，这篇论文提出了一种训练技术用来解决多模态问题。</p>
<h1 id="2-Non-Autoregressive-Transformer"><a href="#2-Non-Autoregressive-Transformer" class="headerlink" title="2. Non-Autoregressive Transformer"></a>2. Non-Autoregressive Transformer</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/e3c7fba55c89d79ee2c7fdee8860a7bd7d48f0.png" alt></p>
<p>模型结构图如上。模型包含四个部分：<em>Encoder</em>、<em>Fertility predictor</em>、<em>Decoder</em>和<em>Translation Predictor</em>。其中黑色实线箭头表示可微分操作，浅色虚线表示不可微分操作，每个<em>sublayer</em>也都包含<em>LayerNorm</em>和残差连接。</p>
<p><em>Encoder</em>部分作者并没有做什么变化，与<em>Transformer</em>保持一致，因此这里我们不做介绍，主要介绍其他几部分，以及训练技巧。</p>
<h2 id="2-1-Decoder-Stack"><a href="#2-1-Decoder-Stack" class="headerlink" title="2.1 Decoder Stack"></a>2.1 Decoder Stack</h2><p>从图中可以看到，<em>Decoder Stack</em>包含了四部分：</p>
<ul>
<li>Multi-Head Self-Attention</li>
<li>Multi-Head Positional Attention</li>
<li>Multi-Head Inter-Attention</li>
<li>MLP</li>
</ul>
<p>其中<em>MLP</em>就是<em>transformer</em>中的<em>position wise feed forward</em>层。<em>Multi-Head Inter-Attention</em>就是<em>transformer</em>中<em>decoder block</em>的第二个<em>multi-head attention</em>层。这两个相比原来的<em>transformer</em>没有什么变化，这里就跳过不讲。下面主要介绍其余两个模块以及<em>Decoder Stack</em>的输入。</p>
<h3 id="2-1-1-Decoder-Inputs"><a href="#2-1-1-Decoder-Inputs" class="headerlink" title="2.1.1 Decoder Inputs"></a>2.1.1 Decoder Inputs</h3><p>在进行解码之前，<em>Non-Autoregressive Transformer</em>（<em>NAT</em>）需要知道要生成的序列的长度。另一方面如果只输入位置编码的话最后的效果会很差，因此解码器的输入也是非常重要的。为此，作者设计了两种解码器的输入：</p>
<ul>
<li><strong>Copy source inputs uniformly</strong>：根据下面的规则从源序列中拷贝一个序列出来作为解码器的输入</li>
</ul>
<script type="math/tex; mode=display">
Round(T't/T)</script><p>假设源序列长度是$T’$，目标序列长度是$T$，那么解码器的输入序列中第$t$个位置的元素为源序列中第$Round(T’t/T)$个元素。举个例子：</p>
<blockquote>
<p>源序列：[Thank, you, !]</p>
<p>目标序列：[谢谢, ！]</p>
</blockquote>
<p>源序列的长度是3，目标序列的长度是2。那么解码器的输入第0个位置的元素应该对应源序列中第 $3 \times 0/2=0$，个元素，即<em>Thank</em>；解码器的输入序列的第1个元素，应该对应源序列中第$3 \times 1/2=1.5$，四舍五入得2，即<em>！</em>。那么解码器的输入序列应该为${Thank , !}$。</p>
<ul>
<li><strong>Copy source inputs using fertilities</strong>：同样是从源序列中拷贝一个序列出来作为解码器的输入，但是拷贝规则有了变化。如同结构图中显示的，编码器会在编码结束后除了输出注意力状态，还会输出一个长度与源序列长度相同的序列，序列中每个元素都是一个整数。将源序列中的每个位置上的元素拷贝相应的整数倍之后作为解码器的输入，而解码器的输入长度$T$则是编码器输出的整数序列之和。举个例子：</li>
</ul>
<blockquote>
<p>源序列：[Thank, you, !]</p>
<p>encoder output: [1, 0, 1] </p>
</blockquote>
<p>那么解码器的输入序列为： ${Thank \times 1, you \times 0, ! \times 1 }$，即${Thank, !}$为解码器的输入。</p>
<h2 id="2-1-2-Non-causal-self-attention"><a href="#2-1-2-Non-causal-self-attention" class="headerlink" title="2.1.2 Non-causal self-attention"></a>2.1.2 Non-causal self-attention</h2><p>由于这里是并行生成目标序列，因此不需要对注意力权重矩阵进行mask。但是在实际的测试过程中作者发现把元素自身所在的位置mask掉后会取得更好的结果，即（深色部分表示被mask掉的）：</p>
<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/beac1906681d79de2a36ba7e872129973babd4.png" alt></p>
<h3 id="2-1-3-Positional-Attention"><a href="#2-1-3-Positional-Attention" class="headerlink" title="2.1.3 Positional Attention"></a>2.1.3 Positional Attention</h3><p>作者还在每个<em>decoder layer</em>中加入了<em>positional attention</em>，使得模型获得序列位置向量的能力更强了。而所谓的<em>positional attention</em>顾名思义就是<em>positional encoding + attention</em>，分别对应<em>transformer</em>中的<em>positional encoding</em>和<em>multi-head attention</em>：</p>
<script type="math/tex; mode=display">
p(j, k) = \sin(j/10000^{k/d}), k为偶数</script><script type="math/tex; mode=display">
p(j,k) = \cos(j/10000^{k/d}), k为奇数</script><script type="math/tex; mode=display">
Attention(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_{model}}})V</script><h2 id="2-2-Modeling-Fertility-解决多模态问题"><a href="#2-2-Modeling-Fertility-解决多模态问题" class="headerlink" title="2.2 Modeling Fertility 解决多模态问题"></a>2.2 Modeling Fertility 解决多模态问题</h2><p>本文提出使用隐变量（<em>Latent Variable</em>）方法来解决这一问题。具体的想法如下图</p>
<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/1adb5756acb8ee4ffb980d2b895080ae0ff905.png" alt></p>
<p>左边代表可能的各种可能的组合元素，所有元素可以两两组合，也可以三三组合等等可以以任意方式组合，因为对于<em>NA</em>来说词与词之间存在一个独立假设。我们可以从大的组合集中进行采样得到小的组合集（如图中蓝色圈圈画出来的小样本集合），这样模型就相当于在一个小的样本空间中进行建模，这样就可以强化模型的多模态处理能力。而我们把采样得到的样本空间称之为隐变量$z$，引入$z$后多模态的翻译分布则变成了：</p>
<script type="math/tex; mode=display">
p_{NA}(Y|X;\theta) = \sum_z\left[ p_z(z|x_{1:T'};\theta)\cdot p_L(T|x_{1:T'};\theta;z)\cdot \Pi_{t=1}^{T}p(y_t|x_{1:T'};\theta;z)\right]</script><p>$z$需要满足以下条件：</p>
<ul>
<li>需要比较简单的从端到端的训练中获得；</li>
<li>$z$要尽可能的考虑到不同说出之间的关系，使得其余位置的输出尽可能条件独立性；</li>
<li>$z$要很容易从平行语料中推理得到，又不能信息量过于丰富让$p$显得无关紧要。</li>
</ul>
<p>所以，模型的关键是对$z$建模。本文中$z$就是<em>decoder</em>的输入——利用<em>Fertility</em>的思想从<em>encoder</em>输入中拷贝元素。所谓<em>fertility</em>指的是，源序列中每个元素会被翻译多少次。这种思想源于早期的统计机器翻译中，每个词被翻译的次数不同，则输出序列会不，比如<em>Thank</em>被翻译一次可能是<em>谢谢</em>，而如果翻译两次的话可能会输出<em>多谢</em>。每个词的翻译次数实际上是个隐藏的含义，并不具有显式意义，因此它是一种<em>Latent Variable</em>，可以用来表示一种<em>translation mode</em>。</p>
<p>因此根据<em>fertitlity</em>我们可以把方程写成：</p>
<script type="math/tex; mode=display">
p_{NA}(Y|X;\theta) = \sum_{f_1, ..., f_T'\in F}\left( \Pi_{t'=1}^{T'}p_F(f_{t'}|x_{1:T'};\theta) \cdot \Pi_{t=1}^{T}p(y_t|x_1\{f_1\}, ...x_{T'}\{f_{T'};\theta\})\right)</script><p>其中$F={f<em>1, …, f</em>{T’} | \sum<em>{t’=1}^{T’}f</em>{t’}=T, f_{t’}\in \mathbb{Z^*}}$。</p>
<p><em>Fertility</em>序列中每个词重复的次数通过一个<em>softmax</em>层预测。</p>
<h1 id="3-训练"><a href="#3-训练" class="headerlink" title="3. 训练"></a>3. 训练</h1><p>由于模型中引入了离散的隐变量，是的整个模型不能直接使用后向传播进行训练。作者引入了一个外部的对齐函数——<em>Fast-align</em>。<em>Fast-align</em>能够将输入输出进行词对齐，即目标序列中的每个词对应源序列中的相应的词。这样我们就可以得到一个外部的<em>Fertility</em>序列。我们可以用这个外部的<em>fertility</em>序列当成<em>fertility</em>的监督项，这样整个模型的损失项来源于两部分：<em>decoding</em>和<em>fertility</em>：</p>
<script type="math/tex; mode=display">
L_{ML}= \log p_{NA}(Y|X;\theta) = \log \sum_{f_{1:T'}\in F} p_F(f_{1:T'}|x_{1:T'};\theta)\cdot p(y_{1:T}|x_{1:T'},f_{1:T'};\theta)</script><script type="math/tex; mode=display">
L_{ML} \ge \mathbb{E}_{f_{1:T'}\sim q}(\underbrace{\sum_{t=1}^T\log(y_t|x_1\{f_1\}, ...,x_{T'}\{f_{T'}\};\theta)}_{\mathrm{Translation Loss}}+\underbrace{\sum_{t'=1}^{T'}\log p_F(f_{t'}|x_{1:T'};\theta)}_{\mathrm{Fertility~Loss}})+H(q)</script><p>其中$q$表示外部对齐函数估计的<em>Fertility</em>序列分布。</p>
<p>这样的话整个模型就相当于由两部分监督学习组成，就可以直接使用后向传播进行训练了。</p>
<h2 id="3-1-知识蒸馏"><a href="#3-1-知识蒸馏" class="headerlink" title="3.1 知识蒸馏"></a>3.1 知识蒸馏</h2><p>之前我们提到解决多模态问题有三种方法：增强模型处理多模态的能力，减少<em>mode</em>数，改变学习目标。其中增强模型处理多模态的能力是核心，前面我们已经介绍过了。下面我们介绍一下后两种方法，注意这几种方法结合使用，而不是分别使用，也就是说是在同一个模型中一起使用者三种方法，<em>fertility</em>是核心。</p>
<p>知识蒸馏其实蒸馏的就是<em>mode</em>，通过减少<em>mode</em>数，从而提升模型的学习效果。这里使用的方法是先用一个标准的<em>Transformer</em>学习一个模型，将这个标准<em>Transformer</em>的推理结果作为<em>NAT</em>的目标序列。</p>
<h2 id="3-2-Fune-Tuning"><a href="#3-2-Fune-Tuning" class="headerlink" title="3.2 Fune Tuning"></a>3.2 Fune Tuning</h2><p>前面我们提到使用<em>Maximum Likelihood</em>本身更倾向于学习概率更大的<em>mode</em>，因此作者在这里引入一个微调项，用来做目标纠正——<em>reverse K-L divergence</em>:</p>
<script type="math/tex; mode=display">
L_{RKL}(f_{1:T'};\theta) = \sum_{t=1}^{T}\sum_{y_t} [\log p_{AR}(y_t|\hat{y}_{1:t},x_{1:T'})\cdot p_{NA}(y_t|x_{1:T'},f_{1:T'};\theta)]</script><p>其中$\hat{y}<em>{1:T}=G(x</em>{1:T’}, f_{1:T’};\theta)$，所以最终的损失函数为：</p>
<script type="math/tex; mode=display">
L_{FT} = \lambda(\underbrace{\mathbb{E}_{f_{1:T'}\sim p_F}(L_{RKL}(f_{1:T'})-L_{RKL}(\overline{f_{1:T'}}))}_{L_{RL}}+\underbrace{\mathbb{E}_{f_{1:T'}}(L_{RKL}(f_{1:T'}))}_{L_{BP}})+(1-\lambda)L_{KD}</script><h1 id="4-推理"><a href="#4-推理" class="headerlink" title="4. 推理"></a>4. 推理</h1><p>推理阶段主要的任务是获取<em>fertility</em>序列，作者提出三种方法：</p>
<ul>
<li><strong>Argmax decoding</strong></li>
</ul>
<p>由于在模型训练阶段，<em>fertility</em>序列随着模型的训练一起呗训练了 ，因此我们可以直接使用$\mathrm{arg~max}$来得到<em>fertility</em>序列：</p>
<script type="math/tex; mode=display">
\hat{Y}_{argmax} = G(x_{1:T'},\hat{f}_{1:T'};\theta),其中\hat{f}_{t'}=\mathrm{arg}\max p_F(f_{t'}|x_{1:T'};\theta)</script><ul>
<li><strong>Average decoding</strong></li>
</ul>
<p>也可以通过求对应的<em>softmax</em>的期望来得到<em>fertility</em>序列：</p>
<script type="math/tex; mode=display">
\hat{Y}_{average} = G(x_{1:T'}, \hat{f}_{1:T'};\theta), 其中\hat{f_{t'}}=Round(\sum_{f_{t'}}^{L}p_F(f_{t'}|x_{1:T'};\theta)f_{t'})</script><ul>
<li><strong>Noisy parallel decoding (NPD)</strong></li>
</ul>
<script type="math/tex; mode=display">
\hat{Y}_{NPD} = G(x_{1:T'},\mathrm{arg}\max_{f_{t'}\sim p_F} p_{AR}(G(x_{1:T'}, f_{1:T'};\theta)|X;\theta);\theta)</script><p>最后使用一个之前在做知识蒸馏的时候训练好的<em>Transformer</em>对输出的句子进行<em>re-ranking</em>，得到一个最佳的翻译结果：</p>
<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/6a37db1447abd4218ebf12c1f19c8d3f0009f3.png" alt></p>
<h1 id="5-实验结果"><a href="#5-实验结果" class="headerlink" title="5. 实验结果"></a>5. 实验结果</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/eb257dc95fc7fc9bea69525e523c9e8cfe5bd8.png" alt></p>
<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/175160ea64dacce506c4413783ff5ac0c4428b.png" alt></p>
<h1 id="6-参考资料"><a href="#6-参考资料" class="headerlink" title="6. 参考资料"></a>6. 参考资料</h1><ol>
<li><p><a href="http://arxiv.org/abs/1711.02281" target="_blank" rel="noopener">Non-autoregressive neural machine translation</a> <em>Gu et al. 2018</em></p>
</li>
<li><p><a href="https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/79547509" target="_blank" rel="noopener">直播实录 | 非自回归神经机器翻译 + ICLR 2018 论文解读</a></p>
</li>
</ol>

        </div>
        
          


  <section class='meta' id="footer-meta">
    <hr>
    <div class='new-meta-box'>
      
        
          <div class="new-meta-item date" itemprop="dateUpdated" datetime="2021-09-15T23:36:17+08:00">
  <a class='notlink'>
    <i class="fas fa-clock" aria-hidden="true"></i>
    <p>最后更新于 2021年9月15日</p>
  </a>
</div>

        
      
        
          
  
  <div class="new-meta-item meta-tags"><a class="tag" href="/tags/transformer/" rel="nofollow"><i class="fas fa-hashtag" aria-hidden="true"></i>&nbsp;<p>Transformer</p></a></div> <div class="new-meta-item meta-tags"><a class="tag" href="/tags/nmt/" rel="nofollow"><i class="fas fa-hashtag" aria-hidden="true"></i>&nbsp;<p>NMT</p></a></div>


        
      
        
          
  <div class="new-meta-item share -mob-share-list">
  <div class="-mob-share-list share-body">
    
      
        <a class="-mob-share-qq" title="QQ好友" rel="external nofollow noopener noreferrer"
          
          href="http://connect.qq.com/widget/shareqq/index.html?url=https://rogerspy.gitee.io/2019/09/26/transformer家族-nat/&title=Transformer家族之NA Trasnsformer | Rogerspy's Home&summary=
本文继续介绍关于transformer在non-auto regression方面的研究，今天要介绍的是Gu et al. 2018 发表在ICLR 2018上的文章Non-autoregressive neural machine translation 。"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/qq.png">
          
        </a>
      
    
      
        <a class="-mob-share-qzone" title="QQ空间" rel="external nofollow noopener noreferrer"
          
          href="https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=https://rogerspy.gitee.io/2019/09/26/transformer家族-nat/&title=Transformer家族之NA Trasnsformer | Rogerspy's Home&summary=
本文继续介绍关于transformer在non-auto regression方面的研究，今天要介绍的是Gu et al. 2018 发表在ICLR 2018上的文章Non-autoregressive neural machine translation 。"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/qzone.png">
          
        </a>
      
    
      
        <a class='qrcode' rel="external nofollow noopener noreferrer" href='data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACpElEQVR42u3a0W7rQAgFwPz/T6evlVo7B1iaVBo/VUmvzfhKCwUej/h6fruuvv35c/KbV//2eXE9Ni48PDy8QehXt7v//J4xgSWfvHgFeHh4eGu83nF/H2KSVO5f3yQePDw8vE/jPYOrWuVWUwseHh7e/+LlyF6BnnyLh4eH915eElDeLJi8puTblV4LHh4eXqfWfeSDq/f+vDLfw8PDwxtP1asF8anWQ6/x8cv98fDw8BZ4+YHbCzEvtasLW4XlMDw8PLwFXm/0VWV/QssDDw8Pb87Lj+ZToUwaDeXGBB4eHt4aLwl6Em6Pna8avGhD4OHh4S3wqkHkC1jJ4f62qR0eHh7eUWqSNqql+dn0kCQMPDw8vA1etUmahNUbic0Txov/MTw8PLyjvOrgKl8vONsOzgv9y50yPDw8vEO8/MHVdkYSYm/lKyrf8fDw8NZ4p9JD3j7Iy+7eBhkeHh7eNq/w532xgJ4zJgteeHh4eNu8yXGfj9B6oedPiZau8PDw8I7yqg2FanATfP55s9eCh4eHl57zI8akAVHFV6PCw8PD2+BVi91JAqjeM2+LRDtleHh4eId4vYO42rqtjtaqqwZRbHh4eHiHeJP2QT7Unx/rzbUtPDw8vAVebzDfK3yrwSVt5Sjv4eHh4f0Jr8rOX0rS0j1VsuPh4eFt8xJkXhCfHZjld8bDw8N7Fy+/UdJUzZcS/mj0hYeHhzfgzUdQ+ULVqQZEPk7Dw8PD2+DlV2+UVW1GVFu3efLAw8PDO8vrrTrl7d08beRti0LiwcPDw1vj5Ud8r6nRK5TzpHUZJx4eHt4H8KpFcHW4lbR685SDh4eH9wm87eZsPjaLEgkeHh7eGq/3mOZhHcfQe1l4eHh427zeH/zVG02G/fOn4+Hh4R16+hfyFZxx6tBaKQAAAABJRU5ErkJggg=='>
        
          <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/wechat.png">
        
        </a>
      
    
      
        <a class="-mob-share-weibo" title="微博" rel="external nofollow noopener noreferrer"
          
          href="http://service.weibo.com/share/share.php?url=https://rogerspy.gitee.io/2019/09/26/transformer家族-nat/&title=Transformer家族之NA Trasnsformer | Rogerspy's Home&summary=
本文继续介绍关于transformer在non-auto regression方面的研究，今天要介绍的是Gu et al. 2018 发表在ICLR 2018上的文章Non-autoregressive neural machine translation 。"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/weibo.png">
          
        </a>
      
    
  </div>
</div>



        
      
    </div>
  </section>


        
        
            <div class="prev-next">
                
                    <section class="prev">
                        <span class="art-item-left">
                            <h6><i class="fas fa-chevron-left" aria-hidden="true"></i>&nbsp;上一页</h6>
                            <h4>
                                <a href="/2019/09/30/transformer家族-sa/" rel="prev" title="Transformer家族之Semi-Autoregressive Transformer">
                                  
                                      Transformer家族之Semi-Autoregressive Transformer
                                  
                                </a>
                            </h4>
                            
                                
                                <h6 class="tags">
                                    <a class="tag" href="/tags/transformer/"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>Transformer</a> <a class="tag" href="/tags/nmt/"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>NMT</a>
                                </h6>
                            
                        </span>
                    </section>
                
                
                    <section class="next">
                        <span class="art-item-right" aria-hidden="true">
                            <h6>下一页&nbsp;<i class="fas fa-chevron-right" aria-hidden="true"></i></h6>
                            <h4>
                                <a href="/2019/09/25/transformer家族-latent/" rel="prev" title="Transformer家族之Latent Transformer">
                                    
                                        Transformer家族之Latent Transformer
                                    
                                </a>
                            </h4>
                            
                                
                                <h6 class="tags">
                                    <a class="tag" href="/tags/transformer/"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>Transformer</a> <a class="tag" href="/tags/nmt/"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>NMT</a>
                                </h6>
                            
                        </span>
                    </section>
                
            </div>
        
      </section>
    </article>
  

  
    <!-- 显示推荐文章和评论 -->



  <article class="post white-box comments">
    <section class="article typo">
      <h4><i class="fas fa-comments fa-fw" aria-hidden="true"></i>&nbsp;评论</h4>
      
      
      
        <section id="comments">
          <div id="gitalk-container"></div>
        </section>
      
      
    </section>
  </article>


  




<!-- 根据页面mathjax变量决定是否加载MathJax数学公式js -->

  <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": {
      preferredFont: "TeX",
      availableFonts: ["STIX","TeX"],
      linebreaks: { automatic:true },
      EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
      inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
      processEscapes: true,
      ignoreClass: "tex2jax_ignore|dno",
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: { autoNumber: "AMS" },
      noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
      Macros: { href: "{}" }
    },
    messageStyle: "none"
  });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += (all[i].SourceElement().parentNode.className ? ' ' : '') + 'has-jax';
    }
  });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>




  <script>
    window.subData = {
      title: 'Transformer家族之NA Trasnsformer',
      tools: true
    }
  </script>


</div>
<aside class='l_side'>
  
    
    
      
        
          
          
            <section class='widget shake author'>
  <div class='content pure'>
    
      <div class='avatar'>
        <img class='avatar' src='https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/65-1Z31313530JC.jpeg'/>
      </div>
    
    
    
      <div class="social-wrapper">
        
          
            <a href="/atom.xml"
              class="social fas fa-rss flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
          
            <a href="mailto:rogerspy@163.com"
              class="social fas fa-envelope flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
          
            <a href="https://github.com/rogerspy"
              class="social fab fa-github flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
          
            <a href="https://music.163.com/#/user/home?id=1960721923"
              class="social fas fa-headphones-alt flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
      </div>
    
  </div>
</section>

          
        
      
        
          
          
            
  <section class='widget toc-wrapper'>
    
<header class='pure'>
  <div><i class="fas fa-list fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;本文目录</div>
  
    <div class='wrapper'><a class="s-toc rightBtn" rel="external nofollow noopener noreferrer" href="javascript:void(0)"><i class="fas fa-thumbtack fa-fw"></i></a></div>
  
</header>

    <div class='content pure'>
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-简介"><span class="toc-text">1. 简介</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-Auto-Regressive-Decoding"><span class="toc-text">1.1 Auto-Regressive Decoding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-Maximum-Likelihood-training"><span class="toc-text">1.2  Maximum Likelihood training</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-Non-Auto-regressive-Decoding"><span class="toc-text">1.3 Non-Auto regressive Decoding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-多模态问题"><span class="toc-text">1.4 多模态问题</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-Non-Autoregressive-Transformer"><span class="toc-text">2. Non-Autoregressive Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Decoder-Stack"><span class="toc-text">2.1 Decoder Stack</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-1-Decoder-Inputs"><span class="toc-text">2.1.1 Decoder Inputs</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-2-Non-causal-self-attention"><span class="toc-text">2.1.2 Non-causal self-attention</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-3-Positional-Attention"><span class="toc-text">2.1.3 Positional Attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-Modeling-Fertility-解决多模态问题"><span class="toc-text">2.2 Modeling Fertility 解决多模态问题</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-训练"><span class="toc-text">3. 训练</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-知识蒸馏"><span class="toc-text">3.1 知识蒸馏</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-Fune-Tuning"><span class="toc-text">3.2 Fune Tuning</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-推理"><span class="toc-text">4. 推理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-实验结果"><span class="toc-text">5. 实验结果</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-参考资料"><span class="toc-text">6. 参考资料</span></a></li></ol>
    </div>
  </section>


          
        
      
        
          
          
            <section class='widget grid'>
  
<header class='pure'>
  <div><i class="fas fa-map-signs fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;站内导航</div>
  
</header>

  <div class='content pure'>
    <ul class="grid navgation">
      
        <li><a class="flat-box" " href="/"
          
          
          id="home">
          
            <i class="fas fa-clock fa-fw" aria-hidden="true"></i>
          
          近期文章
        </a></li>
      
        <li><a class="flat-box" " href="/blog/"
          
          
          id="blog">
          
            <i class="fas fa-edit fa-fw" aria-hidden="true"></i>
          
          我的博客
        </a></li>
      
        <li><a class="flat-box" " href="/paper_note/"
          
          
          id="paper_note">
          
            <i class="fas fa-book fa-fw" aria-hidden="true"></i>
          
          论文笔记
        </a></li>
      
        <li><a class="flat-box" " href="/algorithm/"
          
          
          id="algorithm">
          
            <i class="fas fa-cube fa-fw" aria-hidden="true"></i>
          
          算法基础
        </a></li>
      
        <li><a class="flat-box" " href="/leetcode/"
          
          
          id="leetcode">
          
            <i class="fas fa-code fa-fw" aria-hidden="true"></i>
          
          Leetcode
        </a></li>
      
        <li><a class="flat-box" " href="/video/"
          
          
          id="video">
          
            <i class="fas fa-film fa-fw" aria-hidden="true"></i>
          
          视频小站
        </a></li>
      
        <li><a class="flat-box" " href="/material/"
          
          
          id="material">
          
            <i class="fas fa-briefcase fa-fw" aria-hidden="true"></i>
          
          学习资料
        </a></li>
      
        <li><a class="flat-box" " href="/dataset/"
          
          
          id="dataset">
          
            <i class="fas fa-database fa-fw" aria-hidden="true"></i>
          
          数据集
        </a></li>
      
        <li><a class="flat-box" " href="/articles/"
          
          
          id="articles">
          
            <i class="fas fa-sticky-note fa-fw" aria-hidden="true"></i>
          
          杂文天地
        </a></li>
      
        <li><a class="flat-box" " href="/blog/archives/"
          
            rel="nofollow"
          
          
          id="blogarchives">
          
            <i class="fas fa-archive fa-fw" aria-hidden="true"></i>
          
          文章归档
        </a></li>
      
        <li><a class="flat-box" " href="/personal_center/"
          
          
          id="personal_center">
          
            <i class="fas fa-university fa-fw" aria-hidden="true"></i>
          
          个人中心
        </a></li>
      
        <li><a class="flat-box" " href="/about/"
          
            rel="nofollow"
          
          
          id="about">
          
            <i class="fas fa-info-circle fa-fw" aria-hidden="true"></i>
          
          关于小站
        </a></li>
      
    </ul>
  </div>
</section>

          
        
      
        
          
          
            <section class='widget list'>
  
<header class='pure'>
  <div><i class="fas fa-terminal fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;机器学习框架</div>
  
</header>

  <div class='content pure'>
    <ul class="entry">
      
        <li><a class="flat-box" href="https://rogerspy.gitee.io/pytorch-zh/"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-star fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;PyTorch 中文文档
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://keras-zh.readthedocs.io/"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-star fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Keras 中文文档
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://tensorflow.google.cn/"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-star fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Tensorflow 中文文档
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="http://scikitlearn.com.cn/"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-star fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Scikit Learn 中文文档
          </div>
          
        </a></li>
      
    </ul>
  </div>
</section>

          
        
      
        
          
          
            <section class='widget list'>
  
<header class='pure'>
  <div><i class="fas fa-wrench fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;百宝箱</div>
  
</header>

  <div class='content pure'>
    <ul class="entry">
      
        <li><a class="flat-box" href="https://rogerspy.github.io/excalidraw-claymate/"
          
          
            target="_blank"
          
          >
          <div class='name'>
            
              <i class="fas fa-magic fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Excalidraw-Claymate
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://rogerspy.github.io/jupyterlite/"
          
          
            target="_blank"
          
          >
          <div class='name'>
            
              <i class="fas fa-terminal fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;JupyterLite
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://rogerspy.github.io/kanban/"
          
          
            target="_blank"
          
          >
          <div class='name'>
            
              <i class="fas fa-table fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Kanban
          </div>
          
        </a></li>
      
    </ul>
  </div>
</section>

          
        
      
        
          
          
            <section class='widget list'>
  
<header class='pure'>
  <div><i class="fas fa-eye fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;睁眼看世界</div>
  
</header>

  <div class='content pure'>
    <ul class="entry">
      
        <li><a class="flat-box" href="https://deeplearn.org/"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-link fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Deep Learning Monitor
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://paperswithcode.com/sota"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-link fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Browse State-of-the-Art
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://huggingface.co/transformers/"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-link fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Transformers
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://huggingface.co/models"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-link fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Transformers-models
          </div>
          
        </a></li>
      
    </ul>
  </div>
</section>

          
        
      
        
          
          
            
  <section class='widget category'>
    
<header class='pure'>
  <div><i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;文章分类</div>
  
    <a class="rightBtn"
    
      rel="nofollow"
    
    
    href="/categories/"
    title="categories/">
    <i class="fas fa-expand-arrows-alt fa-fw"></i></a>
  
</header>

    <div class='content pure'>
      <ul class="entry">
        
          <li><a class="flat-box" " href="/categories/nl2sql/"><div class='name'>NL2SQL</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" " href="/categories/nlp/"><div class='name'>NLP</div><div class='badge'>(23)</div></a></li>
        
          <li><a class="flat-box" " href="/categories/博客转载/"><div class='name'>博客转载</div><div class='badge'>(6)</div></a></li>
        
          <li><a class="flat-box" " href="/categories/数据结构与算法/"><div class='name'>数据结构与算法</div><div class='badge'>(11)</div></a></li>
        
          <li><a class="flat-box" " href="/categories/知识图谱/"><div class='name'>知识图谱</div><div class='badge'>(4)</div></a></li>
        
          <li><a class="flat-box" " href="/categories/论文解读/"><div class='name'>论文解读</div><div class='badge'>(2)</div></a></li>
        
          <li><a class="flat-box" " href="/categories/语言模型/"><div class='name'>语言模型</div><div class='badge'>(10)</div></a></li>
        
      </ul>
    </div>
  </section>


          
        
      
        
          
          
            
  <section class='widget tagcloud'>
    
<header class='pure'>
  <div><i class="fas fa-fire fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;热门标签</div>
  
    <a class="rightBtn"
    
      rel="nofollow"
    
    
    href="/tags/"
    title="tags/">
    <i class="fas fa-expand-arrows-alt fa-fw"></i></a>
  
</header>

    <div class='content pure'>
      <a href="/tags/attention/" style="font-size: 16.5px; color: #888">Attention</a> <a href="/tags/cnnlm/" style="font-size: 14px; color: #999">CNNLM</a> <a href="/tags/data-structure/" style="font-size: 14px; color: #999">Data Structure</a> <a href="/tags/deep/" style="font-size: 14px; color: #999">Deep</a> <a href="/tags/ffnnlm/" style="font-size: 14px; color: #999">FFNNLM</a> <a href="/tags/gaussian/" style="font-size: 14px; color: #999">Gaussian</a> <a href="/tags/initialization/" style="font-size: 14px; color: #999">Initialization</a> <a href="/tags/kg/" style="font-size: 17.75px; color: #808080">KG</a> <a href="/tags/lstm/" style="font-size: 14px; color: #999">LSTM</a> <a href="/tags/lstmlm/" style="font-size: 14px; color: #999">LSTMLM</a> <a href="/tags/language-model/" style="font-size: 16.5px; color: #888">Language Model</a> <a href="/tags/log-linear-language-model/" style="font-size: 14px; color: #999">Log-Linear Language Model</a> <a href="/tags/mhsa/" style="font-size: 14px; color: #999">MHSA</a> <a href="/tags/nlp/" style="font-size: 20.25px; color: #6f6f6f">NLP</a> <a href="/tags/nmt/" style="font-size: 22.75px; color: #5e5e5e">NMT</a> <a href="/tags/norm/" style="font-size: 14px; color: #999">Norm</a> <a href="/tags/probabilistic-language-model/" style="font-size: 14px; color: #999">Probabilistic Language Model</a> <a href="/tags/rnnlm/" style="font-size: 14px; color: #999">RNNLM</a> <a href="/tags/roc-auc/" style="font-size: 14px; color: #999">ROC-AUC</a> <a href="/tags/transformer/" style="font-size: 24px; color: #555">Transformer</a> <a href="/tags/context2vec/" style="font-size: 14px; color: #999">context2vec</a> <a href="/tags/divide-conquer/" style="font-size: 14px; color: #999">divide-conquer</a> <a href="/tags/einsum/" style="font-size: 14px; color: #999">einsum</a> <a href="/tags/insertion/" style="font-size: 16.5px; color: #888">insertion</a> <a href="/tags/insertion-deletion/" style="font-size: 15.25px; color: #919191">insertion-deletion</a> <a href="/tags/knowledge-modelling/" style="font-size: 15.25px; color: #919191">knowledge-modelling</a> <a href="/tags/nl2infographic/" style="font-size: 14px; color: #999">nl2infographic</a> <a href="/tags/nl2sql/" style="font-size: 14px; color: #999">nl2sql</a> <a href="/tags/ontology/" style="font-size: 14px; color: #999">ontology</a> <a href="/tags/parallel-recurrent/" style="font-size: 14px; color: #999">parallel-recurrent</a> <a href="/tags/pytorch/" style="font-size: 14px; color: #999">pytorch</a> <a href="/tags/queue/" style="font-size: 19px; color: #777">queue</a> <a href="/tags/sparse/" style="font-size: 14px; color: #999">sparse</a> <a href="/tags/stack/" style="font-size: 14px; color: #999">stack</a> <a href="/tags/survey/" style="font-size: 14px; color: #999">survey</a> <a href="/tags/tensorflow/" style="font-size: 14px; color: #999">tensorflow</a> <a href="/tags/text2viz/" style="font-size: 14px; color: #999">text2viz</a> <a href="/tags/weighted-head/" style="font-size: 14px; color: #999">weighted-head</a> <a href="/tags/半监督语言模型/" style="font-size: 14px; color: #999">半监督语言模型</a> <a href="/tags/双数组前缀树/" style="font-size: 14px; color: #999">双数组前缀树</a> <a href="/tags/推荐系统/" style="font-size: 14px; color: #999">推荐系统</a> <a href="/tags/数据结构/" style="font-size: 21.5px; color: #666">数据结构</a> <a href="/tags/数组/" style="font-size: 14px; color: #999">数组</a> <a href="/tags/时间复杂度/" style="font-size: 14px; color: #999">时间复杂度</a> <a href="/tags/算法/" style="font-size: 14px; color: #999">算法</a> <a href="/tags/评估方法/" style="font-size: 14px; color: #999">评估方法</a> <a href="/tags/词向量/" style="font-size: 14px; color: #999">词向量</a> <a href="/tags/隐式正则化/" style="font-size: 14px; color: #999">隐式正则化</a>
    </div>
  </section>


          
        
      
        
          
          
            


  <section class='widget music'>
    
<header class='pure'>
  <div><i class="fas fa-compact-disc fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;最近在听</div>
  
    <a class="rightBtn"
    
      rel="external nofollow noopener noreferrer"
    
    
      target="_blank"
    
    href="https://music.163.com/#/user/home?id=1960721923"
    title="https://music.163.com/#/user/home?id=1960721923">
    <i class="far fa-heart fa-fw"></i></a>
  
</header>

    <div class='content pure'>
      
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.css">
  <div class="aplayer"
    data-theme="#1BCDFC"
    
    
    data-mode="circulation"
    data-server="netease"
    data-type="playlist"
    data-id="2957571193"
    data-volume="0.7">
  </div>
  <script src="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/meting@1.1.0/dist/Meting.min.js"></script>


    </div>
  </section>


          
        
      
    

  
</aside>

<footer id="footer" class="clearfix">
  <div id="sitetime"></div>
  
  
    <div class="social-wrapper">
      
        
          <a href="/atom.xml"
            class="social fas fa-rss flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
        
          <a href="mailto:rogerspy@163.com"
            class="social fas fa-envelope flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
        
          <a href="https://github.com/rogerspy"
            class="social fab fa-github flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
        
          <a href="https://music.163.com/#/user/home?id=1960721923"
            class="social fas fa-headphones-alt flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
    </div>
  
  <br>
  <div><p>博客内容遵循 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议</a></p>
</div>
  <div>
    本站使用
    <a href="https://xaoxuu.com/wiki/material-x/" target="_blank" class="codename">Material X</a>
    作为主题
    
      ，
      总访问量为
      <span id="busuanzi_value_site_pv"><i class="fas fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span>
      次
    
    。
  </div>
	</footer>

<script>setLoadingBarProgress(80);</script>
<!-- 点击特效，输入特效 运行时间 -->
<script type="text/javascript" src="/cool/cooltext.js"></script>
<script type="text/javascript" src="/cool/clicklove.js"></script>
<script type="text/javascript" src="/cool/sitetime.js"></script>



      <script>setLoadingBarProgress(60);</script>
    </div>
    <a class="s-top fas fa-arrow-up fa-fw" href='javascript:void(0)'></a>
  </div>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>

  <script>
    var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
    var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
    var ALGOLIA_API_KEY = "";
    var ALGOLIA_APP_ID = "";
    var ALGOLIA_INDEX_NAME = "";
    var AZURE_SERVICE_NAME = "";
    var AZURE_INDEX_NAME = "";
    var AZURE_QUERY_KEY = "";
    var BAIDU_API_ID = "";
    var SEARCH_SERVICE = "hexo" || "hexo";
    var ROOT = "/"||"/";
    if(!ROOT.endsWith('/'))ROOT += '/';
  </script>

<script src="//instant.page/1.2.2" type="module" integrity="sha384-2xV8M5griQmzyiY3CDqh1dn4z3llDVqZDqzjzcY+jCBCk/a5fXJmuZ/40JJAPeoU"></script>


  <script async src="https://cdn.jsdelivr.net/npm/scrollreveal@4.0.5/dist/scrollreveal.min.js"></script>
  <script type="text/javascript">
    $(function() {
      const $reveal = $('.reveal');
      if ($reveal.length === 0) return;
      const sr = ScrollReveal({ distance: 0 });
      sr.reveal('.reveal');
    });
  </script>


  <script src="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.js"></script>
  <script type="text/javascript">
    $(function() {
      Waves.attach('.flat-btn', ['waves-button']);
      Waves.attach('.float-btn', ['waves-button', 'waves-float']);
      Waves.attach('.float-btn-light', ['waves-button', 'waves-float', 'waves-light']);
      Waves.attach('.flat-box', ['waves-block']);
      Waves.attach('.float-box', ['waves-block', 'waves-float']);
      Waves.attach('.waves-image');
      Waves.init();
    });
  </script>


  <script async src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-busuanzi@2.3/js/busuanzi.pure.mini.js"></script>




  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
    <script type="text/javascript">
      $(function(){
        if ('.cover') {
          $('.cover').backstretch(
          ["https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/a0c9e6f9efad8b731cb7376504bd10d79d2053.jpg"],
          {
            duration: "6000",
            fade: "2500"
          });
        } else {
          $.backstretch(
          ["https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/a0c9e6f9efad8b731cb7376504bd10d79d2053.jpg"],
          {
            duration: "6000",
            fade: "2500"
          });
        }
      });
    </script>
  







  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script type="text/javascript">
    var gitalk = new Gitalk({
      clientID: "35a5e4dc744cc7d162af",
      clientSecret: "7b5a409e17ce0c1971f284eac9f8902eb4b8feba",
      repo: "rogerspy.github.io",
      owner: "Rogerspy",
      admin: "Rogerspy",
      
        id: "/wiki/material-x/",
      
      distractionFreeMode: false  // Facebook-like distraction free mode
    });
    gitalk.render('gitalk-container');
  </script>





  <script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.5/js/app.js"></script>


  <script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.5/js/search.js"></script>




<!-- 复制 -->
<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  let COPY_SUCCESS = "复制成功";
  let COPY_FAILURE = "复制失败";
  /*页面载入完成后，创建复制按钮*/
  !function (e, t, a) {
    /* code */
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '  <i class="fa fa-copy"></i><span>复制</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });

      clipboard.on('success', function(e) {
        //您可以加入成功提示
        console.info('Action:', e.action);
        console.info('Text:', e.text);
        console.info('Trigger:', e.trigger);
        success_prompt(COPY_SUCCESS);
        e.clearSelection();
      });
      clipboard.on('error', function(e) {
        //您可以加入失败提示
        console.error('Action:', e.action);
        console.error('Trigger:', e.trigger);
        fail_prompt(COPY_FAILURE);
      });
    }
    initCopyCode();

  }(window, document);

  /**
   * 弹出式提示框，默认1.5秒自动消失
   * @param message 提示信息
   * @param style 提示样式，有alert-success、alert-danger、alert-warning、alert-info
   * @param time 消失时间
   */
  var prompt = function (message, style, time)
  {
      style = (style === undefined) ? 'alert-success' : style;
      time = (time === undefined) ? 1500 : time*1000;
      $('<div>')
          .appendTo('body')
          .addClass('alert ' + style)
          .html(message)
          .show()
          .delay(time)
          .fadeOut();
  };

  // 成功提示
  var success_prompt = function(message, time)
  {
      prompt(message, 'alert-success', time);
  };

  // 失败提示
  var fail_prompt = function(message, time)
  {
      prompt(message, 'alert-danger', time);
  };

  // 提醒
  var warning_prompt = function(message, time)
  {
      prompt(message, 'alert-warning', time);
  };

  // 信息提示
  var info_prompt = function(message, time)
  {
      prompt(message, 'alert-info', time);
  };

</script>


<!-- fancybox -->
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
<script>
  let LAZY_LOAD_IMAGE = "";
  $(".article-entry").find("fancybox").find("img").each(function () {
      var element = document.createElement("a");
      $(element).attr("data-fancybox", "gallery");
      $(element).attr("href", $(this).attr("src"));
      /* 图片采用懒加载处理时,
       * 一般图片标签内会有个属性名来存放图片的真实地址，比如 data-original,
       * 那么此处将原本的属性名src替换为对应属性名data-original,
       * 修改如下
       */
       if (LAZY_LOAD_IMAGE) {
         $(element).attr("href", $(this).attr("data-original"));
       }
      $(this).wrap(element);
  });
</script>





  <script>setLoadingBarProgress(100);</script>
</body>
</html>
