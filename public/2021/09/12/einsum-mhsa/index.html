<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  <title>深入理解 einsum：实现多头注意力机制 | Rogerspy&#39;s Home</title>
  
  <meta name="keywords" content="Machine Learning, Deep Learning, NLP">
  
  

  
  <link rel="alternate" href="/atom.xml" title="Rogerspy's Home">
  

  <meta name="HandheldFriendly" content="True">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <!-- meta -->
  
  
  <meta name="theme-color" content="#FFFFFF">
  <meta name="msapplication-TileColor" content="#1BC3FB">
  <meta name="msapplication-config" content="https://cdn.jsdelivr.net/gh/xaoxuu/assets@master/favicon/favicons/browserconfig.xml">
  

  <!-- link -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.css">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.10.1/css/all.min.css">
  
  
  <link rel="shortcut icon" type="image/x-icon" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@master/favicon/favicon.ico">
  <link rel="icon" type="image/x-icon" sizes="32x32" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@master/favicon/favicons/favicon-32x32.png">
  <link rel="apple-touch-icon" type="image/png" sizes="180x180" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@master/favicon/favicons/apple-touch-icon.png">
  <link rel="mask-icon" color="#1BC3FB" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@master/favicon/favicons/safari-pinned-tab.svg">
  <link rel="manifest" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@master/favicon/favicons/site.webmanifest">
  

  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.5/css/style.css">
  

  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>
  

  
  
  <!-- 时间线 -->
  <link rel="stylesheet" href="/css/timeline.css">
  <!-- 血小板-->
  <link rel="stylesheet" href="/live2d/css/live2d.css">
  <style>
	.article p .mjx-math {
	    font-family: Menlo,Monaco,courier,monospace,"Lucida Console",'Source Code Pro',"Microsoft YaHei",Helvetica,Arial,sans-serif,Ubuntu;
        background: none;
        padding: 2px;
        border-radius: 4px;
	}
  </style>
</head>

<body>
  
  
  <header class="l_header pure">
  <div id="loading-bar-wrapper">
    <div id="loading-bar" class="pure"></div>
  </div>

	<div class='wrapper'>
		<div class="nav-main container container--flex">
      <a class="logo flat-box" href='/' >
        
          Rogerspy's Home
        
      </a>
			<div class='menu navgation'>
				<ul class='h-list'>
          
  					
  						<li>
								<a class="nav flat-box" href="/blog/"
                  
                  
                  id="blog">
									<i class='fas fa-edit fa-fw'></i>&nbsp;博客
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/video/"
                  
                  
                  id="video">
									<i class='fas fa-film fa-fw'></i>&nbsp;视频小站
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/material/"
                  
                  
                  id="material">
									<i class='fas fa-briefcase fa-fw'></i>&nbsp;学习资料
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/categories/"
                  
                    rel="nofollow"
                  
                  
                  id="categories">
									<i class='fas fa-folder-open fa-fw'></i>&nbsp;分类
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/archives/"
                  
                    rel="nofollow"
                  
                  
                  id="archives">
									<i class='fas fa-archive fa-fw'></i>&nbsp;归档
								</a>
							</li>
      			
      		
				</ul>
			</div>

			
				<div class="m_search">
					<form name="searchform" class="form u-search-form">
						<input type="text" class="input u-search-input" placeholder="搜索" />
						<i class="icon fas fa-search fa-fw"></i>
					</form>
				</div>
			
			<ul class='switcher h-list'>
				
					<li class='s-search'><a class="fas fa-search fa-fw" href='javascript:void(0)'></a></li>
				
				<li class='s-menu'><a class="fas fa-bars fa-fw" href='javascript:void(0)'></a></li>
			</ul>
		</div>

		<div class='nav-sub container container--flex'>
			<a class="logo flat-box"></a>
			<ul class='switcher h-list'>
				<li class='s-comment'><a class="flat-btn fas fa-comments fa-fw" href='javascript:void(0)'></a></li>
        
          <li class='s-toc'><a class="flat-btn fas fa-list fa-fw" href='javascript:void(0)'></a></li>
        
			</ul>
		</div>
	</div>
</header>
	<aside class="menu-phone">
    <header>
		<nav class="menu navgation">
      <ul>
        
          
            <li>
							<a class="nav flat-box" href="/"
                
                
                id="home">
								<i class='fas fa-clock fa-fw'></i>&nbsp;近期文章
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/archives/"
                
                  rel="nofollow"
                
                
                id="archives">
								<i class='fas fa-archive fa-fw'></i>&nbsp;文章归档
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/blog/"
                
                
                id="blog">
								<i class='fas fa-edit fa-fw'></i>&nbsp;我的博客
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/video/"
                
                  rel="nofollow"
                
                
                id="video">
								<i class='fas fa-film fa-fw'></i>&nbsp;我的视频
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/material/"
                
                  rel="nofollow"
                
                
                id="material">
								<i class='fas fa-briefcase fa-fw'></i>&nbsp;学习资料
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/about/"
                
                  rel="nofollow"
                
                
                id="about">
								<i class='fas fa-info-circle fa-fw'></i>&nbsp;关于小站
							</a>
            </li>
          
       
      </ul>
		</nav>
    </header>
	</aside>
<script>setLoadingBarProgress(40);</script>



  <div class="l_body nocover">
    <div class='body-wrapper'>
      <div class='l_main'>
  

  
    <article id="post" class="post white-box article-type-post" itemscope itemprop="blogPost">
      


  <section class='meta'>
    
    
    <div class="meta" id="header-meta">
      
        
  
    <h1 class="title">
      <a href="/2021/09/12/einsum-mhsa/">
        深入理解 einsum：实现多头注意力机制
      </a>
    </h1>
  


      
      <div class='new-meta-box'>
        
          
        
          
            
  <div class='new-meta-item author'>
    <a href="https://rogerspy.gitee.io" rel="nofollow">
      
        <i class="fas fa-user" aria-hidden="true"></i>
      
      <p>Rogerspy</p>
    </a>
  </div>


          
        
          
            <div class="new-meta-item date">
  <a class='notlink'>
    <i class="fas fa-calendar-alt" aria-hidden="true"></i>
    <p>2021-09-12</p>
  </a>
</div>

          
        
          
            
  
  <div class='new-meta-item category'>
    <a href='/categories/博客转载/' rel="nofollow">
      <i class="fas fa-folder-open" aria-hidden="true"></i>
      <p>博客转载</p>
    </a>
  </div>


          
        
          
            
  
    <div class="new-meta-item browse busuanzi">
      <a class='notlink'>
        <i class="fas fa-eye" aria-hidden="true"></i>
        <p>
          <span id="busuanzi_value_page_pv">
            <i class="fas fa-spinner fa-spin fa-fw" aria-hidden="true"></i>
          </span>
        </p>
      </a>
    </div>
  


          
        
          
            

          
        
          
            
  
    <div style="margin-right: 10px;">
      <span class="post-time">
        <span class="post-meta-item-icon">
          <i class="fa fa-keyboard"></i>
          <span class="post-meta-item-text">  字数统计: </span>
          <span class="post-count">2.6k字</span>
        </span>
      </span>
      &nbsp; | &nbsp;
      <span class="post-time">
        <span class="post-meta-item-icon">
          <i class="fa fa-hourglass-half"></i>
          <span class="post-meta-item-text">  阅读时长≈</span>
          <span class="post-count">13分</span>
        </span>
      </span>
    </div>
  

          
        
      </div>
      
        <hr>
      
    </div>
  </section>


      <section class="article typo">
        <div class="article-entry" itemprop="articleBody">
          <p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/einsum-attention.png" alt></p>
<p>Einsum 表示法是对张量的复杂操作的一种优雅方式，本质上是使用特定领域的语言。 一旦理解并掌握了 einsum，可以帮助我们更快地编写更简洁高效的代码。</p>
<a id="more"></a>
<p>Einsum 是爱因斯坦求和（Einstein summation）的缩写，是一种求和的方法，在处理关于坐标的方程式时非常有效。在 numpy、TensorFlow 和 Pytorch 中都有相关实现，本文通过 Pytorch 实现 Transformer 中的多头注意力来介绍 einsum 在深度学习模型中的应用。</p>
<h1 id="1-矩阵乘法"><a href="#1-矩阵乘法" class="headerlink" title="1. 矩阵乘法"></a>1. 矩阵乘法</h1><p>假设有两个矩阵：</p>
<script type="math/tex; mode=display">
A = \left[\begin{matrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{matrix} \right]
,\quad
B = \left[\begin{matrix}
7 & 8  \\
9 & 10 \\
11 & 12
\end{matrix} \right]</script><p>我们想求两个矩阵的乘积。</p>
<ul>
<li>第一步：</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210912131703.png" style="zoom:67%;"></p>
<ul>
<li>第二步：</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210912132039.png" style="zoom:67%;"></p>
<ul>
<li>第三步：</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210912132541.png" style="zoom:67%;"></p>
<ul>
<li>第四步：</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210912132929.png" style="zoom:67%;"></p>
<h1 id="2-Einstein-Notation"><a href="#2-Einstein-Notation" class="headerlink" title="2. Einstein Notation"></a>2. Einstein Notation</h1><p>爱因斯坦标记法又称爱因斯坦求和约定（Einstein summation convention），基本内容是：</p>
<blockquote>
<p>当两个变量具有相同的角标时，则遍历求和。在此情况下，求和号可以省略。</p>
</blockquote>
<p>比如，计算两个向量的乘积， $\color{red}{a}, \color{blue}{b} \in \mathbb{R}^I$：</p>
<script type="math/tex; mode=display">
\color{green}{c} = \sum_i \color{red}{a_i}\color{blue}{b_i}=\color{red}{a_i}\color{blue}{b_i}</script><p>计算两个矩阵的乘积， <font color="red">$A$</font> $\in \mathbb{R}^{I\times K}$，<font color="blue">$B$</font> $\in \mathbb{R}^{K\times J}$。用爱因斯坦求和符号表示，可以写成：</p>
<script type="math/tex; mode=display">
\color{green}{c}_{ij} \color{black}= \sum_k\color{red}{A_{ik}}\color{blue}{B_{kj}}=\color{red}{A_{ik}}\color{blue}{B_{kj}}</script><p>在深度学习中，通常使用的是更高阶的张量之间的变换。比如在一个 batch 中包含 $N$ 个训练样本的，最大长度是 $T$，词向量维度为 $K$ 的张量，即 $\color{red}{\mathcal{T}}\in \mathbb{R}^{N\times T \times K}$，如果想让词向量的维度映射到 $Q$ 维，则定义一个 $\color{blue}{W} \in \mathbb{R}^{K\times Q}$:</p>
<script type="math/tex; mode=display">
\color{green}{C_{ntq}} = \sum_k\color{red}{\mathcal{T}_{ntk}}\color{blue}{W_{kq}}=\color{red}{\mathcal{T}_{ntk}}\color{blue}{W_{kq}}</script><p>在图像处理中，通常在一个 batch 的训练样本中包含 $N$ 张图片，每张图片长为 $T$，宽为 $K$，颜色通道为 $M$，即 $\color{red}{\mathcal{T}}\in \mathbb{R}^{N\times T \times K \times M}$ 是一个 4d 张量。如果我想进行三个操作：</p>
<ul>
<li>将 $K$ 投影成 $Q$ 维；</li>
<li>对 $T$ 进行求和；</li>
<li>将 $M$ 和 $N$ 进行转置。</li>
</ul>
<p>用爱因斯坦标记法可以表示成：</p>
<script type="math/tex; mode=display">
\color{green}{C_{mqn}}=\sum_t \sum_k \color{red}{\mathcal{T}_{ntkm}} \color{blue}{W_{kq}} = \color{red}{\mathcal{T}_{ntkm}} \color{blue}{W_{kq}}</script><p>需要注意的是，爱因斯坦标记法是一种书写约定，是为了将复杂的公式写得更加简洁。它本身并不是某种运算符，具体运算还是要回归到各种算子上。</p>
<h1 id="3-einsum"><a href="#3-einsum" class="headerlink" title="3. einsum"></a>3. einsum</h1><ul>
<li>Numpy：<code>np.einsum</code></li>
<li>Pytorch：<code>torch.einsum</code></li>
<li>TensorFlow：<code>tf.einsum</code></li>
</ul>
<p>以上三种 <code>einsum</code> 都有相同的特性 <code>einsum(equation, operands)</code>：</p>
<ul>
<li><code>equation</code>：字符串，用来表示爱因斯坦求和标记法的；</li>
<li><code>operands</code>：一些列张量，要运算的张量。</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210912232701.png" alt></p>
<p>其中 <code>口</code> 是一个占位符，代表的是张量维度的字符。比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.einsum(&apos;ij,jk-&gt;ik&apos;, A, B)</span><br></pre></td></tr></table></figure>
<p><code>A</code> 和 <code>B</code> 是两个矩阵，将 <code>ij,jk-&gt;ik</code> 分成两部分：<code>ij, jk</code> 和 <code>ik</code>，那么 <code>ij</code> 代表的是输入矩阵 <code>A</code> 的第 <code>i</code> 维和第 <code>j</code> 维，<code>jk</code> 代表的是 <code>B</code> 第 <code>j</code> 维和第 <code>k</code> 维，<code>ik</code> 代表的是输出矩阵的第 <code>i</code> 维和第 <code>k</code> 维。注意 <code>i, j, k</code> 可以是任意的字符，但是必须保持一致。换句话说，<code>einsum</code> 实际上是直接操作了矩阵的维度（角标）。上例中表示的是， <code>A</code> 和 <code>B</code> 的乘积。</p>
<p><img src="https://obilaniu6266h16.files.wordpress.com/2016/02/einsum-matrixmul.png?w=676" alt></p>
<h2 id="3-1-矩阵转置"><a href="#3-1-矩阵转置" class="headerlink" title="3.1 矩阵转置"></a>3.1 矩阵转置</h2><script type="math/tex; mode=display">
B_{ji} = A_{ij}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ij-&gt;ji'</span>, [a])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">3.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">4.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">5.</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="3-2-求和"><a href="#3-2-求和" class="headerlink" title="3.2 求和"></a>3.2 求和</h2><script type="math/tex; mode=display">
b = \sum_i\sum_j A_{ij}=A_{ij}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ij-&gt;'</span>, [a])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor(<span class="number">15.</span>)</span><br></pre></td></tr></table></figure>
<h2 id="3-3-列求和"><a href="#3-3-列求和" class="headerlink" title="3.3 列求和"></a>3.3 列求和</h2><script type="math/tex; mode=display">
b_j=\sum_iA_{ij}=A_{ij}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ij-&gt;j'</span>, [a])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([ <span class="number">3.</span>,  <span class="number">5.</span>,  <span class="number">7.</span>])</span><br></pre></td></tr></table></figure>
<h2 id="3-4-行求和"><a href="#3-4-行求和" class="headerlink" title="3.4 行求和"></a>3.4 行求和</h2><script type="math/tex; mode=display">
b_i=\sum_jA_{ij}=A_{ij}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ij-&gt;i'</span>, [a])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([  <span class="number">3.</span>,  <span class="number">12.</span>])</span><br></pre></td></tr></table></figure>
<h2 id="3-5-矩阵-向量乘积"><a href="#3-5-矩阵-向量乘积" class="headerlink" title="3.5 矩阵-向量乘积"></a>3.5 矩阵-向量乘积</h2><script type="math/tex; mode=display">
c_i=\sum_kA_{ik}b_k=A_{ik}b_k</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b = torch.arange(<span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ik,k-&gt;i'</span>, [a, b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([  <span class="number">5.</span>,  <span class="number">14.</span>])</span><br></pre></td></tr></table></figure>
<h2 id="3-6-矩阵-矩阵乘积"><a href="#3-6-矩阵-矩阵乘积" class="headerlink" title="3.6 矩阵-矩阵乘积"></a>3.6 矩阵-矩阵乘积</h2><script type="math/tex; mode=display">
C_{ij}=\sum_kA_{ik}B_{kj}=A_{ik}B_{kj}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b = torch.arange(<span class="number">15</span>).reshape(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">torch.einsum(<span class="string">'ik,kj-&gt;ij'</span>, [a, b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">tensor([[  <span class="number">25.</span>,   <span class="number">28.</span>,   <span class="number">31.</span>,   <span class="number">34.</span>,   <span class="number">37.</span>],</span><br><span class="line">        [  <span class="number">70.</span>,   <span class="number">82.</span>,   <span class="number">94.</span>,  <span class="number">106.</span>,  <span class="number">118.</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="3-7-点积"><a href="#3-7-点积" class="headerlink" title="3.7 点积"></a>3.7 点积</h2><script type="math/tex; mode=display">
c = \sum_ia_ib_i=a_ib_i</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">3</span>)</span><br><span class="line">b = torch.arange(<span class="number">3</span>, <span class="number">6</span>)</span><br><span class="line">torch.einsum(<span class="string">'i,i-&gt;'</span>, [a, b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">tensor(<span class="number">14.</span>)</span><br></pre></td></tr></table></figure>
<h2 id="3-8-Hardamard-积"><a href="#3-8-Hardamard-积" class="headerlink" title="3.8 Hardamard 积"></a>3.8 Hardamard 积</h2><script type="math/tex; mode=display">
C_{ij} = A_{ij}B_{ij}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b = torch.arange(<span class="number">6</span>,<span class="number">12</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ij,ij-&gt;ij'</span>, [a, b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">tensor([[  <span class="number">0.</span>,   <span class="number">7.</span>,  <span class="number">16.</span>],</span><br><span class="line">        [ <span class="number">27.</span>,  <span class="number">40.</span>,  <span class="number">55.</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="3-9-外积"><a href="#3-9-外积" class="headerlink" title="3.9 外积"></a>3.9 外积</h2><script type="math/tex; mode=display">
C_{ij}=a_ib_j</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">3</span>)</span><br><span class="line">b = torch.arange(<span class="number">3</span>, <span class="number">7</span>)</span><br><span class="line">torch.einsum(<span class="string">'i, j-&gt;ij'</span>, [a, b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">tensor([[  <span class="number">0.</span>,   <span class="number">0.</span>,   <span class="number">0.</span>,   <span class="number">0.</span>],</span><br><span class="line">        [  <span class="number">3.</span>,   <span class="number">4.</span>,   <span class="number">5.</span>,   <span class="number">6.</span>],</span><br><span class="line">        [  <span class="number">6.</span>,   <span class="number">8.</span>,  <span class="number">10.</span>,  <span class="number">12.</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="3-10-Batch-矩阵乘积"><a href="#3-10-Batch-矩阵乘积" class="headerlink" title="3.10 Batch 矩阵乘积"></a>3.10 Batch 矩阵乘积</h2><script type="math/tex; mode=display">
C_{ijl}=\sum_kA_{ijk}B_{ikl}=A_{ijk}B_{ikl}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>,<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">b = torch.randn(<span class="number">3</span>,<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ijk, jkl-&gt;ijl'</span>, [a, b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">tensor([[[ <span class="number">1.0886</span>,  <span class="number">0.0214</span>,  <span class="number">1.0690</span>],</span><br><span class="line">         [ <span class="number">2.0626</span>,  <span class="number">3.2655</span>, <span class="number">-0.1465</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-6.9294</span>,  <span class="number">0.7499</span>,  <span class="number">1.2976</span>],</span><br><span class="line">         [ <span class="number">4.2226</span>, <span class="number">-4.5774</span>, <span class="number">-4.8947</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-2.4289</span>, <span class="number">-0.7804</span>,  <span class="number">5.1385</span>],</span><br><span class="line">         [ <span class="number">0.8003</span>,  <span class="number">2.9425</span>,  <span class="number">1.7338</span>]]])</span><br></pre></td></tr></table></figure>
<h2 id="3-11-张量收缩"><a href="#3-11-张量收缩" class="headerlink" title="3.11 张量收缩"></a>3.11 张量收缩</h2><p>假设有两个张量 $\mathcal{A}\in \mathbb{R}^{I_1\times \dots\times I_n}$ 和 $\mathcal{B} \in \mathbb{R}^{J_1\times \dots \times J_m}$。比如 $n=4, m=5$，且 $I_2=J_3$ 和 $I_3=J_5$。我们可以计算两个张量的乘积，得到新的张量 $\mathcal{C}\in\mathbb{R}^{I_1\times I_4 \times J_1 \times J_2 \times J_4}$：</p>
<script type="math/tex; mode=display">
C_{pstuv}=\sum_q\sum_r A_{pqrs}B_{tuqvr} = A_{pqrs}B_{tuqvr}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>)</span><br><span class="line">b = torch.randn(<span class="number">11</span>,<span class="number">13</span>,<span class="number">3</span>,<span class="number">17</span>,<span class="number">5</span>)</span><br><span class="line">torch.einsum(<span class="string">'pqrs,tuqvr-&gt;pstuv'</span>, [a, b]).shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">13</span>, <span class="number">17</span>])</span><br></pre></td></tr></table></figure>
<h2 id="3-12-双线性变换"><a href="#3-12-双线性变换" class="headerlink" title="3.12 双线性变换"></a>3.12 双线性变换</h2><script type="math/tex; mode=display">
D_{ij}=\sum_k\sum_lA_{ik}B_{jkl}C_{il} = A_{ik}B_{jkl}C_{il}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.randn(<span class="number">5</span>,<span class="number">3</span>,<span class="number">7</span>)</span><br><span class="line">c = torch.randn(<span class="number">2</span>,<span class="number">7</span>)</span><br><span class="line">torch.einsum(<span class="string">'ik,jkl,il-&gt;ij'</span>, [a, b, c])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([[ <span class="number">3.8471</span>,  <span class="number">4.7059</span>, <span class="number">-3.0674</span>, <span class="number">-3.2075</span>, <span class="number">-5.2435</span>],</span><br><span class="line">        [<span class="number">-3.5961</span>, <span class="number">-5.2622</span>, <span class="number">-4.1195</span>,  <span class="number">5.5899</span>,  <span class="number">0.4632</span>]])</span><br></pre></td></tr></table></figure>
<h1 id="4-einops"><a href="#4-einops" class="headerlink" title="4. einops"></a>4. einops</h1><p>尽管 <code>einops</code> 是一个通用的包，这里哦我们只介绍 <code>einops.rearrange</code> 。同 <code>einsum</code> 一样，<code>einops.rearrange</code> 也是操作矩阵的角标的，只不过函数的参数正好相反，如下图所示。</p>
<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210914003018.png" alt></p>
<div class="container" style="margin-top:40px;margin-bottom:20px;">
    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">
        <div style="margin-left:10px">
            <font color="white" size="4">
                • NOTE
            </font>
        </div>
    </div>
    <div style="background-color:#F3F4F7">
        <div style="padding:15px 10px 15px 20px;line-height:1.5;">
            如果 <code>rearrange</code> 传入的参数是一个张量列表，那么后面字符串的第一维表示列表的长度。
        </div>    
    </div>    
</div>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">qkv = torch.rand(<span class="number">2</span>,<span class="number">128</span>,<span class="number">3</span>*<span class="number">512</span>) <span class="comment"># dummy data for illustration only</span></span><br><span class="line"><span class="comment"># We need to decompose to n=3 tensors q, v, k</span></span><br><span class="line"><span class="comment"># rearrange tensor to [3, batch, tokens, dim] and cast to tuple</span></span><br><span class="line">q, k, v = tuple(rearrange( qkv , <span class="string">'b t (d n) -&gt; n b t d '</span>, n=<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<h1 id="5-Scale-dot-product-self-attention"><a href="#5-Scale-dot-product-self-attention" class="headerlink" title="5. Scale dot product self-attention"></a>5. Scale dot product self-attention</h1><ul>
<li><p><strong>第一步</strong>：创建一个线性投影。给定输入 $X\in \mathbb{R}^{b\times t\times d}$，其中 $b$ 表示 $\text{batch size}$，$t$ 表示 $\text{sentence length}$，$d$ 表示 $\text{word dimension}$。</p>
<script type="math/tex; mode=display">
Q=XW_Q, \quad K=XW_K, \quad V=XW_V</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">to_qvk = nn.Linear(dim, dim * <span class="number">3</span>, bias=<span class="literal">False</span>) <span class="comment"># init only</span></span><br><span class="line"><span class="comment"># Step 1</span></span><br><span class="line">qkv = to_qvk(x)  <span class="comment"># [batch, tokens, dim*3 ]</span></span><br><span class="line"><span class="comment"># decomposition to q,v,k</span></span><br><span class="line">q, k, v = tuple(rearrange(qkv, <span class="string">'b t (d k) -&gt; k b t d '</span>, k=<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>第二步</strong>：计算点积，mask，最后计算 softmax。</p>
<script type="math/tex; mode=display">
\text{dot_score} = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} \right)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Step 2</span></span><br><span class="line"><span class="comment"># Resulting shape: [batch, tokens, tokens]</span></span><br><span class="line">scaled_dot_prod = torch.einsum(<span class="string">'b i d , b j d -&gt; b i j'</span>, q, k) * self.scale_factor</span><br><span class="line"><span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">assert</span> mask.shape == scaled_dot_prod.shape[<span class="number">1</span>:]</span><br><span class="line">    scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)</span><br><span class="line">attention = torch.softmax(scaled_dot_prod, dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>第三步</strong>：计算注意力得分与 $V$ 的乘积。</p>
<script type="math/tex; mode=display">
\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} \right)V</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.einsum(<span class="string">'b i j , b j d -&gt; b i d'</span>, attention, v)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>将上面三步综合起来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SelfAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of plain self attention mechanism with einsum operations</span></span><br><span class="line"><span class="string">    Paper: https://arxiv.org/abs/1706.03762</span></span><br><span class="line"><span class="string">    Blog: https://theaisummer.com/transformer/</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dim)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            dim: for NLP it is the dimension of the embedding vector</span></span><br><span class="line"><span class="string">            the last dimension size that will be provided in forward(x),</span></span><br><span class="line"><span class="string">            where x is a 3D tensor</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="comment"># for Step 1</span></span><br><span class="line">        self.to_qvk = nn.Linear(dim, dim * <span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># for Step 2</span></span><br><span class="line">        self.scale_factor = dim ** <span class="number">-0.5</span>  <span class="comment"># 1/np.sqrt(dim)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask=None)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x.dim() == <span class="number">3</span>, <span class="string">'3D tensor must be provided'</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 1</span></span><br><span class="line">        qkv = self.to_qvk(x)  <span class="comment"># [batch, tokens, dim*3 ]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># decomposition to q,v,k</span></span><br><span class="line">        <span class="comment"># rearrange tensor to [3, batch, tokens, dim] and cast to tuple</span></span><br><span class="line">        q, k, v = tuple(rearrange(qkv, <span class="string">'b t (d k) -&gt; k b t d '</span>, k=<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2</span></span><br><span class="line">        <span class="comment"># Resulting shape: [batch, tokens, tokens]</span></span><br><span class="line">        scaled_dot_prod = torch.einsum(<span class="string">'b i d , b j d -&gt; b i j'</span>, q, k) * self.scale_factor</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> mask.shape == scaled_dot_prod.shape[<span class="number">1</span>:]</span><br><span class="line">            scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)</span><br><span class="line"></span><br><span class="line">        attention = torch.softmax(scaled_dot_prod, dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 3</span></span><br><span class="line">        <span class="keyword">return</span> torch.einsum(<span class="string">'b i j , b j d -&gt; b i d'</span>, attention, v)</span><br></pre></td></tr></table></figure>
<h1 id="6-Multi-Head-Self-Attention"><a href="#6-Multi-Head-Self-Attention" class="headerlink" title="6. Multi-Head Self-Attention"></a>6. Multi-Head Self-Attention</h1><ul>
<li><p><strong>第一步</strong>：为每一个头创建一个线性投影 $Q, K, V$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">to_qvk = nn.Linear(dim, dim_head * heads * <span class="number">3</span>, bias=<span class="literal">False</span>) <span class="comment"># init only</span></span><br><span class="line">qkv = self.to_qvk(x)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>第二步</strong>：将 $Q, K, V$ 分解，并分配给每个头。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Step 2</span></span><br><span class="line"><span class="comment"># decomposition to q,v,k and cast to tuple</span></span><br><span class="line"><span class="comment"># [3, batch, heads, tokens, dim_head]</span></span><br><span class="line">q, k, v = tuple(rearrange(qkv, <span class="string">'b t (d k h) -&gt; k b h t d '</span>, k=<span class="number">3</span>, h=self.heads))</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>第三步</strong>：计算注意力得分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Step 3</span></span><br><span class="line"><span class="comment"># resulted shape will be: [batch, heads, tokens, tokens]</span></span><br><span class="line">scaled_dot_prod = torch.einsum(<span class="string">'b h i d , b h j d -&gt; b h i j'</span>, q, k) * self.scale_factor</span><br><span class="line"><span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">assert</span> mask.shape == scaled_dot_prod.shape[<span class="number">2</span>:]</span><br><span class="line">    scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)</span><br><span class="line">attention = torch.softmax(scaled_dot_prod, dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>第四步</strong>：注意力得分与 $V$ 相乘</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Step 4. Calc result per batch and per head h</span></span><br><span class="line">out = torch.einsum(<span class="string">'b h i j , b h j d -&gt; b h i d'</span>, attention, v)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>第五步</strong>：将所有的头合并</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out = rearrange(out, <span class="string">"b h t d -&gt; b t (h d)"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>第六步</strong>：线性变换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.W_0 = nn.Linear( _dim, dim, bias=<span class="literal">False</span>) <span class="comment"># init only</span></span><br><span class="line"><span class="comment"># Step 6. Apply final linear transformation layer</span></span><br><span class="line">self.W_0(out)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>最终实现 MHSA：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadSelfAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dim, heads=<span class="number">8</span>, dim_head=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Implementation of multi-head attention layer of the original transformer model.</span></span><br><span class="line"><span class="string">        einsum and einops.rearrange is used whenever possible</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            dim: token's dimension, i.e. word embedding vector size</span></span><br><span class="line"><span class="string">            heads: the number of distinct representations to learn</span></span><br><span class="line"><span class="string">            dim_head: the dim of the head. In general dim_head&lt;dim.</span></span><br><span class="line"><span class="string">            However, it may not necessary be (dim/heads)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dim_head = (int(dim / heads)) <span class="keyword">if</span> dim_head <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> dim_head</span><br><span class="line">        _dim = self.dim_head * heads</span><br><span class="line">        self.heads = heads</span><br><span class="line">        self.to_qvk = nn.Linear(dim, _dim * <span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_0 = nn.Linear( _dim, dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.scale_factor = self.dim_head ** <span class="number">-0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask=None)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x.dim() == <span class="number">3</span></span><br><span class="line">        <span class="comment"># Step 1</span></span><br><span class="line">        qkv = self.to_qvk(x)  <span class="comment"># [batch, tokens, dim*3*heads ]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2</span></span><br><span class="line">        <span class="comment"># decomposition to q,v,k and cast to tuple</span></span><br><span class="line">        <span class="comment"># the resulted shape before casting to tuple will be:</span></span><br><span class="line">        <span class="comment"># [3, batch, heads, tokens, dim_head]</span></span><br><span class="line">        q, k, v = tuple(rearrange(qkv, <span class="string">'b t (d k h) -&gt; k b h t d '</span>, k=<span class="number">3</span>, h=self.heads))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 3</span></span><br><span class="line">        <span class="comment"># resulted shape will be: [batch, heads, tokens, tokens]</span></span><br><span class="line">        scaled_dot_prod = torch.einsum(<span class="string">'b h i d , b h j d -&gt; b h i j'</span>, q, k) * self.scale_factor</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> mask.shape == scaled_dot_prod.shape[<span class="number">2</span>:]</span><br><span class="line">            scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)</span><br><span class="line"></span><br><span class="line">        attention = torch.softmax(scaled_dot_prod, dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 4. Calc result per batch and per head h</span></span><br><span class="line">        out = torch.einsum(<span class="string">'b h i j , b h j d -&gt; b h i d'</span>, attention, v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 5. Re-compose: merge heads with dim_head d</span></span><br><span class="line">        out = rearrange(out, <span class="string">"b h t d -&gt; b t (h d)"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 6. Apply final linear transformation layer</span></span><br><span class="line">        <span class="keyword">return</span> self.W_0(out)</span><br></pre></td></tr></table></figure>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol>
<li><p><a href="https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/" target="_blank" rel="noopener">Einstein Summation in Numpy</a>, <em>OLEXA BILANIUK</em></p>
</li>
<li><p><a href="https://ajcr.net/Basic-guide-to-einsum/" target="_blank" rel="noopener">A basic introduction to NumPy’s einsum</a>, <em>Alex Riley</em></p>
</li>
<li><a href="https://rockt.github.io/2018/04/30/einsum" target="_blank" rel="noopener">EINSUM IS ALL YOU NEED - EINSTEIN SUMMATION IN DEEP LEARNING</a>, <em>Tim Rocktäschel</em> </li>
<li><a href="https://theaisummer.com/einsum-attention/" target="_blank" rel="noopener">Understanding einsum for Deep learning: implement a transformer with multi-head self-attention from scratch</a>, <em>Nikolas Adaloglou</em></li>
</ol>

        </div>
        
          


  <section class='meta' id="footer-meta">
    <hr>
    <div class='new-meta-box'>
      
        
          <div class="new-meta-item date" itemprop="dateUpdated" datetime="2022-01-12T16:37:38+08:00">
  <a class='notlink'>
    <i class="fas fa-clock" aria-hidden="true"></i>
    <p>最后更新于 2022年1月12日</p>
  </a>
</div>

        
      
        
          
  
  <div class="new-meta-item meta-tags"><a class="tag" href="/tags/einsum/" rel="nofollow"><i class="fas fa-hashtag" aria-hidden="true"></i>&nbsp;<p>einsum</p></a></div> <div class="new-meta-item meta-tags"><a class="tag" href="/tags/mhsa/" rel="nofollow"><i class="fas fa-hashtag" aria-hidden="true"></i>&nbsp;<p>MHSA</p></a></div>


        
      
        
          
  <div class="new-meta-item share -mob-share-list">
  <div class="-mob-share-list share-body">
    
      
        <a class="-mob-share-qq" title="QQ好友" rel="external nofollow noopener noreferrer"
          
          href="http://connect.qq.com/widget/shareqq/index.html?url=https://rogerspy.gitee.io/2021/09/12/einsum-mhsa/&title=深入理解 einsum：实现多头注意力机制 | Rogerspy's Home&summary=
Einsum 表示法是对张量的复杂操作的一种优雅方式，本质上是使用特定领域的语言。 一旦理解并掌握了 einsum，可以帮助我们更快地编写更简洁高效的代码。"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/qq.png">
          
        </a>
      
    
      
        <a class="-mob-share-qzone" title="QQ空间" rel="external nofollow noopener noreferrer"
          
          href="https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=https://rogerspy.gitee.io/2021/09/12/einsum-mhsa/&title=深入理解 einsum：实现多头注意力机制 | Rogerspy's Home&summary=
Einsum 表示法是对张量的复杂操作的一种优雅方式，本质上是使用特定领域的语言。 一旦理解并掌握了 einsum，可以帮助我们更快地编写更简洁高效的代码。"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/qzone.png">
          
        </a>
      
    
      
        <a class='qrcode' rel="external nofollow noopener noreferrer" href='data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACI0lEQVR42u3aS26EQAwFQO5/6ckBEsizHSJhqlcjEqCLhdX+HEe8Pifr+1/P/v/smWdvOe5YGBgYj2V8Ltf1Rq83ff3MM1LyhB+eg4GB8QJG/ugeLA+m1+89vY6BgYHROgjmwTf5jYGBgXHLcS1ITfMDKAYGBkYeEPN7J+vGXBwDA+OBjLzq/v+/b+lvYGBgPIoxD4KTtsFfBW4MDIzdjDzA5cey6nZ7wxYYGBhvY0xCZO/e6lhYcgUDA2M3o1rAmoTFvKBWbaNiYGDsZuQDE3nKmrck81S52cjEwMBYwchDZ/UgWN10PgJSmBnBwMB4OKNXOMuv93qq1ZCNgYGxm5EHu16QTe6qtgpOSRgYGEsZ+SMm4TUP680UFwMDYzXjl0jcGqHoBfEcXxizwMDAWMTIQ+Edbc5860dvYWBgLGLkQ2D5KydjFtUyHAYGxhsYeXGtmpTmr58MnGFgYGxl9JqFvSGw3rhYVJ7DwMBYzciHJ/KRizztnI+3RtEaAwNjKaMwqVEsoiUb7aW1GBgYb2BMEtE8Ne0NVUSfDwMD4zWMyVBFnpTe0hzFwMBYyvgUV7KheaeikLhiYGC8gFFNRHvbrTYAeq0FDAyM3Yx5kO21QqufrHwbBgbGOkZ1qGISmudJLAYGBkavfJYf+PLi3ai/gYGB8WJGr0BWHQKrHl4xMDDewJinpskm8hmJ6kfBwMDYzaimjr1Sfg6rJsYYGBirGV/RgtsZAp0ElAAAAABJRU5ErkJggg=='>
        
          <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/wechat.png">
        
        </a>
      
    
      
        <a class="-mob-share-weibo" title="微博" rel="external nofollow noopener noreferrer"
          
          href="http://service.weibo.com/share/share.php?url=https://rogerspy.gitee.io/2021/09/12/einsum-mhsa/&title=深入理解 einsum：实现多头注意力机制 | Rogerspy's Home&summary=
Einsum 表示法是对张量的复杂操作的一种优雅方式，本质上是使用特定领域的语言。 一旦理解并掌握了 einsum，可以帮助我们更快地编写更简洁高效的代码。"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/weibo.png">
          
        </a>
      
    
  </div>
</div>



        
      
    </div>
  </section>


        
        
            <div class="prev-next">
                
                    <section class="prev">
                        <span class="art-item-left">
                            <h6><i class="fas fa-chevron-left" aria-hidden="true"></i>&nbsp;上一页</h6>
                            <h4>
                                <a href="/2021/09/15/ptm-data-noising-as-smoothing/" rel="prev" title="预训练语言模型：Data noising smoothing">
                                  
                                      预训练语言模型：Data noising smoothing
                                  
                                </a>
                            </h4>
                            
                                
                                <h6 class="tags">
                                    <a class="tag" href="/tags/language-model/"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>Language Model</a> <a class="tag" href="/tags/data-noising/"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>data noising</a>
                                </h6>
                            
                        </span>
                    </section>
                
                
                    <section class="next">
                        <span class="art-item-right" aria-hidden="true">
                            <h6>下一页&nbsp;<i class="fas fa-chevron-right" aria-hidden="true"></i></h6>
                            <h4>
                                <a href="/2021/09/09/ptm-context2vec/" rel="prev" title="预训练语言模型：context2vec">
                                    
                                        预训练语言模型：context2vec
                                    
                                </a>
                            </h4>
                            
                                
                                <h6 class="tags">
                                    <a class="tag" href="/tags/context2vec/"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>context2vec</a>
                                </h6>
                            
                        </span>
                    </section>
                
            </div>
        
      </section>
    </article>
  

  
    <!-- 显示推荐文章和评论 -->



  <article class="post white-box comments">
    <section class="article typo">
      <h4><i class="fas fa-comments fa-fw" aria-hidden="true"></i>&nbsp;评论</h4>
      
      
      
        <section id="comments">
          <div id="gitalk-container"></div>
        </section>
      
      
    </section>
  </article>


  




<!-- 根据页面mathjax变量决定是否加载MathJax数学公式js -->

  <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": {
      preferredFont: "TeX",
      availableFonts: ["STIX","TeX"],
      linebreaks: { automatic:true },
      EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
      inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
      processEscapes: true,
      ignoreClass: "tex2jax_ignore|dno",
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: { autoNumber: "AMS" },
      noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
      Macros: { href: "{}" }
    },
    messageStyle: "none"
  });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += (all[i].SourceElement().parentNode.className ? ' ' : '') + 'has-jax';
    }
  });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>




  <script>
    window.subData = {
      title: '深入理解 einsum：实现多头注意力机制',
      tools: true
    }
  </script>


</div>
<aside class='l_side'>
  
    
    
      
        
          
          
            <section class='widget shake author'>
  <div class='content pure'>
    
      <div class='avatar'>
        <img class='avatar' src='https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/65-1Z31313530JC.jpeg'/>
      </div>
    
    
    
      <div class="social-wrapper">
        
          
            <a href="/atom.xml"
              class="social fas fa-rss flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
          
            <a href="mailto:rogerspy@163.com"
              class="social fas fa-envelope flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
          
            <a href="https://github.com/rogerspy"
              class="social fab fa-github flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
          
            <a href="https://music.163.com/#/user/home?id=1960721923"
              class="social fas fa-headphones-alt flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
      </div>
    
  </div>
</section>

          
        
      
        
          
          
            
  <section class='widget toc-wrapper'>
    
<header class='pure'>
  <div><i class="fas fa-list fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;本文目录</div>
  
    <div class='wrapper'><a class="s-toc rightBtn" rel="external nofollow noopener noreferrer" href="javascript:void(0)"><i class="fas fa-thumbtack fa-fw"></i></a></div>
  
</header>

    <div class='content pure'>
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-矩阵乘法"><span class="toc-text">1. 矩阵乘法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-Einstein-Notation"><span class="toc-text">2. Einstein Notation</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-einsum"><span class="toc-text">3. einsum</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-矩阵转置"><span class="toc-text">3.1 矩阵转置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-求和"><span class="toc-text">3.2 求和</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-列求和"><span class="toc-text">3.3 列求和</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-行求和"><span class="toc-text">3.4 行求和</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5-矩阵-向量乘积"><span class="toc-text">3.5 矩阵-向量乘积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-6-矩阵-矩阵乘积"><span class="toc-text">3.6 矩阵-矩阵乘积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-7-点积"><span class="toc-text">3.7 点积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-8-Hardamard-积"><span class="toc-text">3.8 Hardamard 积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-9-外积"><span class="toc-text">3.9 外积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-10-Batch-矩阵乘积"><span class="toc-text">3.10 Batch 矩阵乘积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-11-张量收缩"><span class="toc-text">3.11 张量收缩</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-12-双线性变换"><span class="toc-text">3.12 双线性变换</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-einops"><span class="toc-text">4. einops</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-Scale-dot-product-self-attention"><span class="toc-text">5. Scale dot product self-attention</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-Multi-Head-Self-Attention"><span class="toc-text">6. Multi-Head Self-Attention</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Reference"><span class="toc-text">Reference</span></a></li></ol>
    </div>
  </section>


          
        
      
        
          
          
            <section class='widget grid'>
  
<header class='pure'>
  <div><i class="fas fa-map-signs fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;站内导航</div>
  
</header>

  <div class='content pure'>
    <ul class="grid navgation">
      
        <li><a class="flat-box" " href="/"
          
          
          id="home">
          
            <i class="fas fa-clock fa-fw" aria-hidden="true"></i>
          
          近期文章
        </a></li>
      
        <li><a class="flat-box" " href="/blog/"
          
          
          id="blog">
          
            <i class="fas fa-edit fa-fw" aria-hidden="true"></i>
          
          我的博客
        </a></li>
      
        <li><a class="flat-box" " href="/paper_note/"
          
          
          id="paper_note">
          
            <i class="fas fa-book fa-fw" aria-hidden="true"></i>
          
          论文笔记
        </a></li>
      
        <li><a class="flat-box" " href="https://rogerspy.github.io/data_structure_algorithm/"
          
          
          id="https:rogerspy.github.iodata_structure_algorithm">
          
            <i class="fas fa-cube fa-fw" aria-hidden="true"></i>
          
          算法基础
        </a></li>
      
        <li><a class="flat-box" " href="/leetcode/"
          
          
          id="leetcode">
          
            <i class="fas fa-code fa-fw" aria-hidden="true"></i>
          
          Leetcode
        </a></li>
      
        <li><a class="flat-box" " href="/leetcode/"
          
          
          id="leetcode">
          
            <i class="fab fa-kaggle fa-fw" aria-hidden="true"></i>
          
          Kaggle
        </a></li>
      
        <li><a class="flat-box" " href="/paper_note/"
          
          
          id="paper_note">
          
            <i class="fas fa-brain fa-fw" aria-hidden="true"></i>
          
          思维逻辑
        </a></li>
      
        <li><a class="flat-box" " href="/algorithm/"
          
          
          id="algorithm">
          
            <i class="fas fa-keyboard fa-fw" aria-hidden="true"></i>
          
          学习小测
        </a></li>
      
        <li><a class="flat-box" " href="/video/"
          
          
          id="video">
          
            <i class="fas fa-film fa-fw" aria-hidden="true"></i>
          
          视频小站
        </a></li>
      
        <li><a class="flat-box" " href="/material/"
          
          
          id="material">
          
            <i class="fas fa-briefcase fa-fw" aria-hidden="true"></i>
          
          学习资料
        </a></li>
      
        <li><a class="flat-box" " href="https://paperswithcode.com/datasets"
          
          
          id="https:paperswithcode.comdatasets">
          
            <i class="fas fa-database fa-fw" aria-hidden="true"></i>
          
          数据集
        </a></li>
      
        <li><a class="flat-box" " href="/articles/"
          
          
          id="articles">
          
            <i class="fas fa-sticky-note fa-fw" aria-hidden="true"></i>
          
          杂文天地
        </a></li>
      
        <li><a class="flat-box" " href="/archives/"
          
            rel="nofollow"
          
          
          id="archives">
          
            <i class="fas fa-archive fa-fw" aria-hidden="true"></i>
          
          文章归档
        </a></li>
      
        <li><a class="flat-box" " href="/personal_center/"
          
          
          id="personal_center">
          
            <i class="fas fa-university fa-fw" aria-hidden="true"></i>
          
          个人中心
        </a></li>
      
        <li><a class="flat-box" " href="/about/"
          
            rel="nofollow"
          
          
          id="about">
          
            <i class="fas fa-info-circle fa-fw" aria-hidden="true"></i>
          
          关于小站
        </a></li>
      
    </ul>
  </div>
</section>

          
        
      
        
          
          
            <section class='widget list'>
  
<header class='pure'>
  <div><i class="fas fa-terminal fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;机器学习框架</div>
  
</header>

  <div class='content pure'>
    <ul class="entry">
      
        <li><a class="flat-box" href="https://rogerspy.gitee.io/pytorch-zh/"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-star fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;PyTorch 中文文档
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://keras-zh.readthedocs.io/"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-star fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Keras 中文文档
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://tensorflow.google.cn/"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-star fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Tensorflow 中文文档
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="http://scikitlearn.com.cn/"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-star fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Scikit Learn 中文文档
          </div>
          
        </a></li>
      
    </ul>
  </div>
</section>

          
        
      
        
          
          
            <section class='widget list'>
  
<header class='pure'>
  <div><i class="fas fa-wrench fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;百宝箱</div>
  
</header>

  <div class='content pure'>
    <ul class="entry">
      
        <li><a class="flat-box" href="https://rogerspy.github.io/excalidraw-claymate/"
          
          
            target="_blank"
          
          >
          <div class='name'>
            
              <i class="fas fa-magic fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Excalidraw-Claymate
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://rogerspy.github.io/jupyterlite/"
          
          
            target="_blank"
          
          >
          <div class='name'>
            
              <i class="fas fa-terminal fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;JupyterLite
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://rogerspy.github.io/kanban/"
          
          
            target="_blank"
          
          >
          <div class='name'>
            
              <i class="fas fa-table fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Kanban
          </div>
          
        </a></li>
      
    </ul>
  </div>
</section>

          
        
      
        
          
          
            <section class='widget list'>
  
<header class='pure'>
  <div><i class="fas fa-eye fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;睁眼看世界</div>
  
</header>

  <div class='content pure'>
    <ul class="entry">
      
        <li><a class="flat-box" href="https://deeplearn.org/"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-link fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Deep Learning Monitor
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://paperswithcode.com/sota"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-link fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Browse State-of-the-Art
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://huggingface.co/transformers/"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-link fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Transformers
          </div>
          
        </a></li>
      
        <li><a class="flat-box" href="https://huggingface.co/models"
          
          
          >
          <div class='name'>
            
              <i class="fas fa-link fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Transformers-models
          </div>
          
        </a></li>
      
    </ul>
  </div>
</section>

          
        
      
        
          
          
            
  <section class='widget category'>
    
<header class='pure'>
  <div><i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;文章分类</div>
  
    <a class="rightBtn"
    
      rel="nofollow"
    
    
    href="/categories/"
    title="categories/">
    <i class="fas fa-expand-arrows-alt fa-fw"></i></a>
  
</header>

    <div class='content pure'>
      <ul class="entry">
        
          <li><a class="flat-box" " href="/categories/incremental-learning/"><div class='name'>Incremental Learning</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" " href="/categories/nl2sql/"><div class='name'>NL2SQL</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" " href="/categories/nlp/"><div class='name'>NLP</div><div class='badge'>(23)</div></a></li>
        
          <li><a class="flat-box" " href="/categories/博客转载/"><div class='name'>博客转载</div><div class='badge'>(8)</div></a></li>
        
          <li><a class="flat-box" " href="/categories/数据结构与算法/"><div class='name'>数据结构与算法</div><div class='badge'>(11)</div></a></li>
        
          <li><a class="flat-box" " href="/categories/知识图谱/"><div class='name'>知识图谱</div><div class='badge'>(6)</div></a></li>
        
          <li><a class="flat-box" " href="/categories/笔记/"><div class='name'>笔记</div><div class='badge'>(4)</div></a></li>
        
          <li><a class="flat-box" " href="/categories/论文解读/"><div class='name'>论文解读</div><div class='badge'>(2)</div></a></li>
        
          <li><a class="flat-box" " href="/categories/语言模型/"><div class='name'>语言模型</div><div class='badge'>(15)</div></a></li>
        
      </ul>
    </div>
  </section>


          
        
      
        
          
          
            
  <section class='widget tagcloud'>
    
<header class='pure'>
  <div><i class="fas fa-fire fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;热门标签</div>
  
    <a class="rightBtn"
    
      rel="nofollow"
    
    
    href="/tags/"
    title="tags/">
    <i class="fas fa-expand-arrows-alt fa-fw"></i></a>
  
</header>

    <div class='content pure'>
      <a href="/tags/attention/" style="font-size: 16.22px; color: #8a8a8a">Attention</a> <a href="/tags/cnnlm/" style="font-size: 14px; color: #999">CNNLM</a> <a href="/tags/cvt/" style="font-size: 14px; color: #999">CVT</a> <a href="/tags/data-structure/" style="font-size: 14px; color: #999">Data Structure</a> <a href="/tags/deep/" style="font-size: 14px; color: #999">Deep</a> <a href="/tags/dijkstra-s-algorithm/" style="font-size: 14px; color: #999">Dijkstra's Algorithm</a> <a href="/tags/ffnnlm/" style="font-size: 14px; color: #999">FFNNLM</a> <a href="/tags/gaussian/" style="font-size: 14px; color: #999">Gaussian</a> <a href="/tags/graph-algorithm/" style="font-size: 14px; color: #999">Graph Algorithm</a> <a href="/tags/initialization/" style="font-size: 14px; color: #999">Initialization</a> <a href="/tags/kg/" style="font-size: 19.56px; color: #737373">KG</a> <a href="/tags/l1/" style="font-size: 14px; color: #999">L1</a> <a href="/tags/l2/" style="font-size: 14px; color: #999">L2</a> <a href="/tags/lstm/" style="font-size: 14px; color: #999">LSTM</a> <a href="/tags/lstmlm/" style="font-size: 14px; color: #999">LSTMLM</a> <a href="/tags/language-model/" style="font-size: 20.67px; color: #6c6c6c">Language Model</a> <a href="/tags/log-linear-language-model/" style="font-size: 14px; color: #999">Log-Linear Language Model</a> <a href="/tags/mhsa/" style="font-size: 14px; color: #999">MHSA</a> <a href="/tags/nlp/" style="font-size: 20.67px; color: #6c6c6c">NLP</a> <a href="/tags/nmt/" style="font-size: 22.89px; color: #5d5d5d">NMT</a> <a href="/tags/norm/" style="font-size: 14px; color: #999">Norm</a> <a href="/tags/probabilistic-language-model/" style="font-size: 14px; color: #999">Probabilistic Language Model</a> <a href="/tags/rnnlm/" style="font-size: 14px; color: #999">RNNLM</a> <a href="/tags/roc-auc/" style="font-size: 14px; color: #999">ROC-AUC</a> <a href="/tags/smoothing/" style="font-size: 14px; color: #999">Smoothing</a> <a href="/tags/transformer/" style="font-size: 24px; color: #555">Transformer</a> <a href="/tags/context2vec/" style="font-size: 14px; color: #999">context2vec</a> <a href="/tags/cross-entropy/" style="font-size: 14px; color: #999">cross-entropy</a> <a href="/tags/data-noising/" style="font-size: 14px; color: #999">data noising</a> <a href="/tags/divide-conquer/" style="font-size: 14px; color: #999">divide-conquer</a> <a href="/tags/einsum/" style="font-size: 14px; color: #999">einsum</a> <a href="/tags/insertion/" style="font-size: 16.22px; color: #8a8a8a">insertion</a> <a href="/tags/insertion-deletion/" style="font-size: 15.11px; color: #919191">insertion-deletion</a> <a href="/tags/knowledge-distillation/" style="font-size: 14px; color: #999">knowledge distillation</a> <a href="/tags/knowledge-modelling/" style="font-size: 15.11px; color: #919191">knowledge-modelling</a> <a href="/tags/nl2infographic/" style="font-size: 14px; color: #999">nl2infographic</a> <a href="/tags/nl2sql/" style="font-size: 14px; color: #999">nl2sql</a> <a href="/tags/ontology/" style="font-size: 14px; color: #999">ontology</a> <a href="/tags/parallel-recurrent/" style="font-size: 14px; color: #999">parallel-recurrent</a> <a href="/tags/pre-trained-seq2seq/" style="font-size: 14px; color: #999">pre-trained seq2seq</a> <a href="/tags/pytorch/" style="font-size: 14px; color: #999">pytorch</a> <a href="/tags/quantization/" style="font-size: 14px; color: #999">quantization</a> <a href="/tags/queue/" style="font-size: 18.44px; color: #7b7b7b">queue</a> <a href="/tags/sparse/" style="font-size: 14px; color: #999">sparse</a> <a href="/tags/stack/" style="font-size: 14px; color: #999">stack</a> <a href="/tags/survey/" style="font-size: 17.33px; color: #828282">survey</a> <a href="/tags/tensorflow/" style="font-size: 14px; color: #999">tensorflow</a> <a href="/tags/text2viz/" style="font-size: 14px; color: #999">text2viz</a> <a href="/tags/weighted-head/" style="font-size: 14px; color: #999">weighted-head</a> <a href="/tags/半监督语言模型/" style="font-size: 14px; color: #999">半监督语言模型</a> <a href="/tags/双数组前缀树/" style="font-size: 14px; color: #999">双数组前缀树</a> <a href="/tags/推荐系统/" style="font-size: 14px; color: #999">推荐系统</a> <a href="/tags/数据结构/" style="font-size: 21.78px; color: #646464">数据结构</a> <a href="/tags/数组/" style="font-size: 14px; color: #999">数组</a> <a href="/tags/时间复杂度/" style="font-size: 14px; color: #999">时间复杂度</a> <a href="/tags/算法/" style="font-size: 14px; color: #999">算法</a> <a href="/tags/评估方法/" style="font-size: 14px; color: #999">评估方法</a> <a href="/tags/词向量/" style="font-size: 15.11px; color: #919191">词向量</a> <a href="/tags/词向量压缩/" style="font-size: 14px; color: #999">词向量压缩</a> <a href="/tags/隐式正则化/" style="font-size: 14px; color: #999">隐式正则化</a>
    </div>
  </section>


          
        
      
        
          
          
            


  <section class='widget music'>
    
<header class='pure'>
  <div><i class="fas fa-compact-disc fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;最近在听</div>
  
    <a class="rightBtn"
    
      rel="external nofollow noopener noreferrer"
    
    
      target="_blank"
    
    href="https://music.163.com/#/user/home?id=1960721923"
    title="https://music.163.com/#/user/home?id=1960721923">
    <i class="far fa-heart fa-fw"></i></a>
  
</header>

    <div class='content pure'>
      
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.css">
  <div class="aplayer"
    data-theme="#1BCDFC"
    
    
    data-mode="circulation"
    data-server="tencent"
    data-type="playlist"
    data-id="3351822215"
    data-volume="0.7">
  </div>
  <script src="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/meting@1.1.0/dist/Meting.min.js"></script>


    </div>
  </section>


          
        
      
    

  
</aside>

<footer id="footer" class="clearfix">
  <div id="sitetime"></div>
  
  
    <div class="social-wrapper">
      
        
          <a href="/atom.xml"
            class="social fas fa-rss flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
        
          <a href="mailto:rogerspy@163.com"
            class="social fas fa-envelope flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
        
          <a href="https://github.com/rogerspy"
            class="social fab fa-github flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
        
          <a href="https://music.163.com/#/user/home?id=1960721923"
            class="social fas fa-headphones-alt flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
    </div>
  
  <br>
  <div><p>博客内容遵循 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议</a></p>
</div>
  <div>
    本站使用
    <a href="https://xaoxuu.com/wiki/material-x/" target="_blank" class="codename">Material X</a>
    作为主题
    
      ，
      总访问量为
      <span id="busuanzi_value_site_pv"><i class="fas fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span>
      次
    
    。
  </div>
	</footer>

<script>setLoadingBarProgress(80);</script>
<!-- 点击特效，输入特效 运行时间 -->
<script type="text/javascript" src="/cool/cooltext.js"></script>
<script type="text/javascript" src="/cool/clicklove.js"></script>
<script type="text/javascript" src="/cool/sitetime.js"></script>



      <script>setLoadingBarProgress(60);</script>
    </div>
    <a class="s-top fas fa-arrow-up fa-fw" href='javascript:void(0)'></a>
  </div>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>

  <script>
    var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
    var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
    var ALGOLIA_API_KEY = "";
    var ALGOLIA_APP_ID = "";
    var ALGOLIA_INDEX_NAME = "";
    var AZURE_SERVICE_NAME = "";
    var AZURE_INDEX_NAME = "";
    var AZURE_QUERY_KEY = "";
    var BAIDU_API_ID = "";
    var SEARCH_SERVICE = "hexo" || "hexo";
    var ROOT = "/"||"/";
    if(!ROOT.endsWith('/'))ROOT += '/';
  </script>

<script src="//instant.page/1.2.2" type="module" integrity="sha384-2xV8M5griQmzyiY3CDqh1dn4z3llDVqZDqzjzcY+jCBCk/a5fXJmuZ/40JJAPeoU"></script>


  <script async src="https://cdn.jsdelivr.net/npm/scrollreveal@4.0.5/dist/scrollreveal.min.js"></script>
  <script type="text/javascript">
    $(function() {
      const $reveal = $('.reveal');
      if ($reveal.length === 0) return;
      const sr = ScrollReveal({ distance: 0 });
      sr.reveal('.reveal');
    });
  </script>


  <script src="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.js"></script>
  <script type="text/javascript">
    $(function() {
      Waves.attach('.flat-btn', ['waves-button']);
      Waves.attach('.float-btn', ['waves-button', 'waves-float']);
      Waves.attach('.float-btn-light', ['waves-button', 'waves-float', 'waves-light']);
      Waves.attach('.flat-box', ['waves-block']);
      Waves.attach('.float-box', ['waves-block', 'waves-float']);
      Waves.attach('.waves-image');
      Waves.init();
    });
  </script>


  <script async src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-busuanzi@2.3/js/busuanzi.pure.mini.js"></script>




  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
    <script type="text/javascript">
      $(function(){
        if ('.cover') {
          $('.cover').backstretch(
          ["https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/a0c9e6f9efad8b731cb7376504bd10d79d2053.jpg"],
          {
            duration: "6000",
            fade: "2500"
          });
        } else {
          $.backstretch(
          ["https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/a0c9e6f9efad8b731cb7376504bd10d79d2053.jpg"],
          {
            duration: "6000",
            fade: "2500"
          });
        }
      });
    </script>
  







  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script type="text/javascript">
    var gitalk = new Gitalk({
      clientID: "35a5e4dc744cc7d162af",
      clientSecret: "7b5a409e17ce0c1971f284eac9f8902eb4b8feba",
      repo: "rogerspy.github.io",
      owner: "Rogerspy",
      admin: "Rogerspy",
      
        id: "/wiki/material-x/",
      
      distractionFreeMode: false  // Facebook-like distraction free mode
    });
    gitalk.render('gitalk-container');
  </script>





  <script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.5/js/app.js"></script>


  <script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.5/js/search.js"></script>




<!-- 复制 -->
<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  let COPY_SUCCESS = "复制成功";
  let COPY_FAILURE = "复制失败";
  /*页面载入完成后，创建复制按钮*/
  !function (e, t, a) {
    /* code */
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '  <i class="fa fa-copy"></i><span>复制</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });

      clipboard.on('success', function(e) {
        //您可以加入成功提示
        console.info('Action:', e.action);
        console.info('Text:', e.text);
        console.info('Trigger:', e.trigger);
        success_prompt(COPY_SUCCESS);
        e.clearSelection();
      });
      clipboard.on('error', function(e) {
        //您可以加入失败提示
        console.error('Action:', e.action);
        console.error('Trigger:', e.trigger);
        fail_prompt(COPY_FAILURE);
      });
    }
    initCopyCode();

  }(window, document);

  /**
   * 弹出式提示框，默认1.5秒自动消失
   * @param message 提示信息
   * @param style 提示样式，有alert-success、alert-danger、alert-warning、alert-info
   * @param time 消失时间
   */
  var prompt = function (message, style, time)
  {
      style = (style === undefined) ? 'alert-success' : style;
      time = (time === undefined) ? 1500 : time*1000;
      $('<div>')
          .appendTo('body')
          .addClass('alert ' + style)
          .html(message)
          .show()
          .delay(time)
          .fadeOut();
  };

  // 成功提示
  var success_prompt = function(message, time)
  {
      prompt(message, 'alert-success', time);
  };

  // 失败提示
  var fail_prompt = function(message, time)
  {
      prompt(message, 'alert-danger', time);
  };

  // 提醒
  var warning_prompt = function(message, time)
  {
      prompt(message, 'alert-warning', time);
  };

  // 信息提示
  var info_prompt = function(message, time)
  {
      prompt(message, 'alert-info', time);
  };

</script>


<!-- fancybox -->
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
<script>
  let LAZY_LOAD_IMAGE = "";
  $(".article-entry").find("fancybox").find("img").each(function () {
      var element = document.createElement("a");
      $(element).attr("data-fancybox", "gallery");
      $(element).attr("href", $(this).attr("src"));
      /* 图片采用懒加载处理时,
       * 一般图片标签内会有个属性名来存放图片的真实地址，比如 data-original,
       * 那么此处将原本的属性名src替换为对应属性名data-original,
       * 修改如下
       */
       if (LAZY_LOAD_IMAGE) {
         $(element).attr("href", $(this).attr("data-original"));
       }
      $(this).wrap(element);
  });
</script>





  <script>setLoadingBarProgress(100);</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
