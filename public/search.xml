<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>smoothing_techniques</title>
      <link href="/2022/02/15/smoothing-techniques/"/>
      <url>/2022/02/15/smoothing-techniques/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Dijkstra&#39;s Algorithm in 5 steps with Python</title>
      <link href="/2022/01/11/dijkstra-algorithm-python/"/>
      <url>/2022/01/11/dijkstra-algorithm-python/</url>
      
        <content type="html"><![CDATA[<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><p>Dijkstra’s 是最广为人知的图算法之一，同时也是最难发音和拼写的图算法。Dijkstra’s 算法是最短路径算法，在它的基础上还衍生出很多其他变种。本文将介绍两种 Dijkstra’s 算法，并以邻接表为例用 python 实现。</p><a id="more"></a><p>Dijkstra’s 算法伪代码如下：</p><blockquote><ol><li>创建一个“距离”列表，元素个数等于图节点数。每个元素初始化无穷大；</li><li>将起始节点的“距离”设置为 0；</li><li>创建一个“访问”列表，同样将元素个数设定为图节点数。将每个元素设置成 Fasle，因为我们还没有开始访问节点；</li><li>遍历所有节点：<ul><li>再次遍历所有节点，然后从还没有访问的节点中挑选出距离最小的节点；</li><li>将节点设置成已访问；</li><li>将“距离”列表中的距离设置成相应的距离数值。</li></ul></li><li>原始的“距离”列表现在应该已经包含了到每个节点的最短路径，或者如果节点无法到达的话距离为无穷大。</li></ol></blockquote><h1 id="2-邻接表图"><a href="#2-邻接表图" class="headerlink" title="2. 邻接表图"></a>2. 邻接表图</h1><blockquote><p>假设你已经装了 <code>numpy</code>。</p></blockquote><p>首先创建一个有 5 个节点的邻接表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">graph = &#123;</span><br><span class="line">    <span class="number">0</span>: [(<span class="number">1</span>, <span class="number">1</span>)],</span><br><span class="line">    <span class="number">1</span>: [(<span class="number">0</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">3</span>)],</span><br><span class="line">    <span class="number">2</span>: [(<span class="number">1</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">1</span>), (<span class="number">4</span>, <span class="number">5</span>)],</span><br><span class="line">    <span class="number">3</span>: [(<span class="number">1</span>, <span class="number">3</span>), (<span class="number">2</span>, <span class="number">1</span>), (<span class="number">4</span>, <span class="number">1</span>)],</span><br><span class="line">    <span class="number">4</span>: [(<span class="number">2</span>, <span class="number">5</span>), (<span class="number">3</span>, <span class="number">1</span>)]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/pasted_image_0.png" alt></p><h1 id="3-用-python-实现原生-Dijkstra’s"><a href="#3-用-python-实现原生-Dijkstra’s" class="headerlink" title="3. 用 python 实现原生 Dijkstra’s"></a>3. 用 python 实现原生 Dijkstra’s</h1><p>首先先实现原生的 Dijkstra’s 算法，这种实现的算法复杂度是 $O(n^2)$。创建一个函数接收两个参数：邻接表和根节点。</p><p>首先创建一个距离列表，初始化为无穷大：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_dijkstras</span><span class="params">(graph, root)</span>:</span></span><br><span class="line">    n = len(graph)</span><br><span class="line">    <span class="comment"># 将距离列表中的所有元素初始化成无穷大</span></span><br><span class="line">    <span class="comment"># 这里无穷大使用的是 np.Inf，而不是设置成一个很大的数</span></span><br><span class="line">    <span class="comment"># 因为一个很大的数可能造成内存泄露</span></span><br><span class="line">    dist = [np.Inf <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br></pre></td></tr></table></figure><p>第二步，将根节点的距离设置成 0：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dist[root] = <span class="number">0</span></span><br></pre></td></tr></table></figure><p>第三步，创建一个“访问”列表，将所有元素初始化为 False</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">visited = [<span class="literal">False</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br></pre></td></tr></table></figure><p>第四步有三部分：</p><p>① 遍历所有节点然后挑选出距离最近的节点。如果遍历了所有的可用节点还没有找到最近的那个，那就跳出循环：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 遍历所有节点</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(n):</span><br><span class="line">    u = <span class="number">-1</span>  <span class="comment"># 初始节点设置成 -1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="comment"># 如果节点 i 还没有被访问，我们不需要对它进行处理</span></span><br><span class="line">        <span class="comment"># 或者如果它的距离小于 “start” 节点的时候</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> visited[i] <span class="keyword">and</span> (u == <span class="number">-1</span> <span class="keyword">or</span> dist[i] &lt; dist[u]):</span><br><span class="line">            u = i</span><br><span class="line">        <span class="comment"># 访问了所有节点或者该节点无法到达</span></span><br><span class="line">        <span class="keyword">if</span> dist[u] == np.Inf:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>② 将距离最近的节点添加到“访问”列表中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">visited[u] = <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>③ 将已访问的节点的距离设置成可用的最短距离：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> v, l <span class="keyword">in</span> graph(u):</span><br><span class="line">    <span class="keyword">if</span> dist[u] + <span class="number">1</span> &lt; dist[v]:</span><br><span class="line">        dist[v] = dist[u] + <span class="number">1</span></span><br></pre></td></tr></table></figure><p>最后，返回“距离”列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> dist</span><br></pre></td></tr></table></figure><p>完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_dijkstras</span><span class="params">(graph, root)</span>:</span></span><br><span class="line">    n = len(graph)</span><br><span class="line">    <span class="comment"># 将距离列表中的所有元素初始化成无穷大</span></span><br><span class="line">    <span class="comment"># 这里无穷大使用的是 np.Inf，而不是设置成一个很大的数</span></span><br><span class="line">    <span class="comment"># 因为一个很大的数可能造成内存泄露</span></span><br><span class="line">    dist = [np.Inf <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">    <span class="comment"># 将根节点的距离设置成 0</span></span><br><span class="line">    dist[root] = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 创建一个“访问”列表，将所有元素初始化为 False</span></span><br><span class="line">    visited = [<span class="literal">False</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">    <span class="comment"># 遍历所有节点</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(n):</span><br><span class="line">        u = <span class="number">-1</span>  <span class="comment"># 初始节点设置成 -1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="comment"># 如果节点 i 还没有被访问，我们不需要对它进行处理</span></span><br><span class="line">            <span class="comment"># 或者如果它的距离小于 “start” 节点的时候</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> visited[i] <span class="keyword">and</span> (u == <span class="number">-1</span> <span class="keyword">or</span> dist[i] &lt; dist[u]):</span><br><span class="line">                u = i</span><br><span class="line">        <span class="comment"># 访问了所有节点或者该节点无法到达</span></span><br><span class="line">        <span class="keyword">if</span> dist[u] == np.Inf:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 将节点设置成已访问</span></span><br><span class="line">        visited[u] = <span class="literal">True</span></span><br><span class="line">        <span class="comment"># 将已访问的节点的距离设置成可用的最短距离</span></span><br><span class="line">        <span class="keyword">for</span> v, l <span class="keyword">in</span> graph(u):</span><br><span class="line">            <span class="keyword">if</span> dist[u] + l &lt; dist[v]:</span><br><span class="line">                dist[v] = dist[u] + l</span><br><span class="line">    <span class="keyword">return</span> dist</span><br></pre></td></tr></table></figure><p>运行上面的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(naive_dijkstras(graph,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果为</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br></pre></td></tr></table></figure><h1 id="4-用-python-实现-Lazy-Dijkstra’s"><a href="#4-用-python-实现-Lazy-Dijkstra’s" class="headerlink" title="4. 用 python 实现 Lazy Dijkstra’s"></a>4. 用 python 实现 Lazy Dijkstra’s</h1><p>原生版的 Dijkstra’s 我们已经实现了，现在我们来尝试 Lazy Dijkstra’s。为什么叫 “Lazy Dijkstra’s”?因为我们不再遍历所有的节点（上面第四步），这样我们可以更加高效的处理稀疏图（所谓稀疏图就是并非图中的每个点都与其他点相连）。这种实现的算法复杂度是 $O(n\times\log(n))$。</p><blockquote><p>假设你已经装了 <code>heapq</code>。</p></blockquote><p>前三步和之前是一样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lazy_dijkstras</span><span class="params">(graph, root)</span>:</span></span><br><span class="line">    n = len(graph)</span><br><span class="line">    dist = [Inf <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">    dist[root] = <span class="number">0</span></span><br><span class="line">    visited = [<span class="literal">False</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br></pre></td></tr></table></figure><p>从第四步开始就与之前不同了：</p><p>首先给根节点插入一个距离 0：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pq = [(<span class="number">0</span>, root)]</span><br></pre></td></tr></table></figure><p>将前面第四步的①和②合并：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> len(pq) &gt; <span class="number">0</span>:</span><br><span class="line">    _, u = heapq.heappop(pq)</span><br><span class="line">    <span class="keyword">if</span> visited[u]:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    visited[u] = <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>第四步的第三部分基本与之前一致：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> v, l <span class="keyword">in</span> graph[u]:</span><br><span class="line">    <span class="keyword">if</span> dist[u] + l &lt; dist[v]:</span><br><span class="line">        dist[v] = dist[u] + l</span><br><span class="line">        heapq.heappush(pq, (dist[v], v))</span><br></pre></td></tr></table></figure><p>最后，返回“距离”列表。</p><p>完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lazy_dijkstras</span><span class="params">(graph, root)</span>:</span></span><br><span class="line">    n = len(graph)</span><br><span class="line">    dist = [Inf <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">    dist[root] = <span class="number">0</span></span><br><span class="line">    visited = [<span class="literal">False</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">    pq = [(<span class="number">0</span>, root)]</span><br><span class="line">    <span class="keyword">while</span> len(pq) &gt; <span class="number">0</span>:</span><br><span class="line">        _, u = heapq.heappop(pq)</span><br><span class="line">        <span class="keyword">if</span> visited[u]:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        visited[u] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> v, l <span class="keyword">in</span> graph[u]:</span><br><span class="line">            <span class="keyword">if</span> dist[u] + l &lt; dist[v]:</span><br><span class="line">                dist[v] = dist[u] + l</span><br><span class="line">                heapq.heappush(pq, (dist[v], v))</span><br><span class="line">    <span class="keyword">return</span> dist</span><br></pre></td></tr></table></figure><h1 id="5-Reference"><a href="#5-Reference" class="headerlink" title="5. Reference"></a>5. Reference</h1><p><a href="https://pythonalgos.com/dijkstras-algorithm-in-5-steps-with-python/" target="_blank" rel="noopener">Dijkstra’s Algorithm in 5 Steps with Python</a> </p>]]></content>
      
      
      <categories>
          
          <category> 博客转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Dijkstra&#39;s Algorithm </tag>
            
            <tag> Graph Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>预训练语言模型：CVT</title>
      <link href="/2021/09/30/ptm-cvt/"/>
      <url>/2021/09/30/ptm-cvt/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220117183503.png" alt></p><p>之前介绍的预训练模型都是将预训练过程和下游特定任务分成两阶段进行训练， <a href="https://arxiv.org/pdf/1809.08370.pdf" target="_blank" rel="noopener">Cross-View Training</a> 将着来年各个阶段合并成一个统一的半监督学习过程：bi-LSTM 编码器通过有标注数据的监督学习和无标注数据的无监督学习同时训练。</p><a id="more"></a><h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><p>同时使用标注数据和无标注数据进行模型训练，这种训练方式是典型的半监督学习。CVT 引入半监督学习中的自学习机制（self-training）用于神经网络序列建模。</p><p>假设标注数据集和无标注数据集分别为 $D_{l}$ 和 $D_u$，经典的自学习机制过程如下：</p><ul><li>首先从 $D_l$ 中训练一个模型 $M$；</li><li>然后将从 $D_u$ 中抽取部分数据 $D’_u$ 出来，用 $M$ 进行预测：$y’=M(D’_u)$；</li><li>将 $M$ 的预测结果 $y’$ 与 $D’_u$ 视为新的标注数据 $(D’_u, y’)$ 放进 $D_l$ 中；</li><li>重复上面的步骤，知道所有数据都变成 $D_l$。</li></ul><p>自学习机制的缺点也很明显：一旦在第二步中出错，在以后的训练中错误会越来越深。在 CV 领域人们提出给无标注数据加噪声的方法，使模型泛化性得到了有效的提升。但是对于 NLP 这种离散序列数据来说，如何加噪声就变得很棘手了。</p><p>受<a href="https://arxiv.org/pdf/1304.5634.pdf" target="_blank" rel="noopener">多视角学习（multi-view learning）</a> 的启发，CVT 增加了额外的预测模块用来对无标注数据进行预测。</p><h1 id="2-Cross-View-Training"><a href="#2-Cross-View-Training" class="headerlink" title="2. Cross-View Training"></a>2. Cross-View Training</h1><h2 id="2-1-Method"><a href="#2-1-Method" class="headerlink" title="2.1 Method"></a>2.1 Method</h2><ul><li>$D_l = \{(x_1, y_1), (x_2, y_2),…,(x_N, y_N)\}$ 表示标注数据；</li><li>$D_u = \{x_1, x_2,…,x_M\}$ 表示无标注数据；</li><li><p>$p_\theta(y|x_i)$ 表示模型参数为 $\theta$ 时，输入 $x_i$ 模型的输出结果。</p></li><li><p>在有标注的数据上，所有的模型参数都以标准的监督学习方式进行更新，模型的损失函数是标准的交叉熵损失函数：</p><script type="math/tex; mode=display">\mathcal{L}_{\text{sup}}(\theta) = \frac{1}{|D_l|} \sum_{x_i,y_i \in D_l} \text{Cross-Entropy}(y_i, p_\theta(y|x_i))</script><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220117154228.png" style="zoom:80%;"></p></li><li><p>在无标注的数据上：</p><p>① 首先用原始预测模块（primary prediction models）对无标注数据进行预测得：$p_\theta(y|x_i)$；</p><p>② 然后用 $k$ 个附加预测模块（auxiliary prediction modules）将 $p_\theta(y|x_i)$ 作为 ground truth，用加噪声的无标注数据进行预测，然后计算损失：</p><script type="math/tex; mode=display">\mathcal{L}_{\text{CVT}}(\theta) = \frac{1}{D_u} \sum_{x_i\in D_u} \sum_{j=1}^k KL(p_\theta(y|x_i), p_\theta^j(y|x_i^j))</script></li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220117154257.png" style="zoom:80%;"></p><ul><li>最后，整个模型的损失为：<script type="math/tex; mode=display">\mathcal{L} = \mathcal{L}_{\text{sup}} + \mathcal{L}_{\text{CVT}}</script></li></ul><h2 id="2-2-Multi-Task-Learning"><a href="#2-2-Multi-Task-Learning" class="headerlink" title="2.2 Multi-Task Learning"></a>2.2 Multi-Task Learning</h2><p>对多任务同时进行训练时，CVT 增加了几个相应的原始预测模块。所有原始预测模块共享句子表示编码器。进行监督学习时，模型随机选择一个任务进行训练。此时，模型的句子表示编码器和任务预测器的参数会随着训练更新，与选定任务无关的预测器的参数不会更新。比如，同时训练分类和序列标注时，当模型随机选择分类任务进行训练时，序列标注的预测器参数不会随训练更新。</p><p>当进行 CVT 学习时，模型的所有参数都会更新。比如训练分类任务和序列标注任务时，首先用原始预测模块分别预测类别 $c_1$ 和输出序列 $s_1$，然后以此为 ground truth，对 $x_i$ 进行加噪声，利用加噪声后的 $x_i^j$ 再分别预测类别 $c_1^j$ 和输出序列 $s_i^j$，然后分别计算 $(c_1, c_1^j)$ 和 $(s_1, s_1^j)$ 的损失，然后利用后向传播，对参数进行更新。</p><p>多任务学习可以使模型泛化能力得到加强，同时可以得到一个副产物：将所有无标注数据进行标注得到标注数据。</p><h1 id="3-Cross-View-Training-Models"><a href="#3-Cross-View-Training-Models" class="headerlink" title="3. Cross-View Training Models"></a>3. Cross-View Training Models</h1><p>由于 CVT 依赖的附加预测模块需要对输入进行加噪声，即限制输入的“视角”。下面我们介绍一些特定任务下，加噪声的方法。</p><blockquote><p>需要注意的是：当原始预测模块有 dropout 的时候，在进行监督学习时可以让 dropout 正常工作，但是在进行无监督训练时 dropout 需要关闭。</p></blockquote><h2 id="3-1-Bi-LSTM-句子编码"><a href="#3-1-Bi-LSTM-句子编码" class="headerlink" title="3.1 Bi-LSTM 句子编码"></a>3.1 Bi-LSTM 句子编码</h2><ul><li>输入： $x_i = [x_i^1, x_i^2, …, x_i^T]$；</li><li>词向量：$v = \text{word embedding}(x_i) + \text{CNN}(\text{char}(x_i))$：</li><li>$\text{layer}-1 \text{bi-LSTM}$：$h_1=\text{bi-LSTM}(v)=[\overrightarrow{h}{_1^1} \oplus \overleftarrow{h}{_1^1}, \overrightarrow{h}{_1^2} \oplus \overleftarrow{h}{_1^2} …, \overrightarrow{h}{_1^T} \oplus \overleftarrow{h}{_1^T}]$ ；</li><li>$\text{layer}-2 \text{bi-LSTM}$：$h_2=\text{bi-LSTM}(h_1)=[\overrightarrow{h}_2^1 \oplus \overleftarrow{h}{_2^1}, \overrightarrow{h}_2^2 \oplus \overleftarrow{h}{_2^2} …, \overrightarrow{h}{_2^T} \oplus \overleftarrow{h}{_2^T}]$；</li></ul><h2 id="3-2-序列标注"><a href="#3-2-序列标注" class="headerlink" title="3.2 序列标注"></a>3.2 序列标注</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220117173747.png" alt></p><p>序列标注任务（比如词性标注、命名实体识别）中，模型对序列中的每个词进行分类，预测模块包含一个全连接层和一个 softmax 层：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}p(y^t|x_i) &= \text{NN}(h_1^t \oplus h_2^t) \\           &= \text{softmax}(U\cdot \text{ReLU}(W(h_1^t \oplus h_2^t)) + b)\end{aligned}\end{equation}</script><p>在进行额外预测任务时，只给第一层 bi-LSTM 输入单向序列。因为这样的话，模型只观察到部分序列，就必须像语言模型那样对“未来”词进行预测：</p><script type="math/tex; mode=display">p{_\theta}^\text{fwd}(y^t|x_i) = \text{NN}^\text{fwd}(\overrightarrow h{_1^t}(x_i))\\p{_\theta}^\text{bwd}(y^t|x_i) = \text{NN}^\text{bwd}(\overleftarrow h{_1^t}(x_i))\\p{_\theta}^\text{futher}(y^t|x_i) = \text{NN}^\text{future}(\overrightarrow h{_1^t}(x_i))\\p{_\theta}^\text{past}(y^t|x_i) = \text{NN}^\text{past}(\overleftarrow h{_1^t}(x_i))</script><p>其中 forward 表示模型还没看到右侧的信息做出的预测，future 表示该词没有右侧或者该词本身信息做出的预测，两者的区别在于 forward 表示待预测的词右侧是有下文的，而 future 表示的是待预测的词右侧没有下文。</p><h2 id="3-3-Dependency-Parsing"><a href="#3-3-Dependency-Parsing" class="headerlink" title="3.3 Dependency Parsing"></a>3.3 Dependency Parsing</h2><p>依存句法分析任务中，句子中的词被当做是图的节点。词与词之间用有向边进行连接，形成一棵用来描述语法结构的树。$y_i^t = (u, t, r)$ 表示 $x_i^u$ 与 $x_i^t$ 相连，他们的关系是 $r$。</p><script type="math/tex; mode=display">p_\theta((u,t,r)|x_i) \propto e^{s(h_1^u(x_i) \oplus h_2^u(x_i), h_1^t(x_i) \oplus h_2^t(x_i), r)}</script><p>其中</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}s(z_1, z_2, r) = \text{ReLU}(W_\text{head}z_1 + b_\text{head})\cdot (W_r+W) \cdot \text{ReLU}(W_\text{dep}z_2+b_\text{dep})\end{aligned}\end{equation}</script><p>额外预测任务：</p><script type="math/tex; mode=display">p_\theta^\text{fwd-fwd}((u,t,r)|x_i) \propto e^{s^\text{fwd-fwd}(\overrightarrow h{_1^u}(x_i), \overrightarrow h{_1^t}(x_i), r)}\\p_\theta^\text{fwd-bwd}((u,t,r)|x_i) \propto e^{s^\text{fwd-bwd}(\overrightarrow h{_1^u}(x_i), \overleftarrow h{_1^t}(x_i), r)}\\p_\theta^\text{bwd-fwd}((u,t,r)|x_i) \propto e^{s^\text{bwd-fwd}(\overleftarrow h{_1^u}(x_i), \overrightarrow h{_1^t}(x_i), r)}\\p_\theta^\text{bwd-bwd}((u,t,r)|x_i) \propto e^{s^\text{bwd-bwd}(\overleftarrow h{_1^u}(x_i), \overleftarrow h{_1^t}(x_i), r)}\\</script><p>每一个句子都会丢失一部分上下文。</p><h2 id="3-4-Sequence-to-Sequence-Learning"><a href="#3-4-Sequence-to-Sequence-Learning" class="headerlink" title="3.4 Sequence-to-Sequence Learning"></a>3.4 Sequence-to-Sequence Learning</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/151034571.png" alt></p><ul><li>源序列：$x_i = x_i^1,…x_i^T$；</li><li>目标序列：$y_i=y_i^1,…,y_i^K$；</li><li>注意力得分：$\alpha_j \propto e^{h^jW_\alpha h^t}$；</li><li>注意力加权的源序列编码：$c_t = \sum_j\alpha_jh^j$；</li><li>隐状态：$a_t=\tanh(W_a[c_t, h_t])$；</li><li>预测输出：$p(y_i^t|y_i^{&lt;t}, x_i)=\text{softmax}(W_sa_t)$。</li></ul><p>两个附加预测解码器，LSTM 权重共享，但是注意力权重和 softmax 权重是不同的：</p><ol><li><p>对第一个解码器的注意力权重采用 dropout;</p></li><li><p>让第二个解码器预测目标序列的下一个词，而不是当前词：</p><script type="math/tex; mode=display">p_\theta^\text{future}(y_i^t|y_i^{<t}, x_i) = \text{softmax}(W_s^\text{future}a_{t-1}^\text{future})</script></li></ol><h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220117184738.png" alt></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/pdf/1809.08370.pdf" target="_blank" rel="noopener">Semi-Supervised Sequence Modeling with Cross-View Training</a>, <em>Kevin Clark, Minh-Thang Luong, Christopher D. Manning, Quoc V. Le. 2018. arxiv: 1809.08370</em></li><li><a href="https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#cross-view-training" target="_blank" rel="noopener">Generalized Language Models</a>，<em>Lil’Log</em></li><li><a href="https://zhuanlan.zhihu.com/p/32922326" target="_blank" rel="noopener">Lecture13 - Semi-Supervised Learning</a>, <em>幻想家皮霸霸丶</em> </li><li><a href="https://www.cs.cmu.edu/~avrim/Papers/cotrain.pdf" target="_blank" rel="noopener">Combining labeled and unlabeled data with co-training.</a> <em>Avrim Blum and Tom Mitchell. 1998. In COLT. ACM.</em></li><li><a href="https://arxiv.org/pdf/1304.5634.pdf" target="_blank" rel="noopener">A survey on multi-view learning</a>. <em>Chang Xu, Dacheng Tao, and Chao Xu. 2013. arXiv preprint arXiv:1304.5634.</em> </li></ol>]]></content>
      
      
      <categories>
          
          <category> 语言模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Language Model </tag>
            
            <tag> CVT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>预训练语言模型：Pre-trained seq2seq</title>
      <link href="/2021/09/17/ptm-pre-trained-seq2seq/"/>
      <url>/2021/09/17/ptm-pre-trained-seq2seq/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/151034571.png" alt></p><p>之前我们介绍过 <a href="https://rogerspy.github.io/2019/08/26/NLP%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%AE%80%E4%BB%8B%EF%BC%88%E4%B8%80%EF%BC%89/" target="_blank" rel="noopener">seq2seq 模型</a>，通常用作机器翻译，通过编码器（encoder）对源语言进行编码，然后通过解码器（decoder）对编码器的结果进行解码，得到目标语言。原始的 seq2seq 模型是使用平行语料对模型从头开始进行训练，这种训练方式需要大量的平行语料。<a href="https://arxiv.org/pdf/1611.02683.pdf" target="_blank" rel="noopener">Prajit Ramachandran</a> 提出一种方法，可以大幅降低平行语料的需求量：先分别使用源语言和目标语言预训练两个语言模型，然后将语言模型的权重用来分别初始化编码器和解码器，最终取得了 SOTA 的结果。</p><a id="more"></a><h1 id="1-Method"><a href="#1-Method" class="headerlink" title="1. Method"></a>1. Method</h1><h2 id="1-1-Basic-Procedure"><a href="#1-1-Basic-Procedure" class="headerlink" title="1.1 Basic Procedure"></a>1.1 Basic Procedure</h2><p>给定输入序列 $x_1, x_2, …, x_m$，seq2seq 的目的是最大化：</p><script type="math/tex; mode=display">p(y_n, y_{n-1}, ..., y_1|x_1, x_2,...x_m) = \prod_{t=1}^n p(y_t|y_{t-1},...,y_1;x_1, x_2,...,x_m)</script><p>seq2seq 模型是使用编码器（RNN）将 $x_1, x_2, …, x_m$ 表示成一个隐向量，然后将隐向量传递给解码器进行序列解码。我们的方法是将编码器和解码器都当做 RNN 语言模型进行使用大量的语料进行预训练。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220114180418.png" style="zoom:80%;"></p><p>两个语言模型训练完成以后，将两个语言模型的权重用来初始化编码器和解码器。为了方便起见，解码器的 $\text{softmax}$ 使用目标语言的语言模型的 $\text{softmax}$ 进行初始化。</p><h2 id="1-2-Monolingual-language-modeling-losses"><a href="#1-2-Monolingual-language-modeling-losses" class="headerlink" title="1.2 Monolingual language modeling losses"></a>1.2 Monolingual language modeling losses</h2><p>使用语言模型初始化 seq2seq 以后，再用平行语料进行 fine-tuning。根据 <a href="https://arxiv.org/pdf/1312.6211.pdf" target="_blank" rel="noopener">Goodfellow et al. 2013</a> 的研究，fine-tuning 过程很容易造成灾难性遗忘（catastrophic forgetting），使得模型在语言模型上的性能急剧下降，损害模型的泛化能力。</p><p>为了保证模型在平行语料上不会过拟合，在fine-tuning 阶段继续训练语言模型任务，seq2seq 和 语言模型任务的损失等权相加作为最终损失。</p><h2 id="1-3-Other-improvements-to-the-model"><a href="#1-3-Other-improvements-to-the-model" class="headerlink" title="1.3 Other improvements to the model"></a>1.3 Other improvements to the model</h2><p>预训练和损失叠加机制能大幅提升模型性能，但是我们发现另外两个可以小幅提升模型能力的技巧：</p><ol><li>残差连接；</li><li>多层注意力。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220114214211.png" alt></p><h1 id="2-Experiments"><a href="#2-Experiments" class="headerlink" title="2. Experiments"></a>2. Experiments</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220114215019.png" alt></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/pdf/1611.02683.pdf" target="_blank" rel="noopener">Unsupervised Pretraining for Sequence to Sequence Learning</a>, <em>Prajit Ramachandran, Peter J. Liu and Quoc V. Le</em> 2017, arxiv: 1611.02683</li><li><a href="https://arxiv.org/pdf/1312.6211.pdf" target="_blank" rel="noopener">An empirical investigation of catastrophic forgetting in gradient-based neural networks</a>, <em>Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. 2013. arXiv preprint arXiv:1312.6211</em></li></ol>]]></content>
      
      
      <categories>
          
          <category> 语言模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Language Model </tag>
            
            <tag> pre-trained seq2seq </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>预训练语言模型：Data noising smoothing</title>
      <link href="/2021/09/15/ptm-data-noising-as-smoothing/"/>
      <url>/2021/09/15/ptm-data-noising-as-smoothing/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220112213334.png" alt></p><p>数据噪化（data noising）是一种非常有效的神经网络正则化的有段，通常被用在语音和视觉领域，但是在离散序列化数据（比如语言模型）上很少应用。本文尝试探讨给神经网络语言模型加噪声与 n-gram 语言模型中的平滑之间的联系，然后利用这种联系设计出一种噪声机制，帮助我们对语言进行建模。</p><a id="more"></a><h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><p>给定一个序列：$X=(x_1, x_2, …, x_T)$，词表 $V$。我们可以对序列进行建模：</p><script type="math/tex; mode=display">p(X)=\prod_{t=1}^T p(x_t|x_{<t})</script><p>传统的 n-gram 模型很难对 $t$ 很大的序列建模，因为随着 $t$ 的增加，$x_{&lt;t}$ 的数量是以指数增加的。而神经网络可以处理更长的序列，因为神经网络处理的是隐状态而不是序列个数（参考之前的 RNN/CNN语言模型）。</p><p>以一个 $L$ 层的 LSTM 语言模型为例，第 $l$ 层的隐状态为 $h_t^{(l)}=f_\theta(h_{t-1}^{(l)}, h_t^{(t-1)})$。令 $h^{(0)}$ 为 $X$ 的 one-hot 编码，$t$ 时刻的模型输出为：</p><script type="math/tex; mode=display">p_\theta(x_t|x_{<t}) = \text{softmax}(g_\theta(h_t^{(L)}))</script><p>其中 $g_\theta: \mathbb{R}^{|h|} \rightarrow \mathbb{R}^{|V|}$。然后通过优化交叉熵损失函数最大化似然估计 $p_\theta(X)$:</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - \sum_t \log p_\theta (x_t|x_{<t})</script><p>另外，我们还考虑另一个序列任务——seq2seq，输入序列 $X$ 输出序列 $Y$：</p><script type="math/tex; mode=display">p(Y|X) = \prod_{t=1}^{T_Y} p(y_t|X, y_{<t})</script><p>损失函数：</p><script type="math/tex; mode=display">\mathcal{L}_\theta = \sum_t \log p_\theta(y_t|X, y_{<t})</script><h1 id="2-Smoothing-amp-Noising"><a href="#2-Smoothing-amp-Noising" class="headerlink" title="2. Smoothing &amp; Noising"></a>2. Smoothing &amp; Noising</h1><p>神经网络语言模型和 n-gram 语言模型一样，都是通过给定序列去预测下一个位置的元素，使用最大似然估计使模型参数达到最优。因此，两者其实有异曲同工之妙，但是神经网络容易过拟合，现有的正则化方法（比如 L2，dropout 等）都是从模型权重着手，并没有有效地分析和利用序列本身的特征。而 n-gram 模型则充分利用了序列本身的性质，因此分析 n-gram 的序列特征并将这些特征融入到神经网络中，对神经网络序列建模会有很大的帮助。</p><h2 id="2-1-n-gram-中的平滑"><a href="#2-1-n-gram-中的平滑" class="headerlink" title="2.1 n-gram 中的平滑"></a>2.1 n-gram 中的平滑</h2><p>之前介绍<a href="https://rogerspy.github.io/2021/03/16/ptm_probabilistic_language_model/" target="_blank" rel="noopener">统计语言模型</a>的时候，我们介绍过由于 n-gram 语言模型存在稀疏化问题，所以平滑技术至关重要。这里我们考虑插值平滑，比如 bi-gram：</p><script type="math/tex; mode=display">p_{\text{inter}}(x_t|x_{t-1}) = \lambda \cdot p(x_t | x_{t-1}) + (1-\lambda) \cdot p(x_t)</script><p>其中 $0 \le \lambda \le 1$。</p><h2 id="2-2-RNN-中的噪声"><a href="#2-2-RNN-中的噪声" class="headerlink" title="2.2 RNN 中的噪声"></a>2.2 RNN 中的噪声</h2><p>要想将 n-gram 中的平滑技术直接应用于 RNN 中会有一个问题，就是 RNN 中没有明确的计数。所以我们设计了两种加噪声的方法：</p><ul><li><p><strong>unigram noising</strong>：对于 $x_i \in x_{&lt;t}$，以 $\gamma$ 概率从样本中采样一个 unigram 来代替 $x_i$。</p><blockquote><p>原句：张三今年20岁。</p><p>unigram noising：李四今年20岁。/ 老虎今年20岁。</p></blockquote></li><li><p><strong>blank noising</strong>: 对于 $x_i \in x_{&lt;t}$ ，以 $\gamma$ 概率将 $x_i$ 替换成 “_”。</p><blockquote><p>原句：张三今年20岁。</p><p>blank noising：_今年20岁。</p></blockquote></li></ul><p>接下来我们就分析一下，这两种噪声与插值平滑之间的关系。</p><h2 id="2-3-unigram-noising-as-interpolation"><a href="#2-3-unigram-noising-as-interpolation" class="headerlink" title="2.3 unigram noising as interpolation"></a>2.3 unigram noising as interpolation</h2><p>我们先考虑最简单的情形——bigram。令 $c(x)$ 表示 $x$ 在原始数据中的个数，$c_\gamma(x)=\mathbb{E}_\tilde{x}[c(\tilde{x})]$ 表示在 unigram noising 情况下 $x$ 的期望个数，我们可得：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}p_\gamma(x_t|x_{t-1}) &= \frac{c_\gamma(x_{t-1}, x_t)}{c_\gamma(x_{t-1})} \\\\                      &= \frac{(1-\gamma)c(x_{t-1}, x_t) + \gamma p(x_{t-1}) c(x_t)}{c(x_{t-1})} \\\\                      &= (1-\gamma) p(x_t|x_{t-1})  +\gamma p(x_t)\end{aligned}\end{equation}</script><p>其中 $c_\gamma(x) = c(x)$， 因为 $q(x)$ 是 unigram 分布。另外，最后一行是由于：</p><script type="math/tex; mode=display">p(x_t) = \frac{c(x_t)}{n} \quad \& \quad p(x_{t-1}) = \frac{c(x_{t-1})}{n} \\\\n = \frac{c(x_t)}{p(x_t)} = \frac{c(x_{t-1})}{p(x_{t-1})}</script><p> $n$ 表示训练集中总的 token。则最后一行的第二项为：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}\frac{\gamma p(x_{t-1}) c(x_t)}{c(x_{t-1})} &= \gamma c(x_t) \cdot \frac{1}{n} \\\\                                            &= \gamma \frac{c(x_t)}{n} \\\\                                            &= \gamma p(x_t)\end{aligned}\end{equation}</script><p>我们可以看到 $p_\gamma(x_t|x_{t-1})$ 的加噪声形式与插值平滑数学形式上非常相似。我们还可以推导出更一般的形式，令 $\tilde{x_{&lt;t}}$ 表示加噪声后的序列， 则：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}p_\gamma(x_t|x_{<t}) &= \mathbb{E}_{\tilde{x_{<t}}}[p(x_t|\tilde{x}_{<t})] \\\\                      &= \sum_J \underbrace{\pi(|J|)}_{p(|J| \text{swaps})} \sum_{x_K} \underbrace{p(x_t|x_J, x_K)}_{p(x_t|\text{noised context})} \prod_{z\in x_K} \underbrace{p(z)}_{p(\text{drawing z})}\end{aligned}\end{equation}</script><p>其中 $\pi(|J|)=(1-\gamma)^{|J|} \gamma^{t-1-|J|}$， 且 $\sum_J \pi(|J|)=1, J \in \{1,2,…,t-1\}$ 表示 token 没有发生变化的索引，$K$ 表示 token 被替换的索引。</p><h2 id="2-4-blank-noising-as-interpolation"><a href="#2-4-blank-noising-as-interpolation" class="headerlink" title="2.4 blank noising as interpolation"></a>2.4 blank noising as interpolation</h2><p>Blank noising 可以也解释为 “word-dropout”（<a href="https://arxiv.org/pdf/1506.07285.pdf" target="_blank" rel="noopener">Kumar et al. 2015</a>，<a href="https://arxiv.org/pdf/1511.01432.pdf" target="_blank" rel="noopener">Dai &amp; Le 2015</a>，<a href="https://arxiv.org/pdf/1511.06349.pdf" target="_blank" rel="noopener">Bowman et al. 2015</a>）。令 $\tilde{x}_{&lt;t}$ 表示别替换成 “_” 的序列，$x_J$ 表示没有替换的序列：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}p_\gamma(x_t|x_{<t}) &= \mathbb{E}_{\tilde{x_{<t}}}[p(x_t|\tilde{x}_{<t})] \\\\                      &= \sum_J \underbrace{\pi(|J|)}_{p(|J| \text{swaps})}\underbrace{p(x_t|x_J)}_{p(x_t|\text{noised context})} \end{aligned}\end{equation}</script><p>其中 $J \in \{1,2,…,t-1\}$，比如对于 3-gram:</p><script type="math/tex; mode=display">p_\gamma(x_3|x_1, x_2) = \pi(2)p(x_3|x_1, x_2) + \pi(1)p(x_3|x_1, \_)+\pi(1)p(x_3|\_, x_2)+\pi(0)p(x_3|\_, \_)</script><p>其中 $\pi(i)=(1-\gamma)^i\gamma^{2-i}$。</p><h1 id="3-他山之石，可以攻玉"><a href="#3-他山之石，可以攻玉" class="headerlink" title="3. 他山之石，可以攻玉"></a>3. 他山之石，可以攻玉</h1><p>我们已经证明了噪化与平滑有异曲同工之妙，现在我们就可以从以下两个方面考虑如何提升噪声机制;</p><ol><li>自适应计算噪声概率 $\gamma$，用来反应特定输入子序列的置信度；</li><li>利用更高阶的 n-gram 统计信息，选择一个比 unigram 更简单的分布 $q(x)$。</li></ol><h2 id="3-1-Noising-Probability"><a href="#3-1-Noising-Probability" class="headerlink" title="3.1 Noising Probability"></a>3.1 Noising Probability</h2><p>假设有下面两个 bigram:</p><script type="math/tex; mode=display">\text{“and the”} \quad \text{“Humpty Dumpty”}</script><p>第一个二元组在英语语料中非常常见，它的概率非常容易估计，因此不应该用低阶的分布进行插值。直观上来说，我们希望定义一个 $\gamma(x_{1:t})$ 对于常见的二元组尽可能少地被噪化。</p><p>第二个二元组就比较罕见了，但是这个二元组非常特殊，因为在英语语料中 “Humpty” 后面通常跟着的就是 “Dumpty”，同样的 “Dumpty” 前面的通常也是 “Humpty”，即这两个单词通常是成对出现的，这样的二元组我们称之为 “sticky pair”。构成 sticky pair 的词之间有很强的互信息，这样的二元组更类似于 unigram，我们希望可以避免二元组向一元组逼近。 </p><p>令 $N_{1+}(x_1,\cdot)=|\{x_2:c(x_1, x_2)&gt;0\}|$ 表示以 $x_1$ 为开头的二元组的种类，比如 $\{张三: 3, 张四: 4\}$ 其中以“张” 为前缀的二元组总数为 $3+4=7$，而以“张”为前缀的二元组的种类为 $2$（“张三”，“张四”）。根据对上面两个二元组的分析，我们可以设计噪声概率 $\gamma$：</p><script type="math/tex; mode=display">\gamma_{AD}(x_1) = \gamma_0\frac{N_{1+}(x_1, \cdot)}{\sum_{x_2}c(x_1, x_2)}</script><p>其中 $0 \le \gamma_0 \le 1$，因此 $0 \le \gamma_{AD} \le 1$。如果我们忽略掉句子结束符的影响，则 $\sum_{x_2} c(x_1, x_2)=c(x_1)$。</p><ul><li>当以 $x_1$ 为前缀的二元组总数固定，其不同组合的中类越少的时候，$x_1$ 被噪化的概率越小。对应上面第一个分析，当总数一定，但是组合种类越少，那么其中某一种的组合就越常见，$x_1$ 就越不应该被噪化。<script type="math/tex; mode=display">\gamma_{AD}(\text{and}) = \gamma_0 \frac{N_+(\text{and}, \cdot)}{c(\text{and})} \quad</script>假设 “and” 组成的二元组是平均分布的，则 $c(\text{and})= \mathbb{E}(c(\text{and, the})) \times \mathbb{E}(N_+(\text{and, the}))$ ，当 $\mathbb{E}(c(\text{and, the}))$ 越大的时候，$\mathbb{E}(N_+(\text{and, the}))$ 就会越小，则 $\gamma_{AD}(\text{and})$ 就越小。</li></ul><h2 id="3-2-Proposal-distribution"><a href="#3-2-Proposal-distribution" class="headerlink" title="3.2 Proposal distribution"></a>3.2 Proposal distribution</h2><p>假设有下面两个二元组：</p><script type="math/tex; mode=display">\text{“San Francisco”} \quad \text{“New York”}</script><p>这两个二元组在语料中都非常常见，所以 “Francisco” 和 “York” 也非常常见。但是 “Francisco” 和 “York” 通常是跟在 “San” 和 “New” 后面，所以当使用 unigram 频率时它们也不应该有很高的噪声概率。相反，最好增加具有不同历史的一元组的提议概率，或者更准确地说是完成大量二元组类型的一元组。因此，我们令：</p><script type="math/tex; mode=display">q(x) \propto N_{1+}(\cdot, x)</script><ul><li>当以 $x$ 为结尾的二元组种类越少，被采样到的概率就会越低。假设语料中 “New York” 有 1 万条，但是 “York” 只与 “New” 组成二元组，即 $N_+(\cdot, \text{York})=1$，则 $q(x) \sim 1/10000$。 </li></ul><p>注意这个时候，我们除了会对 $x_{1:t-1}$ 进行噪化，同样也会对预测值 $x_t$ 进行噪化。结合 $q(x)$ 和 $\gamma_{AD}(x_1)$ 我们就可以得到   Kneser-Ney 平滑的噪化模拟了。</p><blockquote><ol><li><p>我们以 $\gamma_{AD}(x)$ 的概率决定 $x$ 是否会被噪化（替换成 “_” 或者其他 token）;</p></li><li><p>然后如果我们选择 ngram noising 的话，以 $q(x)$ 的概率对替他 token 进行采样，用来替换被 $\gamma_{AD}(x)$ 选中的 token。</p></li></ol></blockquote><p>下表总结了不同的噪化机制：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220114110524.png" alt></p><h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h1><h2 id="4-1-Language-Model"><a href="#4-1-Language-Model" class="headerlink" title="4.1 Language Model"></a>4.1 Language Model</h2><ul><li><p>Penn Treebank</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220114155146.png" alt></p></li><li><p>Text8</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220114155302.png" alt></p></li></ul><h2 id="4-2-Machine-Translation"><a href="#4-2-Machine-Translation" class="headerlink" title="4.2 Machine Translation"></a>4.2 Machine Translation</h2><ul><li>IWSLT 2015（English-German）</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220114155401.png" alt></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/pdf/1703.02573.pdf" target="_blank" rel="noopener">Data Noising as Smoothing in Neural Network Language Models</a>, <em>Ziang Xie, Sida I. Wang, Jiwei Li, Daniel Lévy, Aiming Nie, Dan Jurafsky, Andrew Y. Ng.</em>  ICLR, 2017</li><li><a href="https://arxiv.org/pdf/1506.07285.pdf" target="_blank" rel="noopener">Ask me anything: Dynamic memory networks for natural language processing</a>, <em>Ankit Kumar, Ozan Irsoy, Jonathan Su, James Bradbury, Robert English, Brian Pierce, Peter Ondruska, Ishaan Gulrajani, and Richard Socher.</em> <em>arXiv preprint arXiv:1506.07285</em>, 2015.</li><li><a href="https://arxiv.org/pdf/1511.06349.pdf" target="_blank" rel="noopener">Generating sentences from a continuous space.</a>, Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio. <em>arXiv preprint arXiv:1511.06349</em>, 2015.</li><li><a href> Semi-supervised sequence learning.</a><a href="https://arxiv.org/pdf/1511.01432.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1511.01432.pdf</a> <em>Andrew M Dai and Quoc V Le.</em>  In <em>Advances in Neural Information Processing Systems</em>, pp. 3061–3069, 2015.</li></ol>]]></content>
      
      
      <categories>
          
          <category> 语言模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Language Model </tag>
            
            <tag> data noising </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入理解 einsum：实现多头注意力机制</title>
      <link href="/2021/09/12/einsum-mhsa/"/>
      <url>/2021/09/12/einsum-mhsa/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/einsum-attention.png" alt></p><p>Einsum 表示法是对张量的复杂操作的一种优雅方式，本质上是使用特定领域的语言。 一旦理解并掌握了 einsum，可以帮助我们更快地编写更简洁高效的代码。</p><a id="more"></a><p>Einsum 是爱因斯坦求和（Einstein summation）的缩写，是一种求和的方法，在处理关于坐标的方程式时非常有效。在 numpy、TensorFlow 和 Pytorch 中都有相关实现，本文通过 Pytorch 实现 Transformer 中的多头注意力来介绍 einsum 在深度学习模型中的应用。</p><h1 id="1-矩阵乘法"><a href="#1-矩阵乘法" class="headerlink" title="1. 矩阵乘法"></a>1. 矩阵乘法</h1><p>假设有两个矩阵：</p><script type="math/tex; mode=display">A = \left[\begin{matrix}1 & 2 & 3 \\4 & 5 & 6\end{matrix} \right],\quadB = \left[\begin{matrix}7 & 8  \\9 & 10 \\11 & 12\end{matrix} \right]</script><p>我们想求两个矩阵的乘积。</p><ul><li>第一步：</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210912131703.png" style="zoom:67%;"></p><ul><li>第二步：</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210912132039.png" style="zoom:67%;"></p><ul><li>第三步：</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210912132541.png" style="zoom:67%;"></p><ul><li>第四步：</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210912132929.png" style="zoom:67%;"></p><h1 id="2-Einstein-Notation"><a href="#2-Einstein-Notation" class="headerlink" title="2. Einstein Notation"></a>2. Einstein Notation</h1><p>爱因斯坦标记法又称爱因斯坦求和约定（Einstein summation convention），基本内容是：</p><blockquote><p>当两个变量具有相同的角标时，则遍历求和。在此情况下，求和号可以省略。</p></blockquote><p>比如，计算两个向量的乘积， $\color{red}{a}, \color{blue}{b} \in \mathbb{R}^I$：</p><script type="math/tex; mode=display">\color{green}{c} = \sum_i \color{red}{a_i}\color{blue}{b_i}=\color{red}{a_i}\color{blue}{b_i}</script><p>计算两个矩阵的乘积， <font color="red">$A$</font> $\in \mathbb{R}^{I\times K}$，<font color="blue">$B$</font> $\in \mathbb{R}^{K\times J}$。用爱因斯坦求和符号表示，可以写成：</p><script type="math/tex; mode=display">\color{green}{c}_{ij} \color{black}= \sum_k\color{red}{A_{ik}}\color{blue}{B_{kj}}=\color{red}{A_{ik}}\color{blue}{B_{kj}}</script><p>在深度学习中，通常使用的是更高阶的张量之间的变换。比如在一个 batch 中包含 $N$ 个训练样本的，最大长度是 $T$，词向量维度为 $K$ 的张量，即 $\color{red}{\mathcal{T}}\in \mathbb{R}^{N\times T \times K}$，如果想让词向量的维度映射到 $Q$ 维，则定义一个 $\color{blue}{W} \in \mathbb{R}^{K\times Q}$:</p><script type="math/tex; mode=display">\color{green}{C_{ntq}} = \sum_k\color{red}{\mathcal{T}_{ntk}}\color{blue}{W_{kq}}=\color{red}{\mathcal{T}_{ntk}}\color{blue}{W_{kq}}</script><p>在图像处理中，通常在一个 batch 的训练样本中包含 $N$ 张图片，每张图片长为 $T$，宽为 $K$，颜色通道为 $M$，即 $\color{red}{\mathcal{T}}\in \mathbb{R}^{N\times T \times K \times M}$ 是一个 4d 张量。如果我想进行三个操作：</p><ul><li>将 $K$ 投影成 $Q$ 维；</li><li>对 $T$ 进行求和；</li><li>将 $M$ 和 $N$ 进行转置。</li></ul><p>用爱因斯坦标记法可以表示成：</p><script type="math/tex; mode=display">\color{green}{C_{mqn}}=\sum_t \sum_k \color{red}{\mathcal{T}_{ntkm}} \color{blue}{W_{kq}} = \color{red}{\mathcal{T}_{ntkm}} \color{blue}{W_{kq}}</script><p>需要注意的是，爱因斯坦标记法是一种书写约定，是为了将复杂的公式写得更加简洁。它本身并不是某种运算符，具体运算还是要回归到各种算子上。</p><h1 id="3-einsum"><a href="#3-einsum" class="headerlink" title="3. einsum"></a>3. einsum</h1><ul><li>Numpy：<code>np.einsum</code></li><li>Pytorch：<code>torch.einsum</code></li><li>TensorFlow：<code>tf.einsum</code></li></ul><p>以上三种 <code>einsum</code> 都有相同的特性 <code>einsum(equation, operands)</code>：</p><ul><li><code>equation</code>：字符串，用来表示爱因斯坦求和标记法的；</li><li><code>operands</code>：一些列张量，要运算的张量。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210912232701.png" alt></p><p>其中 <code>口</code> 是一个占位符，代表的是张量维度的字符。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.einsum(&apos;ij,jk-&gt;ik&apos;, A, B)</span><br></pre></td></tr></table></figure><p><code>A</code> 和 <code>B</code> 是两个矩阵，将 <code>ij,jk-&gt;ik</code> 分成两部分：<code>ij, jk</code> 和 <code>ik</code>，那么 <code>ij</code> 代表的是输入矩阵 <code>A</code> 的第 <code>i</code> 维和第 <code>j</code> 维，<code>jk</code> 代表的是 <code>B</code> 第 <code>j</code> 维和第 <code>k</code> 维，<code>ik</code> 代表的是输出矩阵的第 <code>i</code> 维和第 <code>k</code> 维。注意 <code>i, j, k</code> 可以是任意的字符，但是必须保持一致。换句话说，<code>einsum</code> 实际上是直接操作了矩阵的维度（角标）。上例中表示的是， <code>A</code> 和 <code>B</code> 的乘积。</p><p><img src="https://obilaniu6266h16.files.wordpress.com/2016/02/einsum-matrixmul.png?w=676" alt></p><h2 id="3-1-矩阵转置"><a href="#3-1-矩阵转置" class="headerlink" title="3.1 矩阵转置"></a>3.1 矩阵转置</h2><script type="math/tex; mode=display">B_{ji} = A_{ij}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ij-&gt;ji'</span>, [a])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">3.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">4.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">5.</span>]])</span><br></pre></td></tr></table></figure><h2 id="3-2-求和"><a href="#3-2-求和" class="headerlink" title="3.2 求和"></a>3.2 求和</h2><script type="math/tex; mode=display">b = \sum_i\sum_j A_{ij}=A_{ij}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ij-&gt;'</span>, [a])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor(<span class="number">15.</span>)</span><br></pre></td></tr></table></figure><h2 id="3-3-列求和"><a href="#3-3-列求和" class="headerlink" title="3.3 列求和"></a>3.3 列求和</h2><script type="math/tex; mode=display">b_j=\sum_iA_{ij}=A_{ij}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ij-&gt;j'</span>, [a])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([ <span class="number">3.</span>,  <span class="number">5.</span>,  <span class="number">7.</span>])</span><br></pre></td></tr></table></figure><h2 id="3-4-行求和"><a href="#3-4-行求和" class="headerlink" title="3.4 行求和"></a>3.4 行求和</h2><script type="math/tex; mode=display">b_i=\sum_jA_{ij}=A_{ij}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ij-&gt;i'</span>, [a])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([  <span class="number">3.</span>,  <span class="number">12.</span>])</span><br></pre></td></tr></table></figure><h2 id="3-5-矩阵-向量乘积"><a href="#3-5-矩阵-向量乘积" class="headerlink" title="3.5 矩阵-向量乘积"></a>3.5 矩阵-向量乘积</h2><script type="math/tex; mode=display">c_i=\sum_kA_{ik}b_k=A_{ik}b_k</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b = torch.arange(<span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ik,k-&gt;i'</span>, [a, b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([  <span class="number">5.</span>,  <span class="number">14.</span>])</span><br></pre></td></tr></table></figure><h2 id="3-6-矩阵-矩阵乘积"><a href="#3-6-矩阵-矩阵乘积" class="headerlink" title="3.6 矩阵-矩阵乘积"></a>3.6 矩阵-矩阵乘积</h2><script type="math/tex; mode=display">C_{ij}=\sum_kA_{ik}B_{kj}=A_{ik}B_{kj}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b = torch.arange(<span class="number">15</span>).reshape(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">torch.einsum(<span class="string">'ik,kj-&gt;ij'</span>, [a, b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">tensor([[  <span class="number">25.</span>,   <span class="number">28.</span>,   <span class="number">31.</span>,   <span class="number">34.</span>,   <span class="number">37.</span>],</span><br><span class="line">        [  <span class="number">70.</span>,   <span class="number">82.</span>,   <span class="number">94.</span>,  <span class="number">106.</span>,  <span class="number">118.</span>]])</span><br></pre></td></tr></table></figure><h2 id="3-7-点积"><a href="#3-7-点积" class="headerlink" title="3.7 点积"></a>3.7 点积</h2><script type="math/tex; mode=display">c = \sum_ia_ib_i=a_ib_i</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">3</span>)</span><br><span class="line">b = torch.arange(<span class="number">3</span>, <span class="number">6</span>)</span><br><span class="line">torch.einsum(<span class="string">'i,i-&gt;'</span>, [a, b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">tensor(<span class="number">14.</span>)</span><br></pre></td></tr></table></figure><h2 id="3-8-Hardamard-积"><a href="#3-8-Hardamard-积" class="headerlink" title="3.8 Hardamard 积"></a>3.8 Hardamard 积</h2><script type="math/tex; mode=display">C_{ij} = A_{ij}B_{ij}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b = torch.arange(<span class="number">6</span>,<span class="number">12</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ij,ij-&gt;ij'</span>, [a, b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">tensor([[  <span class="number">0.</span>,   <span class="number">7.</span>,  <span class="number">16.</span>],</span><br><span class="line">        [ <span class="number">27.</span>,  <span class="number">40.</span>,  <span class="number">55.</span>]])</span><br></pre></td></tr></table></figure><h2 id="3-9-外积"><a href="#3-9-外积" class="headerlink" title="3.9 外积"></a>3.9 外积</h2><script type="math/tex; mode=display">C_{ij}=a_ib_j</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">3</span>)</span><br><span class="line">b = torch.arange(<span class="number">3</span>, <span class="number">7</span>)</span><br><span class="line">torch.einsum(<span class="string">'i, j-&gt;ij'</span>, [a, b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">tensor([[  <span class="number">0.</span>,   <span class="number">0.</span>,   <span class="number">0.</span>,   <span class="number">0.</span>],</span><br><span class="line">        [  <span class="number">3.</span>,   <span class="number">4.</span>,   <span class="number">5.</span>,   <span class="number">6.</span>],</span><br><span class="line">        [  <span class="number">6.</span>,   <span class="number">8.</span>,  <span class="number">10.</span>,  <span class="number">12.</span>]])</span><br></pre></td></tr></table></figure><h2 id="3-10-Batch-矩阵乘积"><a href="#3-10-Batch-矩阵乘积" class="headerlink" title="3.10 Batch 矩阵乘积"></a>3.10 Batch 矩阵乘积</h2><script type="math/tex; mode=display">C_{ijl}=\sum_kA_{ijk}B_{ikl}=A_{ijk}B_{ikl}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>,<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">b = torch.randn(<span class="number">3</span>,<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ijk, jkl-&gt;ijl'</span>, [a, b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">tensor([[[ <span class="number">1.0886</span>,  <span class="number">0.0214</span>,  <span class="number">1.0690</span>],</span><br><span class="line">         [ <span class="number">2.0626</span>,  <span class="number">3.2655</span>, <span class="number">-0.1465</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-6.9294</span>,  <span class="number">0.7499</span>,  <span class="number">1.2976</span>],</span><br><span class="line">         [ <span class="number">4.2226</span>, <span class="number">-4.5774</span>, <span class="number">-4.8947</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-2.4289</span>, <span class="number">-0.7804</span>,  <span class="number">5.1385</span>],</span><br><span class="line">         [ <span class="number">0.8003</span>,  <span class="number">2.9425</span>,  <span class="number">1.7338</span>]]])</span><br></pre></td></tr></table></figure><h2 id="3-11-张量收缩"><a href="#3-11-张量收缩" class="headerlink" title="3.11 张量收缩"></a>3.11 张量收缩</h2><p>假设有两个张量 $\mathcal{A}\in \mathbb{R}^{I_1\times \dots\times I_n}$ 和 $\mathcal{B} \in \mathbb{R}^{J_1\times \dots \times J_m}$。比如 $n=4, m=5$，且 $I_2=J_3$ 和 $I_3=J_5$。我们可以计算两个张量的乘积，得到新的张量 $\mathcal{C}\in\mathbb{R}^{I_1\times I_4 \times J_1 \times J_2 \times J_4}$：</p><script type="math/tex; mode=display">C_{pstuv}=\sum_q\sum_r A_{pqrs}B_{tuqvr} = A_{pqrs}B_{tuqvr}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>)</span><br><span class="line">b = torch.randn(<span class="number">11</span>,<span class="number">13</span>,<span class="number">3</span>,<span class="number">17</span>,<span class="number">5</span>)</span><br><span class="line">torch.einsum(<span class="string">'pqrs,tuqvr-&gt;pstuv'</span>, [a, b]).shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">13</span>, <span class="number">17</span>])</span><br></pre></td></tr></table></figure><h2 id="3-12-双线性变换"><a href="#3-12-双线性变换" class="headerlink" title="3.12 双线性变换"></a>3.12 双线性变换</h2><script type="math/tex; mode=display">D_{ij}=\sum_k\sum_lA_{ik}B_{jkl}C_{il} = A_{ik}B_{jkl}C_{il}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.randn(<span class="number">5</span>,<span class="number">3</span>,<span class="number">7</span>)</span><br><span class="line">c = torch.randn(<span class="number">2</span>,<span class="number">7</span>)</span><br><span class="line">torch.einsum(<span class="string">'ik,jkl,il-&gt;ij'</span>, [a, b, c])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([[ <span class="number">3.8471</span>,  <span class="number">4.7059</span>, <span class="number">-3.0674</span>, <span class="number">-3.2075</span>, <span class="number">-5.2435</span>],</span><br><span class="line">        [<span class="number">-3.5961</span>, <span class="number">-5.2622</span>, <span class="number">-4.1195</span>,  <span class="number">5.5899</span>,  <span class="number">0.4632</span>]])</span><br></pre></td></tr></table></figure><h1 id="4-einops"><a href="#4-einops" class="headerlink" title="4. einops"></a>4. einops</h1><p>尽管 <code>einops</code> 是一个通用的包，这里哦我们只介绍 <code>einops.rearrange</code> 。同 <code>einsum</code> 一样，<code>einops.rearrange</code> 也是操作矩阵的角标的，只不过函数的参数正好相反，如下图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210914003018.png" alt></p><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • NOTE            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            如果 <code>rearrange</code> 传入的参数是一个张量列表，那么后面字符串的第一维表示列表的长度。        </div>        </div>    </div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">qkv = torch.rand(<span class="number">2</span>,<span class="number">128</span>,<span class="number">3</span>*<span class="number">512</span>) <span class="comment"># dummy data for illustration only</span></span><br><span class="line"><span class="comment"># We need to decompose to n=3 tensors q, v, k</span></span><br><span class="line"><span class="comment"># rearrange tensor to [3, batch, tokens, dim] and cast to tuple</span></span><br><span class="line">q, k, v = tuple(rearrange( qkv , <span class="string">'b t (d n) -&gt; n b t d '</span>, n=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><h1 id="5-Scale-dot-product-self-attention"><a href="#5-Scale-dot-product-self-attention" class="headerlink" title="5. Scale dot product self-attention"></a>5. Scale dot product self-attention</h1><ul><li><p><strong>第一步</strong>：创建一个线性投影。给定输入 $X\in \mathbb{R}^{b\times t\times d}$，其中 $b$ 表示 $\text{batch size}$，$t$ 表示 $\text{sentence length}$，$d$ 表示 $\text{word dimension}$。</p><script type="math/tex; mode=display">Q=XW_Q, \quad K=XW_K, \quad V=XW_V</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">to_qvk = nn.Linear(dim, dim * <span class="number">3</span>, bias=<span class="literal">False</span>) <span class="comment"># init only</span></span><br><span class="line"><span class="comment"># Step 1</span></span><br><span class="line">qkv = to_qvk(x)  <span class="comment"># [batch, tokens, dim*3 ]</span></span><br><span class="line"><span class="comment"># decomposition to q,v,k</span></span><br><span class="line">q, k, v = tuple(rearrange(qkv, <span class="string">'b t (d k) -&gt; k b t d '</span>, k=<span class="number">3</span>))</span><br></pre></td></tr></table></figure></li><li><p><strong>第二步</strong>：计算点积，mask，最后计算 softmax。</p><script type="math/tex; mode=display">\text{dot_score} = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} \right)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Step 2</span></span><br><span class="line"><span class="comment"># Resulting shape: [batch, tokens, tokens]</span></span><br><span class="line">scaled_dot_prod = torch.einsum(<span class="string">'b i d , b j d -&gt; b i j'</span>, q, k) * self.scale_factor</span><br><span class="line"><span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">assert</span> mask.shape == scaled_dot_prod.shape[<span class="number">1</span>:]</span><br><span class="line">    scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)</span><br><span class="line">attention = torch.softmax(scaled_dot_prod, dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>第三步</strong>：计算注意力得分与 $V$ 的乘积。</p><script type="math/tex; mode=display">\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} \right)V</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.einsum(<span class="string">'b i j , b j d -&gt; b i d'</span>, attention, v)</span><br></pre></td></tr></table></figure></li></ul><p>将上面三步综合起来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SelfAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of plain self attention mechanism with einsum operations</span></span><br><span class="line"><span class="string">    Paper: https://arxiv.org/abs/1706.03762</span></span><br><span class="line"><span class="string">    Blog: https://theaisummer.com/transformer/</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dim)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            dim: for NLP it is the dimension of the embedding vector</span></span><br><span class="line"><span class="string">            the last dimension size that will be provided in forward(x),</span></span><br><span class="line"><span class="string">            where x is a 3D tensor</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="comment"># for Step 1</span></span><br><span class="line">        self.to_qvk = nn.Linear(dim, dim * <span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># for Step 2</span></span><br><span class="line">        self.scale_factor = dim ** <span class="number">-0.5</span>  <span class="comment"># 1/np.sqrt(dim)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask=None)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x.dim() == <span class="number">3</span>, <span class="string">'3D tensor must be provided'</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 1</span></span><br><span class="line">        qkv = self.to_qvk(x)  <span class="comment"># [batch, tokens, dim*3 ]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># decomposition to q,v,k</span></span><br><span class="line">        <span class="comment"># rearrange tensor to [3, batch, tokens, dim] and cast to tuple</span></span><br><span class="line">        q, k, v = tuple(rearrange(qkv, <span class="string">'b t (d k) -&gt; k b t d '</span>, k=<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2</span></span><br><span class="line">        <span class="comment"># Resulting shape: [batch, tokens, tokens]</span></span><br><span class="line">        scaled_dot_prod = torch.einsum(<span class="string">'b i d , b j d -&gt; b i j'</span>, q, k) * self.scale_factor</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> mask.shape == scaled_dot_prod.shape[<span class="number">1</span>:]</span><br><span class="line">            scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)</span><br><span class="line"></span><br><span class="line">        attention = torch.softmax(scaled_dot_prod, dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 3</span></span><br><span class="line">        <span class="keyword">return</span> torch.einsum(<span class="string">'b i j , b j d -&gt; b i d'</span>, attention, v)</span><br></pre></td></tr></table></figure><h1 id="6-Multi-Head-Self-Attention"><a href="#6-Multi-Head-Self-Attention" class="headerlink" title="6. Multi-Head Self-Attention"></a>6. Multi-Head Self-Attention</h1><ul><li><p><strong>第一步</strong>：为每一个头创建一个线性投影 $Q, K, V$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">to_qvk = nn.Linear(dim, dim_head * heads * <span class="number">3</span>, bias=<span class="literal">False</span>) <span class="comment"># init only</span></span><br><span class="line">qkv = self.to_qvk(x)</span><br></pre></td></tr></table></figure></li><li><p><strong>第二步</strong>：将 $Q, K, V$ 分解，并分配给每个头。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Step 2</span></span><br><span class="line"><span class="comment"># decomposition to q,v,k and cast to tuple</span></span><br><span class="line"><span class="comment"># [3, batch, heads, tokens, dim_head]</span></span><br><span class="line">q, k, v = tuple(rearrange(qkv, <span class="string">'b t (d k h) -&gt; k b h t d '</span>, k=<span class="number">3</span>, h=self.heads))</span><br></pre></td></tr></table></figure></li><li><p><strong>第三步</strong>：计算注意力得分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Step 3</span></span><br><span class="line"><span class="comment"># resulted shape will be: [batch, heads, tokens, tokens]</span></span><br><span class="line">scaled_dot_prod = torch.einsum(<span class="string">'b h i d , b h j d -&gt; b h i j'</span>, q, k) * self.scale_factor</span><br><span class="line"><span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">assert</span> mask.shape == scaled_dot_prod.shape[<span class="number">2</span>:]</span><br><span class="line">    scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)</span><br><span class="line">attention = torch.softmax(scaled_dot_prod, dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>第四步</strong>：注意力得分与 $V$ 相乘</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Step 4. Calc result per batch and per head h</span></span><br><span class="line">out = torch.einsum(<span class="string">'b h i j , b h j d -&gt; b h i d'</span>, attention, v)</span><br></pre></td></tr></table></figure></li><li><p><strong>第五步</strong>：将所有的头合并</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out = rearrange(out, <span class="string">"b h t d -&gt; b t (h d)"</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>第六步</strong>：线性变换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.W_0 = nn.Linear( _dim, dim, bias=<span class="literal">False</span>) <span class="comment"># init only</span></span><br><span class="line"><span class="comment"># Step 6. Apply final linear transformation layer</span></span><br><span class="line">self.W_0(out)</span><br></pre></td></tr></table></figure></li></ul><p>最终实现 MHSA：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadSelfAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dim, heads=<span class="number">8</span>, dim_head=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Implementation of multi-head attention layer of the original transformer model.</span></span><br><span class="line"><span class="string">        einsum and einops.rearrange is used whenever possible</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            dim: token's dimension, i.e. word embedding vector size</span></span><br><span class="line"><span class="string">            heads: the number of distinct representations to learn</span></span><br><span class="line"><span class="string">            dim_head: the dim of the head. In general dim_head&lt;dim.</span></span><br><span class="line"><span class="string">            However, it may not necessary be (dim/heads)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dim_head = (int(dim / heads)) <span class="keyword">if</span> dim_head <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> dim_head</span><br><span class="line">        _dim = self.dim_head * heads</span><br><span class="line">        self.heads = heads</span><br><span class="line">        self.to_qvk = nn.Linear(dim, _dim * <span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_0 = nn.Linear( _dim, dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.scale_factor = self.dim_head ** <span class="number">-0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask=None)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x.dim() == <span class="number">3</span></span><br><span class="line">        <span class="comment"># Step 1</span></span><br><span class="line">        qkv = self.to_qvk(x)  <span class="comment"># [batch, tokens, dim*3*heads ]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2</span></span><br><span class="line">        <span class="comment"># decomposition to q,v,k and cast to tuple</span></span><br><span class="line">        <span class="comment"># the resulted shape before casting to tuple will be:</span></span><br><span class="line">        <span class="comment"># [3, batch, heads, tokens, dim_head]</span></span><br><span class="line">        q, k, v = tuple(rearrange(qkv, <span class="string">'b t (d k h) -&gt; k b h t d '</span>, k=<span class="number">3</span>, h=self.heads))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 3</span></span><br><span class="line">        <span class="comment"># resulted shape will be: [batch, heads, tokens, tokens]</span></span><br><span class="line">        scaled_dot_prod = torch.einsum(<span class="string">'b h i d , b h j d -&gt; b h i j'</span>, q, k) * self.scale_factor</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> mask.shape == scaled_dot_prod.shape[<span class="number">2</span>:]</span><br><span class="line">            scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)</span><br><span class="line"></span><br><span class="line">        attention = torch.softmax(scaled_dot_prod, dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 4. Calc result per batch and per head h</span></span><br><span class="line">        out = torch.einsum(<span class="string">'b h i j , b h j d -&gt; b h i d'</span>, attention, v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 5. Re-compose: merge heads with dim_head d</span></span><br><span class="line">        out = rearrange(out, <span class="string">"b h t d -&gt; b t (h d)"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 6. Apply final linear transformation layer</span></span><br><span class="line">        <span class="keyword">return</span> self.W_0(out)</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p><a href="https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/" target="_blank" rel="noopener">Einstein Summation in Numpy</a>, <em>OLEXA BILANIUK</em></p></li><li><p><a href="https://ajcr.net/Basic-guide-to-einsum/" target="_blank" rel="noopener">A basic introduction to NumPy’s einsum</a>, <em>Alex Riley</em></p></li><li><a href="https://rockt.github.io/2018/04/30/einsum" target="_blank" rel="noopener">EINSUM IS ALL YOU NEED - EINSTEIN SUMMATION IN DEEP LEARNING</a>, <em>Tim Rocktäschel</em> </li><li><a href="https://theaisummer.com/einsum-attention/" target="_blank" rel="noopener">Understanding einsum for Deep learning: implement a transformer with multi-head self-attention from scratch</a>, <em>Nikolas Adaloglou</em></li></ol>]]></content>
      
      
      <categories>
          
          <category> 博客转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> einsum </tag>
            
            <tag> MHSA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>预训练语言模型：context2vec</title>
      <link href="/2021/09/09/ptm-context2vec/"/>
      <url>/2021/09/09/ptm-context2vec/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210909234144.png" alt></p><p>上下文的向量表示在许多 NLP 任务中都有至关重要的作用，比如词义消歧、命名实体识别、指代消解等等。以前的方法多是直接用离散上下文词向量组合，缺乏对上下文整体的优化表示方法。本文提出一种双向 LSTM 模型，有效学习句子上下文表征。</p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>通常词向量能获得单个词的语义语法信息，在训练词向量的时候是通过优化与任务无关的目标函数。为了推理出一个具体的词，好的上下文向量表示也是必须的。比如：</p><blockquote><p>我找不到【星期五】了。</p></blockquote><p>其中【星期五】可能是个人，可能是一个宠物等等。我们必须借助“我找不到【】了”才能确定“星期五”并不是一个表示日期的词。</p><p>通常上下文的表示有两种方式：</p><ol><li>无监督。使用上下文的词向量组成一个序列输入到模型，或者直接使用上下文词向量相加求平均。这种方式缺乏对上下文整体表征的优化。</li><li>监督学习。通过标注数据根据具体的任务训练上下文表征。这种方式有两个缺点：① 依赖标注数据，通常标注数据是很难得的；② 训练出来的上下文表征依赖具体的任务，很可能并没有学习到目标词与上下文的依赖关系。</li></ol><p>context2vec 通过在大规模的无标注数据上训练神经网络模型，直接对整个上下文和目标词进行编码，能够获得他们的依赖关系。将训练好的模型应用于下游的任务也获得了很好的表现。</p><h1 id="2-Context2vec-模型"><a href="#2-Context2vec-模型" class="headerlink" title="2. Context2vec 模型"></a>2. Context2vec 模型</h1><table><tr>    <td><img width="600" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210910000919.png"></td>    <td><img width="600" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210910000943.png"></td></tr></table>            <p>Context2vec 的主要目标是学习一个通用的与任务无关嵌入模型，用来表示目标词上下文的变长序列向量表示。我们借鉴了 word2vec 的 CBOW 模型，利用上下文来预测目标词。与 CBOW 不同的是，我们将原来的上下文向量求平均操作替换成了双向 LSTM 模型，如上右图所示。</p><blockquote><p>John [submitted] a paper</p></blockquote><ol><li><p>用双向 LSTM 作为特征抽取器；</p></li><li><p>一个 LSTM 输入句子序列是从左向右；另一个 LSTM 输入序列是从右向左；</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210911121347.png" style="zoom:67%;"></p></li><li><p>将目标词左侧（“John”）的 left-to-right 特征与目标词右侧（“a paper”）的 right-to-left 特征拼接起来；</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210911122227.png" style="zoom:67%;"></p></li><li><p>将拼接后的特征输入到 MLP 中，我们的目标是让 MLP 的输出等于 [submitted] 的向量。</p></li><li><p>采用 Word2vec 中的负采样方法训练神经网络参数，这样就能学到上下文向量和目标词向量。</p></li></ol><h1 id="3-形式化分析"><a href="#3-形式化分析" class="headerlink" title="3. 形式化分析"></a>3. 形式化分析</h1><p>定义：lLS 表示 left-to-right LSTM，rLS 表示 right-to-left LSTM。给定句子 $w_{1:n}$ 和目标词 $w_i$，那么双向 LSTM 的输出为：</p><script type="math/tex; mode=display">biLS(w_{1:n}, i)=\text{lLS}(l_{1:i-1})\oplus\text{rLS}(r_{n:i+1})</script><p>其中 $l$ 和 $r$ 分别表示句子中从左到右和从右到左的词向量。注意在本模型中句子的第 $0$ 个位置和第 $n+1$ 个位置分别表示 $\text{BOS}$ 和 $\text{EOS}$。我们并没有将目标词传入到 LSTM 中去。接下来：</p><script type="math/tex; mode=display">\text{MLP}(x) = L_2(\text{ReLU}(L_1(x)))</script><p>其中 $\text{ReLU}$ 表示激活函数，$L_i$  表示线性变换。令 $c=(w_1, …, w_{i-1}, w_{i+1}, …, w_n)$ 表示句子的上下文词向量。</p><script type="math/tex; mode=display">\vec{c}=\text{MLP}(\text{biLS}(w_{1:n}, i))</script><p>令目标词 $w_i$ 的词向量为 $\vec{t}$：</p><script type="math/tex; mode=display">S=\sum_{t,c}\left( \log\sigma(\vec{t}\cdot \vec{c})+\sum_{i=1}^k\log\sigma(-\vec{t}\cdot\vec{c})\right)</script><p>其中 $\sum_{c,t}$ 表示对训练语料中的每个 $(t,c)$  对求和，$t_1, …, t_k$ 表示负采样的样本。负采样的概率分布为：</p><script type="math/tex; mode=display">p_\alpha(t) \propto (\#t)^\alpha</script><p>$0\le\alpha\le1$ 表示一个平滑系数，$\alpha$ 越大越容易采样到罕见词。$#$ 表示统计个数。</p><p><a href="https://proceedings.neurips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf" target="_blank" rel="noopener">Levy &amp; Goldberg (2014)</a> 证明了将上式用于单字上下文时是可以优化的，当</p><script type="math/tex; mode=display">\vec{t}\cdot\vec{c}=\text{PMI}_\alpha(t,c)-\log(k)</script><p>其中 $\text{PMI}(t,c)=\log\frac{p(t,c)}{p_\alpha(t)p(c)}$ 表示目标词 $t$ 与 上下文 $c$ 的点互信息。 Levy &amp; Goldberg (2014) 的分析适用于两个随机变量的共现矩阵。在我们这里，上下文不是单字而是一个目标词的完整句子表达。据此，我们可以将模型得到的上下文向量视作所有可能的目标词与可能句子上下文的 $\text{PMI}$ 的矩阵分解。</p><p>最终我们注意到 $\alpha$ 越大，则目标词越偏向罕见词。</p><h1 id="4-模型验证"><a href="#4-模型验证" class="headerlink" title="4. 模型验证"></a>4. 模型验证</h1><p>为了验证模型的质量，我们提出三种相似度矩阵：</p><ol><li>target-to-context</li><li>context-to-context</li><li>target-to-target</li></ol><p>所有的相似度都用 $\cos(\cdot)$ 计算。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210911134102.png" style="zoom:67%;"></p><h2 id="4-1-target-to-context"><a href="#4-1-target-to-context" class="headerlink" title="4.1 target-to-context"></a>4.1 target-to-context</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210911134426.png" alt></p><p>当 $\alpha$ 取不同的值的时候，目标词的结果：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210911134541.png" alt></p><h2 id="4-2-context-to-context"><a href="#4-2-context-to-context" class="headerlink" title="4.2 context-to-context"></a>4.2 context-to-context</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210911134650.png" alt></p><h2 id="4-3-target-to-target"><a href="#4-3-target-to-target" class="headerlink" title="4.3 target-to-target"></a>4.3 target-to-target</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210911134743.png" alt></p><h1 id="5-与语言模型的关系"><a href="#5-与语言模型的关系" class="headerlink" title="5. 与语言模型的关系"></a>5. 与语言模型的关系</h1><p>从我们对模型的介绍，以及 target-to-context 实验结果的分析可以看出，我们的模型和基于 LSTM 的语言模型很像。主要的区别在于 LSTM 语言模型给定目标词，优化模型的联合概率。然而 context2vec 的目标是学习通用的向量表示。我们采用了 Word2vec 的学习框架，但是我们利用 $\vec{t}\cdot\vec{v}$ 近似点互信息，而不是 $\log p(t|c)$。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://aclanthology.org/K16-1006.pdf" target="_blank" rel="noopener">context2vec: Learning Generic Context Embedding with Bidirectional LSTM</a>, <em>Oren Melamud, Jacob Goldberger, Ido Dagan. 2016</em></li><li><a href="https://proceedings.neurips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf" target="_blank" rel="noopener">Neural Word Embedding as Implicit Matrix Factorization</a>, <em>Omer Levy, Yoav Goldberg. 2014</em> </li></ol>]]></content>
      
      
      <categories>
          
          <category> 语言模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> context2vec </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>预训练语言模型-Semi-supervised Sequence Learning</title>
      <link href="/2021/09/07/ptm-semi-supervised-sequence-learning/"/>
      <url>/2021/09/07/ptm-semi-supervised-sequence-learning/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210907233207.png" alt></p><p>之前我们介绍了 Word Embedding，将词转换成稠密向量。词向量中包含了大量的自然语言中的先验知识，word2vec 的成功证明了我们可以通过无监督学习获得这些先验知识。随后很多工作试图将句子、段落甚至文档也表示成稠密向量。其中比较有代表性的，比如：</p><a id="more"></a><ul><li>有监督学习<ol><li><a href="https://www.researchgate.net/publication/336156611_Convolutional_Recurrent_Neural_Networks_for_Text_Classification" target="_blank" rel="noopener">Recurrent Convolutional Neural Networks for Text Classification</a> </li><li><a href="https://arxiv.org/pdf/1408.5882.pdf" target="_blank" rel="noopener">Convolutional Neural Networks for Sentence Classification</a> </li></ol></li><li>无监督学习<ol><li><a href="https://arxiv.org/pdf/1405.4053.pdf" target="_blank" rel="noopener">Distributed Representations of Sentences and Documents</a></li><li><a href="https://www.researchgate.net/publication/279068396_Skip-Thought_Vectors" target="_blank" rel="noopener">Skip-Thought</a> 、<a href="https://arxiv.org/pdf/1803.02893.pdf" target="_blank" rel="noopener">Quick-thoughts</a>、<a href="https://rogerspy.github.io/2020/10/13/ptm-introduction/" target="_blank" rel="noopener">InferSent</a> </li></ol></li></ul><p>等等。纯粹的有监督学习是通过分类任务去学习网络参数，最终得到句子向量表示。纯粹的无监督学习是通过预测上下文，比如 skip-thought 利用了 word2vec  的思想，通过预测上下文句子来学习句子表示。</p><p>本文要介绍的这篇论文则是首先尝试使用大规模无标注数据进行预训练，然后将整个句子的向量序列作为有监督任务的初始化值的方法。该方法开创了后来的与训练语言模型+微调下游任务的 NLP 模型训练模式。</p><h1 id="1-Sequence-autoencoders"><a href="#1-Sequence-autoencoders" class="headerlink" title="1.  Sequence autoencoders"></a>1.  Sequence autoencoders</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210907233207.png" alt></p><p>序列自编码器与机器翻译的 seq2seq 架构很相似，主要有两点不同：</p><ol><li>seq2seq 是有监督模型，序列自编码器是无监督模型</li><li>seq2seq 输出是目标语言序列，而序列自编码器输出是输入的句子本身，所以叫做自编码器。</li></ol><p>这个模型中，编码器（绿色部分）和解码器（红色部分）的权重是一样的。</p><p>序列自编码器的一个重要性质就是可以使用大量无标注的数据训练语言模型，这对有限标注数据任务非常有帮助。</p><h1 id="2-Recurrent-language-models"><a href="#2-Recurrent-language-models" class="headerlink" title="2. Recurrent language models"></a>2. Recurrent language models</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210908002430.png" alt></p><p>将序列自编码器，去掉编码器我们就可以得到 LSTM。在我们的任务中，我们使用序列自编码器对 LSTM 的权重进行初始化，我们将使用语言模型初始化后的 LSTM 称之为 LM-LSTM。</p><p>我们再将 LM-LSTM 用于下游的分类任务。通常情况下，LSTM 使用最后一个隐层的输出来预测输入的标签。但是在我们的实验中也尝试了使用 LSTM 每一步输出线性递增组合的方式预测标签，这样我们可以将梯度传递到更靠前的位置上，减轻梯度消失带来的问题。</p><p>另外，我们还尝试了将序列自编码器和下游监督学习模型一起训练的方法，称之为“联合训练”。</p><h1 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3. Experiments"></a>3. Experiments</h1><ul><li><p>IMDB 数据集实验结果</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210908005116.png" alt></p></li><li><p>Rotten Tomatoes 数据集实验结果</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210908004411.png" alt></p></li><li><p>20 newsgroups 数据集实验结果</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210908004438.png" alt></p></li><li><p>DBpedia character level classification</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210908004455.png" alt></p></li><li><p>CIFAR-10 object classification</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210908004510.png" alt></p></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/pdf/1511.01432.pdf" target="_blank" rel="noopener">Semi-supervised Sequence Learning</a>, <em>Andrew M. Dai, Quoc V. Le, 2015, arxiv:1511.01432</em></li><li><a href="https://zhuanlan.zhihu.com/p/21313501" target="_blank" rel="noopener">Semi-supervised Sequence Learning</a>, <em>PaperWeekly, Zhihu</em> </li></ol>]]></content>
      
      
      <categories>
          
          <category> 语言模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 半监督语言模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构与算法：双端队列（deque）</title>
      <link href="/2021/09/05/ds-deque/"/>
      <url>/2021/09/05/ds-deque/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png" alt></p><p>本文介绍双端队列，并用 Python 实现。</p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>双端队列（deque），顾名思义指的是队列的前端和后端都可以进行插入和删除。因此，它不遵循 FIFO 原则。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905144730.png" alt></p><p>双端队列有两种：</p><ul><li>输入限制型双端队列：这种队列中输入被限制在一端，而删除则可以两端同时进行；</li><li>输出限制型双端队列：这种队列只能在一端进行删除，而插入元素则可以两端同时进行。</li></ul><h1 id="2-双端队列的基本操作"><a href="#2-双端队列的基本操作" class="headerlink" title="2. 双端队列的基本操作"></a>2. 双端队列的基本操作</h1><p>下面我们以循环队列实现的双端队列为例尽心介绍。在循环队列中，如果队列是满的，那么我们就从头开始。</p><p>但是，用线性队列实现的双端队列中，如果队列是满的，队列就不允许再往里插入元素了。</p><p>展示双端队列基本操作之前，我们有一些预备工作：</p><ol><li>设置队列的大小 <code>n</code>；</li><li>定义两个指针 <code>FRONT=-1</code> 和 <code>REAR=0</code>。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905150638.png" alt></p><h2 id="2-1-在头部插入元素"><a href="#2-1-在头部插入元素" class="headerlink" title="2.1 在头部插入元素"></a>2.1 在头部插入元素</h2><ol><li><p>检查前端位置</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905181644.png" alt></p></li><li><p>如果 <code>FRONT &lt; 1</code>，重置 <code>FRONT=n-1</code>（最后一个索引）</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905181844.png" alt></p></li></ol><ol><li><p>否则， <code>FRONT-1</code></p></li><li><p>在 <code>FRONT</code> 的位置添加新元素</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905193549.png" alt></p></li></ol><h2 id="2-2-在尾部添加元素"><a href="#2-2-在尾部添加元素" class="headerlink" title="2.2 在尾部添加元素"></a>2.2 在尾部添加元素</h2><ol><li><p>检查队列是否是满队列</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905193906.png" alt></p></li><li><p>如果队列是满的，重置 <code>REAR=0</code></p></li><li><p>否则，<code>REAR=REAR+1</code></p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905194245.png" alt></p></li><li><p>在 <code>REAR</code> 的位置添加新元素</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905194423.png" alt></p></li></ol><h2 id="2-3-从头部删除元素"><a href="#2-3-从头部删除元素" class="headerlink" title="2.3 从头部删除元素"></a>2.3 从头部删除元素</h2><ol><li><p>检查队列是否为空</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905194650.png" alt></p></li><li><p>如果队列是空（即 <code>FRONT=-1</code>），不能删除元素。</p></li><li><p>如果队列只有一个元素（即 <code>FRONT=REAR</code>），设置 <code>FRONT=-1</code> 以及 <code>REAR=-1</code>。</p></li><li><p>否则如果 <code>FORNT=n-1</code>，则令 <code>FRONT</code>去到首位，即令 <code>FRONT=0</code>。</p></li><li><p>否则 <code>FRONT=FORNT+1</code>。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905195221.png" alt></p></li></ol><h2 id="2-4-从尾部删除元素"><a href="#2-4-从尾部删除元素" class="headerlink" title="2.4 从尾部删除元素"></a>2.4 从尾部删除元素</h2><ol><li><p>检查队列是否为空。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905195459.png" alt></p></li><li><p>如果队列为空（<code>FRONT=-1</code>），不能删除元素。</p></li><li><p>如果队列只有一个元素（即 <code>FRONT=REAR</code>），设置 <code>FRONT=-1</code> 以及 <code>REAR=-1</code>。</p></li><li><p>如果 <code>REAR</code> 在前面（<code>REAR=0</code>），则令 <code>REAR=n-1</code>。</p></li><li><p>否则 <code>REAR=REAR-1</code>。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905195900.png" alt></p></li></ol><h2 id="2-5-检查队列是否为空"><a href="#2-5-检查队列是否为空" class="headerlink" title="2.5 检查队列是否为空"></a>2.5 检查队列是否为空</h2><p>检查 <code>FRONT=-1</code>，如果为真则队列为空。</p><h2 id="2-6-检查队列是否为满"><a href="#2-6-检查队列是否为满" class="headerlink" title="2.6 检查队列是否为满"></a>2.6 检查队列是否为满</h2><p>如果 <code>FRONT=0</code> 以及 <code>REAR=n-1</code> 或者 <code>FRONT=REAR+1</code> 则队列为满。</p><h1 id="3-Python-实现双端队列"><a href="#3-Python-实现双端队列" class="headerlink" title="3. Python 实现双端队列"></a>3. Python 实现双端队列</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Deque implementaion in python</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Deque</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.items = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isEmpty</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.items == []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addRear</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        self.items.append(item)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addFront</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        self.items.insert(<span class="number">0</span>, item)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeFront</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.items.pop(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeRear</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.items.pop()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.items)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">d = Deque()</span><br><span class="line">print(d.isEmpty())</span><br><span class="line">d.addRear(<span class="number">8</span>)</span><br><span class="line">d.addRear(<span class="number">5</span>)</span><br><span class="line">d.addFront(<span class="number">7</span>)</span><br><span class="line">d.addFront(<span class="number">10</span>)</span><br><span class="line">print(d.size())</span><br><span class="line">print(d.isEmpty())</span><br><span class="line">d.addRear(<span class="number">11</span>)</span><br><span class="line">print(d.removeRear())</span><br><span class="line">print(d.removeFront())</span><br><span class="line">d.addFront(<span class="number">55</span>)</span><br><span class="line">d.addRear(<span class="number">45</span>)</span><br><span class="line">print(d.items)</span><br></pre></td></tr></table></figure><h1 id="4-时间复杂度"><a href="#4-时间复杂度" class="headerlink" title="4. 时间复杂度"></a>4. 时间复杂度</h1><p>上述操作的时间复杂度为 $O(1)$。</p><h1 id="5-双端队列的应用"><a href="#5-双端队列的应用" class="headerlink" title="5. 双端队列的应用"></a>5. 双端队列的应用</h1><ol><li>软件的撤销操作</li><li>浏览器存储浏览历史</li><li>用来实现队列和栈</li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.programiz.com/dsa/deque" target="_blank" rel="noopener">Deque Data Structure</a> </p>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
            <tag> queue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构与算法：优先队列（priority queue）</title>
      <link href="/2021/09/05/ds-priority-queue/"/>
      <url>/2021/09/05/ds-priority-queue/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png" alt></p><p>本文介绍优先队列，并用 Python 实现。</p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>优先队列是一种特殊的队列类型，队列中每个元素都包含一个优先级值。每个元素根据优先级的大小进行处理，即优先级越高，对应的元素越早处理。</p><p>但是，如果两个元素的优先级一样的话，根据他们在队列中的先后进行处理。</p><h2 id="1-1-分配优先级"><a href="#1-1-分配优先级" class="headerlink" title="1.1 分配优先级"></a>1.1 分配优先级</h2><p>通常情况下，元素值本身就是优先级。比如，元素值越高则优先级越高，或者元素值越低优先级越高。当然，我们也可以根据具体需要来设置优先级。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905125711.png" alt></p><h2 id="1-2-优先队列与常规队列的区别"><a href="#1-2-优先队列与常规队列的区别" class="headerlink" title="1.2 优先队列与常规队列的区别"></a>1.2 优先队列与常规队列的区别</h2><p>在常规队列中，遵守先进先出规则；而在优先队列中，根据优先级删除值，首先删除优先级最高的元素。</p><h2 id="1-3-优先队列的实现方式"><a href="#1-3-优先队列的实现方式" class="headerlink" title="1.3 优先队列的实现方式"></a>1.3 优先队列的实现方式</h2><p>优先队列的实现有多种方式，比如数组、链表、堆以及二叉树等。其中堆更加高效，所以下面我们以堆实现的优先队列为例进行介绍。因此，在此之前需要先了解堆数据结构：<a href="https://www.programiz.com/dsa/heap-sort#heap" target="_blank" rel="noopener">max-heap and mean-heap</a>。</p><p>不同实现方式的复杂度对比：</p><div class="table-container"><table><thead><tr><th>Operations</th><th>peek</th><th>insert</th><th>delete</th></tr></thead><tbody><tr><td>Linked List</td><td><code>O(1)</code></td><td><code>O(n)</code></td><td><code>O(1)</code></td></tr><tr><td>Binary Heap</td><td><code>O(1)</code></td><td><code>O(log n)</code></td><td><code>O(log n)</code></td></tr><tr><td>Binary Search Tree</td><td><code>O(1)</code></td><td><code>O(log n)</code></td><td><code>O(log n)</code></td></tr></tbody></table></div><h1 id="2-优先队列的基本操作"><a href="#2-优先队列的基本操作" class="headerlink" title="2. 优先队列的基本操作"></a>2. 优先队列的基本操作</h1><p>优先队列的基本操作包括：插入、删除、查询。</p><h2 id="2-1-插入"><a href="#2-1-插入" class="headerlink" title="2.1 插入"></a>2.1 插入</h2><p>通过下面的步骤向优先队列中插入元素（max-heap）:</p><ul><li><p>在树的末尾插入元素</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905131519.png" alt></p></li><li><p>将树进行<a href="https://www.programiz.com/dsa/heap-data-structure#heapify" target="_blank" rel="noopener">堆化</a></p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905131744.png" alt></p><p>在优先队列中（max-heap）插入元素的算法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">If there is no node, </span><br><span class="line">  create a newNode.</span><br><span class="line">else (a node is already present)</span><br><span class="line">  insert the newNode at the end (last node from left to right.)</span><br><span class="line">  </span><br><span class="line">heapify the array</span><br></pre></td></tr></table></figure><p>对于 Min heap，上面的算法中 <code>parentNode</code> 永远小于 <code>newNode</code>。</p></li></ul><h2 id="2-2-删除"><a href="#2-2-删除" class="headerlink" title="2.2 删除"></a>2.2 删除</h2><p>通过下面的步骤从优先队列中删除元素（max heap）：</p><ul><li><p>选择要删除的元素</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905132805.png" alt></p></li><li><p>与最后一个元素位置进行交换</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905132935.png" alt></p></li><li><p>删除最后一个元素</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905133111.png" alt></p></li><li><p>将树进行堆化</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905133211.png" alt></p></li></ul><p>从优先队列中删除元素的算法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">If nodeToBeDeleted is the leafNode</span><br><span class="line">  remove the node</span><br><span class="line">Else swap nodeToBeDeleted with the lastLeafNode</span><br><span class="line">  remove noteToBeDeleted</span><br><span class="line">   </span><br><span class="line">heapify the array</span><br></pre></td></tr></table></figure><p>对于 Min Heap，上面算法中的 <code>childNodes</code> 一直 <code>currentNode</code>。</p><h2 id="2-3-查询"><a href="#2-3-查询" class="headerlink" title="2.3 查询"></a>2.3 查询</h2><p>对于 Max heap，返回最大元素；对于 Min heap，返回最小值。</p><p>对于 Max heap 和 Min heap 来说，都是返回根节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">return rootNode</span><br></pre></td></tr></table></figure><h2 id="2-4-选取最大值最小值"><a href="#2-4-选取最大值最小值" class="headerlink" title="2.4 选取最大值最小值"></a>2.4 选取最大值最小值</h2><p>抽取最大值返回从最大堆中删除后具有最大值的节点，而抽取最小值则返回从最小堆中删除后具有最小值的节点。</p><h1 id="3-Python-实现优先队列"><a href="#3-Python-实现优先队列" class="headerlink" title="3. Python 实现优先队列"></a>3. Python 实现优先队列</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"># Priority Queue implementation in Python</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Function to heapify the tree</span><br><span class="line">def heapify(arr, n, i):</span><br><span class="line">    # Find the largest among root, left child and right child</span><br><span class="line">    largest = i</span><br><span class="line">    l = 2 * i + 1</span><br><span class="line">    r = 2 * i + 2</span><br><span class="line"></span><br><span class="line">    if l &lt; n and arr[i] &lt; arr[l]:</span><br><span class="line">        largest = l</span><br><span class="line"></span><br><span class="line">    if r &lt; n and arr[largest] &lt; arr[r]:</span><br><span class="line">        largest = r</span><br><span class="line"></span><br><span class="line">    # Swap and continue heapifying if root is not largest</span><br><span class="line">    if largest != i:</span><br><span class="line">        arr[i], arr[largest] = arr[largest], arr[i]</span><br><span class="line">        heapify(arr, n, largest)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Function to insert an element into the tree</span><br><span class="line">def insert(array, newNum):</span><br><span class="line">    size = len(array)</span><br><span class="line">    if size == 0:</span><br><span class="line">        array.append(newNum)</span><br><span class="line">    else:</span><br><span class="line">        array.append(newNum)</span><br><span class="line">        for i in range((size // 2) - 1, -1, -1):</span><br><span class="line">            heapify(array, size, i)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Function to delete an element from the tree</span><br><span class="line">def deleteNode(array, num):</span><br><span class="line">    size = len(array)</span><br><span class="line">    i = 0</span><br><span class="line">    for i in range(0, size):</span><br><span class="line">        if num == array[i]:</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">    array[i], array[size - 1] = array[size - 1], array[i]</span><br><span class="line"></span><br><span class="line">    array.remove(size - 1)</span><br><span class="line"></span><br><span class="line">    for i in range((len(array) // 2) - 1, -1, -1):</span><br><span class="line">        heapify(array, len(array), i)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">arr = []</span><br><span class="line"></span><br><span class="line">insert(arr, 3)</span><br><span class="line">insert(arr, 4)</span><br><span class="line">insert(arr, 9)</span><br><span class="line">insert(arr, 5)</span><br><span class="line">insert(arr, 2)</span><br><span class="line"></span><br><span class="line">print (&quot;Max-Heap array: &quot; + str(arr))</span><br><span class="line"></span><br><span class="line">deleteNode(arr, 4)</span><br><span class="line">print(&quot;After deleting an element: &quot; + str(arr))</span><br></pre></td></tr></table></figure><h1 id="5-优先队列的应用"><a href="#5-优先队列的应用" class="headerlink" title="5. 优先队列的应用"></a>5. 优先队列的应用</h1><ul><li>Dijkstra 算法</li><li>实现栈结构</li><li>操作系统中的负载平衡和中断处理</li><li>Huffman 编码的数据压缩</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.programiz.com/dsa/priority-queue" target="_blank" rel="noopener">Priority Queue</a> </p>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
            <tag> queue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构与算法：循环队列（circular-queue）</title>
      <link href="/2021/09/05/ds-circular-queue/"/>
      <url>/2021/09/05/ds-circular-queue/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png" alt></p><p>本文介绍循环队列，并用 Python 实现循环队列。</p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>循环队列是常规队列（简单队列）的变种，是将队列中最后一个元素与第一个元素相连。因此，循环队列看起来像下图的样子：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905000636.png" alt></p><p>循环队列是为了解决简单队列的限制。在常规队列中，经过一系列的出队入队操作之后，会有一些空位置。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210904230231.png" alt></p><p>上图中 0 和 1 的位置会被空置，除非等到队列重置。</p><h1 id="2-循环队列的基本操作"><a href="#2-循环队列的基本操作" class="headerlink" title="2. 循环队列的基本操作"></a>2. 循环队列的基本操作</h1><p>循环队列通过循环递增的方式工作，即当我们递增指针并到达队列的末尾时，我们又从队列的开头开始。其中递增是通过模除队列的尺寸，即：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">if REAR + 1 == 5 (overflow!), REAR = (REAR + 1)%5 = 0 (start of queue)</span><br></pre></td></tr></table></figure><p>具体过程如下：</p><ul><li>两个指针 <code>FRONT</code> 和 <code>REAR</code></li><li><code>FRONT</code> 追踪队列中第一个元素</li><li><code>REAR</code> 追踪队列中最后一个元素</li><li>初始化 <code>FRONT</code> 和 <code>REAR</code> 为 -1</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/circular-queue-program.png" style="zoom:50%;"></p><h2 id="2-1-Enqueue"><a href="#2-1-Enqueue" class="headerlink" title="2.1 Enqueue"></a>2.1 Enqueue</h2><ul><li>检查队列是否是满队列</li><li>对于第一个元素，设置 <code>FRONT</code> 的值为 0</li><li>循环增加 <code>REAR</code> ，如果 <code>REAR</code> 到末尾，下一步就从头开始</li><li>在 <code>REAR</code> 指向的位置添加新元素</li></ul><h2 id="2-2-Dequeue"><a href="#2-2-Dequeue" class="headerlink" title="2.2 Dequeue"></a>2.2 Dequeue</h2><ul><li>检查队列是否为空</li><li>返回 <code>FRONT</code> 指向的值</li><li><code>FRONT</code> 循环加 1</li><li>对于最后一个元素，重置 <code>FRONT</code> 和 <code>REAR</code> 为 -1</li></ul><p>然而，检查满队列的时候，有一个新问题：</p><ul><li>第一种情况：<code>FRONT=0</code> &amp;&amp; <code>REAR=size-1</code></li><li>第二种情况：<code>FRONT=REAR+1</code></li></ul><p>第二种情况下，<code>REAR</code> 因为循环递增而从 0  开始，并且其值只比 <code>FRONT</code> 时，队列已满。</p><h1 id="3-Python实现循环队列"><a href="#3-Python实现循环队列" class="headerlink" title="3. Python实现循环队列"></a>3. Python实现循环队列</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Circular Queue implementation in Python</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCircularQueue</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k)</span>:</span></span><br><span class="line">        self.k = k</span><br><span class="line">        self.queue = [<span class="literal">None</span>] * k</span><br><span class="line">        self.head = self.tail = <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Insert an element into the circular queue</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">enqueue</span><span class="params">(self, data)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> ((self.tail + <span class="number">1</span>) % self.k == self.head):</span><br><span class="line">            print(<span class="string">"The circular queue is full\n"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> (self.head == <span class="number">-1</span>):</span><br><span class="line">            self.head = <span class="number">0</span></span><br><span class="line">            self.tail = <span class="number">0</span></span><br><span class="line">            self.queue[self.tail] = data</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.tail = (self.tail + <span class="number">1</span>) % self.k</span><br><span class="line">            self.queue[self.tail] = data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Delete an element from the circular queue</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dequeue</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> (self.head == <span class="number">-1</span>):</span><br><span class="line">            print(<span class="string">"The circular queue is empty\n"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> (self.head == self.tail):</span><br><span class="line">            temp = self.queue[self.head]</span><br><span class="line">            self.head = <span class="number">-1</span></span><br><span class="line">            self.tail = <span class="number">-1</span></span><br><span class="line">            <span class="keyword">return</span> temp</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            temp = self.queue[self.head]</span><br><span class="line">            self.head = (self.head + <span class="number">1</span>) % self.k</span><br><span class="line">            <span class="keyword">return</span> temp</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">printCQueue</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span>(self.head == <span class="number">-1</span>):</span><br><span class="line">            print(<span class="string">"No element in the circular queue"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> (self.tail &gt;= self.head):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.head, self.tail + <span class="number">1</span>):</span><br><span class="line">                print(self.queue[i], end=<span class="string">" "</span>)</span><br><span class="line">            print()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.head, self.k):</span><br><span class="line">                print(self.queue[i], end=<span class="string">" "</span>)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, self.tail + <span class="number">1</span>):</span><br><span class="line">                print(self.queue[i], end=<span class="string">" "</span>)</span><br><span class="line">            print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Your MyCircularQueue object will be instantiated and called as such:</span></span><br><span class="line">obj = MyCircularQueue(<span class="number">5</span>)</span><br><span class="line">obj.enqueue(<span class="number">1</span>)</span><br><span class="line">obj.enqueue(<span class="number">2</span>)</span><br><span class="line">obj.enqueue(<span class="number">3</span>)</span><br><span class="line">obj.enqueue(<span class="number">4</span>)</span><br><span class="line">obj.enqueue(<span class="number">5</span>)</span><br><span class="line">print(<span class="string">"Initial queue"</span>)</span><br><span class="line">obj.printCQueue()</span><br><span class="line"></span><br><span class="line">obj.dequeue()</span><br><span class="line">print(<span class="string">"After removing an element from the queue"</span>)</span><br><span class="line">obj.printCQueue()</span><br></pre></td></tr></table></figure><h1 id="4-循环队列时间复杂度"><a href="#4-循环队列时间复杂度" class="headerlink" title="4. 循环队列时间复杂度"></a>4. 循环队列时间复杂度</h1><p>基于数组实现的循环队列，其 enqueue 和 dequeue 时间复杂度都是 $O(1)$。</p><h1 id="5-循环队列的应用"><a href="#5-循环队列的应用" class="headerlink" title="5. 循环队列的应用"></a>5. 循环队列的应用</h1><ul><li>CPU 任务调度</li><li>内存管理</li><li>任务堵塞管理</li></ul><h1 id="Refernece"><a href="#Refernece" class="headerlink" title="Refernece"></a>Refernece</h1><p><a href="https://www.programiz.com/dsa/circular-queue" target="_blank" rel="noopener">Circular Queue Data Structure</a> </p>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
            <tag> queue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构与算法：队列（queue）</title>
      <link href="/2021/09/04/ds-queue/"/>
      <url>/2021/09/04/ds-queue/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png" alt></p><p>本文介绍队列数据结构，并用 Python 代码实现。</p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>队列是一个非常有用的数据结构。它与电影院外排队买票是一样的，先排先买。队列也是如此，遵循先进先出（First In First Out，FIFO）原则。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210904222908.png" alt></p><p>如上图所示，1 排在 2 前面，也会在 2 之前被删除。</p><p>在编程的术语中，将元素添加进队列中的操作叫做 “<em>enqueue</em>”，从队列中删除的操作叫做 “<em>dequeue</em>”。</p><h1 id="2-队列的基本操作"><a href="#2-队列的基本操作" class="headerlink" title="2. 队列的基本操作"></a>2. 队列的基本操作</h1><ul><li><strong>Enqueue</strong>：向队列中添加元素</li><li><strong>Dequeue</strong>：从队列中删除元素</li><li><strong>IsEmpty</strong>：判断队列是否为空</li><li><strong>IsFull</strong>：判断队列是否为满队列</li><li><strong>Peek</strong>：获取队列最前面的元素而不删除该元素</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/Queue-program-enqueue-dequeue.png" style="zoom: 50%;"></p><ol><li>定义两个指针 <code>FRONT</code> 和 <code>REAR</code></li><li><code>FRONT</code> 追踪队列中第一个元素</li><li><code>REAR</code> 追踪队列中最后一个元素 </li><li>初始化 <code>FRONT</code> 和 <code>REAR</code> 都为 -1</li></ol><h2 id="2-1-Enqueue-操作"><a href="#2-1-Enqueue-操作" class="headerlink" title="2.1 Enqueue 操作"></a>2.1 Enqueue 操作</h2><ul><li>检查队列是否为满序列</li><li>对于第一个元素，设置 <code>FRONT</code> 为 0</li><li><code>REAR</code> 索引加 1</li><li>在 <code>REAR</code> 指向的位置处添加新元素</li></ul><h2 id="2-2-Dequeue"><a href="#2-2-Dequeue" class="headerlink" title="2.2 Dequeue"></a>2.2 Dequeue</h2><ul><li>检查队列是否为空</li><li>返回 <code>FRONT</code> 指向的元素</li><li><code>FRONT</code> 的索引加 1</li><li>对于最后一个元素，重新设置 <code>FRONT</code> 和 <code>REAR</code> 为  -1</li></ul><h1 id="3-Python-实现队列"><a href="#3-Python-实现队列" class="headerlink" title="3. Python 实现队列"></a>3. Python 实现队列</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Queue implementation in Python</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Queue</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.queue = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add an element</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">enqueue</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        self.queue.append(item)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Remove an element</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dequeue</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(self.queue) &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> self.queue.pop(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Display  the queue</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">display</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(self.queue)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.queue)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">q = Queue()</span><br><span class="line">q.enqueue(<span class="number">1</span>)</span><br><span class="line">q.enqueue(<span class="number">2</span>)</span><br><span class="line">q.enqueue(<span class="number">3</span>)</span><br><span class="line">q.enqueue(<span class="number">4</span>)</span><br><span class="line">q.enqueue(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">q.display()</span><br><span class="line"></span><br><span class="line">q.dequeue()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"After removing an element"</span>)</span><br><span class="line">q.display()</span><br></pre></td></tr></table></figure><h1 id="4-队列的限制"><a href="#4-队列的限制" class="headerlink" title="4. 队列的限制"></a>4. 队列的限制</h1><p>如下图所示，经过一系列的入队和出队，队列的尺寸减小了。但是我们只能在队列重置（所有的元素都出队）的时候设置 0 和 1 索引。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210904230231.png" alt></p><p>对于队列的一种变种——循环队列来说，由于入队时尾指针向前追赶头指针；出队时头指针向前追赶尾指针，造成队空和队满时头尾指针均相等。因此，无法通过条件front==rear来判别队列是”空”是”满”。</p><h1 id="5-队列的时间复杂度"><a href="#5-队列的时间复杂度" class="headerlink" title="5. 队列的时间复杂度"></a>5. 队列的时间复杂度</h1><p>Enqueue 和 dequeue 操作在使用数组实现的队列中复杂度都是 $O(1)$。如果你用python中的 <code>pop(n)</code> 方法，那时间复杂度可能是 $O(n)$，取决于你要删除的元素的位置。</p><h1 id="6-队列的应用"><a href="#6-队列的应用" class="headerlink" title="6. 队列的应用"></a>6. 队列的应用</h1><ul><li>CPU  调度，硬盘调度。</li><li>当两个进程之间异步传输数据时，队列用于消息同步。</li><li>处理实时系统的中断。</li><li>呼叫中心电话系统使用队列将呼叫他们的人按顺序排列。</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.programiz.com/dsa/queue" target="_blank" rel="noopener">Queue Data Structure</a> </p>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构与算法：队列类型</title>
      <link href="/2021/09/04/ds-types-queue/"/>
      <url>/2021/09/04/ds-types-queue/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png" alt></p><p>本文介绍不同类型的队列数据结构。</p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>队列就像排队买票，先到先得。有四种不同的队列：</p><ul><li>简单队列（simple queue）</li><li>循环队列（circular queue）</li><li>优先队列（priority queue）</li><li>双端队列（double ended queue，deque）</li></ul><h1 id="2-简单队列"><a href="#2-简单队列" class="headerlink" title="2. 简单队列"></a>2. 简单队列</h1><p>在一个简单的队列中，插入发生在后面，移除发生在前面。 它严格遵循 FIFO（先进先出）规则。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210904233021.png" alt></p><p>更详细内容，查看 <a href="https://rogerspy.github.io//2021/09/05/ds-queue/" target="_blank" rel="noopener">数据结构与算法：队列（queue）</a>。</p><h1 id="3-循环队列"><a href="#3-循环队列" class="headerlink" title="3. 循环队列"></a>3. 循环队列</h1><p>循环队列是指，最后一个元素指向第一个元素，形成一个循环链。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210904233239.png" alt></p><p>与简单队列相比，循环队列的主要优点是更好的内存利用率。 如果最后一个位置已满而第一个位置为空，我们可以在第一个位置插入一个元素。 此操作在简单队列中是不可能的。</p><p>更详细的内容，查看 <a href="https://rogerspy.github.io/2021/09/05/ds-circular-queue/" target="_blank" rel="noopener">数据结构与算法：循环队列（circular-queue）</a>。</p><h1 id="4-优先队列"><a href="#4-优先队列" class="headerlink" title="4. 优先队列"></a>4. 优先队列</h1><p>优先级队列是一种特殊类型的队列，其中每个元素都与一个优先级相关联，并根据其优先级进行处理。 如果出现具有相同优先级的元素，则按照它们在队列中的顺序进行处理。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210904233542.png" alt></p><p>更详细的内容，查看 <a href="https://rogerspy.github.io/2021/09/05/ds-priority-deque/" target="_blank" rel="noopener">数据结构与算法：优先队列（priority queue）</a>。</p><h1 id="5-双端队列"><a href="#5-双端队列" class="headerlink" title="5. 双端队列"></a>5. 双端队列</h1><p>在双端队列中，可以从前面或后面执行元素的插入和删除。 因此，它不遵循 FIFO（先进先出）规则。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210904235805.png" alt></p><p>更详细的内容，查看 <a href="https://rogerspy.github.io/2021/09/05/ds-deque/" target="_blank" rel="noopener">数据结构与算法：双端队列（deque）</a>。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.programiz.com/dsa/types-of-queue" target="_blank" rel="noopener">Types of Queues</a> </p>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
            <tag> queue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构与算法：栈（stack）</title>
      <link href="/2021/09/04/ds-stack/"/>
      <url>/2021/09/04/ds-stack/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png" alt></p><p>本文介绍栈（stack）数据结构，并用 python 代码实现。</p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>栈是一个线性数据结构，遵循后进先出（Last In First Out，LIFO）的原则。这就意味着最后插入的元素会先被删除。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210904122325.png" style="zoom:80%;"></p><p>就像叠盘子一样，</p><ul><li>你可以在最上面放一个新盘子</li><li>拿盘子的时候也是从最上面开始拿</li></ul><p>如果想要最下面的盘子，你就必须先把上面所有的盘子先拿走。这就是栈的基本工作方式。</p><h1 id="2-栈的-LIFO-原则"><a href="#2-栈的-LIFO-原则" class="headerlink" title="2. 栈的 LIFO 原则"></a>2. 栈的 LIFO 原则</h1><p>用编程的术语来说，在栈最上面放置一个元素称之为 “<em>push</em>”，删除元素叫做 “<em>pop</em>”。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210904122919.png" style="zoom: 80%;"></p><h1 id="3-栈的基本操作"><a href="#3-栈的基本操作" class="headerlink" title="3. 栈的基本操作"></a>3. 栈的基本操作</h1><ul><li><strong>push</strong>：在栈上面添加一个元素 </li><li><strong>pop</strong>：从栈中删除一个元素</li><li><strong>isEmpty</strong>：判断栈是否为空</li><li><strong>isFull</strong>：判断栈是否是一个满栈</li><li><strong>peek</strong>：获取栈最上层的元素而不删除它</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210904124034.png" alt></p><ol><li>用 <code>TOP</code> 指针来追踪栈中最上层的元素</li><li>初始化栈的时候，我们设置设置指针为 -1，这样我们就可以通过判断 <code>TOP==-1</code> 来检查栈是否为空</li><li>当往栈里 push 数据的时候，我峨嵋你增加 <code>TOP</code> 的值，将新元素放置在 <code>TOP</code> 指定的位置</li><li>删除元素的时候，返回 <code>TOP</code> 指向的值，然后减小 <code>TOP</code> 值</li><li>向栈 push 数据的时候应该先检查栈是否已满</li><li>删除数据的时候，应该检查栈是否为空</li></ol><h1 id="4-用-Python-实现栈"><a href="#4-用-Python-实现栈" class="headerlink" title="4. 用 Python 实现栈"></a>4. 用 Python 实现栈</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Stack implementation in python</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Creating a stack</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_stack</span><span class="params">()</span>:</span></span><br><span class="line">    stack = []</span><br><span class="line">    <span class="keyword">return</span> stack</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Creating an empty stack</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_empty</span><span class="params">(stack)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> len(stack) == <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Adding items into the stack</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(stack, item)</span>:</span></span><br><span class="line">    stack.append(item)</span><br><span class="line">    print(<span class="string">"pushed item: "</span> + item)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Removing an element from the stack</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(stack)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> (check_empty(stack)):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"stack is empty"</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> stack.pop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">stack = create_stack()</span><br><span class="line">push(stack, str(<span class="number">1</span>))</span><br><span class="line">push(stack, str(<span class="number">2</span>))</span><br><span class="line">push(stack, str(<span class="number">3</span>))</span><br><span class="line">push(stack, str(<span class="number">4</span>))</span><br><span class="line">print(<span class="string">"popped item: "</span> + pop(stack))</span><br><span class="line">print(<span class="string">"stack after popping an element: "</span> + str(stack))</span><br></pre></td></tr></table></figure><h1 id="5-栈的时间复杂度"><a href="#5-栈的时间复杂度" class="headerlink" title="5. 栈的时间复杂度"></a>5. 栈的时间复杂度</h1><p>对于基于数组的栈实现，push 和 pop 操作都是常数时间，即 $O(1)$。</p><h1 id="6-栈的应用"><a href="#6-栈的应用" class="headerlink" title="6. 栈的应用"></a>6. 栈的应用</h1><p>尽管栈是非常简单的数据结构，但是它非常有用，最常见的应用如：</p><ul><li>词倒置。将词中的每个字符方法栈中，然后一个一个删除就可以了。因为栈是 LIFO 的，删除的时候就可以将词中的字符倒置过来。</li><li>编译器中，计算比如 <code>2+4/5*(7-9)</code> 的表达式的时候，用栈将表达式转化成前缀或者后缀的形式。</li><li>浏览器中，后退按钮用栈存储了所有你浏览过的网址（URL），每次你浏览一个新的网站的时候，它就会被加入到栈中，当你回退的时候，现在的网页就会被删除，然后回到倒数第二个页面。</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.programiz.com/dsa/stack" target="_blank" rel="noopener">Stack Data Structure</a> </p>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
            <tag> stack </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知识图谱：知识建模（三）RDFS/OWL 词汇表</title>
      <link href="/2021/08/26/kg-rdf-vocabulary/"/>
      <url>/2021/08/26/kg-rdf-vocabulary/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/kgicon.png" alt></p><p>前面的文章介绍了知识建模，我们提到知识建模使用的是 RDF 知识表示法，而 RDFS 本质上是一个标准化语义词汇表。所以本文总结一些常用的 RDFS/OWL 的语义词汇。</p><a id="more"></a><h1 id="0-絮絮叨叨"><a href="#0-絮絮叨叨" class="headerlink" title="0. 絮絮叨叨"></a>0. 絮絮叨叨</h1><h2 id="0-1-RDF-XML"><a href="#0-1-RDF-XML" class="headerlink" title="0.1 RDF/XML"></a>0.1 RDF/XML</h2><p>在正式介绍 RDFS/OWL 词汇之前，相信很多小伙伴在看知识建模的时候就会有很多疑问。为什么一定要用 RDF？XML 好像也能胜任这份工作，RDF 和 XML 的区别是什么？RDF 标榜的让计算机理解语义体现在哪里？等等一系列的疑问。当然回答这些问题并不是本文的目的，本文只是总结 RDFS/OWL 的词汇。要想弄明白 RDF 到底是怎么一回事，这里推荐一些必读的书籍/文献，希望能帮助到有疑问的人。</p><ul><li><a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=08AF5BC897E861F62FE3800830B02E22?doi=10.1.1.91.8164&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener"> Where are the Semantics in the Semantic Web?</a>, <em>Michael Uschold</em></li><li><a href="https://www.w3.org/DesignIssues/RDF-XML.html" target="_blank" rel="noopener">Why RDF model is different from the XML model</a>, <em>Tim Berners-Lee</em></li><li><a href="https://www.researchgate.net/publication/279393307_A_Developer%27s_Guide_to_the_Semantic_Web" target="_blank" rel="noopener">A developer’s guide to semantic web</a>, <em>Liyang Yu</em></li><li><a href="http://www-900.ibm.com/developerWorks/cn/xml/x-xmlrdf/index.shtml#authorname" target="_blank" rel="noopener">XML+RDF——实现Web数据基于语义的描述</a>, <em>周竞涛、王明微</em></li></ul><h2 id="0-2-IRI-URI-URL-URN-的区别"><a href="#0-2-IRI-URI-URL-URN-的区别" class="headerlink" title="0.2 IRI, URI, URL, URN 的区别"></a>0.2 IRI, URI, URL, URN 的区别</h2><ul><li><p>URL</p><p><strong>Uniform Resource Locator</strong>，统一资源定位符。是用于在网络中传播和访问互联网资源的一个地址，只有通过特定的地址才能够访问的指定的资源或者网页，简而言之就是我们所说的网址，当然这样有些不太恰当，但是确实最容易被理解的，就如你必须通过 <code>https://www.baidu.com</code> 才能访问百度搜索页面，通过其他的链接都是不行的，这个地址就被称之为 URL。</p></li><li><p>URI</p><p><strong>Uniform Resource Identifier</strong>，统一资源标识符。也可以理解为标识、定位资源的字符串。字符集仅限于 US-ASCII 中（额外加一个百分号 %）， 不包括某些保留字符。URI 可用作定位器、名称或两者。 如果 URI 是定位器，则它描述了资源的主要访问机制。 如果一个 URI 是一个名称，它通过给它一个唯一的名称来标识一个资源。 许多人容易将 URL 和 URI 两者混淆，其实两者非常相似，但是也有所不同。URL 包含相对地址和绝对地址，URI 就属于绝对地址，所以 URL 包含 URI，简单的举例就是很多的网站可能都有 <code>/about</code> 这个路径，但是不同的域名或者 IP 访问到的就是不同的资源页面，所以这就只是一个标识，并不能标识其具体未知或者唯一性。</p></li><li><p>IRI</p><p><strong>Internationalized Resource Identifier</strong>，国际化资源标识符。和 URI 类似，区别在于 URI 使用的字符集有限制，所以没有办法兼容不同的文字语言，所以 IRI 就引入了 Unicode 字符来解决这个兼容问题，最后就有了国际化资源标识符（IRI）。</p></li><li><p>URN</p><p><strong>Uniform Resource Name</strong>，统一资源名称。旨在用作持久的，与位置无关的资源标识符。URN 可以提供一种机制，用于查找和检索定义特定命名空间的架构文件。尽管普通的 URL 可以提供类似的功能，但是在这方面，URN 更加强大并且更容易管理，因为 URN 可以引用多个 URL。子凡举个最简单的例子大家就明白了，那就是：磁力链接，它就是 URN 的一种实现，可以持久化的标识一个 BT 资源，资源分布式的存储在 P2P 网络中，无需中心服务器用户即可找到并下载它。</p></li></ul><p>总结一下：</p><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • SUMMARY            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            • IRI ⊃ URI <br>            • URI ⊃ URL <br>            • URI ⊃ URN <br>            • URL ∩ URN = ∅         </div>        </div>    </div><p>下面就进入今天的正题——RDFS/OWL 词汇表。本文摘自 <em>《语义网技术体系》 [瞿裕忠，胡伟，程龚 编著] 2015年版</em> 这本书。想查看完整版词汇表，可前往这两个网页：</p><ul><li><a href="https://www.w3.org/TR/rdf-schema/" target="_blank" rel="noopener">RDF Schema 1.1</a> </li><li><a href="https://www.w3.org/TR/2004/REC-owl-ref-20040210/" target="_blank" rel="noopener">OWL Web Ontology Language</a></li></ul><p>再啰嗦一句，除了以上两个 RDF 词汇表，还有一个 <a href="http://xmlns.com/foaf/spec/" target="_blank" rel="noopener">FOAF</a> 词汇表在对人物进行建模的时候通常会用到。但是这里就不再介绍，想了解更多可自行前往。</p><h1 id="1-序言"><a href="#1-序言" class="headerlink" title="1. 序言"></a>1. 序言</h1><p>RDF Schema（下文简称 RDFS） 是 RDF 词汇表的一个扩展版本（RDF 本身是一个知识表示模型，但同时也是一个词汇表）。RDFS 承认有许多技术可以用来描述类和属性的含义，例如 OWL。</p><p>本文中定义的语言由一组 RDF 资源组成，这些资源可用于在特定于应用程序的 RDF 词汇表中描述其他 RDF 资源。核心词汇 <code>rdfs</code> 非正式地称为命名空间中定义。该命名空间由 IRI 标识：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://www.w3.org/2000/01/rdf-schema#</span><br></pre></td></tr></table></figure><p>并且通常与前缀相关联 <code>rdfs:</code>。本规范还使用前缀 <code>rdf:</code>来指代 RDF 命名空间：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://www.w3.org/1999/02/22-rdf-syntax-ns#</span><br></pre></td></tr></table></figure><p>为了方便和可读性，本规范使用缩写形式来表示 IRI。形式 <code>prefix:suffix</code> 的名称应该被解释为一个 IRI，它由与后缀连接的前缀相关联的 IRI 组成。</p><h1 id="2-RDFS"><a href="#2-RDFS" class="headerlink" title="2. RDFS"></a>2. RDFS</h1><p>资源可以被分成不同的组，这些组称之为“类”（classes）。每个类别下包含的成员称之为“实例”。比如“人”是一个类，“张三”是一个“人”的实例。通常我们把 RDF 和 RDFS 合写成 RDF(S) 或 RDF/S。</p><p>下面分别介绍 RDF(S) 的核心词汇。</p><h2 id="2-1-Classes"><a href="#2-1-Classes" class="headerlink" title="2.1 Classes"></a>2.1 Classes</h2><p>资源可以分成不同的组，这些组就称之为“类”，组内的成员就称之为类的“实例”。我们用 IRI 来标识类，然后用 RDF 属性来描述类。两个不同的类可能有相同的实例，比如“张三”既可以是“导演”这个类，也可以是“演员”这个类。一个类也可能是他自己的实例。</p><blockquote><p>名词解释：“类的外延”</p><p>与一个类别相关的集合，我们称之为类的外延。类的外延集合中的每个成员都是类的实例。举个例子：</p><p>类：食物</p><p>类的外延：a = {鸡，鸭，鱼，肉}</p><p>类的实例：鸡，鸭，鱼，肉</p><p>例子中，“食物”作为一个类别，表示一个抽象概念。跟这个类别相关的一个集合 a 表示“食物”的外延，相对类来说类的外延是具体的概念。但是要注意 a 作为一个集合整体出现。而 a 中的每一个元素称之为实例。</p><p>当我们说“鸡肉是一种食物”的时候，实际上是表明“鸡肉”是“食物”这个概念的外延集合中的一员。</p><script type="math/tex; mode=display">\text{instance} \in a \rightarrow class</script></blockquote><h3 id="2-1-1-rdf-Resource"><a href="#2-1-1-rdf-Resource" class="headerlink" title="2.1.1 rdf:Resource"></a>2.1.1 rdf:Resource</h3><p>所有 RDF 描述的事物都是资源，即都是 <code>rdfs:Resource</code> 的实例。这是所有事物的类，其他所有类都是它的子类。<code>rdfs:Resource</code> 也是 <code>rdfs:Class</code> 的实例。</p><h3 id="2-1-2-rdf-Class"><a href="#2-1-2-rdf-Class" class="headerlink" title="2.1.2 rdf:Class"></a>2.1.2 rdf:Class</h3><p>对应“类”的概念，即资源的类。当定义一个新类的时候，表示该类的资源必须有一个 <code>rdf:type</code> 属性，属性值是 <code>rdfs:Class</code>。比如定义“导演”是一个新类，那么我们必须定义：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">导演 rdf:type rdfs:Class</span><br></pre></td></tr></table></figure><p>注意，如上所述，一个实例可能属于多个类，所以类成员是不互斥的。<code>rdfs:Class</code> 是 <code>rdfs:Class</code> 是实例。</p><h3 id="2-1-3-rdf-Literal"><a href="#2-1-3-rdf-Literal" class="headerlink" title="2.1.3 rdf:Literal"></a>2.1.3 rdf:Literal</h3><p><code>rdf:Literal</code> 表示类或属性的字面量类型，比如数字、字符串等。<code>rdfs:Literal</code> 是 <code>rdfs:Class</code> 的实例，同时也是 <code>rdfs:Resource</code> 的子类。</p><h3 id="2-1-4-rdfs-Property"><a href="#2-1-4-rdfs-Property" class="headerlink" title="2.1.4 rdfs:Property"></a>2.1.4 rdfs:Property</h3><p><code>rdfs:Property</code> 是 RDF 属性类，同时也是 <code>rdfs:Class</code> 的实例。</p><h2 id="2-2-Properties"><a href="#2-2-Properties" class="headerlink" title="2.2.  Properties"></a>2.2.  Properties</h2><p>在 RDF 中，RDF 属性表示 subject 资源和 object 资源之间的关系。为了下文解释方便，我们这里写下三元组的一般形式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">subject predicate object</span><br></pre></td></tr></table></figure><h2 id="2-2-1-rdfs-range"><a href="#2-2-1-rdfs-range" class="headerlink" title="2.2.1 rdfs:range"></a>2.2.1 rdfs:range</h2><p><code>rdfs:range</code> 是 <code>rdfs:Property</code> 的一个实例，用来指明一个属性的值域。例如三元组：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p rdfs:range c</span><br></pre></td></tr></table></figure><p>表示 p 是 <code>rdfs:range</code> 的一个实例， c 是 <code>rdfs:Class</code>  的一个实例。上面的三元组描述的是一个 predicate 是 p 的 object 是 c 的实例。</p><h3 id="2-2-2-rdfs-domain"><a href="#2-2-2-rdfs-domain" class="headerlink" title="2.2.2 rdfs:domain"></a>2.2.2 rdfs:domain</h3><p><code>rdfs:domain</code> 是 <code>rdfs:Property</code> 的一个实例，用来指明一个属性的定义域。例如三元组：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p rdfs:domain c</span><br></pre></td></tr></table></figure><p>表示 p 是 <code>rdfs:Property</code> 的一个实例，c 是 <code>rdfs:Class</code> 的实例。上面的三元组描述的是一个 predicate 是 p 的 subject 是 c 的实例。</p><p>其中，如果 p 有不止一个 <code>rdfs:domain</code> ，那么其对应的所有 subject 都是 c 的实例。</p><p>举个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">人 吃 食物  </span><br><span class="line"></span><br><span class="line">吃 rdf:type rdfs:Property</span><br><span class="line">吃 rdfs:domain 人</span><br><span class="line">吃 rdfs:range 食物</span><br></pre></td></tr></table></figure><p>翻译过来就是，“吃”表示一种属性（关系），它的主语是“人”，宾语是“食物”。</p><h3 id="2-2-3-rdf-type"><a href="#2-2-3-rdf-type" class="headerlink" title="2.2.3 rdf:type"></a>2.2.3 rdf:type</h3><p><code>rdf:type</code> 是 <code>rdf:Property</code> 的一个实例，用于描述一个资源是类的实例，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">R rdf:type C</span><br></pre></td></tr></table></figure><p>表示 C 是 <code>rdfs:Class</code> 的子类，并且 R 是 C 的实例。用一句通俗易懂的话就是，R 是一种 C，比如 <code>人 rdf:type 生物</code> 表示“人是一种动物”。实际上 <code>rdf:type</code> 表示 “is-a” 的关系，可以简写成 <code>a</code>。</p><h3 id="2-2-4-rdfs-subClassOf"><a href="#2-2-4-rdfs-subClassOf" class="headerlink" title="2.2.4 rdfs:subClassOf"></a>2.2.4 rdfs:subClassOf</h3><p><code>rdfs:subClassOf</code> 是 <code>rdfs:Property</code>  的一个实例，用来指明一个类的所有实例也是另一个类的实例，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C1 rdfs:subClassOf C2</span><br></pre></td></tr></table></figure><p>描述的是，C1 是 <code>rdfs:Class</code> 的一个实例，C2 是 <code>rdfs:Class</code> 的一个实例，并且 C1 是 C2 的一个子类。<code>rdfs:subClassOf</code> 是可传递的，即如果 a 是 b 的子类，b 是 c 的子类，那么 a 也是 c 的子类。</p><p><code>rdfs:subClassOf</code> 的 <code>rdfs:domain</code> 是 <code>rdfs:Class</code>。<code>rdfs:subClassOf</code> 的 <code>rdfs:range</code> 是 <code>rdfs:Class</code>。</p><h3 id="2-2-5-rdfs-subPropertyOf"><a href="#2-2-5-rdfs-subPropertyOf" class="headerlink" title="2.2.5 rdfs:subPropertyOf"></a>2.2.5 rdfs:subPropertyOf</h3><p><code>rdfs:subPropertyOf</code> 是 <code>rdfs:Property</code> 的一个实例，用来指明与一个资源相关的所有属性也与另一个资源相关，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">P1 rdfs:subPropertyOf P2</span><br></pre></td></tr></table></figure><p>描述了 P1 是 <code>rdfs:Property</code> 的一个实例，P2 也是 <code>rdfs:Property</code> 的一个实例，并且 P1 是 P2 的一个子属性。<code>rdfs:subPropertyOf</code> 是可传递性的。</p><p><code>rdfs:subPropertyOf</code> 的 <code>rdfs:domain</code> 是 <code>rdf:Property</code>。<code>rdfs:subPropertyOf</code> 的 <code>rdfs:range</code> 是 <code>rdf:Property</code>。</p><p>除了上面介绍的词之外， RD(S) 还有很多其他有用的词汇，这里不一一列举。下图展示了 RDF(S) 各个词汇之间的关系：</p><p> <img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210830172323.png" alt></p><h3 id="2-2-6-RDFS-词汇总结"><a href="#2-2-6-RDFS-词汇总结" class="headerlink" title="2.2.6 RDFS 词汇总结"></a>2.2.6 RDFS 词汇总结</h3><h4 id="2-2-6-1-Classes"><a href="#2-2-6-1-Classes" class="headerlink" title="2.2.6.1 Classes"></a>2.2.6.1 Classes</h4><div class="table-container"><table><thead><tr><th>Class name</th><th>comment</th></tr></thead><tbody><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_resource" target="_blank" rel="noopener">rdfs:Resource</a></td><td>The class resource, everything.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_literal" target="_blank" rel="noopener">rdfs:Literal</a></td><td>The class of literal values, e.g. textual strings and integers.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_langstring" target="_blank" rel="noopener">rdf:langString</a></td><td>The class of language-tagged string literal values.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_html" target="_blank" rel="noopener">rdf:HTML</a></td><td>The class of HTML literal values.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_xmlliteral" target="_blank" rel="noopener">rdf:XMLLiteral</a></td><td>The class of XML literal values.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_class" target="_blank" rel="noopener">rdfs:Class</a></td><td>The class of classes.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_property" target="_blank" rel="noopener">rdf:Property</a></td><td>The class of RDF properties.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_datatype" target="_blank" rel="noopener">rdfs:Datatype</a></td><td>The class of RDF datatypes.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_statement" target="_blank" rel="noopener">rdf:Statement</a></td><td>The class of RDF statements.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_bag" target="_blank" rel="noopener">rdf:Bag</a></td><td>The class of unordered containers.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_seq" target="_blank" rel="noopener">rdf:Seq</a></td><td>The class of ordered containers.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_alt" target="_blank" rel="noopener">rdf:Alt</a></td><td>The class of containers of alternatives.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_container" target="_blank" rel="noopener">rdfs:Container</a></td><td>The class of RDF containers.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_containermembershipproperty" target="_blank" rel="noopener">rdfs:ContainerMembershipProperty</a></td><td>The class of container membership properties, rdf:_1, rdf:_2, …, all of which are sub-properties of ‘member’.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_list" target="_blank" rel="noopener">rdf:List</a></td><td>The class of RDF Lists.</td></tr></tbody></table></div><h4 id="2-2-6-2-Properties"><a href="#2-2-6-2-Properties" class="headerlink" title="2.2.6.2 Properties"></a>2.2.6.2 Properties</h4><div class="table-container"><table><thead><tr><th>Property name</th><th>comment</th><th>domain</th><th>range</th></tr></thead><tbody><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_type" target="_blank" rel="noopener">rdf:type</a></td><td>The subject is an instance of a class.</td><td>rdfs:Resource</td><td>rdfs:Class</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_subclassof" target="_blank" rel="noopener">rdfs:subClassOf</a></td><td>The subject is a subclass of a class.</td><td>rdfs:Class</td><td>rdfs:Class</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_subpropertyof" target="_blank" rel="noopener">rdfs:subPropertyOf</a></td><td>The subject is a subproperty of a property.</td><td>rdf:Property</td><td>rdf:Property</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_domain" target="_blank" rel="noopener">rdfs:domain</a></td><td>A domain of the subject property.</td><td>rdf:Property</td><td>rdfs:Class</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_range" target="_blank" rel="noopener">rdfs:range</a></td><td>A range of the subject property.</td><td>rdf:Property</td><td>rdfs:Class</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_label" target="_blank" rel="noopener">rdfs:label</a></td><td>A human-readable name for the subject.</td><td>rdfs:Resource</td><td>rdfs:Literal</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_comment" target="_blank" rel="noopener">rdfs:comment</a></td><td>A description of the subject resource.</td><td>rdfs:Resource</td><td>rdfs:Literal</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_member" target="_blank" rel="noopener">rdfs:member</a></td><td>A member of the subject resource.</td><td>rdfs:Resource</td><td>rdfs:Resource</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_first" target="_blank" rel="noopener">rdf:first</a></td><td>The first item in the subject RDF list.</td><td>rdf:List</td><td>rdfs:Resource</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_rest" target="_blank" rel="noopener">rdf:rest</a></td><td>The rest of the subject RDF list after the first item.</td><td>rdf:List</td><td>rdf:List</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_seealso" target="_blank" rel="noopener">rdfs:seeAlso</a></td><td>Further information about the subject resource.</td><td>rdfs:Resource</td><td>rdfs:Resource</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_isdefinedby" target="_blank" rel="noopener">rdfs:isDefinedBy</a></td><td>The definition of the subject resource.</td><td>rdfs:Resource</td><td>rdfs:Resource</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_value" target="_blank" rel="noopener">rdf:value</a></td><td>Idiomatic property used for structured values.</td><td>rdfs:Resource</td><td>rdfs:Resource</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_subject" target="_blank" rel="noopener">rdf:subject</a></td><td>The subject of the subject RDF statement.</td><td>rdf:Statement</td><td>rdfs:Resource</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_predicate" target="_blank" rel="noopener">rdf:predicate</a></td><td>The predicate of the subject RDF statement.</td><td>rdf:Statement</td><td>rdfs:Resource</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_object" target="_blank" rel="noopener">rdf:object</a></td><td>The object of the subject RDF statement.</td><td>rdf:Statement</td><td>rdfs:Resource</td></tr></tbody></table></div><h1 id="3-OWL"><a href="#3-OWL" class="headerlink" title="3. OWL"></a>3. OWL</h1><p>由于 RDFS 的表达能力较弱，W3C 2004 年又发布了 Web Ontology Language（OWL）进一步提供更加丰富的知识表示和推理能力。OWL 以描述逻辑为理论基础，可以将概念和属于用结构化的形式表示出来。通过 RDF 中的链接可以是本体分布在不同的系统中，充分体现了其标准化，开放性，扩展性以及适应性。现在 OWL 已经是 W3C 推荐的本体建模标准。OWL 的命名空间是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://www.w3.org/2002/07/owl#</span><br></pre></td></tr></table></figure><p>OWL 提供 3 中表达能力不同的子语言：OWL Full，OWL DL，OWL Lite。其中任意一个都可以映射成一个完整的 RDF 图。</p><ul><li>OWL Full。完全兼容 RDFS，但超出经典一阶逻辑的范畴。与 OWL Full 相关的推理工具现在还在探索中。</li><li>OWL DL。是 OWL Full 的一个子集，表达能力相对较强，可以有效的支持逻辑推理，但不是完全兼容 RDFS。</li><li>OWL Lite。在 OWL DL 的基础上对允许使用公理做了进一步的限制。</li></ul><p>到了 2012 年，W3C 对原先版本的 OWL 进行了修订，发布新的 OWL 版本——OWL 2。OWL 2 对 OWL 向后兼容，包含了 3 个指定的概图：</p><ul><li>OWL 2 EL。允许以高效的多项式时间算法对类型的可满足性检查、分类和实例检查并进行推理，特别适合使用含有大量属性或类本体的应用。</li><li>OWL 2 QL。允许使用传统的关系数据库实现查询问答，特别适合使用大量实例数据并且以查询问答作为主要推理任务的应用。</li><li>OWL 2 RL。允许以一种比较直接的方式，使用基于规则的推理引擎，在不牺牲太多的表达能力的情况下实现大规模推理。</li></ul><h2 id="3-1-OWL-Document"><a href="#3-1-OWL-Document" class="headerlink" title="3.1 OWL Document"></a>3.1 OWL Document</h2><p>一般情况下，描述本体的文档都包含本体本身的信息。一个本体是一个资源，可以采用 OWL 和其他命名空间属性进行描述。这些描述被称为本体头部，通常位于本体文档的开始部分。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;rdf:RDF xmlns=&quot;http://www.semanticweb.org/qiuji/ontologies/2017/9/untitled-ontology-2#&quot;</span><br><span class="line">     xml:base=&quot;http://www.semanticweb.org/qiuji/ontologies/2017/9/untitled-ontology-2&quot;</span><br><span class="line">     xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;</span><br><span class="line">     xmlns:owl=&quot;http://www.w3.org/2002/07/owl#&quot;</span><br><span class="line">     xmlns:xml=&quot;http://www.w3.org/XML/1998/namespace&quot;</span><br><span class="line">     xmlns:untitled-ontology-22=&quot;http://www.semanticweb.org/ontologies/2017/9/untitled-ontology-2#&quot;</span><br><span class="line">     xmlns:xsd=&quot;http://www.w3.org/2001/XMLSchema#&quot;</span><br><span class="line">     xmlns:untitled-ontology-2=&quot;http://www.semanticweb.org/qiuji/ontologies/2017/9/untitled-ontology-2#&quot;</span><br><span class="line">     xmlns:rdfs=&quot;http://www.w3.org/2000/01/rdf-schema#&quot;&gt;</span><br><span class="line">    &lt;owl:Ontology rdf:about=&quot;http://www.semanticweb.org/ontologies/2017/9/untitled-ontology-2&quot;/&gt;</span><br></pre></td></tr></table></figure><h3 id="3-1-1-owl-imports"><a href="#3-1-1-owl-imports" class="headerlink" title="3.1.1 owl:imports"></a>3.1.1 owl:imports</h3><p>允许引用另一个包含定义的 OWL 本体，并将其含义作为定义本体的一部分。每个引用都包含一个 URI，它指向被导入的本体。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">@prefix : &lt;http://example.com/pwl/families/&gt; .</span><br><span class="line">@prefix otherOnt: &lt;http/example.org/otherOntologies/families/&gt; .</span><br><span class="line">@prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt; .</span><br><span class="line">@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .</span><br><span class="line">@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;</span><br><span class="line"></span><br><span class="line">&lt;http://example.com/pwl/families/&gt;</span><br><span class="line">    rdf:type owlOntology ;</span><br><span class="line">    owl:imports &lt;http/example.org/otherOntologies/families.owl&gt; .</span><br></pre></td></tr></table></figure><p>另外，还可以在本体头部添加有关版本的一些信息。相关属性包括：<code>owl:versionInfo</code>，<code>owl:priorVersion</code>，<code>owl:backwardCompatibleWith</code>，<code>owl:incompatibleWith</code> 等等。</p><h2 id="3-2-OWL-Classes"><a href="#3-2-OWL-Classes" class="headerlink" title="3.2 OWL Classes"></a>3.2 OWL Classes</h2><p>与 RDFS 类似， OWL 也有“类”的概念，也是表示我们对资源的分组，也有“类的外延”等概念。需要注意的是，在OWL 中，类的外延中的元素称之为个体（individual），和在 Protege 建模工具菜单栏中的 individual 是同一概念，都表示实例。</p><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • NOTE            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            OWL DL 和 OWL DL 中的资源不能同时是个体（individual）和类（class），即 class 和 individual 是互斥的。            另外，rdfs:Class 和 rdfs:Property 是被禁止使用的。        </div>        </div>    </div><p>从上面的介绍来看，OWL 被设计出来主要是对 RDFS  的逻辑推理能力进行补强。要进行推理我们首先要有一些公理。在 OWL 中采用“类描述”对 OWL 类进行解释描述，然后将 OWL 组合成类公理。</p><h3 id="3-2-1-类描述（class-description）"><a href="#3-2-1-类描述（class-description）" class="headerlink" title="3.2.1 类描述（class description）"></a>3.2.1 类描述（class description）</h3><p>类描述通过类名或通过指定未命名匿名类的类外延来描述 OWL 类。OWL 中有 6 中不同的类描述：</p><ol><li>类标识符（URI）</li><li>穷举组成一个类的个体（enumeration）</li><li>属性限制（property restriction）</li><li>多个类描述的交集（intersection）</li><li>多个类描述的并集（union）</li><li>一个类描述的补集（complement）</li></ol><p>类标识符相当于通过类名（URI）来描述一个类；穷举表示一个类包含可穷举的个体；一个类中的所有个体都要满足特定的属性限制。对于 4、5、6 来说，可以认为是逻辑与（AND）或（OR）非（NOT）操作。</p><h4 id="3-2-1-1-owl-Class"><a href="#3-2-1-1-owl-Class" class="headerlink" title="3.2.1.1 owl:Class"></a>3.2.1.1 owl:Class</h4><p><code>owl:Class</code> 表示一个明明资源是一个类别，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ex：Human rdf:type owl:Class .</span><br></pre></td></tr></table></figure><p>其中 <code>ex</code> 表示本体的命名空间。下面的例子我们都用 RDF/XML 语法进行举例，所以上面的例子改写成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Class rdf:ID=&quot;Human&quot;/&gt;</span><br></pre></td></tr></table></figure><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • NOTE            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            OWL Lite 和 OWL DL 中 <code>owl:Class</code> 必须用在所有的类描述上。        </div>        </div>    </div><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • NOTE            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            在 OWL Lite 和 OWL DL 中<code>owl:Class</code> 是 <code>rdfs:Class</code> 的子类。这个关系说明            在 RDFS 中，并不是所有的类在 OWL DL(Lite) 都是合法的。但是在 OWL Full 中二者是等价的。        </div>        </div>    </div><p>OWL 类标识符是预先定义好的，即 <code>owl:Thing</code> / <code>owl:Nothing</code>。<code>owl:Thing</code> 是所有 OWL 类的父类，而 <code>owl:Nothing</code> 是所有类的子类（可以认为就是空集）。</p><h4 id="3-2-1-2-owl-oneOf"><a href="#3-2-1-2-owl-oneOf" class="headerlink" title="3.2.1.2 owl:oneOf"></a>3.2.1.2 owl:oneOf</h4><p><code>owl:oneOf</code> 用来表示类描述中的穷举，它的值必须是类的实例。为了方便，我们可以用 <code>rdfs:parseType=&quot;Collection&quot;</code> ，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Class&gt;</span><br><span class="line">  &lt;owl:oneOf rdf:parseType=&quot;Collection&quot;&gt;</span><br><span class="line">    &lt;owl:Thing rdf:about=&quot;#Eurasia&quot;/&gt;</span><br><span class="line">    &lt;owl:Thing rdf:about=&quot;#Africa&quot;/&gt;</span><br><span class="line">    &lt;owl:Thing rdf:about=&quot;#NorthAmerica&quot;/&gt;</span><br><span class="line">    &lt;owl:Thing rdf:about=&quot;#SouthAmerica&quot;/&gt;</span><br><span class="line">    &lt;owl:Thing rdf:about=&quot;#Australia&quot;/&gt;</span><br><span class="line">    &lt;owl:Thing rdf:about=&quot;#Antarctica&quot;/&gt;</span><br><span class="line">  &lt;/owl:oneOf&gt;</span><br><span class="line">&lt;/owl:Class&gt;</span><br></pre></td></tr></table></figure><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • NOTE            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            OWL Lite 没有穷举。        </div>        </div>    </div><h4 id="3-2-1-3-owl-Restriction"><a href="#3-2-1-3-owl-Restriction" class="headerlink" title="3.2.1.3 owl:Restriction"></a>3.2.1.3 owl:Restriction</h4><p>属性限制是一类特殊的类描述。它用来描述所有个体都满足一定限制条件的匿名类。OWL 有两种属性限制：值限制和基数限制。</p><ul><li>所谓值限制指的是，限制属性的值域。</li><li>所谓基数限制指的是，限制属性的个数。</li></ul><p>OWL 还提供了全局基数限制：<code>owl:FunctionalProperty</code> 和 <code>owl:InverseFunctionalProperty</code>。</p><p><code>owl:Restriction</code> 是 <code>owl:Class</code> 的子类。一个限制类应该有一个三元组用 <code>owl:onProperty</code> 来连接属性和限制。</p><ul><li><p><strong>值限制</strong></p><ol><li><p><code>owl:allValuesFrom</code>：用来限制一个类的所有个体是否在指定的值域内。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Restriction&gt;</span><br><span class="line">  &lt;owl:onProperty rdf:resource=&quot;#hasParent&quot; /&gt;</span><br><span class="line">  &lt;owl:allValuesFrom rdf:resource=&quot;#Human&quot;  /&gt;</span><br><span class="line">&lt;/owl:Restriction&gt;</span><br></pre></td></tr></table></figure></li><li><p><code>owl:someValuesFrom</code>：用来限制一个类的所有个体中，至少有一个个体来源于指定的值域。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Restriction&gt;</span><br><span class="line">  &lt;owl:onProperty rdf:resource=&quot;#hasParent&quot; /&gt;</span><br><span class="line">  &lt;owl:someValuesFrom rdf:resource=&quot;#Physician&quot; /&gt;</span><br><span class="line">&lt;/owl:Restriction&gt;</span><br></pre></td></tr></table></figure></li><li><p><code>owl:hasValue</code>：用来限制一个类的所有个体中，至少有一个（语义上）等于指定的值。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Restriction&gt;</span><br><span class="line">  &lt;owl:onProperty rdf:resource=&quot;#hasParent&quot; /&gt;</span><br><span class="line">  &lt;owl:hasValue rdf:resource=&quot;#Clinton&quot; /&gt;</span><br><span class="line">&lt;/owl:Restriction&gt;</span><br></pre></td></tr></table></figure><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • NOTE            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            “语义上等价于”的意思是，V 不一定是指定的值，但是 V 和指定的值 V1 之间有一个 <code>owl:sameAs</code>的关系。        </div>        </div>    </div></li></ol></li><li><p><strong>基数限制</strong></p><ol><li><p><code>owl:maxCardinality</code>：用来限制一个类包含了最多 N 个语义不同的个体，其中 N 就是基数限制的值。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Restriction&gt;</span><br><span class="line">  &lt;owl:onProperty rdf:resource=&quot;#hasParent&quot; /&gt;</span><br><span class="line">  &lt;owl:maxCardinality rdf:datatype=&quot;&amp;xsd;nonNegativeInteger&quot;&gt;2&lt;/owl:maxCardinality&gt;</span><br><span class="line">&lt;/owl:Restriction&gt;</span><br></pre></td></tr></table></figure></li><li><p><code>owl:minCardinality</code>：用来限制一个类至少包含 N 个语义不同的个体，其中 N 就是基数限制的值。</p><p>比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Restriction&gt;</span><br><span class="line">  &lt;owl:onProperty rdf:resource=&quot;#hasParent&quot; /&gt;</span><br><span class="line">  &lt;owl:minCardinality rdf:datatype=&quot;&amp;xsd;nonNegativeInteger&quot;&gt;2&lt;/owl:minCardinality&gt;</span><br><span class="line">&lt;/owl:Restriction&gt;</span><br></pre></td></tr></table></figure><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • NOTE            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            一个类中的所有实例都要有 N 个属性。        </div>        </div>    </div></li><li><p><code>owl:cardinality</code>：用来限制一个类必须要有 N 个语义不同的个体，不能多也不能少。其中 N 就是基数限制的值。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Restriction&gt;</span><br><span class="line">  &lt;owl:onProperty rdf:resource=&quot;#hasParent&quot; /&gt;</span><br><span class="line">  &lt;owl:cardinality rdf:datatype=&quot;&amp;xsd;nonNegativeInteger&quot;&gt;2&lt;/owl:cardinality&gt;</span><br><span class="line">&lt;/owl:Restriction&gt;</span><br></pre></td></tr></table></figure></li></ol></li></ul><h4 id="3-2-1-4-Intersection-union-and-complement"><a href="#3-2-1-4-Intersection-union-and-complement" class="headerlink" title="3.2.1.4 Intersection, union and complement"></a>3.2.1.4 Intersection, union and complement</h4><ul><li><p><code>owl:intersectionOf</code>：连接一个类和一个类描述的列表，表示这个类的外延中的个体同时也是列表中所有类描述的外延成员。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Class&gt;</span><br><span class="line">  &lt;owl:intersectionOf rdf:parseType=&quot;Collection&quot;&gt;</span><br><span class="line">    &lt;owl:Class&gt;</span><br><span class="line">      &lt;owl:oneOf rdf:parseType=&quot;Collection&quot;&gt;</span><br><span class="line">        &lt;owl:Thing rdf:about=&quot;#Tosca&quot; /&gt;</span><br><span class="line">        &lt;owl:Thing rdf:about=&quot;#Salome&quot; /&gt;</span><br><span class="line">      &lt;/owl:oneOf&gt;</span><br><span class="line">    &lt;/owl:Class&gt;</span><br><span class="line">    &lt;owl:Class&gt;</span><br><span class="line">      &lt;owl:oneOf rdf:parseType=&quot;Collection&quot;&gt;</span><br><span class="line">        &lt;owl:Thing rdf:about=&quot;#Turandot&quot; /&gt;</span><br><span class="line">        &lt;owl:Thing rdf:about=&quot;#Tosca&quot; /&gt;</span><br><span class="line">      &lt;/owl:oneOf&gt;</span><br><span class="line">    &lt;/owl:Class&gt;</span><br><span class="line">  &lt;/owl:intersectionOf&gt;</span><br><span class="line">&lt;/owl:Class&gt;</span><br></pre></td></tr></table></figure><p><code>owl:intersectionOf</code> 可以看成逻辑连词。</p></li><li><p><code>owl:unionOf</code>：表示一个个体至少会出现在列表中的一个类中。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Class&gt;</span><br><span class="line">  &lt;owl:unionOf rdf:parseType=&quot;Collection&quot;&gt;</span><br><span class="line">    &lt;owl:Class&gt;</span><br><span class="line">      &lt;owl:oneOf rdf:parseType=&quot;Collection&quot;&gt;</span><br><span class="line">        &lt;owl:Thing rdf:about=&quot;#Tosca&quot; /&gt;</span><br><span class="line">        &lt;owl:Thing rdf:about=&quot;#Salome&quot; /&gt;</span><br><span class="line">      &lt;/owl:oneOf&gt;</span><br><span class="line">    &lt;/owl:Class&gt;</span><br><span class="line">    &lt;owl:Class&gt;</span><br><span class="line">      &lt;owl:oneOf rdf:parseType=&quot;Collection&quot;&gt;</span><br><span class="line">        &lt;owl:Thing rdf:about=&quot;#Turandot&quot; /&gt;</span><br><span class="line">        &lt;owl:Thing rdf:about=&quot;#Tosca&quot; /&gt;</span><br><span class="line">      &lt;/owl:oneOf&gt;</span><br><span class="line">    &lt;/owl:Class&gt;</span><br><span class="line">  &lt;/owl:unionOf&gt;</span><br><span class="line">&lt;/owl:Class&gt;</span><br></pre></td></tr></table></figure></li><li><p><code>owl:complementOf</code>：连接一个类和一个类描述，表示类外延中的个体不属于类描述的外延。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Class&gt;</span><br><span class="line">  &lt;owl:complementOf&gt;</span><br><span class="line">    &lt;owl:Class rdf:about=&quot;#Meat&quot;/&gt;</span><br><span class="line">  &lt;/owl:complementOf&gt;</span><br><span class="line">&lt;/owl:Class&gt;</span><br></pre></td></tr></table></figure></li></ul><h3 id="3-2-2-类公理"><a href="#3-2-2-类公理" class="headerlink" title="3.2.2 类公理"></a>3.2.2 类公理</h3><p>类描述通过类公理组合在一起用来定义一个类。这句话说起来很拗口，其实描述的道理很简单。就相当于我们要炒一盘菜，需要一些原材料（类描述），然后通过一些原则（类公理）将这些原料组合在一起形成一盘菜（类）。</p><p>OWL 提供了 3 个词汇，将类描述组合起来：</p><ul><li><code>rdfs:subClassOf</code></li><li><code>owl:equivalentClass</code></li><li><code>owl:disjointWith</code></li></ul><h4 id="3-2-2-1-rdfs-subClassOf"><a href="#3-2-2-1-rdfs-subClassOf" class="headerlink" title="3.2.2.1 rdfs:subClassOf"></a>3.2.2.1 rdfs:subClassOf</h4><script type="math/tex; mode=display">class\ description \quad \text{rdfs:subClassOf} \quad class\ description</script><p>这里的 <code>rdfs:subClassOf</code> 和 RDFS 中的一样。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Class rdf:ID=&quot;Opera&quot;&gt;</span><br><span class="line">  &lt;rdfs:subClassOf rdf:resource=&quot;#MusicalWork&quot; /&gt;</span><br><span class="line">&lt;/owl:Class&gt;</span><br></pre></td></tr></table></figure><h4 id="3-2-2-2-owl-equivalentClass"><a href="#3-2-2-2-owl-equivalentClass" class="headerlink" title="3.2.2.2 owl:equivalentClass"></a>3.2.2.2 owl:equivalentClass</h4><script type="math/tex; mode=display">class\ description\quad \text{owl:equivalentClass}\quad class\ description</script><p><code>owl:equivalentClass</code> 表示两个类描述有相同的类外延。最简单的形式是，两个命名类别是等价的。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Class rdf:about=&quot;#US_President&quot;&gt;</span><br><span class="line">  &lt;equivalentClass rdf:resource=&quot;#PrincipalResidentOfWhiteHouse&quot;/&gt;</span><br><span class="line">&lt;/owl:Class&gt;</span><br></pre></td></tr></table></figure><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • NOTE            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            <code>owl:equivalentClass</code> 的两个类并不表示两个类是等价的。        </div>        </div>    </div><p>比如上例中，“美国总统” 这个概念和“白宫的主要居民”这个概念并不一样。真正的语义等价应该用 <code>owl:sameAs</code>。</p><h4 id="3-2-2-3-owl-disjointWith"><a href="#3-2-2-3-owl-disjointWith" class="headerlink" title="3.2.2.3 owl:disjointWith"></a>3.2.2.3 owl:disjointWith</h4><script type="math/tex; mode=display">class\ description\quad \text{owl:disjointWith}\quad class\ description</script><p><code>owl:disjointWith</code> 表示两个类描述没有公共的个体，或者说两个类描述是互斥的。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Class rdf:about=&quot;#Man&quot;&gt;</span><br><span class="line">  &lt;owl:disjointWith rdf:resource=&quot;#Woman&quot;/&gt;</span><br><span class="line">&lt;/owl:Class&gt;</span><br></pre></td></tr></table></figure><h2 id="3-3-Properties"><a href="#3-3-Properties" class="headerlink" title="3.3 Properties"></a>3.3 Properties</h2><p> OWL 有两种属性：对象属性（object property）和数据类型属性（datatype property）。对象属性用来连接两个实例，而数据类型属性连接一个实例和寿哥数据类型的字面量。换成比较容易理解的话就是，对象属性表示两个实体之间的关系，数据类型属性就是实体和属性之间的关系。比如</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">小明 父亲 大明</span><br><span class="line">小明 生日 1990/1/1</span><br></pre></td></tr></table></figure><p>其中“父亲”就是对象属性，“生日”就是数据类型属性。</p><p>OWL 中支持的属性机构包括：</p><ul><li>RDFS ：<code>rdfs:subPropertyOf</code>, <code>rdfs:domain</code> 和 <code>rdfs:range</code></li><li>与其他属性相关的： <code>owl:equivalentProperty</code> 和 <code>owl:inverseOf</code></li><li>全局基数限制：<code>owl:FunctionalProperty</code> 和 <code>owl:InverseFunctionalProperty</code></li><li>逻辑属性： <code>owl:SymmetricProperty</code> 和 <code>owl:TransitiveProperty</code></li></ul><h3 id="3-3-1-owl-equivalentProperty"><a href="#3-3-1-owl-equivalentProperty" class="headerlink" title="3.3.1 owl:equivalentProperty"></a>3.3.1 owl:equivalentProperty</h3><p><code>owl:equivalentProperty</code> 表示两个属性有相同的属性外延。类似 <code>owl:equivalentClass</code>。</p><h3 id="3-3-2-owl-inverseOf"><a href="#3-3-2-owl-inverseOf" class="headerlink" title="3.3.2 owl:inverseOf"></a>3.3.2 owl:inverseOf</h3><p>属性是有方向的，从定义域指向值域。<code>owl:inverseOf</code> 表示反向属性，即原属性的定义域和值域互换。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:ObjectProperty rdf:ID=&quot;hasChild&quot;&gt;</span><br><span class="line">  &lt;owl:inverseOf rdf:resource=&quot;#hasParent&quot;/&gt;</span><br><span class="line">&lt;/owl:ObjectProperty&gt;</span><br></pre></td></tr></table></figure><h3 id="3-3-3-owl-FunctionalProperty"><a href="#3-3-3-owl-FunctionalProperty" class="headerlink" title="3.3.3 owl:FunctionalProperty"></a>3.3.3 owl:FunctionalProperty</h3><p><code>owl:FunctionalProperty</code> 表示对于实例 $x$ 来说，只有唯一的 $y$ 值。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:ObjectProperty rdf:ID=&quot;husband&quot;&gt;</span><br><span class="line">  &lt;rdfs:domain rdf:resource=&quot;#Woman&quot; /&gt;</span><br><span class="line">  &lt;rdfs:range  rdf:resource=&quot;#Man&quot; /&gt;</span><br><span class="line">&lt;/owl:ObjectProperty&gt;</span><br><span class="line"></span><br><span class="line">&lt;owl:FunctionalProperty rdf:about=&quot;#husband&quot; /&gt;</span><br></pre></td></tr></table></figure><h3 id="3-3-4-owl-InverseFunctionalProperty"><a href="#3-3-4-owl-InverseFunctionalProperty" class="headerlink" title="3.3.4 owl:InverseFunctionalProperty"></a>3.3.4 owl:InverseFunctionalProperty</h3><p><code>owl:InverseFunctionalProperty</code> 表示与 <code>owl:FunctionalProperty</code> 相反的意思，即对于值 $y$ 只能有一个 实例 $x$ 与之对应。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:InverseFunctionalProperty rdf:ID=&quot;biologicalMotherOf&quot;&gt;</span><br><span class="line">  &lt;rdfs:domain rdf:resource=&quot;#Woman&quot;/&gt;</span><br><span class="line">  &lt;rdfs:range rdf:resource=&quot;#Human&quot;/&gt;</span><br><span class="line">&lt;/owl:InverseFunctionalProperty&gt;</span><br></pre></td></tr></table></figure><h3 id="3-3-5-owl-TransitiveProperty"><a href="#3-3-5-owl-TransitiveProperty" class="headerlink" title="3.3.5 owl:TransitiveProperty"></a>3.3.5 owl:TransitiveProperty</h3><p><code>owl:TransitiveProperty</code> 表示属性的可传递性。如果 $(x,y)$ 是 P 的实例，$(y,z)$ 也是 P 的实例，那么 $(x,z)$ 也是 P 的实例。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:TransitiveProperty rdf:ID=&quot;subRegionOf&quot;&gt;</span><br><span class="line">  &lt;rdfs:domain rdf:resource=&quot;#Region&quot;/&gt;</span><br><span class="line">  &lt;rdfs:range  rdf:resource=&quot;#Region&quot;/&gt;</span><br><span class="line">&lt;/owl:TransitiveProperty&gt;</span><br></pre></td></tr></table></figure><h3 id="3-3-6-owl-SymmetricProperty"><a href="#3-3-6-owl-SymmetricProperty" class="headerlink" title="3.3.6 owl:SymmetricProperty"></a>3.3.6 owl:SymmetricProperty</h3><p><code>owl:SymmetricProperty</code> 表示如果 $(x,y)$ 是 P 的实例，那么 $(y,x)$ 也是 P 的实例。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:SymmetricProperty rdf:ID=&quot;friendOf&quot;&gt;</span><br><span class="line">  &lt;rdfs:domain rdf:resource=&quot;#Human&quot;/&gt;</span><br><span class="line">  &lt;rdfs:range  rdf:resource=&quot;#Human&quot;/&gt;</span><br><span class="line">&lt;/owl:SymmetricProperty&gt;</span><br></pre></td></tr></table></figure><h2 id="3-4-Individuals"><a href="#3-4-Individuals" class="headerlink" title="3.4 Individuals"></a>3.4 Individuals</h2><p>个体分为两种：</p><ol><li>类的成员和个体的属性值</li><li>个体身份</li></ol><h3 id="3-4-1-类的成员和个体属性值"><a href="#3-4-1-类的成员和个体属性值" class="headerlink" title="3.4.1 类的成员和个体属性值"></a>3.4.1 类的成员和个体属性值</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;Opera rdf:ID=&quot;Tosca&quot;&gt;</span><br><span class="line">  &lt;hasComposer rdf:resource=&quot;#Giacomo_Puccini&quot;/&gt;</span><br><span class="line">  &lt;hasLibrettist rdf:resource=&quot;#Victorien_Sardou&quot;/&gt;</span><br><span class="line">  &lt;hasLibrettist rdf:resource=&quot;#Giuseppe_Giacosa&quot;/&gt;</span><br><span class="line">  &lt;hasLibrettist rdf:resource=&quot;#Luigi_Illica&quot;/&gt;</span><br><span class="line">  &lt;premiereDate rdf:datatype=&quot;&amp;xsd;date&quot;&gt;1900-01-14&lt;/premiereDate&gt;</span><br><span class="line">  &lt;premierePlace rdf:resource=&quot;#Roma&quot;/&gt;</span><br><span class="line">  &lt;numberOfActs rdf:datatype=&quot;&amp;xsd;positiveInteger&quot;&gt;3&lt;/numberOfActs&gt; </span><br><span class="line">&lt;/Opera&gt;</span><br></pre></td></tr></table></figure><h3 id="3-4-2-个体身份"><a href="#3-4-2-个体身份" class="headerlink" title="3.4.2 个体身份"></a>3.4.2 个体身份</h3><p>通常我们会给不同的事物取不同的名字，但是我们并不能保证不重名。比如“苹果”既可以是电子产品，也可以是水果。为了对个体的身份进行区分或合并，OWL 也设计了一套词汇：</p><ul><li><code>owl:sameAs</code>：表明是相同的个体，只是名字不同</li><li><code>owl:differentFrom</code>：表明是两个不同的个体</li><li><code>owl:AllDifferent</code>：表明列表中所有的个体都不相同</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;rdf:Description rdf:about=&quot;#William_Jefferson_Clinton&quot;&gt;</span><br><span class="line">  &lt;owl:sameAs rdf:resource=&quot;#BillClinton&quot;/&gt;</span><br><span class="line">&lt;/rdf:Description&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;Opera rdf:ID=&quot;Don_Giovanni&quot;/&gt;</span><br><span class="line"></span><br><span class="line">&lt;Opera rdf:ID=&quot;Nozze_di_Figaro&quot;&gt;</span><br><span class="line">  &lt;owl:differentFrom rdf:resource=&quot;#Don_Giovanni&quot;/&gt;</span><br><span class="line">&lt;/Opera&gt;</span><br><span class="line"></span><br><span class="line">&lt;Opera rdf:ID=&quot;Cosi_fan_tutte&quot;&gt;</span><br><span class="line">  &lt;owl:differentFrom rdf:resource=&quot;#Don_Giovanni&quot;/&gt;</span><br><span class="line">  &lt;owl:differentFrom rdf:resource=&quot;#Nozze_di_Figaro&quot;/&gt;</span><br><span class="line">&lt;/Opera&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:AllDifferent&gt;</span><br><span class="line">  &lt;owl:distinctMembers rdf:parseType=&quot;Collection&quot;&gt;</span><br><span class="line">    &lt;Opera rdf:about=&quot;#Don_Giovanni&quot;/&gt;</span><br><span class="line">    &lt;Opera rdf:about=&quot;#Nozze_di_Figaro&quot;/&gt;</span><br><span class="line">    &lt;Opera rdf:about=&quot;#Cosi_fan_tutte&quot;/&gt;</span><br><span class="line">    &lt;Opera rdf:about=&quot;#Tosca&quot;/&gt;</span><br><span class="line">    &lt;Opera rdf:about=&quot;#Turandot&quot;/&gt;</span><br><span class="line">    &lt;Opera rdf:about=&quot;#Salome&quot;/&gt;</span><br><span class="line">  &lt;/owl:distinctMembers&gt;</span><br><span class="line">&lt;/owl:AllDifferent&gt;</span><br></pre></td></tr></table></figure><h1 id="4-结语"><a href="#4-结语" class="headerlink" title="4. 结语"></a>4. 结语</h1><p>关于 RDFS 和 OWL 的词汇总结我们就介绍这么多。当然，这些都只是一小部分，要想看完整版的推荐看 w3c 的官方文档。我们总结出来的这些词汇是比较常用的，同时也是有助于帮助不了解本体，不了解知识建模的同学对这些东西有一个大体的概念。其实本体建模就是在构建一套逻辑体系，这套逻辑体系帮助计算机进行逻辑推理。而无论是 RDFS 还是 OWL 亦或是其他众多我们没有介绍的词汇表都是在尝试将这样一个逻辑体系进行标准化。先阶段计算机的逻辑推理能力仍然处于很弱的阶段，说明我们现在的工作仍然很初级。我们这里总结的相关内容也许在不久的将来就会过期，失效甚至被推翻。但是了解这些知识也有助于我们对未来的发展有一个清晰的认知。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.w3.org/TR/rdf-schema/" target="_blank" rel="noopener">RDF Schema 1.1</a></li><li><a href="https://www.w3.org/TR/2004/REC-owl-ref-20040210/" target="_blank" rel="noopener">OWL Web Ontology Language</a> </li><li><a href="https://fusion.cs.uni-jena.de/fusion/blog/2016/11/18/iri-uri-url-urn-and-their-differences/" target="_blank" rel="noopener">IRI, URI, URL, URN and their differences</a>, <em>JAN MARTIN KEIL</em> </li><li><a href="https://zhangzifan.com/t/7393.html" target="_blank" rel="noopener">浅谈什么是 URL、URI、IRI、URN 及之间的区别</a>, <em>张子凡</em> </li><li>语义网技术体系 [瞿裕忠，胡伟，程龚 编著] 2015年版</li><li><a href="https://www.jianshu.com/p/9e2bfa9a5a06" target="_blank" rel="noopener">知识图谱-浅谈RDF、OWL、SPARQL</a>, <em>吕不韦</em> </li></ol>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KG </tag>
            
            <tag> knowledge-modelling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>预训练语言模型：CoVe</title>
      <link href="/2021/08/25/ptm-cove/"/>
      <url>/2021/08/25/ptm-cove/</url>
      
        <content type="html"><![CDATA[<p><img src="https://aylien.com/images/uploads/general/tumblr_inline_o8tinsmw081u37g00_540.png" alt></p><p>上一篇文章我们介绍了预训练词向量，它的缺点很明显：一旦训练完，每个词的词向量都固定下来了。而我们平时生活中面临的情况却复杂的多，一个最重要的问题就是一词多义，即同一个词在不同语境下有不同的含义。<a href="https://arxiv.org/pdf/1708.00107.pdf" target="_blank" rel="noopener">CoVe（Contextual Word Vectors）</a>同样是用来表示词向量的模型，但不同于 <a href="https://rogerspy.github.io/2021/08/11/ptm-word-embedding/" target="_blank" rel="noopener">word emebdding</a>，它是将整个序列作为输入，根据不同序列得到不同的词向量输出的函数。也就是说，CoVe 会根据不同的上下文得到不同的词向量表示。</p><a id="more"></a><h1 id="1-神经网络机器翻译"><a href="#1-神经网络机器翻译" class="headerlink" title="1. 神经网络机器翻译"></a>1. 神经网络机器翻译</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/nmt-recap.png" alt></p><p>上图是一个经典的 attention seq2seq 模型：</p><ul><li><p>源语言 $x = [x_1, x_2, …, x_n]$;</p></li><li><p>目标语言：$y = [y_1, y_2, …, y_m]$;</p></li><li><p>用 <a href="https://rogerspy.github.io/2021/08/11/ptm-word-embedding/" target="_blank" rel="noopener">GloVe</a> 将源语言的词转换成词向量；</p></li><li><p>编码器是 bi-LSTM，输出一个隐状态序列：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}h &= [h_1, h_2, ..., h_n] \\  &= \text{bi-LSTM}(\text{GloVe}(x))\end{aligned}\end{equation}</script><p>其中 $h_t = [\overrightarrow{h_t}; \overleftarrow{h_t}]$，$\overrightarrow{h_t}=\text{LSTM}(x_t, \overrightarrow{h}_{t-1})$；$\overleftarrow{h_t}=\text{LSTM}(x_t, \overleftarrow{h}_{t-1})$。</p></li><li><p>注意力加持的解码器：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}\text{decoder hidden state:} \quad s_t &= \text{LSTM}([z_{t-1};\tilde{h}_{t-1}], s_{t-1}) \\\text{attention weights:} \quad \alpha_t &= \text{softmax}(H(W_1s_t+b_1)) \\\text{context-adjusted hidden state:} \quad \tilde{h}_t &= \tanh(W_2[H^\top \alpha_t; s_t]+b_2) \\\text{decoder output: } \quad p(y_t|H, y_1, ..., y_{t-1}) &=\text{softmax}(W_{out}\tilde{h}_t+b_{out})\end{aligned} \end{equation}</script></li></ul><p>seq2seq 训练完成之后，将编码器的输出作为 CoVe 用于下游任务。</p><h1 id="2-CoVe-在下游任务中的应用"><a href="#2-CoVe-在下游任务中的应用" class="headerlink" title="2. CoVe 在下游任务中的应用"></a>2. CoVe 在下游任务中的应用</h1><p>seq2seq 编码器的隐状态作为下游任务的语义向量：</p><script type="math/tex; mode=display">\text{CoVe}(x) = \text{bi-LSTM}(\text{GloVe}(x))</script><p>论文中提出将 GloVe 和 CoVe 进行拼接用于问答和分类任务。GloVe 是通过词共现比例学习到的向量，因此它没有句子上下文。而 CoVe 是通过处理文本序列学习到的向量，本身就具有上下文信息：</p><script type="math/tex; mode=display">v = [\text{GloVe}(x);\text{CoVe}(x)]</script><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220117101548.png" alt></p><p>给定下游任务，首先将输入的词用上面的方法转化成向量，然后输入到特定任务模型中进行训练。</p><h1 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h1><p>CoVe 的缺点是显而易见的：</p><ol><li>因为预训练过程是有监督训练，所以训练效果严重依赖标注数据（平行语料）；</li><li>CoVe 的性能受限于特定任务的模型结构。</li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/pdf/1708.00107.pdf" target="_blank" rel="noopener">Learned in Translation: Contextualized Word Vectors</a>， <em>Bryan McCann，James Bradbury，Caiming Xiong，Richard Socher. 2017</em></li><li><a href="https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html" target="_blank" rel="noopener">Generalized Language Models</a>, <em>Jan 31, 2019 by Lilian Weng</em></li></ol>]]></content>
      
      
      <categories>
          
          <category> 语言模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Language Model </tag>
            
            <tag> 词向量 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知识图谱：知识建模（二）构建本体的方法论</title>
      <link href="/2021/08/23/kg-build-ontology-method/"/>
      <url>/2021/08/23/kg-build-ontology-method/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/kgicon.png" alt></p><h1 id="1-什么是本体？"><a href="#1-什么是本体？" class="headerlink" title="1. 什么是本体？"></a>1. 什么是本体？</h1><p>“本体”（<em>ontology</em>）的概念来源于哲学对本体论的研究。随着人工智能（AI）的发展，科学家们将“本体”这一概念引入到计算机领域。不同的文献对本体有着不同的定义，甚至有些定义是相互矛盾的。为了方便起见，我们将本体定义为：<strong>本体是一系列词汇，这些词汇包括机器可读的概念定义和概念之间的关系</strong>。</p><a id="more"></a><ul><li><em>Classes</em>： 类别，或者概念（<em>concepts</em>）表示具体领域内的一些抽象概念，比如“酒”，“人”等。<em>Class</em> 是本体的核心。</li><li><em>Subclasses</em>：表示，大类别下面的子类。比如“酒”的子类包括“白酒”、“红酒”等。</li><li><em>Instances</em>：实例，表示抽象概念下的具体事物。比如“白酒”的实例包括“红星二锅头”、“飞天茅台”等。</li><li><em>Properties</em>：属性，表示的是概念的不同特征和属性。<em>OWL</em> 中的 <em>property</em> 实际上表示的就是关系。主要包括两种关系：<em>object property</em> 和 <em>data property</em> 。<em>Object property</em> 表示两个实体之间的关系，比如小明和小红是兄妹关系，其中“兄妹”就是“小明”和“小红”两个实体的 <em>object property</em>；<em>data property</em> 表示实体属性，比如小明的年龄是12岁，其中“姓名”就是“小明”这个实体的 <em>data property</em>。除此以外， W3C 还规定了一种标注属性（<em>annotation property</em>），它表示一些实体的注释元信息，用来对实体进行注释说明。</li><li><em>slots</em>：槽，可以认为就是实体具体属性，比如“小明的年龄是12岁”，其中 “年龄” 就是一个 <em>slot</em>，而 “12岁” 就是 <em>slot value</em>。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/image-20210729171714117.png" alt></p><p>总结一下，本体需要包含以下要素：</p><ul><li>定义本体类别</li><li>构建各类别之间的层级关系（父类 -&gt; 子类）</li><li>定义 <em>slot</em> 以及可接受的值，比如年龄 <em>slot</em> 必须是数字</li><li>在实例中对 <em>slot</em> 进行填充</li></ul><h1 id="2-为什么需要本体？"><a href="#2-为什么需要本体？" class="headerlink" title="2. 为什么需要本体？"></a>2. 为什么需要本体？</h1><ul><li>共享人类或者计算机理解的信息结构。假设多个的网页包含不同的医疗信息，如果这些网页能够共享相同的本体，那么计算机可以轻易从这些网页抽取有用信息提供给用户。不仅如此，计算机还可以聚合不同来源的信息，准确回答用户的问题。</li><li>领域知识的复用。举个例子，很多地方都会需要时间信息，包括时间段、时间点、相对时间等等。如果有人能够构建一个关于时间的本体，那么其他人就可以轻易将这个本体应用到自己的领域。另外，当我们自己构建本体的时候，也可以使用已有的本体知识在上面进行扩充或者缩减等。</li><li>明确领域假设。相当于我们将某一领域内的知识点利用一些假设关系相互串联起来，使我们对整个领域有更加清晰准确的认识，尤其是对一些新人。</li><li>将领域知识与可操作性的知识分离。这就有点类似于我们在设计一款产品的时候，我们将具体的产品和组件分离开来（模块化）。比如手机，多年前的手机充电器，基本上是一个品牌甚至同一个品牌的不同型号手机就有一个充电器，充电器不同共用，相当于手机和充电器是深度绑定的。后来为了解决这种深度绑定带来的各种问题，业内开始制定统一标准实现充电器与手机分离，一个充电器可以使用不同的充电器，而一个充电器可以给不同的手机充电。其中充电器标准就可以认为是领域知识本体，而手机就是可操作数据。</li><li>领域知识分析。当我们要复用和扩展领域知识的时候，这些领域知识元素就会变得非常有价值，说白了其实还是避免重复造轮子。</li></ul><p>通常定义领域的本体并不是我们的最终目的。开发本体类似于定义一组数据及其结构以供其他程序使用。 解决问题的方法、独立于领域的应用程序和软件代理使用从本体构建的本体和知识库作为数据。 例如，在本文中，我们开发了葡萄酒和食物的本体以及葡萄酒与膳食的适当组合。 然后，该本体可以用作一套餐厅管理工具中某些应用程序的基础：一个应用程序可以为当天的菜单创建葡萄酒建议或回答服务员和顾客的查询。 另一个应用程序可以分析酒窖的库存清单，并建议扩展哪些葡萄酒类别以及为即将到来的菜单或食谱购买哪些特定的葡萄酒。</p><h1 id="3-如何构建本体？"><a href="#3-如何构建本体？" class="headerlink" title="3. 如何构建本体？"></a>3. 如何构建本体？</h1><p>现实中，并没有一个标准的、统一的本体构建方法。本文只是讨论一种比较通用的方法：先构建比较粗糙、大粒度的本体，然后不断的迭代细化。</p><p>在详细介绍构建本体的流程之前，我们先强调本体设计时的一些规则，虽然看起来有些教条，但是在很多情况下这些规则确实能帮助我们。</p><ul><li>没有一个标准的、统一的本体构建方法。最好的方法就是根据实际业务需求去构建。</li><li>本体构建是一个需要不断迭代的过程</li><li>本体中的概念和关系必须是一些比较相近的对象（无论是物理上还是逻辑上）。这些对象可能是某领域内描述性句子中的名词或者动词。</li></ul><p>接下来，我们以构建酒类和食物领域的本体为例，介绍构建本体的方法。</p><h2 id="3-1-第一步、确定本体的领域和范围"><a href="#3-1-第一步、确定本体的领域和范围" class="headerlink" title="3.1 第一步、确定本体的领域和范围"></a>3.1 第一步、确定本体的领域和范围</h2><p>构建本体的第一步是确定领域和范围，因此我们需要回答下面几个问题：</p><blockquote><ul><li>我们要构建什么领域的本体？</li><li>这些本体用来做什么？</li><li>这些本体可以回答什么问题？</li><li>谁会使用这些本体？</li></ul></blockquote><p>在本体设计过程中，这些问题的答案可能会发生变化，但是任何时候这些问题都可以帮助我们限定本体范围。</p><p>考虑酒类和食物的例子。首先我们已经确定要构建酒类和食物的本体了，我们的目的是用这些本体来推荐一些好的食物和酒的搭配。</p><p>那么显然，不同酒类的概念，食物类型的概念，以及酒和食物的搭配就必须包含在我们的本体中。同时，在我们的本体中不太可能包括管理酒厂库存或餐厅员工的概念，即使这些概念与酒和食物的概念有些相关。</p><p>如果我们的本体是用来帮助酒类杂志文章进行自然语言处理，那么词性、同义词等自然语言信息可能就会变得非常重要。如果本体用于帮助餐厅顾客决定订购哪种酒，我们需要包括零售定价信息。如果用于酒品买家储存酒窖，则可能需要批发定价和可用性信息。如果本体描述语言不同于本体用户语言，我们可能需要提供语言之间的映射。</p><h3 id="能力问题（competency-questions）"><a href="#能力问题（competency-questions）" class="headerlink" title="能力问题（competency questions）"></a>能力问题（competency questions）</h3><p>确定本体范围的方法之一就是勾勒出基于本体的知识库能够回答的问题（<a href="https://www.semanticscholar.org/paper/Methodology-for-the-Design-and-Evaluation-of-Gruninger/497abc0ddace6a7772a5f5a3edb3d7b751476755" target="_blank" rel="noopener">Gruninger and Fox 1995</a>）。这些问题能帮助我们确定我们是否有足够的信息去回答这些问题，本体粒度都不够，覆盖的范围全不全。只需要一些大致的问题即可，无需穷举。</p><p>在我们的例子中，我们可能包含以下能力问题：</p><blockquote><ul><li>当我挑选酒品的时候应该考虑什么？</li><li>Cabernet Sauvignon 适合搭配海鲜吗？</li><li>什么酒与烤肉最配？</li><li>酒类的哪些特点会影响与食物的搭配？</li><li>特定的酒的香气或者酒本身会随着时间发生变化吗？</li><li>Napa Zinfandel 最好的年份？</li></ul></blockquote><p>从上面的问题中可以总结出，在我们的本体中石少应该包含：不同酒的特点、酒的类型、年份（及其品质的好坏）、食物分类以及酒和食物的搭配。</p><h2 id="3-2-第二步、现有本体的复用"><a href="#3-2-第二步、现有本体的复用" class="headerlink" title="3.2 第二步、现有本体的复用"></a>3.2 第二步、现有本体的复用</h2><p>程序员的圣经之一就是“不要重复造轮子”。查找已有的可用本体是一件非常重要的事情。网上有很多相关的资源，下面列举一些比较重要的资源（大多是英文的资源，中文开放本体资源目前还比较少）：</p><blockquote><ul><li><p>OpenKG</p><p>OpenKG是最大的中文开放知识图谱库，其中包含了很多本体。</p><p>地址：<a href="http://openkg.cn/home" target="_blank" rel="noopener">http://openkg.cn/home</a></p></li><li><p>Protege Ontology Library</p><p>地址：<a href="https://protegewiki.stanford.edu/wiki/Protege_Ontology_Library" target="_blank" rel="noopener">https://protegewiki.stanford.edu/wiki/Protege_Ontology_Library</a></p></li><li><p>Ontolingua ontology library</p><p>地址：<a href="http://www.ksl.stanford.edu/software/ontolingua/" target="_blank" rel="noopener">http://www.ksl.stanford.edu/software/ontolingua/</a></p></li><li><p>DAML ontology library</p><p>地址：<a href="http://www.daml.org/ontologies/" target="_blank" rel="noopener">http://www.daml.org/ontologies/</a></p></li><li><p>UNSPSC</p><p>地址：<a href="https://www.unspsc.org" target="_blank" rel="noopener">https://www.unspsc.org</a></p></li><li><p>RosettaNet</p><p>地址：<a href="https://www.rosettanet.org" target="_blank" rel="noopener">https://www.rosettanet.org</a></p></li><li><p>DMOZ</p><p>地址：<a href="https://www.dmoz.org" target="_blank" rel="noopener">https://www.dmoz.org</a></p></li></ul></blockquote><p>实际上现在确实有酒类开放实体可用，但是我们假设不存在，从头构建一个酒类本体。</p><h2 id="3-3-第三步、枚举本体中的重要对象"><a href="#3-3-第三步、枚举本体中的重要对象" class="headerlink" title="3.3 第三步、枚举本体中的重要对象"></a>3.3 第三步、枚举本体中的重要对象</h2><p>当提到一个对象的时候你会讨论些什么？这些对象有什么属性？关于这个对象你会说些什么？思考这些问题对我们构建本体是非常有用的。我们可以把这些对象写成一个列表记录下来，比如提到“酒”，你会想到“葡萄”、“酿酒厂”、“原产地”、“酒的颜色”、“口感”、“含糖量”、“酒精含量”等等。而提到“食物”，我们通常会想到“鱼”、“虾“、”肉“、”蛋“、”奶“等等。起初，对于对象的一个综合理解更重要，无需过于关注概念和关系之间的相互覆盖。</p><h2 id="3-4-第四步、定义类和类的层级结构"><a href="#3-4-第四步、定义类和类的层级结构" class="headerlink" title="3.4 第四步、定义类和类的层级结构"></a>3.4 第四步、定义类和类的层级结构</h2><p>有几种不同的方法定义类的层级结构（<a href="https://www.cambridge.org/core/journals/knowledge-engineering-review/article/abs/ontologies-principles-methods-and-applications/2443E0A8E5D81A144D8C611EF20043E6" target="_blank" rel="noopener">Uschold &amp; Gruninger 1996</a>）：</p><ul><li><strong>自上而下</strong>的方法是先定义领域内最通用的顶层概念，然后依次向下扩展。比如，我们先定义两个类别：“酒”和“食物”。然后定义酒的子类：白酒、红酒、玫瑰酒等等。然后对红酒进一步分类：<em>Syrah</em>，<em>Red Burgundy</em>，<em>Cabernet Sauvignon</em> 等等。对“食物”也是如此。</li><li><strong>自下而上</strong>的方法是先定义一些具体的类别，然后讲这些类别聚合成更加通用的类别。比如“衡水老白干”和“红星二骨头”可以聚合成“白酒”。另外，“拉菲”，“桃乐丝”可以聚合成“红酒”。而“白酒”和“红酒”可以聚合成“酒”。</li><li><strong>上下结合</strong>的方法是先定义一些比较重要的类，然后向上聚合和向下扩展。相当于将上面两种方法结合在一起。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/image-20210802113435286.png" alt></p><p>这三种方法中，没有一种是一定比其他两种更好的方法。最终采取哪种方法取决于开发人对领域的认知。如果开发人员对领域有着系统性的了解，那么采取自上而下的方法应该是首选。通常对于大多数的开发人员来说，上下结合的方法是比较适合的，因为多数人对领域都是有一定的了解而又了解不深。所以可以通过先构建一些比较通用的本体，然后再从实例中进行总结向上补充的方法会比较合适。</p><p>比如，多数人都知道酒可以分成“白酒”、“红酒”、“鸡尾酒”等，经常看广告也可以知道“白酒”有“酱香型”、“浓香型”对于其他的香型不太了解。而对于具体的酒进行总结归类，就可以发现，原来“酒鬼酒”是馥郁香型的，那么我们就可以将“馥郁香型”补充到酒类香型的层级上去。</p><p>无论是那种方法，都是从定义类开始的。第三步中，我们列举出了一些对象，现在我们可以从列表中的选择那些用于描述独立存在的对象作为类别，以这些对象作为锚点构建层级关系。一个重要的假设如下：</p><blockquote><p>  <em>如果类别 A 是类别 B 的父类，那么属于 B 类的所有实例也同样是类别 A 的实例。</em></p></blockquote><h2 id="3-5-第五步、定义类的属性——slots"><a href="#3-5-第五步、定义类的属性——slots" class="headerlink" title="3.5 第五步、定义类的属性——slots"></a>3.5 第五步、定义类的属性——<em>slots</em></h2><p>单纯的类别不足以为回答能力问题提供足够的信息，一旦我们用以上的方法定义了类别之后，需要为这些类别提供额外的信息，比如酒的颜色、口感、含糖量、产地等。这些信息就是类别的属性。</p><p>通常，一下集中类型的对象可以成为本体的属性：</p><ul><li>内秉属性，比如：颜色、口感、含糖量等；</li><li>外部属性，比如：产地、酒名等；</li><li>子结构，如果一个对象是结构化的，那么它的实体结构和抽象结构都可以成为它的属性；</li><li>与其他对象的关系。比如，酒的厂家、酒的原料等。</li></ul><p>所有子类都要继承父类的属性。比如“酒”的属性包括“厂家”、“颜色”、“口感”、“含糖量”、“产地”等，那么“酒”的子类“白酒”也要继承这些属性。因此，定义 <em>slots</em> 的时候通常是附加在具有该属性的最顶级类别上。</p><h2 id="3-6-第六步、定义-slots-的刻面"><a href="#3-6-第六步、定义-slots-的刻面" class="headerlink" title="3.6 第六步、定义 slots 的刻面"></a>3.6 第六步、定义 <em>slots</em> 的刻面</h2><p><em>slots</em> 的刻面包括 <em>slots</em> 基数、数据类型、定义域和值域等。比如酒的名称（<em>name</em>）是一个字符串类型的数据，酒厂生产（<em>produces</em>）酒，这些酒是具体的酒的实例，因此其对应的数据类型应该是实例（<em>instance</em>）。</p><ul><li><p>基数（<em>cardinality</em>）</p><p><em>slot</em> 基数定义了一个 <em>slot</em> 可以有多少个值。有些 <em>slot</em> 只能至多有一个值，而有些则可以有多个值。比如一种酒只能有一种颜色，却可以有多个产地。</p><p>有些系统会规定 <em>slot</em> 基数的最大值和最小值。最小值 <em>N</em> 表明该 <em>slot</em> 至少有 <em>N</em> 个值，比如葡萄酒的原料葡萄 <em>slot</em> 最小值为 1，表明该种葡萄酒的原料中至少包含一种葡萄。最大值 <em>M</em> 表明该 <em>slot</em> 最多有 <em>M</em> 个值。比如葡萄酒的原料葡萄 <em>slot</em> 最大值为 2，表明该种葡萄酒最多有两种不同品种的葡萄酿制而成。如果最大值最小值都是 1，说明这种葡萄酒就是 1 种葡萄酿制而成的。所有时候将最大值设置成 0 也是非常有用的，表明对于某些特定的子类没有任何值满足条件。</p></li><li><p>数据类型</p><p>数据类型定义了 <em>slot</em> 的数据类型。</p><ul><li>字符串（<em>string</em>）：最简单，最常用的数据类型。</li><li>数字（<em>number</em>）：数值类型的 <em>slot</em>，比如年龄、价格等。</li><li>布尔型（<em>boolean</em>）：<em>yes</em> 或者 <em>no</em>，<em>true</em> 或者 <em>false</em> 等</li><li>可枚举（<em>enumerated</em>）：给定 <em>slot</em> 可取到的值的列表，比如酒的口味可以是 [“重”，“中等”，“清淡”] 中的任意一种，而不能超过这三种的范围。</li><li>实例类型（<em>instance</em>）：<em>slot</em> 允许定义两个单独实体的关系，但是必须定义清楚哪些类别的实体是可以作为 <em>slot</em> 的值。比如“酒”这个类别，可以作为 “<em>produces</em>” 的值。</li></ul></li><li><p>定义域和值域</p><p>允许使用实例类型作为 <em>slot</em> 的类别称之为值域，比如 “酒” 作为 “<em>produces</em>” 的 <em>slot</em> 值，“酒” 就是  “<em>produces</em>” 的值域。简单来说，就是 $x \rightarrow y$，其中 $y$ 的所有实例数据类型的取值就是值域。</p><p>而定义域就是 <em>slot</em> 描述的对象。比如 “酿酒厂”，“<em>produces</em>”，“酒”，其中 “酿酒厂” 就是定义域。是简单来说，就是 $x \rightarrow y$，其中 $x$​ 的取值范围就是定义域。</p><p>确定定义域和值域的规则是相似的：</p><blockquote><ul><li>找到最通用的类或者最具代表性的类别；</li><li>另一方面，不要将定义域和值域定义得范围太大，定义域中所有的值都可以被 <em>slot</em> 描述，值域中的所有值应该是 <em>slot</em> 的潜在填充值。不要选择过于笼统的类别，而是应该选择涵盖所有填充值的类别。</li></ul></blockquote><p>比如，“酿酒厂”，“<em>produces</em>” 的值域不应该是所有“酒” 的子类（“白酒”，“啤酒”，“红酒”等），而直接就是“酒”。同时，也不应该将“酒”进一步泛化到 “<em>THING</em>”。</p><p>具体来讲：</p><blockquote><p>  <em>如果 slot 的值域或者定义域包括一个类别以及该类别下的子类，那么将其子类全部删掉</em></p></blockquote><p>比如，一个 <em>slot</em> 的值域包括“酒”和“红酒”，那么应该将“红酒”删掉，因为“红酒”是“酒”的子类。</p><blockquote><p>  <em>如果 slot 的值域或者定义域包含了 A 类的所有子类，但不包含 A 类本身，那么值域应该只包含 A 类本身而不包括其子类。</em></p></blockquote><p>比如，一个 <em>slot</em> 的值域是“白酒”、“红酒”、“啤酒”等，我们可以将值域设为“酒”本身。</p><blockquote><p>  <em>如果 slot 的值域或者定义域包含类别 A 中除少数子类以外的所有子类，那么我们应该考虑将类别 A 本身进行重新定义。</em></p></blockquote><p>将一个 <em>slot</em> 挂在到一个类别上，和将该类别设为 <em>slot</em> 的定义域是完全等价的。一方面，我们应该尽可能泛化，另一方面我们应该保证 <em>slot</em> 对应的类别确实有相应的属性。总之一句话，我们既要一个都不差，也要避免张冠李戴。</p></li></ul><h2 id="3-7-第七步、构建实例"><a href="#3-7-第七步、构建实例" class="headerlink" title="3.7 第七步、构建实例"></a>3.7 第七步、构建实例</h2><p>最后一步就是根据我们建立的类别层级结构构建实例。定义一个实例需要：</p><ol><li>选择一个类别；</li><li>创建该类的单一实例</li><li>填充 <em>slot</em> 值</li></ol><p>比如，我们创建一个 <em>飞天茅台</em> 的实体用来表示 “白酒” 的实例。它的属性如下：</p><blockquote><p>  酒精度：53%</p><p>  颜色：无色透明</p><p>  香气：幽雅细腻</p><p>  口味：回味悠长</p><p>  产地：贵州省仁怀市</p><p>  生产商：贵州茅台酒股份有限公司</p></blockquote><h1 id="4-定义类别和类别层级结构"><a href="#4-定义类别和类别层级结构" class="headerlink" title="4. 定义类别和类别层级结构"></a>4. 定义类别和类别层级结构</h1><p>本节讨论定义类别和类别层级结构时需要注意的点和容易出现的错误。对于任意领域来说都没有一个唯一正确的层级结构。我们所定义的层级结构依赖于我们要怎样使用本体，应用中必要的细节，个人喜好以及有时候可能还需要与其他模型进行兼容。但是我们还是要讨论一些开发层级结构时的一些指南，当我们开发完新的层级结构以后，回过头重新审视我们的定义是否满足这些指南，可以帮助我们避免很多错误。</p><h2 id="4-1-保证类别层级结构的正确性"><a href="#4-1-保证类别层级结构的正确性" class="headerlink" title="4.1 保证类别层级结构的正确性"></a>4.1 保证类别层级结构的正确性</h2><ul><li><p>“is-a” 关系</p><p>如果类别 A 中的所有实例同时也是类别 B 的实例，此时我们就说类别 A 是类别 B 的子类，我们就可以定义关系（A，is-a，B）。比如，（“茅台酒”，“is-a”，“白酒”）。另一个也可以表示这种关系的是 “kind-of”，（“茅台酒” ，“kind-of”，“白酒”），（“肉”，“kind-of”，“食物”）等等。</p></li><li><p>单一的酒不是所有酒的子类</p><p>一个常见的错误是，在层级结构中包含同一个概念的单数版本和复数版本，然后令单数版本是复数版本的子类。比如，定义 “<em>wine</em>” 是 “<em>wines</em>” 的子类。然而这种关系是错误的。为了避免这种情况发生，在给类别命名的时候最好都采用单数形式或者都采用复数形式（第六节中讨论类别命名）。</p></li><li><p>层级关系的可传递性</p><p>满足以下条件的关系是可传递的：</p><blockquote><p>  <em>如果 B 是 A 的子类，C 是 B 的子类，那么 C 也是 A 的子类。</em></p></blockquote><p>比如，我们定义一个类别是 “酒”，然后定义 “白酒” 是 “酒” 的子类。然后再定义 “茅台酒” 是 “白酒” 的子类。那么可传递性表示 “茅台酒” 也是 “酒” 的一个子类。有时候我们会区分直接子类和间接子类。直接子类表示在层级结构中两个类别之间没有其他子类，即某一类别与其父类直接相连。而间接子类就是需要一个中间子类再与父类相连。实际上该子类也是中间父类的直接子类。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/image-20210803104950521.png" alt></p></li><li><p>类别层级结构的演化</p><p>随着定义域的变化，要维护一个不变的层级结构可能会是一件很困难的事。比如通常我们见到的 “茅台酒” 是无色透明的，所以我们将 “茅台酒” 定义为 “白酒” 的子类。但是有可能在未来的某一天，酒厂发明了一种新的酿酒技术使得酒变成了黄色或者红色。此时，我们再将 “茅台酒” 归类到 “白酒” 里面可能就不太合适了。（这个例子实际上并不是很典型，一个比较典型的例子是组织结构的本体。组织结构的变动是很频繁的，一些部门今天还在，明天可能就取消了。）</p></li><li><p>类别及其名字的区别</p><p>区分类别和它的名字是至关重要的，通常也很难被注意到。</p><blockquote><p>  类别代表的是某一领域内的概念本身，而不是代表这个概念的几个单词。</p></blockquote><p>我们选择的术语不同，其类别名就会发生变化，但实际上不同的术语表示的是同一个概念。比如 “克劳修斯表述”，“开尔文表述”，“熵增定律” 等等虽然名字不同，但都表示 “热力学第二定律” 这一概念。只是各自的名字不同罢了。再比如 “飞人乔丹”，“乔帮主” 都可以表示 “迈克尔·乔丹” 这个概念。</p><p>现实情况下，我们应该遵循以下规则：</p><blockquote><p>  <em>表示相同概念的同义词不可代表不同的类别</em></p></blockquote><p>同义词仅仅是相同概念的不同术语而已，因此，我们不能使用同义词来命名不同的类别。很多本体系统允许将同义词与表示类别名称相关联。比如，我们可以定义 “<em>same_as</em>” 关系，将“熵增定律”、“克劳修斯表述”和“开尔文表述”都关联到“热力学第二定律”上。如果不允许这种关联，则应该在类别文档中列出同义词。</p></li><li><p>避免类别套娃</p><p>类别层级结构中的循环指的是，类别 A 是类别 B 的子类的同时，类别 B 又是 类别 A 的子类，即两个类别互为子类和父类。是在构建层级结构的时候，我们应该避免出现这种情况。一旦出现这种情况就说明 A 和 B 是等价的：A 的所有实例也是 B 的实例，同时 B 的所有实例也是 A 的实例。</p></li></ul><h2 id="4-2-分析同级类别"><a href="#4-2-分析同级类别" class="headerlink" title="4.2 分析同级类别"></a>4.2 分析同级类别</h2><ul><li><strong>层次结构中的同级类别</strong></li></ul><p>同级类别（<em>siblings</em>）指的是具有相同直接父类的类别。</p><blockquote><p>  除了根节点，所有同级类别必须处于同一层。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/image-20210803143900031.png" alt></p><p>如图所示，“红酒”、“白酒”、“玫瑰酒” 同级别，“五粮液”、“鸭溪窖酒”、“贵阳大曲” 是同级别。</p><ul><li><strong>多少是太多？多少是太少？</strong></li></ul><p>并没有一个硬性指标规定一个类别至少应该有多少个直接子类。但是很多结构比较规范的本体中的类别通常有 2 个到 12 个直接子类。因此，我们可以有以下经验：</p><blockquote><p>  <em>如果一个给定类别只有一个直接子类，可能是本体建模有问题，或者本体不完全</em>；</p><p>  <em>如果一个给定类别的子类过多（超过 12 个），可能需要一些中间类别重新归类</em></p></blockquote><p>比如上图，如果我们构建的本体，“酒” 只有 “白酒” 一个直接子类，说明我们丢了 “红酒”、“玫瑰酒”等其他酒品。而如果把所有白酒都挂到 “白酒” 下面可能说明，我们对 “白酒” 的分类过于粗糙。因此，我们可以对 “白酒” 再进一步细分成 “酱香型”、“浓香型”、“清香型”等等。然后，再将对应的白酒挂上去。</p><h2 id="4-3-多继承"><a href="#4-3-多继承" class="headerlink" title="4.3 多继承"></a>4.3 多继承</h2><p>大多数知识表示系统都允许多继承：一个类别可以同时是多个类别的子类。比如，“啤酒”既可以是 “酒” 的直接子类，也可以是 “食物” 本体中 “调味料” 的直接子类。因此，“啤酒” 可以有两个父类：“酒” 和 “调味料”。“啤酒” 的所有实例同时也是 “酒” 和 “调味料” 的实例。当然，“啤酒” 也会同时继承 “酒” 和 “调味料” 的一些属性。</p><h2 id="4-4-什么时候应该（不应该）引入新的类别？"><a href="#4-4-什么时候应该（不应该）引入新的类别？" class="headerlink" title="4.4 什么时候应该（不应该）引入新的类别？"></a>4.4 什么时候应该（不应该）引入新的类别？</h2><p>本体构建最难的部分应该就是什么时候引入新的类别，或者什么时候通过不同的属性值加以区分。也就是说，对于一个新的对象，我们是把它归到已有的类别然后给予不同的属性，还是新建一个类别？如果新建过多类别，会造成类别过多，甚至会出现彼此嵌套。而如果是新加属性加以区分的话 ，又会造成属性过于复杂。如何找到一个平衡点并不容易。</p><p>为了寻找这样一个平衡点，我们可以设定一些规则：</p><blockquote><p>  一个子类通常需要满足以下条件之一：</p><ol><li>有一些父类不具备的属性；</li><li>与父类的限制条件不同；</li><li>与父类参与的关系类型不同</li></ol></blockquote><p>比如，“烤肉” 有一个属性 “几分熟”，但是其父类 “肉” 通常不会有这个属性。或者 “白酒” 的颜色限制为 “无色透明”（或者 “微黄透明”），而 “酒” 没有这个限制。换句话说，当我们想要描述的对象无法通过父类来描述的时候，就需要定义新的子类。</p><p>实际情况下，每个子类都应该有新的 <em>slot</em>，或者有新的 <em>slot</em> 值，又或者要覆盖原有的继承自父类的刻面。</p><p>有时候，没有新的属性的时候也可以引入新子类：</p><blockquote><p>  <em>术语层级结构不需要引入新的属性</em></p></blockquote><p>比如，电子病历系统基础的本体可以包括对各种疾病的分类。这个分类可能只是没有属性（或者有相同属性集）的术语层级结构。比如，“糖尿病”、“心脏病”、“高血压” 都是不带属性的，但是我们还是应该将这些术语分成不同的类，而不应该看成属性。</p><p>另一个无新增属性而要新建类别的情况是——约定俗成。某些领域内的对象，在领域专家眼中通常是区分对待的，我们构建的本体系统应该反映出领域专家对该领域的看法。因此，这种情况下，还是需要新增子类。</p><p>最后，我们不应该为每个附加的限制创建一个子类。比如，我们可以构建 “红酒”、“白酒”、“玫瑰酒” 的分类，但是不能构建 “扬州酒”、“贵州酒”、“法国酒” 等，单独根据产地属性的类别。</p><h2 id="4-5-新的类别或者属性值？"><a href="#4-5-新的类别或者属性值？" class="headerlink" title="4.5 新的类别或者属性值？"></a>4.5 新的类别或者属性值？</h2><p>当我们对一个领域进行建模的时候，通常需要考虑将某些对象定义为属性还是类别的问题。</p><p>我们是要定义 “白酒”、“红酒” 作为 “酒” 的子类，还是要将 “白酒”、“红酒” 作为 “酒” 的 “颜色” 属性值？这通常取决于我们构建的本体的范围。“白酒” 在你的领域内重要性如何？如果 “白酒” 只是为我们提供一些边缘信息，或者与其他对象没有很重要的关系，那么我们就不应该将 “白酒” 作为一个类别来对待。</p><blockquote><p>  <em>如果有不同 slot 的概念会变成其他类别的不同 slot 的限制，那么我们应该新建一个类别。否则我们就在属性中加以区分即可。</em></p><p>  换句话说就是，如果 slot 的值发生了变，会使得类别也发生变化的话，那么我们应该新建一个类别。 </p></blockquote><p>比如，“红啤”、“白啤”、“黑啤” 等，这几种酒确实不是同一种酒。</p><blockquote><p>  <em>如果领域内对对象的区分非常重要，并且我们将具有不同值的对象视为不同种类的对象，那么我们应该创建一个新类</em></p></blockquote><p>同时我们还应该注意：</p><blockquote><p>  <em>一个实例所属的类别不应该时常发生变动</em></p></blockquote><p>通常情况下，当我们用外部属性而不是内秉属性来划分类别的时候，实例所属的类别经常会发生变化。比如，“热牛奶” 和 “常温牛奶” 并不应该分成两个类别，而是应该把温度设置成 “牛奶” 的属性。</p><p>另外，数字、颜色、地点通常应该是属性而不是类别。但是，对于 “酒” 来说，颜色应该是一个很重要的分类标准，所以在 “酒” 的分类中颜色应该属于类别而不是属性。</p><p>另一个人体解剖学本体的例子。当我们表示 “肋骨” 的时候，是否应该将肋骨分成 “左侧第一根肋骨”、“左侧第二根肋骨” 等？或者我们将肋骨的顺序和位置当成属性？如果在我们的本体中每根肋骨承载的信息非常不同的话，我们么确实应该为每根肋骨构建一个类别。比如，如果我们想要对肋骨不同位置的邻接信息建模，以及运动过程中每根肋骨所起的作用，或者不同肋骨保护的不同器官等等，这时候我们就需要对每根肋骨新建一个类别。如果我们只是对人体解剖学进行大致建模，那么我们只需要构建一个 “肋骨” 的类别，然后把 “位置” 和 “顺序” 作为属性即可。</p><h2 id="4-6-实例还是类别？"><a href="#4-6-实例还是类别？" class="headerlink" title="4.6 实例还是类别？"></a>4.6 实例还是类别？</h2><p>决定一个对象是类别还是实例，还是取决于我们构建的本体的潜在应用场景。类别结束实例开始的位置决定了本体的细节粒度。比如，“酸奶” 应该算是一个类别，还是实例？如果作为类别，它下面还有 “成都老酸奶”、“青海老酸奶” 等更细粒度的类别，如果 “酸奶” 作为实例，那么就不需要区分 “成都老酸奶” 和 “青海老酸奶”了。</p><p>要确定本体的细节粒度，可以回到本体构建步骤的第一步——我们想要利用这个本体回答什么问题？</p><blockquote><p>  知识库中，单实例是粒度最细的概念。</p></blockquote><p>比如，如果我们关心的是是否易消化，那么 “酸奶”、“纯牛奶” 就可以作为实例，而如果还要考察 “酸奶” 的制作工艺，口味特点等。那么 “酸奶” 就需要成为一个类别。</p><p>另外，如果满足以下条件，则可以将实例转化成类别：</p><blockquote><p>  <em>如果一个概念天然有层级结构，那么我们应该把它当成类别。</em></p></blockquote><p>比如，“地球有七个大洲”，我们可以把 “七大洲”（“亚洲”，“欧洲”，…） 当成 “地球” 的实例，但是 “七大洲” 是由不同国家组成的。因此，通常我们把每个大洲作为类别，而不是实例。</p><p>需要注意的是，只有类别有值域。在知识表示系统中，不存在 “subinstance” 的概念。因此，如果我们还想对一个概念进行细分，即使概念本身没有任何实例，也要把它当成类别，而不是实例。</p><h2 id="4-7-限制本体范围"><a href="#4-7-限制本体范围" class="headerlink" title="4.7 限制本体范围"></a>4.7 限制本体范围</h2><blockquote><p>  <em>本体系统不需要包括领域内所有的信息：我们不需要细化或者泛化超过实际需求的本体。</em></p></blockquote><p>比如，如果我们的用处是酒和食物的单配，那么我们就不需要知道如何酿酒和如何烹饪。</p><blockquote><p>  <em>本体系统不需要包含所有的属性以及不同类别之间的区别。</em></p><p>  <em>本体系统不需要包含所有的关系</em></p></blockquote><h2 id="4-8-无交集子类"><a href="#4-8-无交集子类" class="headerlink" title="4.8 无交集子类"></a>4.8 无交集子类</h2><p>如果两个类别的实例中没有公共实例，我们就认为这两个类别是无交集类别。比如 “红酒” 和 “白酒” 就是无交集类别：没有一种酒即是红酒又是白酒。在构建本体系统的时候，我们可以指定两个类别是无交集类别。指定无交集类别的好处是可以使本体更好进行验证——如果不指定的话，我们要看两个类别是否有交集还要将两个类别的实例全部读取出来，然后求交集看是否为空。不仅浪费空间还浪费时间。</p><p>同时，如果我们指定 “红酒” 和 “白酒” 是无交集类别的话，在进行建模的时候，如果创建了一个多继承子类，父类中包括了 “红酒” 和 “白酒”，那么系统可以很快识别出建模错误。</p><h1 id="5-定义属性——更多细节"><a href="#5-定义属性——更多细节" class="headerlink" title="5. 定义属性——更多细节"></a>5. 定义属性——更多细节</h1><p>本节主要讨论可逆属性和属性默认值。</p><h2 id="5-1-互逆属性（slot）"><a href="#5-1-互逆属性（slot）" class="headerlink" title="5.1 互逆属性（slot）"></a>5.1 互逆属性（slot）</h2><p>一个 <em>slot</em> 的值可能依赖于另一个 <em>slot</em> 的值。比如，（<em>wine</em>，<em>produced_by</em>，<em>winery</em>）和（<em>winery</em>，<em>produces</em>，<em>wine</em>）这两个关系就是互逆关系。如果在本体系统中将两个关系都存下来，会显得整个本体冗杂。当我们知道某种酒的生产厂家是某某某的时候，我们就可以推断出某某某厂家生产了某种酒。从知识获取的角度来说，明确这种互逆关系对知识获取来说是很方便的。知识获取系统可以自动填写互逆关系的值，以确保知识库的一致性。</p><p>比如，当我们明确了 “<em>produced_by</em>” 和 “<em>produces</em>” 是互逆关系之后，当我们填写了 “茅台酒 <em>produced_by</em> 贵州茅台酒股份有限公司” 以后，系统可以自动填写 “贵州茅台酒股份有限公司 <em>produces</em> 茅台酒”。</p><h2 id="5-2-默认属性值（slot-value）"><a href="#5-2-默认属性值（slot-value）" class="headerlink" title="5.2 默认属性值（slot value）"></a>5.2 默认属性值（slot value）</h2><p>许多基于框架的系统允许指定默认属性值。如果多数实例的特定属性是相同的，那么我们可以给这个属性指定一个默认值。然后，每次往该类别下添加有该属性的实例的时候，系统可以自动填充属性值。如果我们默认的属性值与该实例的实际属性值不符，我们还可以手动修改。</p><p>比如，如果多数白酒都是 53° 的，那么我们在 “酒精度数” 中可以默认为 53°。如果有些白酒不是 53°，还可以手动改成其他度数。</p><p>需要注意的是，默认属性值与属性值是不同的，默认属性值是可以修改的，而属性值是不可修改的。即如果我们定义了 “白酒” 的酒精度是 53°，那么所有 “白酒” 的子类和实例的酒精度都是 53°，这个度数在任意子类和实例中都不可修改。</p><h1 id="6-名字包含什么？"><a href="#6-名字包含什么？" class="headerlink" title="6. 名字包含什么？"></a>6. 名字包含什么？</h1><blockquote><p>  本节讨论对概念命名规则，主要集中在英文名称中会出现的一些问题，比如大小写、分隔符、单复数等。这些问题在中文中都基本不会出现。但是就个人而言，还是建议使用英文进行知识建模。众所周知，现在很多系统对中文的支持并不是十分友好，使用中文建模的话很可能出现各种意想不到的问题，因此，能用英文建模的就尽量使用英文建模吧。</p></blockquote><p>为本体中的概念设定一些命名规则，不仅可以使本体更容易理解，还能够帮助我们避免一些常见的建模错误。命名方法有很多，实际应用的时候可以选择合适的方法。但是，我们要：</p><blockquote><p>  <em>定义一种类别和属性的命名规范，然后遵守它。</em></p></blockquote><p>在知识表示系统中，我们可以考虑以下特征用于对概念进行命名：</p><ul><li>本体中是否存在同名的类别、属性、实例？比如 “酿酒厂” 既是类别又是属性？</li><li>本体系统大小写敏感吗？比如，系统是否人为 “<em>Wine</em>” 和 “<em>wine</em>” 是同一个概念？</li><li>名称中允许出现什么样的分隔符？空格、逗号、星号等等？</li></ul><h2 id="6-1-大小写与分隔符"><a href="#6-1-大小写与分隔符" class="headerlink" title="6.1 大小写与分隔符"></a>6.1 大小写与分隔符</h2><p>首先，如果我们在本体中保证概念名称的大小写一致性能够大幅提升本体的可读性。比如，通常的做法是大写类别名称，小写属性名称（假设大小写敏感）。</p><p>当概念名称中有不止一个词的时候，我们需要在词与词之间添加分隔符。通常分隔符有以下几种选择：</p><ul><li>空格</li><li>词与词之间没有分隔符，而是将每个词的首字母大写，比如 “MealCourse”</li><li>使用下划线或者连接符，比如 “meal_course”，“meal-course” 等</li></ul><p>在使用空格的时候，需要考虑你所使用的本体建模工具是否支持空格，以及你构建出来的本体是否会与其他本体系统交互使用，如果有交互，需要交互的本体系统是否支持空格。因此，虽然用空格作为分隔符更符合人类的习惯，但是需要考虑的因素比较多，更建议使用后两种方案。</p><h2 id="6-2-单数还是复数？"><a href="#6-2-单数还是复数？" class="headerlink" title="6.2 单数还是复数？"></a>6.2 单数还是复数？</h2><p>一个类别的名称代表的是一些列对象的集合。所以，建议使用复数作为类别的名称。但是无论使用单数还是复数，都要在整个本体中保持一致，不要出现在这里是单数，淡了另一处就变成了复数。甚至有些本体建模工具会要求用户指定概念名称的单复数。</p><h2 id="6-3-前缀和后缀的规则"><a href="#6-3-前缀和后缀的规则" class="headerlink" title="6.3 前缀和后缀的规则"></a>6.3 前缀和后缀的规则</h2><p>有些知识库会建议使用前缀或者后缀还区分类别名和属性名。属性名中常用的两种前缀或者后缀：“has-” 或者 “-of”。比如 “<em>has-maker</em>” 或者 “<em>*maker-of</em>”。通过这种方式区分类别名和属性名可以提高可读性。</p><h2 id="6-4-命名中的一些其他考量"><a href="#6-4-命名中的一些其他考量" class="headerlink" title="6.4 命名中的一些其他考量"></a>6.4 命名中的一些其他考量</h2><ul><li>不要在概念名称中出现 “<em>class</em>”、“<em>property</em>”、“<em>slot</em>” 等词汇</li><li>避免使用缩写</li><li>对直接子类进行命名的时候，要么所有子类都包含父类的名称，要么都不包含父类的名称。不要出现有些子类包含父类有些不包含的情况。比如 “<em>red wine</em>” 和 “<em>*white</em>”</li></ul><h1 id="7-其他可参考资料"><a href="#7-其他可参考资料" class="headerlink" title="7. 其他可参考资料"></a>7. 其他可参考资料</h1><ol><li><a href="https://dl.acm.org/doi/10.1006/ijhc.1999.0366" target="_blank" rel="noopener">WonderTools? A comparative study of ontological engineering tools</a>, <em>Duineveld, A.J., Stoter, R., Weiden, M.R., Kenepa, B. and Benjamins, V.R. (2000).</em></li><li>Knowledge sharing and reuse. <em>Gómez-Pérez, A. (1998).</em></li><li><a href="https://www.cambridge.org/core/journals/knowledge-engineering-review/article/abs/ontologies-principles-methods-and-applications/2443E0A8E5D81A144D8C611EF20043E6" target="_blank" rel="noopener">Ontologies: Principles, Methods and Applications</a>, <em>Uschold, M. and Gruninger, M. (1996).</em></li><li><a href="http://www.ksl.stanford.edu/software/ontolingua/tutorial.pdf" target="_blank" rel="noopener">Ontolingua tutorial</a>. <em>Farquhar, A. (1997).</em></li><li><a href="https://www.researchgate.net/publication/profile/Richard_Fikes/publication/221393548_An_Environment_for_Merging_and_Testing_Large_Ontologies/links/564d604f08ae4988a7a44137/An-Environment-for-Merging-and-Testing-Large-Ontologies.pdf" target="_blank" rel="noopener">An Environment for Merging and Testing Large Ontologies</a>. <em>McGuinness, D.L., Fikes, R., Rice, J. and Wilder, S. (2000).</em></li></ol><h1 id="8-总结"><a href="#8-总结" class="headerlink" title="8. 总结"></a>8. 总结</h1><p>本文描述了构建本体的方法和步骤。讨论了构建过程中需要注意的问题。但是我们需要记住一点：</p><p><strong>对于任意领域来说都没有一个唯一正确的方法</strong></p><p>本体的构建是一个创造的过程，即使是以相同的目的和应用场景构建相同领域的本体，不同的人都会得到不同的本体。主要能满足我们的需求，就是好的本体。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://protege.stanford.edu/publications/ontology_development/ontology101.pdf" target="_blank" rel="noopener">Ontology Development 101: A Guide to Creating Your First Ontology</a>. <em>Natalya F. Noy and Deborah L. McGuinness</em> </p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ontology </tag>
            
            <tag> KG </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>双数组前缀树</title>
      <link href="/2021/08/16/double_array_trie/"/>
      <url>/2021/08/16/double_array_trie/</url>
      
        <content type="html"><![CDATA[<p>前缀树（trie）又叫字典树，顾名思义通过字符串的前缀进行查找、匹配的数据结构。Trie 树的应用场景主要包括：分词、词频统计、字符串查询和模糊匹配、字符串排序等。Trie 树大幅降低重复字符串的比较，所以执行效率非常高。</p><a id="more"></a><h1 id="1-Trie-树简介"><a href="#1-Trie-树简介" class="headerlink" title="1. Trie 树简介"></a>1. Trie 树简介</h1><p>前缀树是将字符串存储在一棵树结构内，该树是将字符串的公共前缀作为父节点。以下图为例：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210816213207.png" alt></p><p>假设有三个词，分别是“茶叶”、“茶树”、“走廊”。将这三个词存储在 Trie 树里如上图所示。“茶树”和“茶叶”有公共前缀“茶”，所以“茶”作为“叶”和“树”的父节点。而“走廊”和前两个词无公共前缀，所以独立成一个分支。另外 <code>root</code> 节点表示根节点，所有词的匹配、查找都是从根节点开始的。而 <code>null</code> 为叶节点表示从根节点 <code>root</code> 到 叶子节点 <code>null</code> 的路径组成一个完整的词。Trie 树具有以下几个特点：</p><ul><li>具有相同前缀的词必须位于同一个路径下。“叶”和“树”要共用一个父节点“茶”。</li><li>Trie 树中的词只可共用前缀，不可共用词的其他部分。比如，现在有一个新词“卖茶”，虽然都有“茶”，但是它并不是“卖茶”的前缀，所以“卖茶”要与“茶叶”和“茶树”在不同的分支上。</li><li>Trie 树中任何一个完整的词，都必须是从根节点开始至叶子节点结束，这意味着对一个词进行检索也必须从根节点开始，至叶子节点才算结束。</li></ul><h1 id="2-搜索-Trie-树的时间复杂度"><a href="#2-搜索-Trie-树的时间复杂度" class="headerlink" title="2. 搜索 Trie 树的时间复杂度"></a>2. 搜索 Trie 树的时间复杂度</h1><p>在 Trie 树中搜索一个字符串，会从根节点出发，沿着某条路径向下逐字比对字符串的每个字符，直到抵达底部的叶子节点才能确认字符串为该词，这种检索方式具有以下两个优点：</p><ol><li><p>公共前缀的词都位于同一个串内，查词范围因此被大幅缩小（比如首字不同的字符串，都会被排除）。</p></li><li><p>Trie 树实质是一个有限状态自动机（Deterministic Finite Automaton, DFA），这就意味着从 Trie 树 的一个节点（状态）转移到另一个节点（状态）的行为完全由状态转移函数控制，而 <strong>状态转移函数本质上是一种映射</strong>，这意味着：<strong>逐字搜索 Trie 树时，从一个字符到下一个字符比对是不需要遍历该节点的所有子节点的。</strong></p><blockquote><p>确定的有限自动机 M 是一个五元组：</p><script type="math/tex; mode=display">M = (\Sigma, Q, \delta, q_0, F)</script><p>其中，</p><ul><li><p>$\Sigma$ 是输入符号的有穷集合；</p></li><li><p>$Q$ 是状态的有限集合；</p></li><li><p>$\delta$ 是 $Q$ 与 $\Sigma$ 的直积 $Q × \Sigma$ 到 $Q$ (下一个状态) 的映射。它支配着有限状态控制的行为，有时也称为状态转移函数。</p></li><li><p>$q_0 \in Q$ 是初始状态；</p></li><li><p>$F$ 是终止状态集合，$F \subseteq Q$；</p></li></ul><p>可以把 DFA 想象成一个单放机，插入一盘磁带，随着磁带的转动，DFA 读取一个符号，依靠状态转移函数改变自己的状态，同时磁带转到下一个字符。</p></blockquote></li></ol><p>这两个优点相结合可以最大限度地减少无谓的字符比较，使得搜索的时间复杂度理论上仅与检索词的长度有关：$O(m)$，其中 $m$ 为检索词的长度。</p><h1 id="3-Trie-树的缺点"><a href="#3-Trie-树的缺点" class="headerlink" title="3. Trie 树的缺点"></a>3. Trie 树的缺点</h1><p>综上可知， Trie 树主要是利用词的公共前缀缩小查词范围、通过状态间的映射关系避免了字符的遍历，从而达到高效检索的目的。这一思想有赖于字符在词中的前后位置能够得到表达，因此其设计哲学是典型的“<strong>以信息换时间</strong>”，当然，这种优势同样是需要付出代价的：</p><ol><li>由于结构需要记录更多的信息，因此 Trie 树的实现稍显复杂。好在这点在大多数情况下并非不可接受。</li><li>Trie 型词典不仅需要记录词，还需要记录字符之间、词之间的相关信息，因此字典构建时必须对每个词和字逐一进行处理，而这无疑会减慢词典的构建速度。对于强调实时更新的词典而言，这点可能是致命的，尤其是采用双数组实现的 Trie 树，更新词典很大概率会造成词典的全部重构，词典构建过程中还需处理各种冲突，因此重构的时间非常长，这导致其大多用于离线；不过也有一些 Trie 可以实现实时更新，但也需付出一定的代价，这个缺点一定程度上影响了 Trie 树的应用范围。</li><li>公共前缀虽然可以减少一定的存储空间，但 Trie 树相比普通字典还需表达词、字之间的各种关系，其实现也更加复杂，因此实际空间消耗相对更大（大多少，得根据具体实现而定）。尤其是早期的“Array Trie”，属于典型的以空间换时间的实现，（其实 Trie 本身的实现思想是是以信息换时间，而非以空间换时间，这就给 Trie 树的改进提供了可能），然而 Trie 树现今已经得到了很好的改进，总体来说，对于类似词典这样的应用，Trie 是一个优秀的数据结构。</li></ol><h1 id="4-Trie-树的几种实现"><a href="#4-Trie-树的几种实现" class="headerlink" title="4. Trie 树的几种实现"></a>4. Trie 树的几种实现</h1><p>Trie 树实现:一般的链表指针方式，三数组实现，双数组实现，HAT，burst trie 等。</p><ul><li><p><strong>链表指针方式</strong></p><p>即每个节点对应一个字符，并有多个指针指向子节点，查找和插入从根节点按照指针的指向向下查询。这种方案，实现较为简单，但指针较多，较为浪费空间；树形结构，指针跳转，对缓存不够友好，节点数目上去之后，效率不够高。</p></li><li><p><strong>Hash Trie 树以及 Burst trie</strong></p><p>是将 trie 树和其他数据结构，比如 HashMap，结合起来，提高效率。但主要用于键值查找，对于给定一个字符串匹配其前缀这种场景不适用。</p></li><li><p><strong>三数组实现</strong></p><p>利用三个数组（分别叫做 base, next, check）来实现状态的转移，将前缀树压缩到三个数据里，能够较好的节省内存；数组的方式也能较好的利用缓存。</p></li><li><p><strong>双数组实现</strong></p><p>是在三数组的基础上，将 base 数组重用为 next 数组，节省了一个数组，并没有增加其他开销。与三数组相比，内存使用和效率进一步提升。</p></li></ul><p>综上，双数组 trie（Double Array trie，简称为 DATrie）的实现有明显的优势，以下讨论 DATrie 的细节（只介绍构造和查询，删除节点不常用，而且比较复杂，暂时略过）。</p><h1 id="5-Double-Array-Trie-树"><a href="#5-Double-Array-Trie-树" class="headerlink" title="5. Double-Array Trie 树"></a>5. Double-Array Trie 树</h1><h2 id="5-1-DATrie-构造方法"><a href="#5-1-DATrie-构造方法" class="headerlink" title="5.1 DATrie 构造方法"></a>5.1 DATrie 构造方法</h2><ol><li><p>数组表示 trie 树的状态转移，父节点跳转到子节点转化为父状态跳转到子状态。</p></li><li><p>利用两个数组 <code>base</code>, <code>check</code>表示状态的转移：</p><ul><li><code>base</code> 数组的索引用来表示状态</li><li><code>base</code> 数组里存的数据称为 offset</li><li><code>check</code> 数组里存的数据是父状态的索引</li><li><code>check</code> 与<code>base</code> 大小相同，一一对应，用于保存父状态，以及解决冲突</li></ul></li><li><p>状态 S 接收到字符 c 后转移到状态 T:</p><script type="math/tex; mode=display">S \overset{c}{\to} T</script><p>满足：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">check[base[S] + c] = S</span><br><span class="line">base[S] + c = T</span><br><span class="line">base[T] = base[S]</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/trie1.png" alt></p><ul><li><code>base</code> 数组的索引为 0，1，…, base[s], …, S, …, T，均表示 trie 树的状态</li><li>从 S 状态接收到 c 跳转到 T, 则表示为 base 数组索引为 S 的内容 base[S] 为基地址，加上跳转偏移 c，得到下一个 T 状态在 base 的索引 <code>T=base[S] + C</code></li><li><code>check</code> 数组对应 T 的内容 check[T] 为跳转过来的父状态，即 S。</li></ul></li></ol><h2 id="5-2-DATrie-查询"><a href="#5-2-DATrie-查询" class="headerlink" title="5.2 DATrie 查询"></a>5.2 DATrie 查询</h2><ol><li>从 <code>base</code> 数组索引 0 开始，初始状态为 S=base[0]，其中偏移的基地址为 base[S]</li><li>接受到 c，则跳转到 <code>base</code> 数组索引 T=base[S] + c，检查此时 <code>check</code> 数组的 check[T] == S，为真跳转到 3，否则匹配失败。</li><li>如果 <code>base[T] == LEAF_VALUE</code> （这里 <code>LEAF_VALUE</code> 用来表示叶子节点的特殊值），则匹配完成；否则，令 S = T, 跳转到 2。</li></ol><p>状态更新的伪码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">T := base[S] + c</span><br><span class="line"></span><br><span class="line">if check[T] = S then</span><br><span class="line">    next state := T</span><br><span class="line">else</span><br><span class="line">    fail</span><br><span class="line">endif</span><br></pre></td></tr></table></figure><h2 id="5-3-举个例子"><a href="#5-3-举个例子" class="headerlink" title="5.3 举个例子"></a>5.3 举个例子</h2><ul><li><p>假定输入两个前缀为 ‘ab’ ,  ‘ad’ ，将字母 a-z 映射为数字 1，2，3,…, 26.</p></li><li><p>这里用 -1 代表数组元素为空，-2 代表叶子节点，-3 代表根节点</p></li><li><p>状态如下：</p><ol><li><p>初始状态</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/dat_example1.png" alt></p></li><li><p>输入 ‘a’ （’ab’）</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/dat_example2.png" alt></p><p><code>base[0]+a</code>，由状态 0 跳转到状态 2。<code>check[2]</code> 为 -1，说明为空，更新为父状态 0；<code>base[2]</code>更新为跳转过来的 <code>base</code>, 即 <code>base[0]</code> 的值 1。</p></li><li><p>输入 ‘b’ （’ab’）</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/dat_example3.png" alt></p><p><code>base[2]+b</code>，由状态 2 跳转到状态 3，<code>check[3]</code>为 -1，说明为空，更新为父状态 2；由于字符串结束，将 <code>base[3]</code> 更新为 -2，代表叶节点。</p></li><li><p>输入 ‘a’（’ad’）</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/dat_example4.png" alt></p><p>图中 <code>base</code> 和 <code>check</code> 的状态不会变化。 根据 <code>base[0]+a</code>，从状态 0 跳转到 2。<code>check[2]</code> 不为空，但<code>check[2]</code> 的值 0 与其父状态 S=0 相等，则无需更新，进入状态 2，等待输入下一个字符。这个过程相当于一个查询过程。</p></li><li><p>输入 ‘d’ （’ad’）</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/dat_example5.png" alt></p><p><code>base[2]+d</code>，由状态 2 跳转到状态 5，<code>check[5]</code> 为 -1，说明为空，更新为父状态 2；由于字符串结束，将 <code>base[5]</code> 更新为 -2，代表叶节点。</p></li></ol></li></ul><h2 id="5-4-解决冲突"><a href="#5-4-解决冲突" class="headerlink" title="5.4 解决冲突"></a>5.4 解决冲突</h2><p>DATrie 不可避免会出现冲突。仍以上面的例子说明，继续插入 ‘ca’：</p><ul><li><p>输入 ‘c’（’ca’）</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/dat_example6.png" alt></p><p>状态由 0 跳转到状态 4，<code>check[4]</code> 空闲，将 <code>check[4]</code> 赋值为 0，<code>base[4]</code> 赋值为1。</p></li><li><p>输入 ‘a’ （’ca’）</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/dat_example7.png" alt></p><p>根据 <code>base[4]+4</code> 状态从4跳转到2， 但是 <code>check[2]</code> 非空，并且 <code>check[2]=0</code> 不等于父状态 4，此时发生冲突。</p></li><li><p>解决冲突</p><ol><li><p>挪动以 4 为父状态状态转移，查找对应 <code>base</code>，<code>check</code> 的连续的空闲空间以放入状态。这里只有最新的输入 ‘a’ 带来的状态转移以 4 为父状态。<code>base[6]</code>, <code>check[6]</code> 有空闲。</p></li><li><p>修改 <code>base[4]</code>, 使其能够根据输入跳转到空闲空间，即 <code>base[4] = 6 - a = 5</code>。</p></li><li><p>重新插入 ‘a’，如下图</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/dat_example8.png" alt></p></li></ol></li></ul><h1 id="6-Trie-树的压缩"><a href="#6-Trie-树的压缩" class="headerlink" title="6. Trie 树的压缩"></a>6. Trie 树的压缩</h1><p>双数组 Trie 树虽然大幅改善了经典 Trie 树的空间浪费，但是由于冲突发生时，程序总是向后寻找空地址，导致数组不可避免的出现空置，因此空间上还是会有些浪费。另外， 随着节点的增加，冲突的产生几率也会越来越大，字典构建的时间因此越来越长，为了改善这些问题，有人想到对双数组 Trie 进行尾缀压缩，具体做法是：将非公共前缀的词尾合并为一个节点（tail 节点），以此大幅减少节点总数，从而改善树的构建速度；同时将合并的词尾单独存储在另一个数组之中（Tail array）， 并通过 tail 节点的 base 值指向该数组的相应位置，以 <code>{baby, bachelor, badge, jar}</code> 四词为例，其实现示意图如下:</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/dat_example9.png" alt></p><ul><li>速度：减少了 <code>base</code>， <code>check</code> 的状态数，以及冲突的概率，提高了插入的速度。</li><li>内存：状态数的减少的开销大于存储 tail 的开销，节省了内存。</li><li>删除：能很方便的实现删除，只需将 tail 删除即可。</li></ul><h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h2><ol><li>Trie 树是一种以信息换时间的数据结构，其查询的复杂度为 $O(m)$。</li><li>Trie 的单数组实现能够达到最佳的性能，但是其空间利用率极低，是典型的以空间换时间的实现。</li><li>Trie 树的哈希实现可以很好的平衡性能需求和空间开销，同时能够实现词典的实时更新。</li><li>Trie 树的双数组实现基本可以达到单数组实现的性能，同时能够大幅降低空间开销；但是其难以做到词典的实时更新。</li><li>对双数组 Trie 进行 tail 改进可以明显改善词典的构建速度，同时进一步减少空间开销。</li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://segmentfault.com/a/1190000008877595" target="_blank" rel="noopener">小白详解 Trie 树</a>, <em>xu_zhoufeng</em></li><li><a href="https://www.iteye.com/blog/huangwei1024-813697" target="_blank" rel="noopener">Double-Array Trie（双数组字典树）</a>, <em>huangwei1024</em></li><li><a href="https://turbopeter.github.io/2013/09/02/prefix-match/" target="_blank" rel="noopener">前缀树匹配(Double Array Trie)</a>, <em>minzhan’s blog</em></li><li><a href="https://zhuanlan.zhihu.com/p/35193582" target="_blank" rel="noopener">双数组前缀树（Double-Array Trie）</a>, <em>两片</em> </li></ol>]]></content>
      
      
      <categories>
          
          <category> 博客转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 双数组前缀树 </tag>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>预训练语言模型：Word Embedding</title>
      <link href="/2021/08/11/ptm-word-embedding/"/>
      <url>/2021/08/11/ptm-word-embedding/</url>
      
        <content type="html"><![CDATA[<p><img src="https://aylien.com/images/uploads/general/tumblr_inline_o8tinsmw081u37g00_540.png" alt></p><p>词嵌入（word embedding）是一种用稠密向量来表示词义的方法，其中每个词对应的向量叫做词向量（word vector）。词嵌入通常是从语言模型中学习得来的，其中蕴含着词与词之间的语义关系，比如 “猫” 和 “狗” 的语义相似性大于 “猫” 和 “计算机” 。这种语义相似性就是通过向量距离来计算的。</p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><h2 id="1-1-词表示法简史"><a href="#1-1-词表示法简史" class="headerlink" title="1.1 词表示法简史"></a>1.1 词表示法简史</h2><p>自然语言文本在很长时间里并没有一个统一的表示法，用于计算机进行计算。通常人们给每个词分配一个 id，将词作为离散符号输入计算机系统。</p><ul><li><p><strong>查字典</strong></p><p>最直接的方法是创建一个词表，每个词分配一个唯一的 ID，比如：</p><blockquote><p>  我， 0</p><p>  是， 1</p><p>  谁， 2</p><p>  …</p></blockquote></li><li><p><strong>One-hot 编码</strong></p><p>同样是先建立一个词表，然后给词表中的每个词分配一个大小为词表大小的向量来表示词。每个词对应的向量中，只有一个位置的数字为 1，其他位置上的数字全部是 0。词与词的 one-hot 向量两两正交。整个词表就是一个 $1\times (N+1)$ 的矩阵，其中 $N$ 表示词表大小，额外的 1 表示 <em>UNK</em> ，即不在词表中的词的统一标识。比如：</p><blockquote><p>  我，[1, 0, 0, 0, …]</p><p>  是，[0, 1, 0, 0, …]</p><p>  谁，[0, 0, 1, 0, …]</p><p>  …</p></blockquote></li><li><p><strong>Distributional 表示法</strong></p><p>以上两种方法存在着一下几个问题：</p><ol><li>正交。词与词之间的语义丢失，我们没有办法从向量表示中得到词与词之间的关联性，</li><li>维度爆炸。通常一个词表会有几万个词，如果用 one-hot 表示，那么整个词表的 one-hot 就是一个几万乘几万的矩阵，极大地消耗了计算机资源。</li><li>矩阵稀疏。one-hot 矩阵中，除了特定位置上的数字是 1， 其余位置全部是 0，造成整个矩阵极端稀疏化，运算过程中极大地浪费了算力，</li></ol><p>因此，人们提出了分布式表示法，希望通过稠密向量来获得词嵌入矩阵。而得到稠密向量的方法就是我们下面要介绍的。</p></li></ul><h2 id="1-2-发展里程碑"><a href="#1-2-发展里程碑" class="headerlink" title="1.2 发展里程碑"></a>1.2 发展里程碑</h2><div class="timeline"><div class="timenode"><div class="meta"><p></p><p>2003 年 —— 前馈神经网络语言模型</p><p></p></div><div class="body"><p>2003 年 <em>Bengio</em> 等人提出前馈神经网络语言模型（FFNNLM），该模型的一个重要副产物就是词向量。相当于提出了一种利用语言模型训练词向量的方法，同样为后来的 Word2vec 打下了基础。</p></div></div><div class="timenode"><div class="meta"><p></p><p>2005 年 —— 层级 Softmax</p><p></p></div><div class="body"><p><em>Morin &amp; Bengio</em> 提出层级 softmax 思想。给定大小为 $V$ 的词表，通过一棵二叉树计算输出词的概率分布，将计算复杂度从 $O(V)$ 降到 $O(\log(V))$。这一思想成为后来 word2vec 模型的重要组成部分。</p></div></div><div class="timenode"><div class="meta"><p></p><p>2010 年 —— Noise Contrastive Estimation</p><p></p></div><div class="body"><p><em>Gutmann &amp; Hyvarinen</em> 提出噪声对比估计（NCE）方法。其基本思想是：一个好的模型可以利用<strong>逻辑回归</strong>从噪声中识别有用数据。后来 NCE 被 <em>Mnih &amp;Teh</em> 用于语言模型。后来 Word2vec 中的负采样技术就是 NCE 的简化版。</p></div></div><div class="timenode"><div class="meta"><p></p><p>2013 年 —— word2vec</p><p></p></div><div class="body"><p><em>Mikolov</em> 等人提出 word2vec 模型，使得大规模训练词向量成为现实。Word2vec 包含两个模型：<em>skip-gram</em> 和 <em>CBOW</em>。为了加速计算，word2vec 将 softmax 替换成层级 softmax，二叉树用的是哈夫曼树（Huffman tree）。</p></div></div><div class="timenode"><div class="meta"><p></p><p>2013 年 —— 负采样</p><p></p></div><div class="body"><p><em>Mikolov</em> 等人对原来的 word2vec 模型进行了优化，提出负采样的方法。负采样是噪声对比估计的简化版，比层级 softmax 更简单、更快。</p></div></div><div class="timenode"><div class="meta"><p></p><p>2014 年 —— GloVe</p><p></p></div><div class="body"><p><em>Pennington</em> 等人基于词共现的方法，提出另一种训练词向量的方法：Glove。与 word2vec 相比，两个模型表现相差不大，而 GloVe 更容易并行化训练。</p></div></div></div><p>接下来我们介绍两种主要的训练词嵌入的方法：</p><ul><li><strong>Context-based</strong>：给定上下文，设计模型预测中心词。</li><li><strong>Count-based</strong>：统计文本中词的共现矩阵，然后利用矩阵分解的方法对矩阵进行降维。</li></ul><h1 id="2-Context-based-Word2Vec"><a href="#2-Context-based-Word2Vec" class="headerlink" title="2. Context-based: Word2Vec"></a>2. Context-based: Word2Vec</h1><p>2013 年 <a href="http://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">Mikolov</a> 等人提出一种模型 —— Word2Vec。该模型包含两种架构：<em>Continuous Bag-of-Words（CBOW）</em> 和  <em>Continuous Skip-gram（Skip-gram）</em>，然后在随后的文章中提出了两种模型训练的优化方法：<em>hierarchical softmax（层级 softmax）</em> 和 <em>negative sampling（负采样）</em>。Mikolov 等人不是第一个提出连续向量表示词的人，但是他们提出的 word2vec 模型是第一个能应用在大规模语料上的模型，具有非常重要的意义。</p><p>假设有一个固定大小的滑动窗口，沿着句子从头到尾滑动取片段，每个窗口中中心词即为目标词（target），其他的词为上下文（context）。举个例子（假设已经分词）：</p><blockquote><p>  天才 就是 百分之一 的 灵感 加 百分之九十九 的 汗水。</p></blockquote><div class="table-container"><table><thead><tr><th>滑动窗口（size=5）</th><th>target</th><th>context</th></tr></thead><tbody><tr><td>[<font color="red">天才</font>, 就是, 百分之一]</td><td>天才</td><td>就是, 百分之一</td></tr><tr><td>[天才, <font color="red">就是</font>, 百分之一, 的]</td><td>就是</td><td>天才, 百分之一,的</td></tr><tr><td>[天才, 就是, <font color="red">百分之一</font>, 的, 灵感]</td><td>百分之一</td><td>天才, 就是, 的, 灵感</td></tr><tr><td>…</td><td>…</td><td>…</td></tr><tr><td>[灵感, 加, <font color="red">百分之九十九</font>, 的, 汗水]</td><td>百分之九十九</td><td>灵感, 加, 的, 汗水</td></tr><tr><td>[加, 百分之九十九, <font color="red">的</font>, 汗水]</td><td>的</td><td>加, 百分之九十九, 汗水</td></tr><tr><td>[百分之九十九, 的, <font color="red">汗水</font>]</td><td>汗水</td><td>百分之九十九, 的</td></tr></tbody></table></div><h2 id="2-1-Skip-Gram-Model"><a href="#2-1-Skip-Gram-Model" class="headerlink" title="2.1 Skip-Gram Model"></a>2.1 Skip-Gram Model</h2><p>Skip-gram 模型的核心思想是通过<strong>中心词</strong>预测<strong>上下文</strong>，即：</p><script type="math/tex; mode=display">p(w_{i-2}, w_{i-1}, w_{i+1}, w_{i+2}|w_i)</script><p>Skip-gram 模型采用的是一个浅层神经网络来计算这个概率分布。该一共只有三层：输入层、投影层（隐藏层）、输出层。模型结构如下图：</p><p><img width="500" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210811213240.jpg"></p><p>假设上下文中的词相互独立，则：</p><script type="math/tex; mode=display">p(w_{i-2}, w_{i-1}, w_{i+1}, w_{i+2}|w_i) = p(w_{i-2}|w_i)\cdot p(w_{i-1}|w_i) \cdot p(w_{i+1}|w_i)\cdot p(w_{i+2}|w_i)</script><p>相当于训练样本的（target，context）对拆解成 $2m$个（target，context word）对，其中 $m$ 表示滑动窗口除中心词外一半大小（很多地方会直接把 $m$ 定义为窗口大小），context word 表示上下文中每个词。例如，中心词为 “百分之九十九”，那么训练样本就是：</p><blockquote><p>  （百分之九十九，灵感）</p><p>  （百分之九十九，加）</p><p>  （百分之九十九， 的）</p><p>  （百分之九十九， 汗水）</p></blockquote><p>此时，上面的模型结构则等效于下图：</p><p><img width="500" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210811213223.jpg"></p><p>模型的输入是中心词，输出是上下文词之一。</p><p>假设词表 $\mathcal{V}$​​​​​ 的大小为 $V=|\mathcal{V}|$​​​​​，中心词在词典中的索引为 $i$​​​​​，上下文对应的词在词表中的索引为 $j$， $N$ 表示词向量 $\boldsymbol{v}_i$ 的维度，即 $\boldsymbol{v}_i \in \mathbb{R}^N$​​​​​。</p><p>关于模型的一些细节：</p><ul><li><p>$\boldsymbol{x}$​​​ 和 $\boldsymbol{y}$​​​ 都是 one-hot 编码，编码中 $i$​ 和 $j$​ 对应的位置为 1，其余位置全部为 0，$\boldsymbol{x},\boldsymbol{y} \in \mathbb{R}^{1\times V}$​。​​​</p></li><li><p>首先，将输入 $\boldsymbol{x}$​​ 与一个 $\boldsymbol{W}$​​ 矩阵相乘得到隐藏层 $\boldsymbol{h}$​​，其中 $\boldsymbol{W}\in \mathbb{R}^{V\times N}$​​​​​​​ ，则 $\boldsymbol{h}\in \mathbb{R}^{1\times N}$​​。实际上 $\boldsymbol{h}$​ 相当于 $\boldsymbol{W}$​ 的第 $i$​ 行：</p><script type="math/tex; mode=display">[0, ..., 1, ..., 0] \times \left[\begin{matrix}w_{00}, w_{01}, ..., w_{0N} \\\\\vdots \\\\w_{i0}, w_{i1}, ..., w_{iN} \\\\\vdots \\\\w_{V0}, w_{V1}, ..., w_{VN}\end{matrix}\right] = \left[w_{i0}, ...,w_{ii}, ..., w_{iN}\right]</script></li><li><p>用 $\boldsymbol{h}$​ 与另一个矩阵 $\boldsymbol{W’}\in \mathbb{R}^{N\times V}$​​ 相乘得到一个 $1\times V$ 的向量 $\boldsymbol{h’}$。</p></li><li><p>将 $\boldsymbol{h’}$ 进行归一化即可得到 $\boldsymbol{y}$​ 的 one-hot 概率分布：</p><script type="math/tex; mode=display">\boldsymbol{y} = \mathrm{softmax}(\boldsymbol{h'})</script></li><li><p>$\boldsymbol{y}$ 中概率最大的位置 $j$ 即对应词表第 $j$ 个词：</p><script type="math/tex; mode=display">w_j = \mathcal{V}_{j=\arg \max (\boldsymbol{y})}</script><p>比如：</p><blockquote><p>  假设 $\mathcal{V}=[我，的，灵感，天才，…]$</p><p>  $\boldsymbol{y} = [0.1, 0.2, 0.3, 0.2, 0.15, 0.05]$</p><p>  $\boldsymbol{y}$​ 中最大概率为 0.3，对应的索引是 2，即 $j=2$​​，</p><p>  则 $w_j = \mathcal{V}_2 = 灵感$​。</p></blockquote></li><li><p>模型中有两个矩阵 $\boldsymbol{W}$ 和 $\boldsymbol{W’}$​，非别对应着中心词的向量编码和上下文的向量编码。在自然语言处理应用中，一般使用中心词向量作为词的表征向量，即 $\boldsymbol{W}$ 就是我们最终得到的 word embedding。</p></li></ul><h2 id="2-2-CBOW-Model"><a href="#2-2-CBOW-Model" class="headerlink" title="2.2 CBOW Model"></a>2.2 CBOW Model</h2><p>连续词袋模型（CBOW）模型与 skip-gram 模型正相反，CBOW 是利用<strong>上下文</strong>来预测<strong>中心词</strong>，即：</p><script type="math/tex; mode=display">p(w_i|w_{i-2},w_{i-1},w_{i_1},w_{i+1})</script><p>模型结构如下图所示：</p><p><img width="500" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210811213150.png"></p><p>由于 CBOW 模型的输入有多个，所以我们将得到的 context 向量取平均，然后使用和 skip-gram 一样的方法来计算中心词的概率分布。</p><script type="math/tex; mode=display">\boldsymbol{h} = \frac{1}{2m}\sum \boldsymbol{x}_i \cdot \boldsymbol{W}</script><h2 id="2-3-Loss-object-Functions"><a href="#2-3-Loss-object-Functions" class="headerlink" title="2.3 Loss/object Functions"></a>2.3 Loss/object Functions</h2><p>无论是 skip-gram 模型还是 CBOW 模型，模型参数就是中心词向量和上下文词向量对应的嵌入矩阵 $\boldsymbol{W}$​​​​ 和 $\boldsymbol{W’}$​​​。给定输入词 $w_I$​​​ ，其在 $\boldsymbol{W}$​​​ 中对应的向量为 $\boldsymbol{v}_I$​​​​（即 $\boldsymbol{h}$​​）。$\boldsymbol{W’}$​​ 中每一列对应的词向量为 $\boldsymbol{v’}_j$​​​​​​。输出词 $w_O$​​ 对应的词向量为 $\boldsymbol{v’}_o$​​。</p><p>通过最小化损失函数对模型进行训练，下面以 skip-gram 为例介绍一些常用的损失/目标函数。</p><h3 id="2-3-1-标准-Softmax（Full-Softmax）"><a href="#2-3-1-标准-Softmax（Full-Softmax）" class="headerlink" title="2.3.1 标准 Softmax（Full Softmax）"></a>2.3.1 标准 Softmax（Full Softmax）</h3><p>用数学语言来描述上面的模型，即对于单个样本我们的目标函数为：</p><script type="math/tex; mode=display">p(w_O|w_I) = \frac{\exp(\boldsymbol{v'}_O^\mathsf{T} \cdot \boldsymbol{v}_I)}{\sum_{j=1}^V\exp(\boldsymbol{v'}_j^\mathsf{T} \cdot \boldsymbol{v}_I)}</script><p>从上式可以看出，对于任意单一样本，我们都需要对全词表进行指数求和，然而当 $V$ 非常大的时候（实际情况下 $V$​ 通常会有几万到几十万），计算将会变得非常复杂，根据 2.3.3 节关于交叉熵损失函数的介绍中，我们也可以看出进行后向传播的时候，计算过程同样是需要计算完整词表。因此，<a href="https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf" target="_blank" rel="noopener">Morin and Bengio</a> 等人在 2005 年的时候，提出了层级 Softmax，采用二叉树来加速计算。</p><h3 id="2-3-2-层级-Softmax（Hierarchical-Softmax）"><a href="#2-3-2-层级-Softmax（Hierarchical-Softmax）" class="headerlink" title="2.3.2 层级 Softmax（Hierarchical Softmax）"></a>2.3.2 层级 Softmax（Hierarchical Softmax）</h3><p><img width="500" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210811213210.png"></p><p>由于标准的 softmax 的计算复杂度较高，所以人们就不断思考对其进行优化。2001 年 <a href="https://arxiv.org/abs/cs/0108006" target="_blank" rel="noopener"><em>Goodman</em></a> 提出基于分类思想的加速方案。简单来说，假设我们词表中有 10000 个词，在传统的方法是在这 10000 个词上做 <em>softmax</em> 获得每个词的概率分布，然后取出概率最大的词，这样我们需要计算 10000 次。如果我们将这 10000 个词进行分类，假设分成 100 个类别，每个类别 100 个词。这个时候我们的计算过程是，先用一个 <em>softmax</em> 计算下一个词是属于什么类别，然后再用一个 <em>softmax</em> 计算概率最大的类别中的词的概率分布，这样我们只需要两个 100 次的计算量，计算速度直接提升 50 倍。</p><p>基于这个思想，<a href="https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf" target="_blank" rel="noopener"><em>Morin &amp; Bengio</em></a> 于 2005 年提出层级 softmax 的方法：使用平衡二叉树来构建这种分类关系，能够将计算复杂度降到 $O(\log_2(|\mathcal{V}|))$。由于他们利用的是先验知识（wordnet 中的 is-a 关系）来构建二叉树，最终的而效果并不理想。随后 <em>Mnih &amp; Hinton</em> 采用 boostrapping 的方法，从一个随机树开始自动学习一棵平衡二叉树。</p><p>直到 2013 年 <em>Mikolov</em> 等人提出使用 Huffman 树来代替平衡二叉树，使得层级 softmax 在效果和效率上都达到了新的高度。</p><h4 id="2-3-2-1-Huffman-树"><a href="#2-3-2-1-Huffman-树" class="headerlink" title="2.3.2.1 Huffman 树"></a>2.3.2.1 Huffman 树</h4><p>Huffman 树是一个用于数据压缩的算法。计算机中所有的数据都是以 0 和 1 进行存储的，最简单的数据编码方式是 <strong>等长编码</strong>。假设我们的数据中有 6 个字母，那么我们要将这些字母区分开，就至少需要三位二进制数来表示，$2^3=8&gt;6$，如果数据中的字符数更多，那就需要更长的二进制数进行编码。然而我们希望用尽可能少的二进制数对数据进行编码，尤其是实际生活中，有些字符使用频率非常高，另一些字符很少使用。我们希望使用频率高的字符编码长度更短，这样就可以节省存储空间了。所以这里就涉及到 <strong>变长编码</strong>。</p><p>比如，给定一个字符串 <code>aabacdab</code>，包含了 8 个字符，我们发现这个这个字符串中包含了 4 个不同的字符 <code>a</code>、<code>b</code>、<code>c</code>、<code>d</code>，分别对应的频率为 4、2、1、1。由于 <code>a</code> 的频率大于 <code>b</code>，<code>b</code> 的频率大于 <code>c</code> 和 <code>d</code>。所以，我们可以给 <code>a</code> 分配一个 1 位的编码长度，<code>b</code> 分配 2 位的编码长度，<code>c</code> 和 <code>d</code> 分配 3 位的编码长度：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a: 0</span><br><span class="line">b: 11</span><br><span class="line">c: 100</span><br><span class="line">d: 011</span><br></pre></td></tr></table></figure><p>所以，<code>aabacdab</code> 就被编码成了 <code>00110100011011</code>（<code>0|0|11|0|100|011|0|11</code>）。但是这个编码会有问题，那就是歧义性。因为我们不仅需要编码，还需要解码。当我们把数据存储到计算机以后，还需要从计算机中将数据读取出来。读取数据的过程就是解码的过程。如果我们用上面的编码进行存储解码的时候，会出现不同的解码方式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">0|011|0|100|011|0|11    adacdab</span><br><span class="line">0|0|11|0|100|0|11|011   aabacabd</span><br><span class="line">0|011|0|100|0|11|0|11   adacabab</span><br><span class="line">…</span><br></pre></td></tr></table></figure><p>为了避免解码歧义，我们需要保证编码满足 “<strong>前缀规则</strong>”：任意编码不能是其他编码的前缀。在上例中，<code>0</code> 是 <code>011</code> 的前缀，所以才会出现解码歧义性问题。</p><p>Huffman 树就是用来做这种变长编码的数据结构，构造过程如下：</p><ol><li><p>计算字符频率</p><p><img width="250" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/hf-character-frequency.png"></p></li><li><p>根据词频对字符进行排序，并按升序进行排列，得到序列 <code>Q</code>：</p><p><img width="250" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/hf-character-frequency-sorted.png"></p></li><li><p>创建一个空节点 <code>z</code>。节点 <code>z</code> 的左子节点是频率最低的字符，右子节点是频率第二低的字符。节点 <code>z</code> 的频率为左右子节点字符频率之和</p><p><img width="200" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/hf-encoding-1.png"></p></li><li><p>从 <code>Q</code> 中删除两个上一步中两个频率最低的字符，然后将两者频率之和添加到 <code>Q</code> 中。</p></li><li><p>重复 3-4 两步</p><table><tr>    <td><center><img width="250" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/hf-encoding-2.png"></center></td>    <td><center><img width="250" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/hf-encoding-3.png"></center></td></tr></table>            </li><li><p>将左侧的边赋值为 0，右侧的边为 1。</p><p><img width="250" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/hf-encoding-4.png"></p></li></ol><p>这样就构建好了一棵 Huffman 树。Huffman 编码就是找到从根节点到对应的字符之间的路径，然后将路径上的边对应的值拼接在一起。比如，上例中的 <code>A</code>、<code>B</code>、<code>C</code>、<code>D</code> 的编码分别为：<code>11</code>、<code>100</code>、<code>0</code>、<code>101</code>。</p><p>解码过程就是按照编码找到相应的路径：</p><p><img width="250" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/hf-decoding.png"></p><h4 id="2-3-2-2-基于-Huffman-树的层级-softmax"><a href="#2-3-2-2-基于-Huffman-树的层级-softmax" class="headerlink" title="2.3.2.2 基于 Huffman 树的层级 softmax"></a>2.3.2.2 基于 Huffman 树的层级 softmax</h4><p>Word2vec 中是预先统计语料中的词频，根据词频构建起一棵 Huffman 树。</p><blockquote><p>Huffman 树的每个叶子节点是词表中的一个词，每个除叶子节点和根节点以外的节点都表示一个二分类的概率，这个概率用来决定去往左右子节点的路径。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210811213210.png" alt></p><p>如上图所示，每个叶子结点（白圈）表示一个词表中的词 $w_i$，每个非叶子节点（灰圈）表示该路径上的概率。每个词都有一条唯一可达的路径，$n(w_i, j)$ 表示 $w_i$ 的路径上 第 $j$ 个节点。比如 $w_2$ 的路径就是 $n(w_2,1)n(w_2,2)n(w_2,3)w_2$。这条路径就对应 Huffman 编码。$w_2$ 的概率就是这条路径上每个节点的概率累积：</p><script type="math/tex; mode=display">p(w_O \vert w_I) = \prod_{j-1}^{L(w_O)-1} p(n(w_O,j))</script><p>其中 $L(w_O)$  表示 $w_O$ 的路径长度（Huffman 编码长度）。由于这是一个二叉树，相当于 $p(n(w_O,j))$ 是一个二分类，所以可以使用 $\sigma$ 函数进行计算：</p><script type="math/tex; mode=display">p(w_O \vert w_I) = \prod_{j=1}^{L(w_O)-1} \sigma({\mathbb{I}_{\text{turn}} \cdot\boldsymbol{v'}_{n(w_O, j)}}^{\top} \cdot \boldsymbol{v}_{w_I})</script><p>其中 $v’_{n(w_O,j)}$ 表示 $n(w_,j)$ 节点对应的向量，$\mathbb{I}_{\text{turn}}$ 表示特殊的标识函数：如果 $n(w_O,j+1)$ 是 $n(w_O,j)$ 的左子节点，则 $\mathbb{I}_{\text{turn}}=1$ ，否则为 $\mathbb{I}_{\text{turn}}=-1$。比如，上图中，我们要计算 $w_2$ 的概率：</p><script type="math/tex; mode=display">P(w_2 \mid w_I) = \sigma(\boldsymbol{v'}_{n(w_2,1)}^\top \boldsymbol{v}_I) \cdot \sigma(\boldsymbol{v'}_{n(w_2,2)}^\top \boldsymbol{v}_I) \cdot \sigma(-\boldsymbol{v'}_{n(w_2,3)}^\top \boldsymbol{v}_I)</script><p>内部节点的向量 $\boldsymbol{v’}_{n(w_i, j)}$ 可以通过训练得到。由 $\sigma(\cdot)$ 的定义：</p><script type="math/tex; mode=display">\sigma(z) = \frac{1}{1+\exp(-z)}</script><p>可知，整个概率的计算都无需遍历整个词表，只需计算 $\log_2(V)$ 次 $\sigma(\cdot)$ 即可，相当于将计算复杂度降低到了 $\log_2(V)$，大幅提升了计算效率。</p><p>由于 $\sigma(x)+\sigma(-x)=1$，给定中心词 $w_I$，生成词典 $\mathcal{V}$ 中任意词的体哦阿健概率之和也满足：</p><script type="math/tex; mode=display">\sum_{w\in \mathcal{V}} p(w|w_I)=1</script><p>由于 Huffman 树是用来对数据进行压缩编码的，其主要思想是高频的词距离根节点越近，那么它的路径就会越短，所需要计算的 $\sigma(\cdot)$ 函数的次数也会越少。所以相比平衡二叉树，Huffman 树的计算更有效率。</p><p>需要注意的是，我们在训练过程中，由于已知我们需要预测的词是哪一个，所以只需要计算对应的词的概率，然后进行优化即可。但是在推理过程中，我们并不知道哪个词是最优解，所以还是需要遍历整个词表。所以基于 Huffman 树的 word2vec 加速了训练过程而没有加速推理过程。</p><h3 id="2-3-3-交叉熵（Cross-Entropy）"><a href="#2-3-3-交叉熵（Cross-Entropy）" class="headerlink" title="2.3.3 交叉熵（Cross Entropy）"></a>2.3.3 交叉熵（Cross Entropy）</h3><p>交叉熵用于度量两个概率（$p$ 和 $q$​​）分布间的差异性信息的一个指标。计算公式如下：</p><script type="math/tex; mode=display">H(p, q) = -\sum_xp(x)\log q(x)</script><p>当交叉熵用于损失函数的时候，我们需要度量的是真实标签概率分布（$\boldsymbol{y}_{true}$）和模型输出标签概率分布（$\boldsymbol{y}_{pred}$）之间的差异，即：</p><script type="math/tex; mode=display">H(\boldsymbol{y}_{true}, \boldsymbol{y}_{pred}) = -\sum \boldsymbol{y}_{true}\cdot \log(\boldsymbol{y}_{pred})</script><p>在我们的情况下，$\boldsymbol{y}_{true}$​ 中只有 $y_{i=O}=1$​，其余位置 $y_j$​ 全部是 0，$\boldsymbol{y}_{pred} = p(w_i|w_I)$​。也就是说，我们只需要计算 $w_i=w_O$​ 位置的交叉熵即可，如下图所示。 </p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210813001314.jpg" alt></p><script type="math/tex; mode=display">\mathcal{L}_{\theta} = H(y_i, w_i) = -\sum_{i=1}^{V}y_i\log p(w_i|w_I) \overset{i=O}{=} -\log p(w_O|w_I)</script><p>式中 $\theta$ 表示我们需要训练的参数。如上面介绍的，交叉熵是用来度量两个分布的差异性的指标。对于我们的模型来说，当然是 $\boldsymbol{y}_{ture}$ 和 $\boldsymbol{y}_{pred}$​ 的差异越小越好。所以我们模型训练最终的目的是<strong>最小化交叉熵</strong>。</p><p>将 $p(w_O|w_I)$ 的 full softmax 公式代入交叉熵损失函数中得到：</p><script type="math/tex; mode=display">\mathcal{L}_{\theta} = -\log \frac{\exp(\boldsymbol{v'}_{O}^\mathsf{T} \cdot \boldsymbol{v}_I)}{\sum_{j=1}^V\exp(\boldsymbol{v'}_{j}^\mathsf{T} \cdot \boldsymbol{v}_I)}=-\boldsymbol{v'}_{O}^\mathsf{T} \cdot \boldsymbol{v}_I + \log \sum_{j=1}^V \exp(\boldsymbol{v'}_{j}^\mathsf{T} \cdot \boldsymbol{v}_I)</script><p>使用随机梯度下降算法对模型开始训练，需要计算损失函数的梯度。为了简化，我们令 $z_{IO}=\boldsymbol{v’}_{O}^\mathsf{T} \cdot \boldsymbol{v}_I$​ 及 $z_{Ij}=\boldsymbol{v’}_{j}^\mathsf{T} \cdot \boldsymbol{v}_I$​。</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}\nabla_\theta \mathcal{L}_\theta &= \nabla_\theta(-z_{IO}+\log\sum_{j=1}^V\exp(z_{Ij}))\\\\                                 &= -\nabla_\theta z_{IO} + \nabla_\theta(\log \sum_{j=1}^V \exp(z_{Ij})) \\\\                                 &= -\nabla_\theta z_{IO} + \frac{1}{\sum_{j=1}^V\exp(z_{Ij})} \sum_{j=1}^V \exp(z_{Ij}) \cdot \nabla_\theta z_{Ij} \\\\                                 &= -\nabla_\theta z_{IO} + \sum_{j=1}^V \frac{\exp(z_{Ij})}{\sum_{j=1}^V\exp(z_{Ij})} \cdot \nabla_\theta z_{Ij} \\\\                                 &= -\nabla_\theta z_{IO} + \sum_{j=1}^V p(w_j|w_I) \cdot \nabla_\theta z_{Ij} \\\\                                 &= -\nabla_\theta z_{IO} + \mathbb{E}_{w_j \sim Q(\bar{w})} \cdot \nabla_\theta z_{Ij}\end{aligned}\end{equation}</script><p>将 $z_{IO}$ 和 $z_{Ij}$ 代回原式，根据下面两式：</p><script type="math/tex; mode=display">\nabla_\theta z_{IO} =  \frac{\partial (\boldsymbol{v'}_{O}^\mathsf{T} \cdot \boldsymbol{v}_I)}{\partial \boldsymbol{v}_I} = \boldsymbol{v'}_{O} ,\quad\nabla_\theta z_{Ij} = \frac{\partial (\boldsymbol{v'}_{j}^\mathsf{T} \cdot \boldsymbol{v}_I)}{\partial \boldsymbol{v}_I} = \boldsymbol{v'}_{j} \\\\</script><p>可得：</p><script type="math/tex; mode=display">\nabla_\theta \mathcal{L}_\theta = -\boldsymbol{v'}_{O} + \mathbb{E}_{w_j \sim Q(\tilde{w})} \cdot \boldsymbol{v'}_{j}</script><p>上式中 $Q(\tilde{w})$​ 表示噪声概率分布。根据上式，输出词的词向量越大，损失越小；而其他词的词向量越小，则损失越小。因此，交叉熵损失函数会使模型将正确的输出更加凸显，而对错误的输出进行压制，从而使参数达到最优。</p><h3 id="2-3-4-Noise-Contrastive-Estimation"><a href="#2-3-4-Noise-Contrastive-Estimation" class="headerlink" title="2.3.4 Noise Contrastive Estimation"></a>2.3.4 Noise Contrastive Estimation</h3><p>噪声对比估计（NCE）是通过简单的逻辑回归来区分目标词和非目标词的。</p><p>给定输入词 $w_I$，正确的输出词是 $w_O$。同时，我们可以从噪声词分布 $Q(\tilde{w})$ 中进行采样得到 $N$ 个负样本词：</p><script type="math/tex; mode=display">\tilde{w}_1,\tilde{w}_2,\dots,\tilde{w}_N \sim Q(\tilde{w})</script><p>此时，我们的样本就成了 $w_O$  为正样本，$\tilde{w}_1,\tilde{w}_2,\dots,\tilde{w}_N$ 为负样本，然后再用一个二分类器进行分类：</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - \left[ \log p(d=1 \vert w_O, w_I) + \sum_{i=1, \tilde{w}_i \sim Q}^N \log p(d=0|\tilde{w}_i, w_I) \right]</script><p>$d$ 表示二分类器的输出标签。</p><p>当 $N$ 足够大时，根据<a href="https://en.wikipedia.org/wiki/Law_of_large_numbers" target="_blank" rel="noopener">大数定理</a>可得:</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - \left[ \log p(d=1 \vert w_O, w_I) + N\mathbb{E}_{\tilde{w}_i \sim Q} \log p(d=0|\tilde{w}_i, w_I) \right]</script><p>为了计算概率分布 $p(d=1 \vert w_O, w_I)$，我们可以从联合概率 $p(d, w_j \vert w_I), w_j \in [w_O, \tilde{w}_1, \tilde{w}_2, \dots, \tilde{w}_N]$。我们有 $1/(N+1)$ 的概率得到 $w_j=w_O$，这个概率是一个条件概率 $p(w_j=w_O\vert w_I)$，同时我们有 $N/(N+1)$ 的概率得到噪声词 $q(\tilde{w}_{1:N})$。</p><script type="math/tex; mode=display">p(d, w_j | w_I) = \begin{cases} \frac{1}{N+1} p(w_O \vert w_I) & \text{if } d=1 \\\\ \frac{N}{N+1} q(\tilde{w}_{1:N}) & \text{if } d=0 \end{cases}</script><p>然后我们可以计算 $p(d=1 \vert w, w_I)$ 和 $p(d=0 \vert w, w_I)$：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned} p(d=1 \vert w, w_I) &= \frac{p(d=1, w \vert w_I)}{p(d=1, w \vert w_I) + p(d=0, w \vert w_I)} \\\\                     &\overset{贝叶斯公式}{=} \frac{p(w \vert w_I)}{p(w \vert w_I) + Nq(\tilde{w})}\end{aligned}\end{equation}</script><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}p(d=0 \vert w, w_I) &= \frac{p(d=0, w \vert w_I)}{p(d=1, w \vert w_I) + p(d=0, w \vert w_I)}\\\\ &\overset{贝叶斯公式}{=} \frac{Nq(\tilde{w})}{p(w \vert w_I) + Nq(\tilde{w})} \end{aligned}\end{equation}</script><p>最后，NCE 二分类器的损失函数为：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned} \mathcal{L}_\theta & = - \left[ \log p(d=1 \vert w, w_I) + \sum_{\substack{i=1 \\\\ \tilde{w}_i \sim Q}}^N \log p(d=0|\tilde{w}_i, w_I) \right] \\\\                    & = - \left[ \log \frac{p(w \vert w_I)}{p(w \vert w_I) + Nq(\tilde{w})} + \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^N \log \frac{Nq(\tilde{w}_i)}{p(w \vert w_I) + Nq(\tilde{w}_i)} \right] \end{aligned}\end{equation}</script><p>然而，我们会发现公式中仍然有 $p(w \vert w_I)$ ，即仍然要对整个词表进行求和。为了方便，令 $Z(w_I)$ 为 $p(w\vert w_I)$ 的分母。NCE 对于 $Z(w_I)$ 的处理有两种假设：</p><ol><li><p>将 $Z(w_I)$ 视作常数。<a href="https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf" target="_blank" rel="noopener">Mnih &amp; Teh, 2012</a> 证明对于参数量很大的神经网络模型来说，将 $Z(w_I)$ 固定为 1 对每个 $w_I$ 仍是成立的。此时，上面的损失函数可以简化成：</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - \left[ \log \frac{\exp({v'_w}^{\top}{v_{w_I}})}{\exp({v'_w}^{\top}{v_{w_I}}) + Nq(\tilde{w})} + \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^N \log \frac{Nq(\tilde{w}_i)}{\exp({v'_w}^{\top}{v_{w_I}}) + Nq(\tilde{w}_i)}\right]</script><ul><li><p>这种情况下，我们可以证明，当 $N \to \infty$ 时，$\nabla_\theta \mathcal{L}_{NCE}=\nabla_\theta\mathcal{L}_{entrpy}$。证明过程可参看 <a href="https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf" target="_blank" rel="noopener">Mnih &amp; Teh, 2012</a>。所以 NCE 的优化目标和交叉熵是一样的。作者还发现，当 $N=25$ 时，效果就已经与标准 softmax 效果差不多了，但是速度提升了 45 倍。</p></li><li><p>实际上 $Z(w_I)$ 到底取值是多少，不同作者都有过不同的尝试。但是从表现来看，不同点只是开始的时候收敛速度不同，最终的结果相差不大。</p></li><li><p>噪声分布 $Q(\tilde{w})$ 是一个可调参数，在选择 $Q$ 的分布的时候应该考虑两点：</p><p>① 接近真实数据分布；</p><p>② 容易采样</p></li></ul></li><li><p>将 $Z(w_I)$ 看作一个可训练的参数。</p></li></ol><p>从实践来看，当训练语料比较小的时候，$Z(w_I)$ 直接设置为常数效果更好。当有足够语料的时候，$Z(w_I)$ 作为可训练的一个参数效果更好。</p><p>NCE 处理似乎是故意绕开了标准 Softmax 计算量最大的分母，但其背后有充分的理论推导和证明。如果直接在最大似然估计上用这两种假设（之一）是否可行？</p><p>答案还真是不行。两种情况：</p><ol><li>如果最大似然估计中的 $Z(w_I)$ 为常数，那么 $\mathcal{L}_\theta$ 的第二项 $\log Z(w_I)$ 就是常数，这就意味着 $\mathcal{L}_\theta$ 的导数的第二项就为 0。也就是噪声词的词向量缺少约束，模型只需要让目标词的概率变大即可，最坏情况下预测所有词的概率为 1 即可。</li><li>如果 $Z(w_I)$ 为可训练的一个参数，这个参数没有和数据产生任何联系，只需要简单的变小，就可以让似然概率变大，得到一个完全与数据无关的结果，所以也不可行。</li></ol><h3 id="2-3-5-Negative-Sampling"><a href="#2-3-5-Negative-Sampling" class="headerlink" title="2.3.5 Negative Sampling"></a>2.3.5 Negative Sampling</h3><p><em><a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener">Mikolov</a></em> 等人 2013 年提出的负采样方法是 NCE 的一个简化版变种。因为 word2vec 的目标是训练高质量的词向量，而不是对自然语言中的词进行建模。所以，<em>Mikolov</em> 等人在 NCE 的基础上进一步简化。</p><p>在 NCE 假设 $Z(w_I)=1$ 的基础上，进一步令 $N q(\tilde{w})=1$，则</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}p(d=1\vert w, w_I) &= \frac{p(w \vert w_I)}{p(w \vert w_I)+1} \\\\                   &= \sigma({v'_{w}}^\top v_{w_I}) \\\\p(d=0\vert w, w_I) &= \frac{1}{p(w \vert w_I) + 1} \\\\                    &= 1 - \sigma({v'_{w}}^\top v_{w_I}) \\\\                   &= \sigma(-{v'_{w}}^\top v_{w_I})\end{aligned}\end{equation}</script><p>那么负采样的损失函数为：</p><script type="math/tex; mode=display">\mathcal{L}_\theta =  - \left[ \log \sigma({v'_{w}}^\top v_{w_I}) + \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^N \log \sigma(-{v'_{\tilde{w}_i}}^\top v_{w_I}) \right]</script><p>因为 $Nq(\tilde{w})=1$，所以 $q(\tilde{w})=1/N$ 是一个均匀分布。这里的均匀采样并不是每个词采样概率相同，而是在总的语料中进行均匀采样。这就意味着，它实际上是按照每个词本身的词频来进行采样的，词频越高，采样的概率就越高。这种情况下，模型最终拟合的实际是词的互信息。详细解答看这里：<a href="https://spaces.ac.cn/archives/5617" target="_blank" rel="noopener">“噪声对比估计”杂谈：曲径通幽之妙</a>。互信息与条件概率的区别就类似：条件概率反映“我认识周杰伦，周杰伦却不认识我”，而互信息反映的是“你认识我，我也认识你”。所以，通常负采样的效果比层次 softmax 要好一些。</p><h2 id="2-4-一些小技巧"><a href="#2-4-一些小技巧" class="headerlink" title="2.4 一些小技巧"></a>2.4 一些小技巧</h2><ul><li><p><strong>Soft slide window</strong>。利用滑动窗口构建输入词和输出词样本对的时候，我们可以给距离较远的词更低的权重。比如，设置窗口就最大值 $s_{\text{max}}$，然后每次训练时的真实窗口大小是从 $[1, s_{\text{max}}]$ 中进行随机采样。因此，每个上下文词都有 $1/(d)$ 的概率被取到，其中 $d$ 表示到中心词的距离。</p></li><li><p><strong>下采样高频词</strong>。极端高端的词可能由于太常见而无法得以区分（比如停用词）。而低频词可能会带有很重要的信息。为了平衡高频词和低频词，<em>Mikolov</em> 等人提出采样时对每个词施加一个采样概率 $1-\sqrt{t/f(w)}$。其中 $f(w)$ 表示词频，$t$ 表示相关性阈值，通常取值为 $10^{-5}$。</p></li><li><p><strong>先学词组</strong>。词组表示一个有意义的概念单元，而非简单的独立单词的组合。先学习这些词组将他们作为一个词单元来处理可以提升词向量的质量。比如基于 unigram 和 bigram 统计：</p><script type="math/tex; mode=display">s_{\text{phrase}} = \frac{C(w_i w_j) - \delta}{ C(w_i)C(w_j)}</script><p>其中 $C(\cdot)$ 表示 unigram $w_i$ 或 bigram $w_iw_j$ 的数量，$\delta$ 表示衰减阈值，防止过高频的词或词组。$s_{\text{phrase}}$ 得分越高则采样几率越高。为了形成长于两个单词的短语，我们可以随着分数截止值的降低多次扫描词汇表。</p></li></ul><h1 id="3-Count-based-GloVe"><a href="#3-Count-based-GloVe" class="headerlink" title="3. Count-based: GloVe"></a>3. Count-based: GloVe</h1><p>GloVe（<em>The Global Vector</em>）是 <a href="http://www.aclweb.org/anthology/D14-1162" target="_blank" rel="noopener">Pennington</a> 等人于 2014 年提出的模型。 GloVe 结合了 矩阵分解和 skip-gram 模型。</p><p>众所周知，统计数量和共现可以表示词义。为了区分上下文的词嵌入 $p(w_O \vert w_I)$，我们定义共现概率：</p><script type="math/tex; mode=display">p_{\text{co}}(w_k \vert w_i) = \frac{C(w_i, w_k)}{C(w_i)}</script><p>$C(w_i, w_k)$ 表示 $w_i$ 和 $w_k$ 的共现频率。假设有两个词 $w_i=”ice”$ 和 $w_j=”steam”$，第三个词 $\tilde{w}_k=”solid”$ 与 $”ice”$ 相关，但是与 $”steam”$ 无关，我们希望：</p><script type="math/tex; mode=display">p_{\text{co}}(\tilde{w}_k \vert w_i) > p_{\text{co}}(\tilde{w}_k \vert w_j)</script><p>因此 $\frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert w_j)}$ 会非常大。而如果 $\tilde{w}_k=”water”$ 与 $”ice”$ 和 $”steam”$ 都有关系，或者 $\tilde{w}_k=”fashion”$ 与两者都没有关系，$\frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert w_j)}$ 会接近 1。</p><p>以上描述给我们的直观感受就是，词义是通过共现概率分布的比例得到的，而非共现概率本身。所以，GloVe 模型是将第三个词的向量取决于另两个词之间的关系：</p><script type="math/tex; mode=display">F(w_i, w_j, \tilde{w}_k) = \frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert w_j)}</script><p>确定 $F$ 的函数形式过程如下：</p><ol><li><p>$F(w_i, w_j, \tilde{w}_k)$ 是考察 $i, j, k$ 三个词的相似关系，不妨单独考察 $i, j$ 两个词。在线性空间中，两个向量的相似性最简单的就是欧氏距离 $v_i, v_j$，所以 $F$ 可以是</p><script type="math/tex; mode=display">F(w_i-w_j, \tilde{w}_k) = \frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert w_j)}</script></li><li><p>$\frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert w_j)}$ 是一个标量，而 $F$ 是作用在两个向量上的，向量与矢量之间的关系自然就可以想到内积，所以进一步确定 $F$ 的形式：</p><script type="math/tex; mode=display">F((w_i-w_j) \top \tilde{w}_k) = F(w_i\top \tilde{w}_k-w_j \top \tilde{w}_k) = \frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert w_j)}</script></li><li><p>上式中，左边是差，右边是商。可以通过 $\exp(\cdot)$ 函数将两者结合在一起：</p><script type="math/tex; mode=display">\exp(w_i\top \tilde{w}_k-w_j \top \tilde{w}_k) = \frac{\exp(w_i \top \tilde{w}_k)}{\exp(w_j \top \tilde{w}_k)} = \frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert w_j)}</script></li><li><p>现在只要让分子分母分别相等，上式就可以成立：</p><script type="math/tex; mode=display">\exp(w_i \top \tilde{w}_k) = p_{co}(\tilde{w}_k \vert w_i) \\\\\exp(w_j \top \tilde{w}_k) = p_{co}(\tilde{w}_k \vert w_j)</script></li><li><p>只需要满足：</p><script type="math/tex; mode=display">{w_i}^\top \tilde{w}_k = \log p_{\text{co}}(\tilde{w}_k \vert w_i) = \log \frac{C(w_i, \tilde{w}_k)}{C(w_i)} = \log C(w_i, \tilde{w}_k) - \log C(w_i)</script></li><li><p>由于 $w_i$ 和 $\tilde{w}_k$ 是向量，所以 $\tilde{w}_k \top w_i = w_i \top \tilde{w}_k$ ，这就意味着上式中 $i, k$ 是顺序不敏感的，但是右边交换 $i,k$ 的顺序结果就会不同。为了解决这个对称性问题，模型引入两个偏置项 $b_i, b_k$，则模型变成：</p><script type="math/tex; mode=display">\log C(w_i, \tilde{w}_k) = w_i \top \tilde{w}_k + b_i +\tilde{b}_k</script></li><li><p>上面的公式只是理想状态下，实际上左右只能无限接近，所以损失函数定义为：</p><script type="math/tex; mode=display">\mathcal{L}_\theta = \sum_{i=1, k=1}^V ({w_i}^\top \tilde{w}_k + b_i + \tilde{b}_k - \log C(w_i, \tilde{w}_k))^2</script></li><li><p>根据经验，如果两个词共现次数越多，那么两个词在损失函数中的影响就应该越大，所以可以根据两个词共现的次数设计一个权重来对损失函数进行加权：</p><script type="math/tex; mode=display">\mathcal{L}_\theta = \sum_{i=1, j=1}^V f(C(w_i,\tilde{w}_k)) ({w_i}^\top \tilde{w}_k + b_i + \tilde{b}_k - \log C(w_i, \tilde{w}_k))^2</script><p>权重函数 $f(\cdot)$ 应该有以下性质：</p><p>① $f(0)=0$，即如果两个词没有共现过，那么权重为 0；</p><p>② $f(x)$ 必须是一个单调递增的函数。两个词共现次数越多，反而权重越小违反了设置权重项的初衷；</p><p>③ $f(x)$ 对于共现次数过多的词对，不能有太大的值，比如停用词。</p><p>有了这三个性质，可以将 $f(x)$ 定义为：</p><script type="math/tex; mode=display">f(x) = \begin{cases}(\frac{x}{x_{\text{max}}})^\alpha,\quad & \text{if}\quad x<x_{\text{max}}\\\\1, \quad & \text{otherwise}\end{cases}</script><p>根据经验 GloVe 作者认为 $x_\text{max}=100, \alpha=3/4$ 是一个比较好的选择。</p></li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p><a href="https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/" target="_blank" rel="noopener">The amazing power of word vectors</a>, <em>Adrian Colyer</em></p></li><li><p><a href="https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#glove-global-vectors" target="_blank" rel="noopener">Learning Word Embedding</a>, <em>Lilian Weng</em></p></li><li><p><a href="https://jalammar.github.io/illustrated-word2vec/" target="_blank" rel="noopener">Illustrated word2vec</a>, <em>Jay Alammar</em></p></li><li><p><a href="https://zh.d2l.ai/chapter_natural-language-processing/word2vec.html" target="_blank" rel="noopener">Dive into deep learning: word2vec</a></p></li><li><p><a href="https://zh.d2l.ai/chapter_natural-language-processing/glove.html" target="_blank" rel="noopener">Dive into deep learning: GloVe</a></p></li><li><p><a href="http://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">Efficient Estimation of Word Representations in Vector Space</a> <em>Mikolov et al. 2013</em></p></li><li><p><a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener">Distributed Representations of Words and Phrases and their Compositionality</a> <em>Mikolov et al. 2013</em></p></li><li><p><a href="http://www.aclweb.org/anthology/N13-1090" target="_blank" rel="noopener">Linguistic Regularities in Continuous Space Word Representations</a> <em>Mikolov et al. 2013</em></p></li><li><p><a href="http://arxiv.org/pdf/1411.2738v3.pdf" target="_blank" rel="noopener">word2vec Parameter Learning Explained</a> <em>Rong 2014</em></p></li><li><p><a href="http://arxiv.org/pdf/1402.3722v1.pdf" target="_blank" rel="noopener">word2vec Explained: Deriving Mikolov et al’s Negative Sampling Word-Embedding Method</a> <em>Goldberg and Levy 2014</em></p></li><li><p><a href="https://nlp.stanford.edu/pubs/glove.pdf" target="_blank" rel="noopener">GloVe: Global Vectors for Word Representation</a>, <em>Jeffrey Pennington et al. 2014</em></p></li><li><p><a href="https://www.gavagai.io/text-analytics/a-brief-history-of-word-embeddings/" target="_blank" rel="noopener">A Brief History of Word Embeddings</a>, <em>gavagai</em></p></li><li><p><a href="https://licor.me/post/word-representation/" target="_blank" rel="noopener">Word Representation</a>, <em>Chuanrong Li</em></p></li><li><p>Devopedia. 2020. “Word2vec.” Version 4, September 5. Accessed 2021-03-28. <a href="https://devopedia.org/word2vec" target="_blank" rel="noopener">https://devopedia.org/word2vec</a></p></li><li><p><a href="https://www.cnblogs.com/peghoty/p/3857839.html" target="_blank" rel="noopener">word2vec 中的数学原理详解</a>, <em>peghoty</em></p></li><li><p><a href="https://www.techiedelight.com/huffman-coding/" target="_blank" rel="noopener">Huffman Coding Compression Algorithm</a> </p></li><li><p><a href="https://www.programiz.com/dsa/huffman-coding" target="_blank" rel="noopener">Huffman Coding</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/368939108" target="_blank" rel="noopener">噪声对比估计 Noise Contrastive Estimation</a>, <em>码农要术</em></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/42073620" target="_blank" rel="noopener">(十五）通俗易懂理解——Glove算法原理</a>, <em>梦里寻梦</em> </p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 语言模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 词向量 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法与数据结构（Python）：array</title>
      <link href="/2021/08/09/ds-array/"/>
      <url>/2021/08/09/ds-array/</url>
      
        <content type="html"><![CDATA[<p>数组是一个容器，它容纳的元素应该是相同的数据类型。数组有两个重要概念：</p><ul><li><strong>元素</strong> —— 存储的数组中的数据称为元素。</li><li><strong>索引</strong> —— 数组中每个元素所在的位置。</li></ul><a id="more"></a><h1 id="1-数组的表示"><a href="#1-数组的表示" class="headerlink" title="1. 数组的表示"></a>1. 数组的表示</h1><p><img src="https://codingdict.com/static/assets/tutorials/python/ds/array_declaration.jpg" alt></p><ul><li><code>int</code>  表示数组中数字的类型为整型</li><li><code>array</code> 表示数组的名字</li><li><code>[10]</code> 表示数组的尺寸，即数组中有多少个元素</li><li><code>{35, 33, 42, ...}</code> 表示数组存储的数据</li></ul><p><img src="https://codingdict.com/static/assets/tutorials/python/ds/array_representation.jpg" alt></p><ul><li>索引从 0 开始</li><li>数组的尺寸是 10，表示它可以存储 10 个元素</li><li>每个元素可以通过索引访问</li></ul><h1 id="2-基本操作"><a href="#2-基本操作" class="headerlink" title="2. 基本操作"></a>2. 基本操作</h1><p>数组的基本操作包括：</p><ul><li><strong>遍历</strong> —— 逐个获得数组中的元素</li><li><strong>插入</strong> —— 在指定的位置（索引）处添加一个元素</li><li><strong>删除</strong> —— 删除指定位置（索引）处的元素</li><li><strong>搜索</strong> —— 搜索指定位置（索引）处的元素</li><li><strong>更新</strong> —— 更新指定位置（索引）处的元素</li></ul><p><code>python</code> 内置的 <code>array</code> 模块可以用来创建数组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> array <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">arrayName = array(typecode, [Initializerss])</span><br></pre></td></tr></table></figure><p>其中 <code>typecode</code> 用于定义数组中元素的数据类型，一些常用的 <code>typecode</code> 如下：</p><div class="table-container"><table><thead><tr><th style="text-align:center">typecode</th><th style="text-align:left">表示</th></tr></thead><tbody><tr><td style="text-align:center">b</td><td style="text-align:left">大小为1字节/ td&gt;的有符号整数</td></tr><tr><td style="text-align:center">B</td><td style="text-align:left">大小为1字节的无符号整数</td></tr><tr><td style="text-align:center">C</td><td style="text-align:left">大小为1字节的字符</td></tr><tr><td style="text-align:center">i</td><td style="text-align:left">大小为2个字节的带符号整数</td></tr><tr><td style="text-align:center">I</td><td style="text-align:left">大小为2个字节的无符号整数</td></tr><tr><td style="text-align:center">F</td><td style="text-align:left">大小为4字节的浮点</td></tr><tr><td style="text-align:center">d</td><td style="text-align:left">大小为8个字节的浮点</td></tr></tbody></table></div><p>举个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> array <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">array1 = array(<span class="string">'i'</span>, [<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>,<span class="number">50</span>])</span><br></pre></td></tr></table></figure><h2 id="2-1-遍历"><a href="#2-1-遍历" class="headerlink" title="2.1 遍历"></a>2.1 遍历</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> array1:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10</span></span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="number">30</span></span><br><span class="line"><span class="number">40</span></span><br><span class="line"><span class="number">50</span></span><br></pre></td></tr></table></figure><h2 id="2-2-搜索"><a href="#2-2-搜索" class="headerlink" title="2.2 搜索"></a>2.2 搜索</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法一</span></span><br><span class="line">print(array1[<span class="number">0</span>])</span><br><span class="line">print(array1[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法二</span></span><br><span class="line">print(array1.index(<span class="number">40</span>))</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10</span></span><br><span class="line"><span class="number">30</span></span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure><h2 id="2-3-插入"><a href="#2-3-插入" class="headerlink" title="2.3 插入"></a>2.3 插入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array1.insert(<span class="number">1</span>,<span class="number">60</span>)</span><br><span class="line">print(array1)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array(<span class="string">'i'</span>, [<span class="number">10</span>, <span class="number">60</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>])</span><br></pre></td></tr></table></figure><h2 id="2-4-删除"><a href="#2-4-删除" class="headerlink" title="2.4 删除"></a>2.4 删除</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array1.remove(<span class="number">40</span>)</span><br><span class="line">print(array1)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array(<span class="string">'i'</span>, [<span class="number">10</span>, <span class="number">60</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">50</span>])</span><br></pre></td></tr></table></figure><h2 id="2-5-更新"><a href="#2-5-更新" class="headerlink" title="2.5 更新"></a>2.5 更新</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array1[<span class="number">2</span>] = <span class="number">80</span></span><br><span class="line">print(array1)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array(<span class="string">'i'</span>, [<span class="number">10</span>, <span class="number">60</span>, <span class="number">80</span>, <span class="number">30</span>, <span class="number">50</span>])</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://codingdict.com/article/4830" target="_blank" rel="noopener">Python-数组</a></p>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数组 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ROC-AUC原理及计算方法</title>
      <link href="/2021/07/29/roc-auc/"/>
      <url>/2021/07/29/roc-auc/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文转载自知乎用户<strong>码农要术</strong>的文章 <a href="https://zhuanlan.zhihu.com/p/141266017" target="_blank" rel="noopener">衡量指标篇：ROC-AUC</a>。</p></blockquote><h1 id="1-历史起源"><a href="#1-历史起源" class="headerlink" title="1. 历史起源"></a>1. 历史起源</h1><p>1941年，日军偷袭珍珠港，太平洋战争由此爆发。美军的雷达操作员（Radar operator）开始忙碌了起来，他们要识别出雷达屏幕上的光点是不是日本的战机。</p><a id="more"></a><p><img width="300" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/radar.jpg"></p><p>因为光点也有可能是我方军舰，也有可能是噪声。为了衡量识别的性能，研究者们设计了ROC曲线（Receiver operating characteristic curve）。所谓 Receiver 就是雷达接收器，operating characteristic 则是表明雷达操作员（Radar operator）的识别能力。</p><p>后来，ROC曲线被应用到了医学领域，还有机器学习领域。虽然名字比较奇怪，但是从诞生之初，ROC 曲线的目的就是衡量分类的性能。AUC 是 ROC 曲线下的面积（ <strong>A</strong>rea <strong>U</strong>nder the ROC <strong>C</strong>urve），有一些优雅的性质，我们后面再说。</p><p>想讲清楚 ROC曲线，先要讲一下混淆矩阵。</p><h1 id="2-混淆矩阵"><a href="#2-混淆矩阵" class="headerlink" title="2. 混淆矩阵"></a>2. 混淆矩阵</h1><p>先从两类开始说起，Positive 和 Negative，医学上叫阳性和阴性，机器学习称之为正例和负例。经过分类器的决策后，一般情况下，正例预测的有对有错，负例预测的也有对有错。这样数据会被划分成4部分：<strong>正例预测对（True Positive），正例预测错（False Negtative），负例预测对（True Negative），负例预测错（False Positive）。</strong></p><p><img width="600" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/confusion_matrix1.jpg"></p><h1 id="3-如何衡量分类器的好坏？"><a href="#3-如何衡量分类器的好坏？" class="headerlink" title="3. 如何衡量分类器的好坏？"></a>3. 如何衡量分类器的好坏？</h1><p>如何衡量一个分类器是有效的，而不是随机结果？还是以雷达识别敌舰这个场景来说明。</p><h2 id="3-1-两个假设"><a href="#3-1-两个假设" class="headerlink" title="3.1 两个假设"></a>3.1 两个假设</h2><ul><li><p>正负例等比例分布 </p></li><li><p>分类器输出是离散值, 也就是 label 的集合</p></li></ul><p>此时预测为正的结果可以划分成两部分：$TP$ 和 $FP$。比较两者关系，有如下结论：</p><ol><li>如果分类器是随机抽样，那么模型的输出和正负例比例一致。也就是 $TP=FP$。这个时候向识别出来的敌舰(预测为正的样本)开炮就是无差别攻击。</li><li>如果 $TP&gt;FP$, 可以说分类器有一定的甄别能力，战争中可以做到伤敌一千，自损八百。</li><li>如果是 $TP&lt;FP$ ,则说明分类器太差了，都不如随机抽样。用在战争中可以做到伤敌八百，自损一千。</li></ol><h2 id="3-2-一个假设"><a href="#3-2-一个假设" class="headerlink" title="3.2 一个假设"></a>3.2 一个假设</h2><blockquote><p>分类器输出是离散值, 也就是 label 的集合</p></blockquote><p>这个时候在用TP和FP的绝对值做对比就显得不公平, 举个例子，我方军舰10艘，敌方军舰100艘。预测并且击沉我方军舰 8 艘，敌方军舰 9 艘.绝对数量上确实是占优势，但是我方基本全军覆没，敌方绝大多数战力仍然保留。样本不均衡时，就得做归一化，看相对值。</p><p>这里引入两个概念：$TPR$ （True Positive Rate），$FPR$（False Positive Rate）</p><script type="math/tex; mode=display">TPR = \frac{TP}{TP+FN} \\\\FPR = \frac{\mathrm{FP}}{FP+TN}</script><p>$TPR$ 就是正例中预测正确的比率。$FPR$ 就是负例预测错的比例。</p><p>$TPR$ 和 $FPR$，比较两者关系，有如下结论：</p><ol><li>如果分类器是随机抽样，那么模型的输出和正负例比例一致。也就是 $TPR=FPR$。这个时候向识别出来的敌舰(预测为正的样本)开炮就是无差别攻击。</li><li>如果 $TPR&gt;FPR$, 可以说分类器有一定的甄别能力，战争中伤敌的比率高于自损的比率。</li><li>如果是 $TPR&lt;FPR$ ,则说明分类器太差，不如随机抽样。战争中伤敌的比率低于自损的比率。</li></ol><p>把 $TPR$ 和 $FPR$ 可视化，在 “<em>分类器输出是离散值, 也就是 label</em>“ 的假设下，$TPR$ 和 $FPR$ 是确定的，在二维坐标系上就是一个点。这个点就是 ROC 曲线的雏形。如下图：</p><p><img width="300" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/roc-auc8.jpg"></p><p>图中，E 点就是随机抽样 （$TPR=FPR$）。A，B，D点表示分类器有一定的甄别能力（$TPR&gt;FPR$）。其中 A 点对应的是一个完美的分类器，所有的正例被识别正确（$TPR=1$），所有的负例没有识别错误（$FPR=0$）。F 点就是分类器太差（$TPR&lt;FPR$），不如随机抽样。</p><h2 id="3-3-另一个假设"><a href="#3-3-另一个假设" class="headerlink" title="3.3 另一个假设"></a>3.3 另一个假设</h2><blockquote><p> 分类器输出连续值</p></blockquote><p>此时需要确定一个阈值来决定混淆矩阵和 $TPR$，$FPR$。</p><p>$TPR$ 的计算如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/roc-auc1.gif" alt></p><p>$FPR$ 的计算如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/roc-auc2.gif" alt></p><p>对于同一个分类器，不同的阈值对应不同的 $TPR$ 和 $FPR$，遍历阈值，即可得到 ROC 曲线。如下图：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/roc-auc3.gif" alt></p><p>对于一个分类器，固定阈值，则得到一条 ROC 曲线。不同分类器会使预测的数据分布不同，在固定阈值的情况下，ROC 曲线变化如下图：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/roc-auc4.gif" alt></p><p>直观来看，分类器的区分度越好，ROC 曲线则越往左上角靠拢。AUC 就越大。怎么解释？</p><h1 id="4-AUC-的概率解释"><a href="#4-AUC-的概率解释" class="headerlink" title="4. AUC 的概率解释"></a>4. AUC 的概率解释</h1><p>如果把 ROC 曲线看成是 $TPR$ 对 $FPR$ 的函数，$TPR=F(x)$ 我们对这个函数进行积分。如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/roc-auc5.jpg" alt></p><script type="math/tex; mode=display">AUC = \int_{0}^1F(x)dx</script><p>假设样本标签为 $y$，模型预测得分为 $s$，阈值为 $t$，正例的概率密度函数为 $f_1(s)$，负例的概率密度函数为 $f_0(s)$，则有</p><script type="math/tex; mode=display">TPR = F(x) = \int_t^\infty f_1(s)ds = P(s>t|y=1) \\\\FPR = x = \int_t^\infty f_0(s)ds = 1-\int_{-\infty}^t f_0(s)ds</script><p>$x$ 是 $t$ 的积分上限函数，根据积分上限函数的性质，得到</p><script type="math/tex; mode=display">\frac{dx}{dt} = \frac{d}{dt}(1-\int_{-\infty}^t f_0(s)ds) = -f_0(t) \\\\dx = -f_0(t)dt = -P(s'=t|y'=0)dt</script><p>则有</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}AUC &= \int_0^1F(x)dx \\\\    &= \int_{\infty}^{-\infty} F(x)(-f_0(t))dt \\\\    &= \int_{-\infty}^{\infty} F(x)f_0(t)dt \\\\    &= \int_{-\infty}^{\infty} P(s>t|y=1)f_0(t)dt \\\\    &= \int_{-\infty}^{\infty} P(s>t|y=1)\times P(s/=t|y'=0)dt \\\\    &= \int_{-\infty}^{\infty} P(s>s'\ \&\ s'=t|y=1\ \&\ y'=0)dt \\\\    &= P(s>s'|y=1\ \&\ y'=1)\end{aligned}\end{equation}</script><p>上面推导需要解释一下：</p><ol><li>第二行，因为 $FPR$ 的取值范围从 0 到 1，对应着阈值是从大到小的变化。可以从动图中看出，只不过动图中阈值是从小到大，$FPR$ 是从 1 到 0。</li><li>第五行，$f_0(t)$ 的含义就是该样本为负例，得分为 $t$ 的概率。加引号是为了和正例区分。</li><li>第七行，该积分相当于是遍历阈值 $t$，同时负例得分和 $t$ 相同，也就是负例遍历所有可能的得分情况。</li></ol><p>最终得到这么一个结论：</p><blockquote><p><strong><em>AUC 的值，就是从样本中任意取一个正例和一个负例，正例得分大于负例得分的概率。</em></strong></p></blockquote><h1 id="5-AUC-的一些性质"><a href="#5-AUC-的一些性质" class="headerlink" title="5. AUC 的一些性质"></a>5. AUC 的一些性质</h1><p>从公式可以看出，$TPR$ 的计算只局限在正例中，$FPR$ 的计算只局限在负例中。正例（或负例）如果同分布的增加或者减小，对于 ROC 曲线来说没有区别，因为在正例（或负例）内部已经做了归一化。如下图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/roc-auc6.gif" alt></p><p>但如果正例（或负例）的比例在变化的同时，分布也发生了变化，那么 ROC 和 AUC 也会随之变化。如下图</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/roc-auc7.gif" alt></p><p>AUC 使用时，有几点需要注意：</p><ol><li>AUC 只关注预测的正负例的顺序关系，对于和 label 的拟合情况则不关注。比如正例得分都是 0.2，负例得分都是 0.1，AUC 很完美，但是 loss 就会比较大。</li><li>在非常不均衡的样本上，AUC 表现可能很好，但 precision 可能比较差。比如 $TP=80$，$FN=20$，$FP=200$，$TN=8000$，此时从 ROC 空间上看，效果还不错，但是 precision 低的可怜。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 博客转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ROC-AUC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data Structures With Python</title>
      <link href="/2021/07/27/data-structures-with-python/"/>
      <url>/2021/07/27/data-structures-with-python/</url>
      
        <content type="html"><![CDATA[<p><img src="https://mymasterdesigner.com/wp-content/uploads/2021/07/Data-Structures-With-Python-Big-Guide.png" alt></p><p>对于编程和计算机科学来说，数据结构是主要的主题，几乎涉及所有计算机领域。</p><p>本文介绍 <code>python</code> 中的一些数据结构。</p><a id="more"></a><h1 id="1-什么是数据结构"><a href="#1-什么是数据结构" class="headerlink" title="1. 什么是数据结构"></a>1. 什么是数据结构</h1><p>计算机科学中，数据结构是一种为了便于数据获取和修改的组织、管理和存储的形式。所有编程语言中，列表、字典和数组是最简单的数据结构。尽管语法不同，但是其内在的逻辑是相同的。因此，本文介绍的例子也适用于其它编程语言。</p><h1 id="2-字典、映射、哈希表"><a href="#2-字典、映射、哈希表" class="headerlink" title="2. 字典、映射、哈希表"></a>2. 字典、映射、哈希表</h1><p><code>Python</code> 中的字典（<code>dictionary</code>）可以用来存储任意数据，每条数据都有一个关键词。映射（<code>map</code>）也被称为哈希表（<code>hash table</code>）、查找表（<code>lookup table</code>）或者关联数组（<code>associative array</code>）。它可以更轻松地组织与特定关键字关联的数据，并以更有条理的形式呈现。</p><p>比如，用字典来存储每个人的年龄：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'Mark'</span>: <span class="number">12</span>,</span><br><span class="line">    <span class="string">'Alice'</span>: <span class="number">23</span>,</span><br><span class="line">    <span class="string">'David'</span>: <span class="number">8</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当我们想要查看特定的人的年龄时：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">'Mark'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: 12</span></span><br></pre></td></tr></table></figure><blockquote><p>当然，你可以把数据写在同一行内，但是如果数据量比较大的时候，卸载一行看起来会比较乱。</p></blockquote><h2 id="2-1-OrderedDict-defaultdict-ChainMap"><a href="#2-1-OrderedDict-defaultdict-ChainMap" class="headerlink" title="2.1 OrderedDict, defaultdict, ChainMap"></a>2.1 <code>OrderedDict</code>, <code>defaultdict</code>, <code>ChainMap</code></h2><ul><li>字典是无序的，如果我们想按照顺序来存储数据，显然原生的字典就无能为力了，这个时候就可以用 <code>OrderedDict</code>：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections <span class="keyword">as</span> cs</span><br><span class="line"></span><br><span class="line">dict1 = cs.OrderedDict(</span><br><span class="line">    Mark=<span class="number">12</span>,</span><br><span class="line">    Alice=<span class="number">23</span>,</span><br><span class="line">    David=<span class="number">8</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>查看一下 <code>dcit1</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(dict1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Ouput: ([('Mark', 12), ('Alice', 22), ('David', 8)])</span></span><br></pre></td></tr></table></figure><ul><li>当我们从字典里面取值的时候，遇到字典里并没有对应的 key 的时候，程序就会报错。这时 <code>defaultdict</code> 就派上用场了。<code>defaultdict</code> 的作用是在于，当字典里的key不存在但被查找时，返回的不是 <code>keyError</code> 而是一个默认值。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">dict1 = defaultdict(int)</span><br><span class="line">dict2 = defaultdict(set)</span><br><span class="line">dict3 = defaultdict(str)</span><br><span class="line">dict4 = defaultdict(list)</span><br></pre></td></tr></table></figure><p><code>defaultdict</code> 接受 <code>int</code>, <code>set</code>, <code>str</code>, <code>list</code> 作为参数，也可以自定义函数作为参数。我们来看下，上面的例子中默认值分别是什么：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(dict1[<span class="number">1</span>]) </span><br><span class="line">print(dict2[<span class="number">1</span>])</span><br><span class="line">print(dict3[<span class="number">1</span>])</span><br><span class="line">print(dict4[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line">set()</span><br><span class="line"></span><br><span class="line">[]</span><br></pre></td></tr></table></figure><p>说明 <code>int</code> 默认值是 0，<code>set</code> 默认值是空集合，<code>str</code> 默认值是空字符串，<code>list</code> 默认值是空列表。</p><ul><li>当你有多个字典的时候，可以使用 <code>ChainMap</code> 将它们变成一个字典。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> ChainMap</span><br><span class="line"></span><br><span class="line">dict1 = &#123;<span class="string">"1"</span>: <span class="number">1</span>, <span class="string">"2"</span>: <span class="number">2</span>&#125;</span><br><span class="line">dict2 = &#123;<span class="string">"3"</span>: <span class="number">3</span>, <span class="string">"4"</span>: <span class="number">4</span>&#125;</span><br><span class="line">main = ChainMap(dict1, dict2)</span><br><span class="line"></span><br><span class="line">print(main[<span class="string">"3"</span>] , main[<span class="string">"1"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line"><span class="number">3</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><h1 id="3-数组型数据结构"><a href="#3-数组型数据结构" class="headerlink" title="3. 数组型数据结构"></a>3. 数组型数据结构</h1><h2 id="3-1-列表"><a href="#3-1-列表" class="headerlink" title="3.1 列表"></a>3.1 列表</h2><p>列表可以存储任意类型的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># from 0 to 10 value</span></span><br><span class="line">arr = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># String Array</span></span><br><span class="line">arr1 = [<span class="string">"a"</span> , <span class="string">"b"</span> , <span class="string">"c"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get First Indexing</span></span><br><span class="line">arr1[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get from 0 to 4</span></span><br><span class="line">arr[<span class="number">0</span>:<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Deleting Element</span></span><br><span class="line"><span class="keyword">del</span> arr[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Adding Element</span></span><br><span class="line">arr.append(<span class="number">11</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output:  [1, 2, 3, 4]</span></span><br></pre></td></tr></table></figure><h2 id="3-2-元组"><a href="#3-2-元组" class="headerlink" title="3.2 元组"></a>3.2 元组</h2><p>元组是另一个可以存储任意类型数据的数据结构。与列表不同的是，元组是不可变的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">tuple = (<span class="number">1</span> , <span class="number">2</span> , <span class="number">3</span>)</span><br><span class="line">tuple[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: 1</span></span><br><span class="line"></span><br><span class="line">tuple1 = (<span class="string">"x"</span> , <span class="number">1</span> , <span class="number">1.25</span>)</span><br><span class="line">tuple1[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: 1.25</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># you'll get error</span></span><br><span class="line"><span class="keyword">del</span> tuple[<span class="number">0</span>]</span><br><span class="line">tuple[<span class="number">1</span>] = <span class="string">"y"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 报错</span></span><br></pre></td></tr></table></figure><h2 id="3-3-array-数组"><a href="#3-3-array-数组" class="headerlink" title="3.3 array 数组"></a>3.3 <code>array</code> 数组</h2><p><code>python</code> 的 <code>array</code> 模块存储的数据包括整型、浮点型等等，它的空间占用率会更低。因为 <code>array</code> 只接受相同类型的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Accessing array</span></span><br><span class="line"><span class="keyword">from</span> array <span class="keyword">import</span> array</span><br><span class="line"></span><br><span class="line"><span class="comment"># use type code</span></span><br><span class="line">arr = array(<span class="string">"f"</span> , [<span class="number">1.0</span> , <span class="number">1.2</span>])</span><br></pre></td></tr></table></figure><h2 id="3-4-字符串——字符编码的数组"><a href="#3-4-字符串——字符编码的数组" class="headerlink" title="3.4 字符串——字符编码的数组"></a>3.4 字符串——字符编码的数组</h2><p>字符串可以节省空间，因为它们被密集打包并专门处理特定的数据类型。 如果要保存 <code>Unicode</code> 文本，则应使用字符串。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">str = <span class="string">"55555"</span></span><br><span class="line">emoji = <span class="string">"😀"</span></span><br><span class="line"></span><br><span class="line">print(str , emoji)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Outtput: 55555 😀</span></span><br></pre></td></tr></table></figure><h2 id="3-5-字节-amp-字节数组"><a href="#3-5-字节-amp-字节数组" class="headerlink" title="3.5 字节 &amp; 字节数组"></a>3.5 字节 &amp; 字节数组</h2><p>字节（<em>Bytes</em>）存储的是 0 到 255 的数字，如果超过了这个范围程序会报错。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = bytes([<span class="number">1</span> , <span class="number">2</span> , <span class="number">3</span>])</span><br><span class="line">y = bytes([<span class="number">-1</span> , <span class="number">2</span> , <span class="number">3</span>])</span><br><span class="line">z = bytes([<span class="number">100</span> , <span class="number">200</span> , <span class="number">300</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Output: <span class="string">b'\x01\x02\x03'</span></span><br><span class="line">Output: error</span><br><span class="line">Output: error</span><br></pre></td></tr></table></figure><h1 id="4-集合-amp-多数组数据结构"><a href="#4-集合-amp-多数组数据结构" class="headerlink" title="4. 集合 &amp; 多数组数据结构"></a>4. 集合 &amp; 多数组数据结构</h1><h2 id="4-1-集合"><a href="#4-1-集合" class="headerlink" title="4.1 集合"></a>4.1 集合</h2><p>集合中不能包含相同的数据，且集合存储的是无序的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">set = &#123;<span class="number">1</span> , <span class="number">2</span> , <span class="number">3</span>&#125;</span><br><span class="line"></span><br><span class="line">set.add(<span class="number">4</span>)</span><br><span class="line">set.remove(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><h2 id="4-2-Frozen-Set"><a href="#4-2-Frozen-Set" class="headerlink" title="4.2 Frozen Set"></a>4.2 Frozen Set</h2><p>原始的集合元素可增可删，如果我们不想让集合发生改变，可以使用 <code>frozenset</code> 方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">frozen = frozenset(&#123;<span class="string">"x"</span> , <span class="string">"y"</span> , <span class="string">"z"</span>&#125;)</span><br><span class="line">frozen.add(<span class="string">"k"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 报错</span></span><br></pre></td></tr></table></figure><h2 id="4-3-Counter"><a href="#4-3-Counter" class="headerlink" title="4.3 Counter"></a>4.3 Counter</h2><p><code>counter</code> 可以对多个集合进行合并，并且可以对每个元素进行计数，得到每个元素的个数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">merge = Counter()</span><br><span class="line"></span><br><span class="line">fruits = &#123;<span class="string">"apple"</span> , <span class="string">"banana"</span> , <span class="string">"orange"</span>&#125;</span><br><span class="line">merge.update(fruits)</span><br><span class="line">print(merge)</span><br><span class="line"></span><br><span class="line">fruits1 = &#123;<span class="string">"apple"</span> , <span class="string">"banana"</span> , <span class="string">"watermelon"</span>&#125;</span><br><span class="line">merge.update(fruits1)</span><br><span class="line">print(merge)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'orange'</span>: <span class="number">1</span>, <span class="string">'apple'</span>: <span class="number">1</span>, <span class="string">'banana'</span>: <span class="number">1</span>&#125;)</span><br><span class="line">&#123;<span class="string">'apple'</span>: <span class="number">2</span>, <span class="string">'banana'</span>: <span class="number">2</span>, <span class="string">'orange'</span>: <span class="number">1</span>, <span class="string">'watermelon'</span>: <span class="number">1</span>&#125;)</span><br></pre></td></tr></table></figure><h1 id="5-堆栈"><a href="#5-堆栈" class="headerlink" title="5. 堆栈"></a>5. 堆栈</h1><p>堆栈是支持用于插入和删除的快速输入/输出语义 (LIFO) 的项目集合。与数组和列表不同，你不能做随机访问，你需要使用函数进行插入和删除。</p><h2 id="5-1-list-实现堆栈"><a href="#5-1-list-实现堆栈" class="headerlink" title="5.1 list 实现堆栈"></a>5.1 <code>list</code> 实现堆栈</h2><p>你可以用 <code>append</code> 把数据加到最后，再用<code>pop</code>从 LIFO 队列中取出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">stack = []</span><br><span class="line"></span><br><span class="line">stack.append(<span class="number">1</span>)</span><br><span class="line">stack.append(<span class="number">2</span>)</span><br><span class="line">stack.append(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">print(stack)</span><br><span class="line"></span><br><span class="line">stack.pop()</span><br><span class="line">stack.pop()</span><br><span class="line"></span><br><span class="line">print(stack)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Output: [1,2,3]</span></span><br><span class="line"><span class="comment"># Output: [1]</span></span><br></pre></td></tr></table></figure><h2 id="5-2-deque-实现堆栈"><a href="#5-2-deque-实现堆栈" class="headerlink" title="5.2 deque 实现堆栈"></a>5.2 <code>deque</code> 实现堆栈</h2><p><code>deque</code> 与列表的区别还支持在固定时间添加和删除数组开头的元素。</p><p>因此，它比列表更有效、更快。 它还支持随机访问。</p><p>如果您尝试删除双端队列之间的数据，您可能会失去性能，主要原因是直到两端的所有元素都移动以腾出空间或填补空白。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line">stack = deque()</span><br><span class="line"></span><br><span class="line">stack.append(<span class="string">"a"</span>)</span><br><span class="line">stack.append(<span class="string">"b"</span>)</span><br><span class="line">stack.append(<span class="string">"c"</span>)</span><br><span class="line"></span><br><span class="line">print(stack)</span><br><span class="line"></span><br><span class="line">print(stack.pop())</span><br><span class="line">print(stack.pop())</span><br><span class="line"></span><br><span class="line">print(stack)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Output: deque(['a', 'b', 'c'])</span></span><br><span class="line"><span class="comment"># Output: deque(['a'])</span></span><br></pre></td></tr></table></figure><h1 id="6-Queues"><a href="#6-Queues" class="headerlink" title="6. Queues"></a>6. <code>Queues</code></h1><p>堆栈上的逻辑在这里略有不同，其中采用先进先出 (FIFO)，而在堆栈中采用先进后出。</p><p>我们这里可以使用栈中使用的 <code>list</code>和 <code>deque</code> 数据结构，也可以使用队列中的 <code>Queue</code> 类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">queue = []</span><br><span class="line"></span><br><span class="line">queue.append(<span class="string">"x"</span>)</span><br><span class="line">queue.append(<span class="string">"y"</span>)</span><br><span class="line">queue.append(<span class="string">"z"</span>)</span><br><span class="line"></span><br><span class="line">print(queue)</span><br><span class="line"></span><br><span class="line">queue.pop(<span class="number">0</span>)</span><br><span class="line">queue.pop(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">print(queue)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Output: ['x', 'y', 'z']</span></span><br><span class="line"><span class="comment"># Output: ['z']</span></span><br></pre></td></tr></table></figure><h2 id="6-1-deque"><a href="#6-1-deque" class="headerlink" title="6.1 deque"></a>6.1 <code>deque</code></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line">queue = deque()</span><br><span class="line"></span><br><span class="line">queue.append(<span class="string">"x"</span>)</span><br><span class="line">queue.append(<span class="string">"y"</span>)</span><br><span class="line">queue.append(<span class="string">"z"</span>)</span><br><span class="line"></span><br><span class="line">print(queue)</span><br><span class="line"></span><br><span class="line">queue.popleft()</span><br><span class="line">queue.popleft()</span><br><span class="line"></span><br><span class="line">print(queue)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Output: deque(['x', 'y', 'z'])</span></span><br><span class="line"><span class="comment"># Output: deque(['z'])</span></span><br></pre></td></tr></table></figure><h2 id="6-2-queue"><a href="#6-2-queue" class="headerlink" title="6.2 queue"></a>6.2 <code>queue</code></h2><p>队列是一种结构，通过它我们可以确定队列可以容纳和存储多少数据。 它主要用于实现队列。</p><p>您可以通过将 <code>max size</code> 参数设置为 0 来创建无限队列，这符合 FIFO 规则。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> queue <span class="keyword">import</span> Queue</span><br><span class="line"></span><br><span class="line">queue = Queue(maxsize = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Adding element</span></span><br><span class="line">queue.put(<span class="number">10</span>)</span><br><span class="line">queue.put(<span class="number">20</span>)</span><br><span class="line">queue.put(<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">print(queue.queue)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Removing element</span></span><br><span class="line">queue.get()</span><br><span class="line">queue.get()</span><br><span class="line"></span><br><span class="line">print(queue.queue)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Output: deque([10, 20, 30])</span></span><br><span class="line"><span class="comment"># Output: deque([30])</span></span><br></pre></td></tr></table></figure><h1 id="7-自定义数据类型"><a href="#7-自定义数据类型" class="headerlink" title="7. 自定义数据类型"></a>7. 自定义数据类型</h1><p>要更可控，您只需要您自己。 不要害怕创建和使用自己的类。 创建复杂的类有时会很累，但它会提高您的工作效率。</p><p>当您想通过方法向记录对象添加业务逻辑和活动时，创建自定义类是一个极好的解决方案。 然而，这意味着这些东西不再只是数据对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, note)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.note = note</span><br><span class="line"></span><br><span class="line">x = Student(<span class="string">"David"</span> , <span class="number">55</span>)</span><br><span class="line">y = Student(<span class="string">"Mark"</span> , <span class="number">35</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Access Data</span></span><br><span class="line">print(x.name , x.note)</span><br><span class="line"></span><br><span class="line">print(Student)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Output: David 55</span></span><br><span class="line"><span class="comment"># Output: &lt;main.Student object at 0x7f53925c2400&gt;</span></span><br></pre></td></tr></table></figure><h1 id="8-Reference"><a href="#8-Reference" class="headerlink" title="8. Reference"></a>8. Reference</h1><p><a href="https://mymasterdesigner.com/2021/07/06/data-structures-with-python-big-guide/" target="_blank" rel="noopener">Data Structures With Python – Big Guide</a></p>]]></content>
      
      
      <categories>
          
          <category> 博客转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Structure </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知识图谱：知识建模（一）不那么简要的知识建模简介</title>
      <link href="/2021/07/23/kg-data-modelling/"/>
      <url>/2021/07/23/kg-data-modelling/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/kgicon.png" alt></p><h1 id="1-序言"><a href="#1-序言" class="headerlink" title="1. 序言"></a>1. 序言</h1><h2 id="1-1-什么是知识建模（语义建模）"><a href="#1-1-什么是知识建模（语义建模）" class="headerlink" title="1.1 什么是知识建模（语义建模）?"></a>1.1 什么是知识建模（语义建模）?</h2><blockquote><p>通过赋予数据指定的概念和数据之间的关系使数据包含语义。</p></blockquote><a id="more"></a><p>本质上讲，我们是通过明确数据代表的概念以及概念之间的关系来对数据进行建模。这样一个知识模型必须同时被人类和计算机所理解。对于人类来说相对比较容易，因为我们可以通过文字和数字对任意我们想要的东西进行建模，更重要的是赋予计算机更多的思考。</p><h2 id="1-2-我们为什么要知识建模？"><a href="#1-2-我们为什么要知识建模？" class="headerlink" title="1.2 我们为什么要知识建模？"></a>1.2 我们为什么要知识建模？</h2><p>对于数据来说，上下文信息非常重要。比如 “苹果”，我们是在讨论苹果电子产品还是水果？作为人类，当给定上下文的时候，我们可以很轻易的判断我们在讨论什么。当我说，“昨天买的苹果真好吃！”没有人会觉得我啃了一部手机。这就是问题的关键，没有上下文，数据所携带的信息总是不明确的。知识模型就是赋予数据以意义，避免这种歧义。</p><p>另外，还可以通过概念之间的关系帮助我们发散思维，找到数据之间的关联性。比如，“张三”是“张四”的父亲，而“张四”又是“张五”的父亲。人类可以很轻易的发现“张三”和“张五”是祖孙关系，但是如果没有知识模型构建的数据之间的关系，那么计算机是无法得知“张三”与“张五”的关系的。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210823222116.png" alt></p><h2 id="1-3-Led-Zeppelin-乐队知识模型"><a href="#1-3-Led-Zeppelin-乐队知识模型" class="headerlink" title="1.3 Led Zeppelin 乐队知识模型"></a>1.3 Led Zeppelin 乐队知识模型</h2><p>解释知识模型最好的方式就是举例子。下图展示了一个 Led Zeppelin 乐队简单的知识模型，包括一些与 Led Zeppelin 相关的概念和概念之间的关系。从图中我们可以看到， Led Zeppelin 是一个乐队，它有一张专辑叫做 “ Led Zeppelin IV”，这张专辑发布于 “1971年11月8日”，专辑中有一首歌叫做 “Black Dog”。“Jimmy Page” 是一个人，也是乐队的成员之一。当然这只是一小部分数据，我们仅仅用这部分数据作为一个例子进行介绍。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210823225518.png" style="zoom:67%;"></p><p>知识模型也可以将信息排列成层次结构。比如“唱片”和“歌曲”都是“具有创造性的作品”，同时“具有创造性的作品”还包括“书籍”、“电影”、“雕塑”等等。</p><p>上图中大多数的关系都是用实线箭头连接的，除了 “is a” 关系是用虚线箭头连接的。这是因为 “is a” 代表了一类特殊的关系——“rdf:type”。现在先不用管这啥，在后面的章节会介绍。</p><h1 id="2-知识模型基础"><a href="#2-知识模型基础" class="headerlink" title="2. 知识模型基础"></a>2. 知识模型基础</h1><p>虽然我么现在有了一张看起来很漂亮的图，但是如果计算机不能理解这张图，再漂亮也白搭。如何让计算机从图中抽取语义信息？下面我们就来介绍一个非常有用的工具——<strong>RDF</strong>。</p><h2 id="2-1-理解-RDF"><a href="#2-1-理解-RDF" class="headerlink" title="2.1 理解 RDF"></a>2.1 理解 RDF</h2><p>RDF（<em><a href="https://www.w3.org/TR/rdf11-concepts/" target="_blank" rel="noopener">Resource Description Framework</a></em>），翻译成中文：“资源描述框架”。顾名思义，它是一个用来描述数据的框架。在 RDF 数据模型中，我们用三元组来描述数据。一个三元组包含：<strong>subject</strong>、<strong>object</strong> 和 <strong>predicate</strong>，我们可以简单理解为，一个三元组就是有最简单的“主谓宾”构成的事实描述。RDF 图就是将很多三元组组合起来，其中 subject 和 object 是节点，predicate 是边。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210824001055.png" style="zoom:60%;"></p><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • NOTE            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            predicate 的方向很重要！如有必要，我们可以在三元组中定义双向关系，如下图：        </div>        </div>    </div><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210824002230.png" style="zoom:60%;"></p><p>对于 RDF 来说，节点有三种类型：</p><ul><li><strong>Resources</strong>：你想要描述的概念，比如 Led Zeppelin 乐队知识模型图中的矩形框中的内容。所有的概念都必须有一个唯一的标识符。</li><li><strong>Literals</strong>：即字面量，所有的字面量必须是字符串、数字和日期，如 Led Zeppelin 乐队知识模型图中的圆形框中的内容。</li><li><strong>Blank nodes</strong>：空节点表示没有唯一标识符的数据。</li></ul><h2 id="2-2-URI"><a href="#2-2-URI" class="headerlink" title="2.2 URI"></a>2.2 URI</h2><blockquote><p> 所有的 resources 和 predicates  都必须有<strong>机器可读</strong>的唯一标识符。</p></blockquote><p>为什么标识符的唯一性这么重要？比如，我们要对一个大型组织结构的雇员进行建模，其中可能有很多同名同姓的人，不考虑每个人的昵称的话，你怎么区分到底是财务室的张三，还是技术部的张三，又或者是食堂大厨张三？解决的办法就是每个人都分配一个唯一的标识符——Uniform Resource Identififiers（URIs）。</p><blockquote><p>URI 是一个字符串，用来准确的辨认特定的资源。为了保证统一性，所有的 URI 都有预定义的语法规则。</p></blockquote><p>URI 与我们平时上网的时候，各个网站的 URL 很像。URI 可以是层级结构的（hierarchical URIs），也可以是不透明的（opaque URI）。Hierarchical URI 会包含不同级别的信息，它可以编码资源的位置信息，即资源存放在模型中的哪个位置，或者共享资源的上下文信息。而 opaque URI 不会编码任何信息，它的后缀通常是随机字符串。</p><blockquote><ul><li>Hierarchical：&lt;<a href="http://mycompany.com/people/JediDepartment/LukeSkywalker" target="_blank" rel="noopener">http://mycompany.com/people/JediDepartment/LukeSkywalker</a> &gt;</li><li>Opaque：&lt;<a href="http://mycompany.com/AE04801" target="_blank" rel="noopener">http://mycompany.com/AE04801</a> &gt;</li></ul></blockquote><p>上例中 hierarchical URI 是人类可读的。从中我们知道 <em>Luke Skywalker</em> 为 <em>My Company</em> 工作，他是 <em>Jedi Department</em> 部门的员工。Opaque URI 同样是代表的 <em>Luke Skywalker</em>，但我们很难从中读取到任何信息。这就意味着 Opaque URI 的隐私性非常强。Opaque URI 的另一个优点是，不需要经常更新。比如如果 <em>Luke Skywalker</em> 变更了工作，从 <em>Jedi Department</em> 部门调到财务部门。Opaque URI 不需要做任何变更，而 Hierarchical URI 就要跟着改变。</p><p>从上面的例子中，我们可以看到两种 URI 各有优劣势，那么什么样的 URI 是一个好的 URI 呢？对此 <a href="https://www.ebi.ac.uk/rdf/documentation/good_practice_uri/" target="_blank" rel="noopener">European Bioinformatics Institute</a> 给出了几个性质，总结起来就是：</p><ul><li><strong>全局唯一性</strong>：一个 URI 决不能被两个不同的概念同时引用，即使两个概念是等价的。</li><li><strong>持久性</strong>：在可预见的将来，URI 要保证可用性。</li><li><strong>稳定性</strong>：一个 URI 决不能在不同的数据上重复使用，即使原来的版本已经删除了。</li><li><strong>可解析性（不可引用性）</strong>：简单来说就是，当用户在自己的浏览器上点击 URI 的时候，我们希望浏览器能重定向到合适的文档。</li></ul><p>在介绍 <em>Skywalker</em> 的时候，我随便造了一个 URI，用浏览器访问的时候会返回给你“404 Page Not Found”，也就是说这个 URI 不是一个好的 URI，因为它不满足上面第四个性质：可解析性。</p><p>下面我们回到 Led Zeppelin 的例子。我们用 URI 来表示三元组：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210824102652704.png" alt></p><p>当我们点击上面的 URI 的时候，浏览器会给我们展示相关的资源页面，比如：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210824103813.png" alt></p><p>上面我们是将一个三元组用 URI 来表示，而我们要做的是将所有的三元组都用 URI 来表示。但是全部都用完整的 URI 的话显得多余，所以我们可以用<strong>前缀</strong>：</p><ol><li>用 <code>@prefix</code> 作为定义前缀的开始；</li><li>选择前缀名；</li><li>描述前缀的命名空间；</li><li>以句号 <code>.</code> 结尾.</li></ol><div style="text-align: center">    <span style="background-color:#FFE4B5">@prefix</span>&nbsp;&nbsp;    <span style="background-color:#EE82EE">prefix name</span>&nbsp;&nbsp;    <span style="background-color:#FFC1C1">:</span>&nbsp;&nbsp;    <span style="background-color:#98FB98"> &lt; resource namespace &gt;</span>&nbsp;&nbsp;    <span style="background-color:#00BFFF">.</span></div><p>比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">@prefix wd: &lt;https://www.wikidata.org/wiki/&gt;.</span><br></pre></td></tr></table></figure><p>现在回到上面的例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">@prefix wd: &lt;https://www.wikidata.org/wiki/&gt;.</span><br><span class="line">@prefix wdp: &lt;https://www.wikidata.org/wiki/Property&gt;</span><br><span class="line"></span><br><span class="line">&lt;wd:Q2331&gt; &lt;wdp:P527&gt; &lt;wd:Q16546&gt;</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210824112850.png" alt></p><p>现在我们就可以将整个图用 URI 来表示了：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/23.png" style="zoom: 67%;"></p><p>从图中我们可以看到，大多数的 URI 都是源自 Wikidata，少部分看起来有点奇怪，比如 <code>schema</code>、 <code>rdf</code>、<code>rdfs</code>。在介绍这几个比较奇怪的 URI 之前，我们再多说点关于<strong>空节点</strong>，即 “blank node” 的事情。</p><h2 id="2-3-Blank-node"><a href="#2-3-Blank-node" class="headerlink" title="2.3 Blank node"></a>2.3 Blank node</h2><p>空节点表示没有 URI 或者没有 Literal 的资源。听起来好像空节点是非法节点，但是实际上 RDF 允许这种情况的存在。必须说明的是，空节点只能出现在节点上，边是不允许的。空节点的 URI 并非是未知的，而是匿名的。当你想要表示一个复杂的资源特征，但是又不知道特征的名字，比如具体地址的街道门牌号。又或者你希望保护一些信息的隐私。类似于 “&lt;张三&gt;，&lt;生日&gt;，&lt;<em>*</em>&gt;”，我告诉你张三有一个属性叫做生日，但是我不告诉你他的生日是多少。</p><h2 id="2-4-RDFS"><a href="#2-4-RDFS" class="headerlink" title="2.4 RDFS"></a>2.4 RDFS</h2><p>RDFS 即 “RDF Schema”，本质上是一个知识建模的词表，用来表示 RDF 知识模型。RDFS 包含了一系列的属性和其他机制用于描述知识以及知识之间的关系。</p><p>在介绍 RDFS 之前需要搞清楚，我们为什么需要这样一个词表？在我们最初引入 Led Zeppelin 例子的时候，把虚线箭头以下的部分去掉似乎并不影响整体性。实际上这种知识建模的技术就是诞生于 20 世纪 60 年代的语义网络。语义网络有一定的优点，比如容易理解、相关概念容易聚类等。如上面例子展示的，对我们人类来说，整张图的概念很清晰，关系也很明确。但是这样一个图有一个非常严重的问题：</p><p><strong>没有标准！</strong></p><ol><li>节点和边的值没有标准，完全由用户自己定义。</li><li>多源数据融合困难。比如三元组1（鱼，住在，水里），三元组2（鱼，生活在，水里），对于我们人类来说，两个三元组描述的是同一个事实，应该融合成一个三元组。但是对于计算机来说，它无法理解“住在”和“生活在”是同一概念，所以就没有办法自动融合在一起。</li><li>无法区分概念和实例。比如在语义网络中（鱼，是，动物）中，“鱼”和“动物”是同级的概念。而我们很清楚，他们是有上下位关系的。</li></ol><p>为了解决语义网络存在的各种缺点，所以 W3C 制定了统一的标准，而这些标准就是相当于是由权威机构发布一些词汇，用来标准化常见的概念。比如，语义网络中</p><blockquote><p>鱼，是，动物</p><p>羊，是一种，动物</p></blockquote><p>我们将 “是”、“是一种”统一成 <code>rdf:type</code>，其中 <code>rdf</code> 表示前缀，完整的 URI 为 <code>http://www.w3.org/1999/02/22-rdf-syntax-ns#type</code>。这样既完成了标准化，也实现了 URI。</p><h3 id="2-4-1-RDFS-常用词汇"><a href="#2-4-1-RDFS-常用词汇" class="headerlink" title="2.4.1 RDFS 常用词汇"></a>2.4.1 RDFS 常用词汇</h3><p>下面我们就来介绍，RDFS 是如何构建标准化词表的。首先先介绍 “类” 的概念：</p><ul><li><p>知识可以被分成很多组，这些组称之为“<strong>类（class）</strong>”，这些类的成员就是“<strong>实例（instance）</strong>”。类和实例可以通过 URI 加以区分，也可以通过 RDF 属性加以描述。比如 <code>rdf:type</code> 就是用来描述是个实例属于某个类别：</p><blockquote><p>鱼，<code>rdf:type</code>，动物</p></blockquote></li><li><p>类也可以有子类 “<strong>subclass</strong>”，所有子类的实例也是类的实例。</p></li><li><p><strong>属性（property）</strong> 表示连接 subject 和 object 的边，即 predicate。</p></li></ul><p>用 RDFS 中一些重要的 Schema：</p><ul><li><p><strong><code>rdf:Class</code></strong>：定义类节点。比如 “Led Zeppelin”，“Led Zeppelin IV”，“Black Dog”，“Jimmy Page” 都是类。</p></li><li><p><strong><code>rdfs:Literal</code></strong>：用于定义节点的字面量，即字符串或者数字等。比如 “1971/8/11”。</p><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • NOTE            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            <code>rdfs:Literal</code> 本身是一个类，同时也是 <code>rdf:Class</code> 的实例。        </div>        </div>    </div></li><li><p><strong><code>rdf:Property</code></strong>：属性，即连接节点的边，还可以通过 <code>rdfs:subPropertyOf</code> 定义子属性。有一个很特殊而又常用的属性 <code>rdf:type</code> 用来描述一个实体是一个类别的实例，特殊在于我们经常缩写成 <code>a</code>。</p></li><li><p><strong><code>rdfs:domain</code> </strong> 和 <strong><code>rdfs:range</code></strong>：用来指定 <code>rdf:Property</code> 的定义域和值域。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;hasMember&gt; a rdf:Property . </span><br><span class="line">&lt;hasMember&gt; rdfs:domain &lt;Band&gt; . </span><br><span class="line">&lt;hasMember&gt; rdfs:range &lt;Person&gt; .</span><br><span class="line"></span><br><span class="line"># 用自然语言描述就是:</span><br><span class="line"># &lt;hasMember&gt; 是一条边</span><br><span class="line"># 定义域（subject）是&lt;Band&gt;</span><br><span class="line"># 值域（object）是&lt;Person&gt;</span><br><span class="line"># 等效于： &lt;Band&gt; &lt;hasMember&gt; &lt;Person&gt;</span><br></pre></td></tr></table></figure></li><li><p><strong><code>rdfs:subClassOf</code></strong>：定义类的子类，可以用来构建层级关系。比如，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;musician&gt; &lt;rdfsLsubClassOf&gt; &lt;Person&gt;</span><br></pre></td></tr></table></figure></li><li><p><strong><code>rdfs:label</code></strong> 和 <strong><code>rdfs:comments</code></strong>：之前介绍的 RDFS 词汇使数据变成计算机可读，但是我们还是希望对人类也可读。<code>rdfs:label</code> 是给一个节点人类可读的名字，帮助我们理解该节点，对 opaque URI 尤其有用。<code>rdfs:comment</code> 给节点添加文本描述，有点类似于给代码加注释。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;hasMember&gt; rdfs:label “has member” . </span><br><span class="line">&lt;hasMember&gt; rdfs:comment “Property relating a band to one of its band members” .</span><br></pre></td></tr></table></figure></li></ul><h2 id="2-5-OWL"><a href="#2-5-OWL" class="headerlink" title="2.5 OWL"></a>2.5 OWL</h2><p>RDFS 是节点和关系的标准化词汇表。随着技术的发展，人们发现 RDFS 的表达能力相当有限，因此提出了OWL。我们可以把 OWL 当做是 RDFS 的一个扩展，其添加了额外的预定义词汇。</p><p>OWL 也定义了类、属性和实例。不同于 RDFS，OWL 有更丰富，更严格的词汇表。这里可能会有一个疑问：既然 OWL 覆盖了 RDFS 大部分词汇，而且比 RDFS 表达能力更强，那我们为什么还要用 RDFS？</p><p>这就有点像牛刀可以用来杀鸡，但是不提倡，道理是一样的。虽然 OWL 表达能力更强，但是同时要求也更严格。当我们构建的知识模型本身就比较简单的时候， RDFS 就足够了。</p><p>OWL 的好处是，它支持和集成了 RDFS 的元素。比如 在OWL 里你仍可以用 <code>rdf:type</code>、<code>rdfs:range</code>、<code>rdfs:subPropertyOf</code> 等。</p><p>另外， OWL 也定义了自己的词汇，这些词汇比 RDFS 更细致。可以对一些属性添加约束，比如 <code>owl:allValuesFrom</code> 可以定义类的取值范围。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;hasParent&gt; owl:allValuesFrom &lt;Human&gt; .</span><br></pre></td></tr></table></figure><p>OWL 也支持用一系列操作来描述知识。比如，<code>owl:unionOf</code> 可以用来表示类，比如水果，包含甜水果和不甜的水果。<code>owl:unionOf</code> 的 subject 也可以是空节点。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;Fruit&gt; owl:unionOf ( &lt;SweetFruit&gt; &lt;NonSweetFruit ) .</span><br></pre></td></tr></table></figure><p>我们还可以用 OWL 定义反向关系。还记得上面 Darth Vader 和 Luke 的例子吗？我们可以通过 <code>owl:inverseOf</code> 来定义 “<hasson>” 和 “<hasfather>” 是一对互逆关系：</hasfather></hasson></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;hasSon&gt; owl:inverseOf &lt;hasFather&gt; .</span><br></pre></td></tr></table></figure><p>OWL 中另一个重要的词汇是 <code>owl:sameAs</code>，它用来表示两个实体是相同的。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;BillClinton&gt; owl:sameAs &lt;WilliamJeffersonClinton&gt; .</span><br></pre></td></tr></table></figure><p>另一个用来表示两个类是等价的词汇是 <code>owl:equivalentClass</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;USPresident&gt; owl:equivalentClass &lt;PrincipalResidentOfWhiteHouse&gt; .</span><br></pre></td></tr></table></figure><p>说两个事情是等价的看起来有点多余，但是这确实是 OWL 的一大优势。用“等价”描述可以很轻松的引用外部知识模型和本体。比如，你可以说 wikidata 中的 Al Pacino 和 IMDB 中的 Al Pacino 是等价的。这个在帮助你构建知识模型的时候省掉很多工作。</p><p>最后， WOL 还可以定义两种不同的 property：</p><ul><li><strong>Object property</strong>：实体与实体之间的关系（&lt;hasMember &gt;）</li><li><strong>Data propery</strong>：实体与属性的关系（&lt;birthDay &gt;）</li></ul><h1 id="3-知识建模的步骤"><a href="#3-知识建模的步骤" class="headerlink" title="3. 知识建模的步骤"></a>3. 知识建模的步骤</h1><p>前面我们介绍了关于知识模型的理论，接下来就是如何用上面的理论一步一步从头构建一个知识模型。在实际的项目当中，不建议自己从头构建知识模型。现在有很多领域已经有专家、工程师构建好的知识模型，我们可以以此为基础进行开发。本文是为了介绍相关知识，所以介绍从头构建知识模型的内容。</p><p>这里我们主要介绍两点：</p><ul><li>RDF 知识模型的语法，或者更正式一点的说法——RDF 数据的序列化。</li><li>帮助我们进行知识建模的工具——Protege。</li></ul><h2 id="3-1-RDF-序列化"><a href="#3-1-RDF-序列化" class="headerlink" title="3.1 RDF 序列化"></a>3.1 RDF 序列化</h2><p>RDF 是一种知识描述框架，本质上也是一种模型。而序列化就是要讲这种描述框架落到实处。就像算法本身只是一种数学模型，而要如何实现算法就具体依赖你使用什么编程语言。RDF 三元组的序列化就是使用“编程语言”把它实现出来。RDF 序列化方法有多种：</p><ul><li><p>RDF/XML：就是用XML的格式来表示 RDF 数据。之所以提出这个方法，是因为 XML 的技术比较成熟，有许多现成的工具来存储和解析 XML。然而，对于 RDF 来说，XML 的格式太冗长，也不便于阅读，通常我们不会使用这种方式来处理 RDF 数据。</p></li><li><p>N-Triples：即用多个三元组来表示 RDF 数据集，是最直观的表示方法。在文件中，每一行表示一个三元组，方便机器解析和处理。</p></li><li>Turtle：应该是使用得最多的一种 RDF 序列化方式了。它比 RDF/XML 紧凑，且可读性比 N-Triples 好。</li><li>RDFa：即“The Resource Description Framework in Attributes”，是 HTML5 的一个扩展，在不改变任何显示效果的情况下，让网站构建者能够在页面中标记实体，像人物、地点、时间、评论等等。也就是说，将 RDF 数据嵌入到网页中，搜索引擎能够更好的解析非结构化页面，获取一些有用的结构化信息。</li><li>Json-LD：即“JSON for Linking Data”，用键值对的方式来存储 RDF 数据。</li></ul><p>我们以 Turtle 为例进行介绍，因为如上所述，它是目前用的最多的一种序列方法。Turtle 简称 TTL，表示 Time To Live。</p><h3 id="Turtle"><a href="#Turtle" class="headerlink" title="Turtle"></a>Turtle</h3><p>为了解释 TTL，我们从 TTL 的<a href="https://www.w3.org/TR/turtle/" target="_blank" rel="noopener">官网</a> 借来一个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">@base &lt;http://example.org/&gt; .</span><br><span class="line">@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .</span><br><span class="line">@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .</span><br><span class="line">@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; .</span><br><span class="line">@prefix rel: &lt;http://www.perceive.net/schemas/relationship/&gt; .</span><br><span class="line"></span><br><span class="line">&lt;#green-goblin&gt; rel:enemyOf &lt;#spiderman&gt; ;</span><br><span class="line">&lt;#green-goblin&gt; a foaf:Person ;    # in the context of the Marvel universe</span><br><span class="line">&lt;#green-goblin&gt; foaf:name &quot;Green Goblin&quot; .</span><br><span class="line"></span><br><span class="line">&lt;#spiderman&gt; rel:enemyOf &lt;#green-goblin&gt; ;</span><br><span class="line">&lt;#spiderman&gt; a foaf:Person ;</span><br><span class="line">&lt;#spiderman&gt; foaf:name &quot;Spiderman&quot;, &quot;Человек-паук&quot;@ru .</span><br></pre></td></tr></table></figure><p>下面我们详细介绍这个例子。</p><ul><li><p>1-5 行：定义前缀。任何好的 TTL 文件都是先定义前缀。其中 2-3 行是我们之前介绍的 <code>rdf</code> 和 <code>rdfs</code>，4-5 行是两个新词 <code>foaf</code> 和 <code>rel</code>。第 1 行 <code>base</code> 是比较特殊的前缀，它表示一个基础的 URI，所有 “&lt;&gt;” 内的内容都是属于它的命名空间。需要注意的是，每一行都必须有 “.” 作为结束。</p></li><li><p>7-9 行：描述了一条关于“&lt;#green-goblin&gt;”的知识。</p><ol><li><code>&lt;#green-goblin&gt;</code> 就是以 <code>@base</code> 为前缀的节点，等价于 <code>&lt;http://example.org/#green-goblin&gt;</code>。</li><li><code>rel:enemyOf</code> 表示以 <code>rel</code> 为前缀的关系，等价于 <code>&lt;http://www.perceive.net/schemas/relationship/enemyOf&gt;</code>。后面的 <code>&lt;#spiderman&gt;</code> 和 <code>&lt;#green-goblin&gt;</code> 类似。</li><li><code>a</code> 表示 <code>rdf:type</code>，后面的表示法都与之前类似。</li><li>这三行描述的信息是：Green Goblin 有一个敌人叫做 Spiderman，Green Goblin 是一个人，Green Goblin 的名字叫做 Green Goblin。</li><li>字符串用双引号表示。</li><li>注释用 “#” 。</li><li>每一个三元组都以 “<strong>空格  .</strong>” 结尾。</li></ol></li><li><p>每一个三元组都写 &lt;#green-goblin&gt; 有点重复，显得笨重又耗时。我们可以用 <strong>predicate list</strong> 的方法，将有相同 subject 的描述组合在一起，每条 predicate-object 描述用 “;” 分割。然后最后一条 predicate list 用空格和句号结束。我们将 7-9 行和11-13 行改造如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;#green-goblin&gt;</span><br><span class="line">    rel:enemyOf &lt;#spiderman&gt; ;</span><br><span class="line">    a foaf:Person ;    # in the context of the Marvel universe</span><br><span class="line">    foaf:name &quot;Green Goblin&quot; .</span><br><span class="line">    </span><br><span class="line">&lt;#spiderman&gt;</span><br><span class="line">    rel:enemyOf &lt;#green-goblin&gt; ;</span><br><span class="line">    a foaf:Person ;</span><br><span class="line">    foaf:name &quot;Spiderman&quot;, &quot;Человек-паук&quot;@ru .</span><br></pre></td></tr></table></figure></li><li><p>我们来看改造后的第 9 行。Spiderman 有两个名字，一个是英文名，另一个是俄文名。两个英文名之间用 “,” 分割。在俄文名的引号后面有一个语言标签 <code>@ru</code>。语言标签有两部分组成 <code>@</code> 和 语言关键词。</p></li></ul><p>除了上面的例子展示的一些语法，Turtle 还有一些其他语法。比如可以使用 <code>xsd</code> 里指定数据类型，比如日期、字符串、数字等。更详细内容可查看<a href="https://www.w3.org/TR/turtle/" target="_blank" rel="noopener">官网</a>。</p><p>另外，如果你之前已经用一种序列化方法实现了一个知识模型，现在处于某种原因，你想换成另一种序列化方法。不要紧，可以试试这个<a href="https://www.easyrdf.org/converter" target="_blank" rel="noopener">小工具</a>。</p><h2 id="3-2-知识建模工具"><a href="#3-2-知识建模工具" class="headerlink" title="3.2 知识建模工具"></a>3.2 知识建模工具</h2><p>在我看来，有两种方法构建知识模型：</p><ol><li>在文本编辑器中写三元组（手动或者自动）。</li><li>用工具创建三元组。</li></ol><p>就个人而言，我更喜欢前者。因为这样我们可以完全掌控我们要构建的知识模型。更重要的是，不需要从头学习一个工具的使用。当然，这纯属是个人原因。如果你找到一款趁手的工具，完全没必要手写三元组。如果你要手写三元组的话，这里推荐一个 python 包 —— <a href="https://rdflflib.readthedocs.io/en/stable/" target="_blank" rel="noopener">rdflib</a>。</p><p>考虑使用工具的话，现在市面上的开源而又实用的工具只有 <a href="https://protege.stanford.edu" target="_blank" rel="noopener">Protégé</a>。</p><p>Protégé 是斯坦福大学开发的一款本体（本文提到的“本体”等效于“知识模型”）建模工具。包括网页版和桌面版。</p><p>基本上，Protégé 允许用户添加类、对象和数据属性和实例，不需要手写三元组。用过给类添加子类来构建层级结构。知识建模完成以后可以将文件保存成 OWL 文件。</p><h2 id="3-3-按步骤构建知识模型"><a href="#3-3-按步骤构建知识模型" class="headerlink" title="3.3 按步骤构建知识模型"></a>3.3 按步骤构建知识模型</h2><p>斯坦福大学不仅开发了知识建模工具，还发表了一篇文章叫我们怎样进行知识建模——<a href="https://protege.stanford.edu/publications/ontology_development/ontology101.pdf" target="_blank" rel="noopener">《Ontology Development 101 guide》</a>。更详细的内容可以看另一篇文章：<a href="https://rogerspy.gitee.io/2021/08/23/kg-build-ontology-method/">知识图谱：知识建模（二）构建本体的方法论</a>，或者去看原文。</p><p>总的来说，知识建模有三大原则：</p><blockquote><ol><li>对于一个领域来说，没有一个唯一正确的建模方法。我们可以用不同的方法进行建模。最好的方法依赖于实际应用和需求。</li><li>本体构建（知识建模）需要不断迭代。</li><li>本体中的概念和关系应该接近你感兴趣的领域（物理上或逻辑上）。</li></ol></blockquote><p>具体步骤如下：</p><ol><li>确定知识领域及范围，即应用场景。</li><li>确定知识模型的重要概念。</li><li>本体复用。</li><li>定义类，类的层级结构以及关系。</li><li>定义限制条件。</li></ol><h2 id="3-4-可视化知识模型"><a href="#3-4-可视化知识模型" class="headerlink" title="3.4 可视化知识模型"></a>3.4 可视化知识模型</h2><p>无论你是怎么建模的，最后你都希望看下你建模出来的知识模型长什么样子。可视化工具有两种：</p><ol><li><p>Protege 自带的插件 OntoGraf。</p><p>在菜单中选择 Windows -&gt; Tabs -&gt; OntoGraf</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210826102126.png" alt></p></li><li><p>网页工具 <a href="http://vowl.visualdataweb.org/webvo" target="_blank" rel="noopener">WebVOWL</a>。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/第34页-35.PNG" alt></p></li></ol><h1 id="4-知识模型查询"><a href="#4-知识模型查询" class="headerlink" title="4. 知识模型查询"></a>4. 知识模型查询</h1><p>在知识建模的时候，我们提到知识建模的目的是希望它能回答我们一些什么问题。现在，我们就要介绍一下我们该怎么像知识模型问问题。</p><h2 id="4-1-SPARQL"><a href="#4-1-SPARQL" class="headerlink" title="4.1 SPARQL"></a>4.1 SPARQL</h2><p><a href="https://www.w3.org/TR/2008/REC-rdf-sparql-query-20080115/" target="_blank" rel="noopener">SPARQL</a> 是 “SPARQL Protocol and RDF Query Language” 的首字母缩写。SPARQL 对 RDF 来说，就像 SQL 对关系型数据库一样。如果你会一点 SQL，那 SPARQL 学起来也会比较快。</p><p>我们用下面的例子做一个简单的介绍：</p><ul><li><p>例 1：</p><p><strong>数据</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@prefix schemaL &lt;http://schema.org&gt; .</span><br><span class="line">@prefix wd: &lt;http://www.wikidata.org/wiki/&gt; .</span><br><span class="line"></span><br><span class="line">wd:Q2331 schema:album wd:Q201940 .</span><br><span class="line">wd:Q2331 schema:album wd:Q209539 .</span><br></pre></td></tr></table></figure><p>上面这两个三元组相当于：Led Zeppelin (wd:Q2331) 有两张专辑，分别是 wd:Q201940 和 Q209539。</p><p><strong>Query</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">@prefix schemaL &lt;http://schema.org&gt; .</span><br><span class="line">@prefix wd: &lt;http://www.wikidata.org/wiki/&gt; .</span><br><span class="line"></span><br><span class="line">SELECT ?album_name</span><br><span class="line">WHRER &#123;</span><br><span class="line">    wd:Q2331 schema:album ?album_name .</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这一条查询语句包括两部分：</p><ul><li>SELECT：定义我们要查询的变量（album_name），以 “?” 开头。</li><li>WHERE：定义了基本的匹配模式。</li></ul><p>上面的查询语句返回结果是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wd:Q201940</span><br><span class="line">wd:Q209539</span><br></pre></td></tr></table></figure></li><li><p>例 2：</p><p><strong>数据</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@prefix schemaL &lt;http://schema.org&gt; .</span><br><span class="line">@prefix wd: &lt;http://www.wikidata.org/wiki/&gt; .</span><br><span class="line">@prefix wpd: &lt;http://www.wikidata.org/wiki/Property&gt; .</span><br><span class="line"></span><br><span class="line">wd:Q201940 a wd:208569;  # Led Zeppelin IV 是一张专辑</span><br><span class="line">    wd:P1449 &quot;Led Zeppelin IV&quot;  # 有一个名字叫做 “Led Zeppelin IV”</span><br></pre></td></tr></table></figure><p>上面两条三元组的意思是：Led Zeppelin IV 是一张专辑，Led Zeppelin IV 的名字是 “Led Zeppelin IV”。现在我想查询所有叫做 “Led Zeppelin IV” 的专辑，那我可以用下面的查询语句：</p><p><strong>Query</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@prefix schemaL &lt;http://schema.org&gt; .</span><br><span class="line">@prefix wd: &lt;http://www.wikidata.org/wiki/&gt; .</span><br><span class="line">@prefix wpd: &lt;http://www.wikidata.org/wiki/Property&gt; .</span><br><span class="line"></span><br><span class="line">SELECT ?album</span><br><span class="line">WHERE &#123;</span><br><span class="line">    ?alubm wd:P1449 &quot;Led Zeppelin IV&quot; .</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>同样，在 SELECT 语句中以 “?” 开头定义我们需要查询的变量，在 WHERE 中定义查询模式。最后返回结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wd:Q201940</span><br></pre></td></tr></table></figure></li></ul><p>以上两个例子都是非常基础的查询语句。<a href="https://www.stardog.com/tutorials/sparql/" target="_blank" rel="noopener">Stardog tutorial</a> 是一份非常好的 SPARQL 教程，我们可以在这里找到更详细的介绍。</p><h2 id="4-2-SPARQL-endpoint"><a href="#4-2-SPARQL-endpoint" class="headerlink" title="4.2 SPARQL endpoint"></a>4.2 SPARQL endpoint</h2><p>前面我们已经将某一领域的知识建模完成了，存成了 <code>.ttl</code> 文件。实际上这个 <code>.ttl</code> 文件是一个文本文件。我们说知识查询需要用到 SPARQL 查询语言。但是对于一个文本文件来说，我们能做的似乎只有字符串匹配或者正则匹配。SPARQL 语句应该在哪里输入？又在哪里执行？在哪里输出结果？也就是说，现在我们的知识模型文件和知识查询之间形成了一个断层。填补这个断层的就是 endpoint。这里我们介绍三种 SPARQL endpoint：</p><ul><li>D2RQ SPARQL endpoint</li><li>Apache jena SPARQL endpoint</li><li>rdflib</li></ul><h3 id="4-2-1-D2RQ-SPARQL-endpoint"><a href="#4-2-1-D2RQ-SPARQL-endpoint" class="headerlink" title="4.2.1 D2RQ SPARQL endpoint"></a>4.2.1 D2RQ SPARQL endpoint</h3><p>SPARQL endpoint 用于处理客户端的请求，可以类比web server提供用户浏览网页的服务。通过endpoint，我们可以把数据发布在网上，供用户查询。</p><p>D2RQ 是以虚拟 RDF 的方式访问关系型数据库的一个工具。也就是说，假设我们原来的数据是存储在关系型数据库中的，我们不需要把这些数据手动转成 RDF 型数据，就可以通过 D2RQ 使用 SPARQL 语句而不是 SQL 语句进行查询。</p><p>它的工作原理也很简单，就是 D2RQ 会根据关系型数据库的表结构生成一个 mapping 文件，然后 D2RQ 会根据这个 mapping 文件将 SPARQL 语句翻译成 SQL 语句，然后进行查询。这里隐藏着一个 D2RQ 很重要的一个功能：将关系型数据库的数据转化成 RDF 数据。这也是我们常用来批量生成 RDF 数据的方式。关于这个功能不是我们要介绍的，想了解更多可以去 <a href="http://d2rq.org/getting-started" target="_blank" rel="noopener">D2RQ 的官网</a>进行了解。</p><p>我们通过 D2RQ 中的 D2RQ server 功能来进行 SPARQL 查询。D2RQ server 架构图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/d2rq_server.png" alt></p><p>进入 D2RQ 目录，使用下面的命令启动 D2R Server:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d2r-server.bat mapping.ttl  # windows</span><br><span class="line">d2r-server mapping.ttl  # linux</span><br></pre></td></tr></table></figure><p>其中 “mapping.ttl” 就是我们上面说的 mapping 文件。</p><blockquote><p>这里多说两句：</p><p>以 mysql 关系型数据库为例。生成 mapping.ttl 的方式如下：</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">generate-mapping -u root -o mapping.ttl jdbc:mysql:///demo</span><br></pre></td></tr></table></figure><blockquote><ul><li><code>generate-mapping</code> 是转换命令</li><li><code>-u root</code> 表示关系型数据库的用户名</li><li><code>-o mapping.ttl</code> 表示输出 mapping 文件名，可自定义</li><li><code>jdbc:mysql:///demo</code> 关系型数据库</li></ul><p>生成 mapping 文件之后，根据我们用 protege 知识建模的时候生成的 <code>.owl</code> 文件对 mapping文件进行修改，得到我们最终要用的 mapping 文件。更多关于 mapping 的语法参看：<a href="http://d2rq.org/d2rq-language" target="_blank" rel="noopener">The D2RQ Mapping Language</a>。</p></blockquote><p>此时，D2RQ 服务就启动的。我们有两种方式进行 RDF 查询：</p><ol><li>浏览器中查询</li><li>命令行查询</li><li>Python 脚本查询</li></ol><h4 id="4-2-1-1-浏览器查询"><a href="#4-2-1-1-浏览器查询" class="headerlink" title="4.2.1.1 浏览器查询"></a>4.2.1.1 浏览器查询</h4><p>在浏览器中输入 “<a href="http://localhost:2020/" target="_blank" rel="noopener">http://localhost:2020/</a> ”，可以看到如下页面：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/d2rs-screenshot-start.png" alt></p><p>点击页面右下角红框地方的链接，进入 endpoint。然后就可以进行查询了，如下图：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/sparql_browser.png" style="zoom:80%;"></p><h4 id="4-2-1-2-命令行查询"><a href="#4-2-1-2-命令行查询" class="headerlink" title="4.2.1.2 命令行查询"></a>4.2.1.2 命令行查询</h4><p>使用 <code>d2rq-query</code> 工具进行命令行查询：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d2r-query mapping.ttl &quot;SELECT * &#123; ?s ?p ?o &#125; LIMIT 10&quot;</span><br></pre></td></tr></table></figure><p>或者加载一个查询文件，比如 <code>query.sparql</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d2r-query mapping.ttl @query.sparql</span><br></pre></td></tr></table></figure><h4 id="4-2-1-3-Python-脚本查询"><a href="#4-2-1-3-Python-脚本查询" class="headerlink" title="4.2.1.3 Python 脚本查询"></a>4.2.1.3 Python 脚本查询</h4><p>通常情况下，我们对 RDF 的查询是集成在代码中的。为了能在代码中进行查询，人们就开发了一个 python 库—— <a href="https://github.com/RDFLib/sparqlwrapper" target="_blank" rel="noopener">SPARQLWrapper</a>。这是一个Python下的包装器，可以让我们十分方便地和endpoint进行交互。下面是通过SPARQLWrapper，向 D2RQ endpoint发送查询“巩俐参演的评分大于 7 的电影有哪些”，得到结果的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> SPARQLWrapper <span class="keyword">import</span> SPARQLWrapper, JSON</span><br><span class="line"></span><br><span class="line">sparql = SPARQLWrapper(<span class="string">"http://localhost:2020/sparql"</span>)</span><br><span class="line">sparql.setQuery(<span class="string">"""</span></span><br><span class="line"><span class="string">    PREFIX : &lt;http://www.kgdemo.com#&gt;</span></span><br><span class="line"><span class="string">    PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    SELECT ?n WHERE &#123;</span></span><br><span class="line"><span class="string">      ?s rdf:type :Person.</span></span><br><span class="line"><span class="string">      ?s :personName '巩俐'.</span></span><br><span class="line"><span class="string">      ?s :hasActedIn ?o.</span></span><br><span class="line"><span class="string">      ?o :movieTitle ?n.</span></span><br><span class="line"><span class="string">      ?o :movieRating ?r.</span></span><br><span class="line"><span class="string">    FILTER (?r &gt;= 7)</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">"""</span>)</span><br><span class="line">sparql.setReturnFormat(JSON)</span><br><span class="line">results = sparql.query().convert()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results[<span class="string">"results"</span>][<span class="string">"bindings"</span>]:</span><br><span class="line">    print(result[<span class="string">"n"</span>][<span class="string">"value"</span>])</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">2046</span><br><span class="line">Memoirs of a Geisha</span><br><span class="line">荆轲刺秦王</span><br><span class="line">大红灯笼高高挂</span><br><span class="line">霸王别姬</span><br><span class="line">活着</span><br><span class="line">唐伯虎点秋香</span><br><span class="line">秋菊打官司</span><br><span class="line">菊豆</span><br><span class="line">Hong gao liang</span><br><span class="line">画魂</span><br><span class="line">风月</span><br><span class="line">Piao Liang Ma Ma</span><br><span class="line">The Hand</span><br></pre></td></tr></table></figure><h3 id="4-2-2-Apache-jena-SPARQL-endpoint"><a href="#4-2-2-Apache-jena-SPARQL-endpoint" class="headerlink" title="4.2.2 Apache jena SPARQL endpoint"></a>4.2.2 Apache jena SPARQL endpoint</h3><p> <a href="https://jena.apache.org/getting_started/index.html" target="_blank" rel="noopener">Apache jena</a> 严格来说是语义网框架，包括存储、查询和推理组件，架构图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/jena-architecture.png" style="zoom:70%;"></p><p>我们可以将 <code>.ttl</code> 数据存储到 jena 数据库中，然后通过 Fuseki 查询组件进行查询。操作流程同样是可以在浏览器端和命令行和通过调用 api 在代码里进行操作。这里我们不再详细介绍，在接下来的知识存储相关文章中进行详细介绍。</p><h3 id="4-2-3-RDFLib"><a href="#4-2-3-RDFLib" class="headerlink" title="4.2.3 RDFLib"></a>4.2.3 RDFLib</h3><p><a href="https://rdflib.readthedocs.io/en/stable/intro_to_sparql.html" target="_blank" rel="noopener">RDFLib</a> 是一个 python 包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> rdflib</span><br><span class="line">g = rdflib.Graph()</span><br><span class="line">g.parse(<span class="string">"demo.ttl"</span>)  <span class="comment"># 导入 ttl 文件</span></span><br><span class="line"></span><br><span class="line">knows_query = <span class="string">"""</span></span><br><span class="line"><span class="string">SELECT DISTINCT ?aname ?bname</span></span><br><span class="line"><span class="string">WHERE &#123;</span></span><br><span class="line"><span class="string">    ?a foaf:knows ?b .</span></span><br><span class="line"><span class="string">    ?a foaf:name ?aname .</span></span><br><span class="line"><span class="string">    ?b foaf:name ?bname .</span></span><br><span class="line"><span class="string">&#125;"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> qres:</span><br><span class="line">    print(<span class="string">f"<span class="subst">&#123;row.aname&#125;</span> knows <span class="subst">&#123;row.bname&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><h3 id="4-2-4-Wikidata-Query-Service"><a href="#4-2-4-Wikidata-Query-Service" class="headerlink" title="4.2.4 Wikidata Query Service"></a>4.2.4 Wikidata Query Service</h3><p>如果你想快速体验 SPARQL 的话，wikidata 提供了一个服务——<a href="https://query.wikidata.org/" target="_blank" rel="noopener">Wikidata Query Service</a>：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210826173014.png" style="zoom:80%;"></p><h2 id="4-3-更多-SPARQL-语法"><a href="#4-3-更多-SPARQL-语法" class="headerlink" title="4.3 更多 SPARQL 语法"></a>4.3 更多 SPARQL 语法</h2><p>接下来，我们再介绍一些比较常用的 SPARQL 语法。</p><h3 id="4-3-1-DISTINCT"><a href="#4-3-1-DISTINCT" class="headerlink" title="4.3.1 DISTINCT"></a>4.3.1 DISTINCT</h3><p>用于数据去重。</p><p><strong>数据</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">@prefix  foaf:  &lt;http://xmlns.com/foaf/0.1/&gt; .</span><br><span class="line"></span><br><span class="line">_:x    foaf:name   &quot;Alice&quot; .</span><br><span class="line">_:x    foaf:mbox   &lt;mailto:alice@example.com&gt; .</span><br><span class="line"></span><br><span class="line">_:y    foaf:name   &quot;Alice&quot; .</span><br><span class="line">_:y    foaf:mbox   &lt;mailto:asmith@example.com&gt; .</span><br><span class="line"></span><br><span class="line">_:z    foaf:name   &quot;Alice&quot; .</span><br><span class="line">_:z    foaf:mbox   &lt;mailto:alice.smith@example.com&gt; .</span><br></pre></td></tr></table></figure><p><strong>Query</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PREFIX foaf:    &lt;http://xmlns.com/foaf/0.1/&gt;</span><br><span class="line">SELECT DISTINCT ?name WHERE &#123; ?x foaf:name ?name &#125;</span><br></pre></td></tr></table></figure><p><strong>返回结果</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;Alice&quot;</span><br></pre></td></tr></table></figure><h3 id="4-3-2-OPTIONAL"><a href="#4-3-2-OPTIONAL" class="headerlink" title="4.3.2 OPTIONAL"></a>4.3.2 OPTIONAL</h3><p>通常的 query 语句只会返回匹配到的数据，OPTIONAL 可以返回一些匹配到的数据包含的额外信息：</p><p><strong>数据</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">@prefix foaf:       &lt;http://xmlns.com/foaf/0.1/&gt; .</span><br><span class="line">@prefix rdf:        &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .</span><br><span class="line"></span><br><span class="line">_:a  rdf:type        foaf:Person .</span><br><span class="line">_:a  foaf:name       &quot;Alice&quot; .</span><br><span class="line">_:a  foaf:mbox       &lt;mailto:alice@example.com&gt; .</span><br><span class="line">_:a  foaf:mbox       &lt;mailto:alice@work.example&gt; .</span><br><span class="line"></span><br><span class="line">_:b  rdf:type        foaf:Person .</span><br><span class="line">_:b  foaf:name       &quot;Bob&quot; .</span><br></pre></td></tr></table></figure><p><strong>Query</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">PREFIX foaf: &lt;http://xmlns.com/foaf/0.1/&gt;</span><br><span class="line">SELECT ?name ?mbox</span><br><span class="line">WHERE  &#123; ?x foaf:name  ?name .</span><br><span class="line">         OPTIONAL &#123; ?x  foaf:mbox  ?mbox &#125;</span><br><span class="line">       &#125;</span><br></pre></td></tr></table></figure><p><strong>返回结果</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;Alice&quot;&lt;mailto:alice@example.com&gt;</span><br><span class="line">&quot;Alice&quot;&lt;mailto:alice@work.example&gt;</span><br><span class="line">&quot;Bob&quot;</span><br></pre></td></tr></table></figure><h3 id="4-3-3-UNION"><a href="#4-3-3-UNION" class="headerlink" title="4.3.3 UNION"></a>4.3.3 UNION</h3><p>求并集。</p><p><strong>数据</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">@prefix dc10:  &lt;http://purl.org/dc/elements/1.0/&gt; .</span><br><span class="line">@prefix dc11:  &lt;http://purl.org/dc/elements/1.1/&gt; .</span><br><span class="line"></span><br><span class="line">_:a  dc10:title     &quot;SPARQL Query Language Tutorial&quot; .</span><br><span class="line">_:a  dc10:creator   &quot;Alice&quot; .</span><br><span class="line"></span><br><span class="line">_:b  dc11:title     &quot;SPARQL Protocol Tutorial&quot; .</span><br><span class="line">_:b  dc11:creator   &quot;Bob&quot; .</span><br><span class="line"></span><br><span class="line">_:c  dc10:title     &quot;SPARQL&quot; .</span><br><span class="line">_:c  dc11:title     &quot;SPARQL (updated)&quot; .</span><br></pre></td></tr></table></figure><p><strong>Query</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">PREFIX dc10:  &lt;http://purl.org/dc/elements/1.0/&gt;</span><br><span class="line">PREFIX dc11:  &lt;http://purl.org/dc/elements/1.1/&gt;</span><br><span class="line"></span><br><span class="line">SELECT ?title</span><br><span class="line">WHERE  &#123; &#123; ?book dc10:title  ?title &#125; UNION &#123; ?book dc11:title  ?title &#125; &#125;</span><br></pre></td></tr></table></figure><p><strong>返回结果</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&quot;SPARQL Protocol Tutorial&quot;</span><br><span class="line">&quot;SPARQL&quot;</span><br><span class="line">&quot;SPARQL (updated)&quot;</span><br><span class="line">&quot;SPARQL Query Language Tutorial&quot;</span><br></pre></td></tr></table></figure><h3 id="4-3-4-FILTER"><a href="#4-3-4-FILTER" class="headerlink" title="4.3.4 FILTER"></a>4.3.4 FILTER</h3><p>过滤器。</p><p><strong>数据</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@prefix dc:   &lt;http://purl.org/dc/elements/1.1/&gt; .</span><br><span class="line">@prefix :     &lt;http://example.org/book/&gt; .</span><br><span class="line">@prefix ns:   &lt;http://example.org/ns#&gt; .</span><br><span class="line"></span><br><span class="line">:book1  dc:title  &quot;SPARQL Tutorial&quot; .</span><br><span class="line">:book1  ns:price  42 .</span><br><span class="line">:book2  dc:title  &quot;The Semantic Web&quot; .</span><br><span class="line">:book2  ns:price  23 .</span><br></pre></td></tr></table></figure><p><strong>Query</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">PREFIX  dc:  &lt;http://purl.org/dc/elements/1.1/&gt;</span><br><span class="line">SELECT  ?title</span><br><span class="line">WHERE   &#123; ?x dc:title ?title</span><br><span class="line">          FILTER regex(?title, &quot;^SPARQL&quot;) </span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p><strong>返回结果</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;SPARQL Tutorial&quot;</span><br></pre></td></tr></table></figure><h3 id="4-3-5-ORDER-BY"><a href="#4-3-5-ORDER-BY" class="headerlink" title="4.3.5 ORDER BY"></a>4.3.5 ORDER BY</h3><p>排序。</p><p><strong>数据</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">@prefix dc:   &lt;http://purl.org/dc/elements/1.1/&gt; .</span><br><span class="line">@prefix :     &lt;http://example.org/book/&gt; .</span><br><span class="line">@prefix ns:   &lt;http://example.org/ns#&gt; .</span><br><span class="line">@prefix schema: &lt;https://schema.org&gt; .</span><br><span class="line"></span><br><span class="line">:book1  dc:title  &quot;SPARQL Tutorial&quot; .</span><br><span class="line">:book1  ns:price  42 .</span><br><span class="line">:book2  dc:title  &quot;The Semantic Web&quot; .</span><br><span class="line">:book2  ns:price  23 .</span><br><span class="line">:book3 schema:name &quot;A data engineer&apos;s guide to semantic models&quot; .</span><br><span class="line">:book3 ns:price 0 .</span><br></pre></td></tr></table></figure><p><strong>Query</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PREFIX  dc:  &lt;http://purl.org/dc/elements/1.1/&gt;</span><br><span class="line">PREFIX  schema: &lt;https://schema.org&gt; .</span><br><span class="line">SELECT  ?title</span><br><span class="line">WHERE   &#123; </span><br><span class="line">    &#123; ?x dc:title ?title&#125; UNION &#123;?x schema:name ?title&#125;</span><br><span class="line">        &#125;</span><br><span class="line">ORDER BY DESC(?title)</span><br></pre></td></tr></table></figure><p><strong>返回结果</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;A data engineer&apos;s guide to semantic models&quot;</span><br><span class="line">&quot;SPARQL Tutorial&quot;</span><br><span class="line">&quot;The Semantic Web&quot;</span><br></pre></td></tr></table></figure><h3 id="4-3-6-LIMIT"><a href="#4-3-6-LIMIT" class="headerlink" title="4.3.6 LIMIT"></a>4.3.6 LIMIT</h3><p><strong>数据</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">@prefix dc:   &lt;http://purl.org/dc/elements/1.1/&gt; .</span><br><span class="line">@prefix :     &lt;http://example.org/book/&gt; .</span><br><span class="line">@prefix ns:   &lt;http://example.org/ns#&gt; .</span><br><span class="line">@prefix schema: &lt;https://schema.org&gt; .</span><br><span class="line"></span><br><span class="line">:book1  dc:title  &quot;SPARQL Tutorial&quot; .</span><br><span class="line">:book1  ns:price  42 .</span><br><span class="line">:book2  dc:title  &quot;The Semantic Web&quot; .</span><br><span class="line">:book2  ns:price  23 .</span><br><span class="line">:book3 schema:name &quot;A data engineer&apos;s guide to semantic models&quot; .</span><br><span class="line">:book3 ns:price 0 .</span><br></pre></td></tr></table></figure><p><strong>Query</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">PREFIX  dc:  &lt;http://purl.org/dc/elements/1.1/&gt;</span><br><span class="line">PREFIX  schema: &lt;https://schema.org&gt; .</span><br><span class="line">SELECT  ?title</span><br><span class="line">WHERE   &#123; </span><br><span class="line">    &#123; ?x dc:title ?title&#125; UNION &#123;?x schema:name ?title&#125;</span><br><span class="line">        &#125;</span><br><span class="line">ORDER BY DESC(?title)</span><br><span class="line">LIMIT 2</span><br></pre></td></tr></table></figure><p><strong>返回结果</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;A data engineer&apos;s guide to semantic models&quot;</span><br><span class="line">&quot;SPARQL Tutorial&quot;</span><br></pre></td></tr></table></figure><p>更详细的 SPARQL 语法可参考之前提到的资源。</p><h1 id="5-结语"><a href="#5-结语" class="headerlink" title="5. 结语"></a>5. 结语</h1><p>到这里，我们的知识建模简介就结束了。但是对于我们来说，它才刚刚开始。学习这个理论是一个很好的开始，但最好的学习方法是将理论付诸实践。注意建议：当您决定构建一个模型时，尝试与一个团队一起构建模型。由于建模是如此的主观，所以把一群不同的思想家聚集在一起总是一个好主意。语义建模并不一定需要以语义模型开始和结束。</p><p>不管怎样，无论你可能读了多少。我希望我能让你对知识建模领域更了解一点，最重要的是，我希望你能对链接数据感到最小兴奋。毕竟，我们不需要更多的数据，我们需要更多有意义的数据。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p><a href="https://www.semanticarts.com/a-data-engineers-guide-to-semantic-modelling/" target="_blank" rel="noopener">A DATA ENGINEER’S GUIDE TO SEMANTIC MODELLING</a>, <em>Ilaria Maresi, 2020</em> </p></li><li><p><a href="https://zhuanlan.zhihu.com/p/32122644" target="_blank" rel="noopener">知识图谱基础之RDF，RDFS与OWL</a>, <em>SimmerChan</em></p></li><li><a href="https://zhuanlan.zhihu.com/p/32880610" target="_blank" rel="noopener">实践篇（三）：D2RQ SPARQL endpoint与两种交互方式</a>, <em>SimmerChan</em></li><li><a href="https://zhuanlan.zhihu.com/p/33224431" target="_blank" rel="noopener">实践篇（四）：Apache jena SPARQL endpoint及推理</a>, <em>SimmerChan</em> </li></ol>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KG </tag>
            
            <tag> knowledge-modelling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知识图谱：综述（三）Schema, Identity, Context</title>
      <link href="/2021/05/29/kg-survey-3/"/>
      <url>/2021/05/29/kg-survey-3/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/kgicon.png" alt></p><p>接下来，我们介绍 Schema、Identity 和 Context。在上一章，我们将以节点和边组成的数据集合称之为“Data graph”，而真正意义上的知识图谱（knowledge graph）是经过了 Schema（数据模式）、Identity（数据一致性）、Context（上下文）、ontology（本体）和 rules（规则）等表示方法增强过的 data graph。本章我们讨论 Schema、Identity 和 Context。Ontology 和 rules 在后面章节讨论。</p><a id="more"></a><h1 id="3-Schema-Identity-Context"><a href="#3-Schema-Identity-Context" class="headerlink" title="3. Schema, Identity, Context"></a>3. Schema, Identity, Context</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220208095917.png" alt></p><center> Fig 1. 示例知识图谱</center><h2 id="3-1-Schema"><a href="#3-1-Schema" class="headerlink" title="3.1 Schema"></a>3.1 Schema</h2><p>相比于关系型数据库的数据模型，图数据的一大优势就是不需要数据模式（schema）或者可以延后定义数据模式。但是给图数据制定数据模式可以规范一些更加高级的结构或者语义。接下来我们讨论三种图数据模式：semantic，validating，emergent。</p><h3 id="3-1-1-Semantic-schema"><a href="#3-1-1-Semantic-schema" class="headerlink" title="3.1.1 Semantic schema"></a>3.1.1 Semantic schema</h3><p>语义模式的允许定义图中的高级项的意义，我们可以用这些项进行推理。</p><p>根据示例图谱，我们可以定义一些类别（class），比如 Event，City 等等，然后根据类别定义一些子类（subclass），从而形成一个层级结构：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220209164651.png" alt></p><p>有了这样一个层级结构，我们可以直接从数据中检索到<code>（EID15，type，Food Festival）</code>，然后根据语义模式推理出<code>（EID15，type，Festival）</code>，进一步推理出<code>（EID15，type，Event）</code>，如下所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220209165443.png" alt></p><p>除了类别，我们还希望定义边上标签的语义，即属性（property）。回到示例图谱，我们可以认为 city 和 venue 是 location 的子属性（sub-property），或者 bus 和 flight 是 connections to 的子属性。这样，属性也可以形成一个层级结构。</p><p>更进一步，我们可以给属性定义定义域（domain）和值域（range）。定义域表示节点的类别，而边是从该节点扩展出来的。比如，定义 connections to 的定义域为 city，那么对于<code>（Arica connections to Santiago）</code>来说，我们可以推理出<code>（Arica，type，city）</code>。而值域同样表示节点的类别，而边是扩展到该节点上。比如，定义 connections to 的值域是 city，那么那么对于<code>（Arica, connections, to Santiago）</code>来说，我们可以推理出<code>（Santiago，type，city）</code>。</p><p>RDF Schema（RDFS）是定义语义模式的一个重要标准，在 RDFS 中我们可以定义类（class）、子类（subclass）、属性（property）、子属性（sub-property）、定义域（domain）和值域（range），并且可以将这些定义进行序列化。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220210101146.png" alt></p><p>下面给出一个例子：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220210101438.png" alt></p><p>由于 RDFS 的表达能力较弱，比如无法表示两个实体是同一实体（乔丹，same as，飞人），后来又衍生出另一个非常重要的语义模式——OWL（Web Ontology Language）。OWL 的语义表达能力更强，同时也支持 RDFS，在实际的工程中 OWL 的应用更加广泛。</p><blockquote><p>无论是 RDFS 还是 OWL，包括 OWL 2 等所有的语义模式标准，本质上都是词表。这些词表被认为是语义模式的标准词表，我们用这个词表中的词进行语义建模实现了语义标准化，避免了利用自然语言进行语义建模的歧义性和多样性为后续的推理带来困扰，同时也为多图谱融合提供了方便。我们在后续的文章中进行详细讲解。</p></blockquote><p>语义模式同时也满足一个知识图谱的重要假设——开放世界假设：</p><blockquote><p>当前没有陈述的事情是未知的。</p></blockquote><p>即，没有在知识图谱中描述的知识，我们认为是未知的，而不是不存在的。比如在我们的示例图谱中找不到<code>（Viña del Mar，flight ，Arica）</code>这样一条关系，这不代表在 Viña del Mar 和 Arica 之间是没有航班的，只能说明我们现在还不知道这两地之间有没有航班。</p><p>开放世界假设从逻辑上讲是很合理的，因为我们不可能在知识图谱中描述所有的知识，我们只需要描述我们所需要的知识就可以了。但是这样也会给我们带来一定的麻烦，在开放世界假设下，知识图谱无法回答“是”或“否”的问题。比如“<em>在 Viña del Mar 和 Arica 之间有航班吗？</em>”这样一个问题，知识图谱无法直接回答“有”或者“没有”。</p><p>相对于开放世界假设，还存在一个封闭世界假设：</p><blockquote><p>没有描述的事情就是不存在的。</p></blockquote><p>即，假设我们的图谱已经是完备的了，它包含了一切知识。</p><p>处于开放世界假设和封闭世界假设之间，还有一个局部封闭世界假设：</p><blockquote><p>假设部分没有描述的事情真的不存在。</p></blockquote><p>即，认为我们的图谱是部分完备的。比如假设我们的图谱中已经包含了所有航空公司的所有航线，在所有这些航线中都没有 Viña del Mar 和 Arica 之间的航班，那么我们就认为这两地之间确实没有航班。</p><h3 id="3-1-2-Validating-schema"><a href="#3-1-2-Validating-schema" class="headerlink" title="3.1.2 Validating schema"></a>3.1.2 Validating schema</h3><p>如果图谱中的数据多样性较高且存在大量不完备的信息，那么开放世界假设作为默认语义是比较合理的。但是在某些场景下，我们必须保证数据的完备性。比如我们必须保证示例图谱中每一项活动都至少有一个名字、举办地点、开始时间和结束时间，这样才能保证用户对这项活动有最基础的了解。为了保证这些数据的完备性，我们必须对图谱添加一些约束，而 validating schema 就是去验证图谱是否满足我们添加的约束的。</p><blockquote><ul><li>Semantic schema 用于从现有的知识中推理出新的知识</li><li>Validating schema 用于验证已有的知识的合法性</li></ul></blockquote><p>定义 validating schema 的标准方法是使用 <strong><em>shapes</em></strong>。</p><script type="math/tex; mode=display">\text{shape} = [nodes, constraints]</script><p>其中的 $nodes$ 选取可以是一个类别的所有实例，或者某个属性的定义域或者值域，或者某条查询语句的结果等等。$constraints$ 就是作用在这些节点上的约束条件，比如给定属性上节点的数量，节点的数据类型等等。多个 shape 相互关联就形成了 shapes graph。</p><p>Shapes graph 可以用类似 UML 示意图来表示，如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220210150506.png" alt></p><p>这个例子中包含 4 个 shape：Event、Venue、City 以及 Place，每个 shape 中都对该类别下的节点做出了限制，比如 Event 要求必须包含 name（数据类型是字符串类型）、start（数据类型是日期类型）、end（数据类型是日期类型） 和 type（任意类型）等。其中的 $[1..*]$ 表示可以有 $1-\infty$ 个值，$[1..1]$ 表示只能有 1 个值，$[0..1]$ 表示要么没有值（即可以缺省），要么只能有一个值。</p><p>当我们定义了这样一个 shape graph之后，就可以通过递归的方法对数据进行检查，比如 EID15 包含定义的 4 个属性，与之相连的 Santa Lucía 必须属于 Venue 且 Santiago 属于 City。因为 City 属于 Place，就要求 Viña del Mar 和 Arica 也必须属于 Place 等等。而 EID16 就是一个不合法的节点，因为它缺少了 start 和 end。</p><p>当我们定义 shape graph 时，不可能知道每个节点包含的所有属性。这种情况下，open shape 允许节点存在在 shape graph 中未定义的属性，而 close shape 则不允许这种情况。比如我们在示例图谱中添加一个关系（Santiago，founder，Pedro de Valdivia），在我们定义的 shape graph 中没有包含 founder 这个属性，在 open shape 情况下，Santiago 仍然是一个合法的节点，但是在 close shape 情况下就变成了非法节点。</p><p>实际的查询语言通常还支持额外的特性：AND、OR、NOT 等操作。比如 $\text{Venue}~ AND~ (NOT~ \text{City})$ 表示 venue 不能是 City。但是这些额外的操作符的自由组合可能会造成语义问题，比如著名的理发师悖论：</p><blockquote><p>假设有一个 shape 是（Barber，shave，Person），并且要求 $\text{Person}~ AND~ (NOT~ \text{Barber})$。现在给定（Bob，shave，Bob），其中 Bob 满足 （Bob,，type，Person），那么 Bob 满足（Bob，type，Barber）吗？</p><p>如果满足，那么这条数据就是非法的，因为 $\text{Person}~ AND~ (NOT~ \text{Barber})$；</p><p>如果不满足，那么这条数据也是非法的，因为它不满足（Bob，type，Barber）。</p></blockquote><p>为了避免这些悖论的产生，人们提出了各种方法，比如分层（ stratification）、部分赋值（partial assignments）、稳定模型（stable models）等。</p><p>虽然，Semantic schema 和 Validating schema 有不同的应用场景，但是二者也可以互补。我们可以利用 Semantic schema 对 shape 进行推理，也可以利用 Validating schema 对推理结果进行筛选。通常这种情况下需要用 open shape。</p><p>目前有两种验证模式语言：<em>Shape Expressions</em> (<em>ShEx</em>) 和 <em>SHACL</em> (<em>Shapes Constraint Language</em>)。更加详细的介绍可以看 <a href="https://doi.org/10.2200/s00786ed1v01y201707wbe016" target="_blank" rel="noopener">Labra Gayo</a> 等人的论文。</p><h3 id="3-1-3-Emergent-schema"><a href="#3-1-3-Emergent-schema" class="headerlink" title="3.1.3 Emergent schema"></a>3.1.3 Emergent schema</h3><p>无论是 semantic schema 还是 validating schema 都需要领域专家来构建，但是通常我们不具备这样的条件。我们可以从图谱中抽取一些隐式结构形成 emergent schema。 </p><p>用于定义 emergent schema 的框架是 quotient graphs，它可以根据一些等价关系划分数据图中的节点组，同时保留图的一些结构属性。比如示例图谱中，我们可以区分不同的 type，比如 event、name、venue、class、date-time、city 等。将每一个 type 的节点融合进一个节点然后保留边，最终形成 quotient graph，如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220211145433.png" alt></p><p>quotient graphs 的定义取决于如何选取节点和边，不同的 quotient graphs 展现出来的图结构不同。严格意义上来说，quotient graphs 是在模拟输入图谱，这就意味着当且仅当 $x \in X, z \in Z$，且 $x \xrightarrow{y} z$ 时，$X \xrightarrow{y} Z$ 才成立。然而示例图谱中 EID16 是没有开始和结束时间的，所以我们应该将上图换成：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220211153801.png" alt></p><p>将 event 分成两个节点来保存满足不同边的节点，这样的图叫做 bisimilarity。关于 bisimilarity 的一个比较严格的定义是：当存在 $X \xrightarrow{y} Z$ 关系时，对于任意 $x \in X$，必定存在 $z \in Z$ 满足 $x \xrightarrow{y} z$。</p><h2 id="3-2-Identity"><a href="#3-2-Identity" class="headerlink" title="3.2 Identity"></a>3.2 Identity</h2><p>在示例图谱中，我们有一个节点 “Santiago”，但是到底是哪个 Santiago？是 Santiago de Chile？Santiago de Cuba？Santiago de Compostela？甚至说是摇滚乐队 Santiago？从（Santa Lucía，city，Santiago）我们可以推出 Santiago 是一个城市而不是乐队，另一方面，这个图谱是关于智利的旅游图谱，那么我们可以推理出这里的 Santiago 应该是 Santiago de Chile。如果没有足够多的信息，节点的歧义性就会给后续的任务带来麻烦。为了避免节点的歧义性，首先要做的就是给节点分配全局统一的标识符，其次就是给节点添加外部链接标识符加以以区分。</p><h3 id="3-2-1-Persistent-identifiers"><a href="#3-2-1-Persistent-identifiers" class="headerlink" title="3.2.1 Persistent identifiers"></a>3.2.1 Persistent identifiers</h3><p>假设我们想对比智利和古巴的旅游景点，我们收集了足够多的数据然后分别构建了图谱。当我们想要将两个图谱融合在一起的时候就会出现问题：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220211172819.png" alt></p><p>如图所示，两个图谱都有 Santiago 节点，我们如果直接合并的话，两个 Santiago 就会合并成一个。但是我们知道实际上两个 Santiago 是两个不同的地点。为了避免这种情况，我们可以给每个 Santiago 分配一个 Persistent identifiers（PIDs），每一个 PID 保证其全局唯一性。</p><p>除了全局唯一性，我们通常还需要在网页对图谱进行展示，所以 RDF 推荐使用 IRI 来表示实体和关系。比如：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220214102307.png" alt></p><p>IRI 的形式与 URL 非常相近，如果我们把上面的 IRI 复制到浏览器的话，我们会发现进入到了维基百科的页面。那么 IRI 和 URL 有什么区别呢？URL 是用于信息资源的定位，所谓信息简单来说其实就是网页，比如 URL “<a href="https://www.wikidata.org/wiki/Q2887”" target="_blank" rel="noopener">https://www.wikidata.org/wiki/Q2887”</a> 表示的是关于 “Santiago” 这个网页而不是 “Santiago” 这个实体。为了更进一步解释其中的区别，我们来看下面的例子：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220214104303.png" alt></p><p>与上面 IRI 表示法一模一样，但是如果这里是 URL 的话，实际上会给我们带来歧义：到底是 Santiago 这个城市是由 Pedro de valdivia 建立的，还是 “Santiago” 这个网页是 Pedro de valdivia 创建的？</p><h3 id="3-2-2-External-identity-links"><a href="#3-2-2-External-identity-links" class="headerlink" title="3.2.2 External identity links"></a>3.2.2 External identity links</h3><p>假设在我们的图谱中定义了 Santiago 节点为 <code>chile:Santiago</code></p><blockquote><p>chile 是 “<a href="http://turismo.cl/entity/”" target="_blank" rel="noopener">http://turismo.cl/entity/”</a> 的缩写，这个节点定义相当于 “<a href="http://turismo.cl/entity/Santiago”" target="_blank" rel="noopener">http://turismo.cl/entity/Santiago”</a></p></blockquote><p>在另一个地理相关的图谱中定义了 Santiago 为 <code>geo:SantiagoDeChile</code>。两个节点实际上指得是同一座城市，但是由于命名不同，我们就灭哟哟办法直接知道这两个节点是相同的。如果我们对来弄个图谱进行融合的时候，会造成同一个实体有两个不同的节点。</p><p>将两个名称不同，但是表示同一实体的节点统一的方法有以下几种：</p><ol><li><p>使用实体在图谱中信息的唯一性，比如地理坐标、邮政编码、成立时间等；</p></li><li><p>使用标识链接来声明本体实体与外部源中发现的实体是相同实体。在 OWL 中定义了 <code>owl:sameAs</code> 属性来关联两个实体。例如：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220214113104.png" alt></p></li></ol><h3 id="3-2-3-Datatypes"><a href="#3-2-3-Datatypes" class="headerlink" title="3.2.3 Datatypes"></a>3.2.3 Datatypes</h3><p>思考一下示例图谱中左侧的两个日期：“2020-03-29T20:00:00” 和 “2020-03-22T12:00:00”，我们应该怎样分配 PID？直观上来说，我们给它们分配的 IRI 需要告诉我们特定的日期和时间，并且可以被机器或者软件识别，然后我们还可以对这些值进行排序，抽取我们需要的年份、月份等。</p><p>大多数图数据模型允许定义节点的数据类型。RDF 使用 <em>XML Schema Datatypes</em> (<em>XSD</em>)，表示为（$l, d$）对 ，其中 $l$ 表示词汇字符串，$d$ 表示数据类型。比如 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">“2020-03-29T20:00:00” 就表示成 &quot;2020-03-29T20:00:00&quot;^^xsd:dateTime</span><br></pre></td></tr></table></figure><p>RDF 中数据类型节点被称为 “literals”，并且不允许有向外的边。RDF 中其他常用的数据类型还包括：<code>xsdstring</code>、<code>xsd:integer</code>、<code>xsd:decimal</code>、<code>xsd:boolean</code> 等等。</p><p>建立在图谱之上的应用可以通过识别这些数据类型，对其进行解析然后做排序、变换等等后续的处理。</p><h3 id="3-2-4-Lexicalisation"><a href="#3-2-4-Lexicalisation" class="headerlink" title="3.2.4 Lexicalisation"></a>3.2.4 Lexicalisation</h3><p>虽然通常 PID 的形式具有人类可解释性，但是 PID 本身不具有任何语义信息，比如 <code>chile:Santiago</code>。甚至有时候 PID 连人类可解释性都不具备，比如 <code>wd:Q2887</code>。这两种表示法分别称为显式表示和隐式表示，显式表示法有利于人类理解，而隐式表示有利于持久化，比如假设我，们用 <code>wd:Q2887</code> 表示某公司某部门的员工，如果该员工工作部门发生了变动，采用显式表示的话其对应的节点就需要改变，而如果采用隐式表示就无序改变。</p><p>由于 PID 具有任意性，所以通常会给节点添加一个人类可读的边作为标记，比如：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220214144446.png" alt></p><p>实际上可以将这种标记看成是编程语言中的注释，用来说明特定节点在真实世界中包含的信息。我们可以使用昵称或者注释来实现：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220214144930.png" alt></p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220214144946.png" alt></p><p>形如 <code>&quot;Santiago&quot;</code> 这样的节点通常是 literal，而不是标识符。不同的图谱会有不同的表示，比如 <code>(Santiago, rdfs:label, &quot;City&quot;@en)</code> 或者 <code>(Santiago, rdfs:label, &quot;Ciudad&quot;@es)</code> 表示不同语言下的注释。这种用人类可读的 labels，aliases，comments 来对节点进行注释的图谱，我们称之为 “ (<em>multilingual</em>) <em>lexicalised knowledge graphs</em> ”。</p><h3 id="3-2-5-Existential-nodes"><a href="#3-2-5-Existential-nodes" class="headerlink" title="3.2.5 Existential nodes"></a>3.2.5 Existential nodes</h3><p>当建模不完整的信息时，我们在某些情况下可能知道图中必须存在一个特定的节点，与其他节点有特定的关系，但无法识别所讨论的节点。比如两个活动 <code>chile:EID42</code> 和 <code>chile:EID43</code> 有共同的活动地点，但是这个活动地点还没有公布。这种情况的一种处理方法就是忽略活动地点这个关系，但是这样我们会丢失一些信息：</p><ul><li>活动是有活动地点这个属性的；</li><li>两个活动有相同的举办地点。</li></ul><p>另一种处理方法就是创建一个新的 IRI，但是这样的话又无法区分未知地点和已知地点的区别。因此，一些图谱中允许空节点的存在，即所谓的 existential nodes：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220214152404.png" alt></p><p>在 RDF 中支持空节点的存在，即 blank nodes。Blank nodes 也被用来对复杂元素进行建模，比如 RDF list：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220214152637.png" alt></p><h2 id="3-3-Context"><a href="#3-3-Context" class="headerlink" title="3.3 Context"></a>3.3 Context</h2><p>我们可以认为在某些特定情况下，图谱中的数据都是真实的。但是如果考虑时效性的话，Santiago 是从 1541 年成为一个城市的，而 Arica 到 Santiago 的航班是 1956 年开通的。考虑地理因素的话，示例图谱描述的是 chile。考虑数据来源的话，与 <code>EID15</code> 相关的数据来源于 2020 年 1 月 4 日的 Ñam 网页。因此，通过上面的例子，我们可以看到，图谱中的知识都是具有片面性的，只在某些特定情境下成立，这些特定情形我们称之为 context。现在我们就来介绍不同级别的 context 的不同表示方法。</p><h3 id="3-3-1-Direct-representation"><a href="#3-3-1-Direct-representation" class="headerlink" title="3.3.1 Direct representation"></a>3.3.1 Direct representation</h3><p>第一种表示方法就是把 context 当成图谱数据的一部分，用节点和边来表示，比如 <code>EID15</code> 关联的日期时间，我们就认为在这个时间上 <code>（EID15，venueSanta Lucia）</code> 是成立的。另一种方法就如我们在第二章介绍属性图那样，改变边的模式。虽然我们的例子中用的是比较随意的方法，但是实际上有一些特定的 context 已经有一些标准了，比如 <em>Time Ontology</em> 就对时刻、时间段等信息做出了规定。另外 <em>PROV Data Model</em> 规定了在 RDF 中如何表示数据来源。</p><h3 id="3-3-2-Reification"><a href="#3-3-2-Reification" class="headerlink" title="3.3.2 Reification"></a>3.3.2 Reification</h3><p>通常我们希望直接定义边本身的 context，比如 <code>(Santiago, flight, Arica)</code> 这个关系从 1956 年才成立。虽然我们可以用直接表示法，通过改变边的结构来加以声明，但是我们还是希望能有一种更加通用的方法来表示，这就是 Reification。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220214165036.png" alt></p><p>上图展示了三种 reification 方法。我们使用 $e$ 来表示任意标识符，表示可以与之关联到的边的 context 信息。（a）中定义了 $e$ 节点表示边和与之相连的节点（subject, predict, object）。（b）则是用 $e$ 代替目标节点，然后将原来的目标节点作为 $e$ 的值赋予给它。（c）用 $e$ 代替边，然后将 context 作为 $e$ 的标签。</p><h3 id="3-3-3-Higher-arity-representation"><a href="#3-3-3-Higher-arity-representation" class="headerlink" title="3.3.3 Higher-arity representation"></a>3.3.3 Higher-arity representation</h3><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220214172615.png" alt></p><ul><li>（a）命名图的方法</li><li>（b）属性图的方法</li><li>（c） RDF*，RDF* 是 RDF 的扩展方法，它允许将整个三元组当成一个节点。</li></ul><p>以上三种方法中，最灵活的就是命名图的方法。RDF* 方法是最不灵活的，比如 <code>(Chile, president, M. Bachelet)</code>，Bachelet 担任了两届总统，2006-2010 年 和 2014-2018 年，这种情况无法用 RDF* 来描述。</p><h3 id="3-3-4-Annotations"><a href="#3-3-4-Annotations" class="headerlink" title="3.3.4  Annotations"></a>3.3.4  Annotations</h3><p>到目前为止我们讨论了图谱中的 context，但是还没有涉及到 context 的自动推理机制。比如假设 Santiago 到 Arica 之家只有夏季航班，我们要从 Santiago 到 Arcia 只能换一种途径。虽然直接将所有汽车、航班的日期全部表示在图谱中，或者写冗长复杂的查询语句也可以，但是这样的话可能会比较麻烦，甚至根本做不到。这个时候，我们可以考虑使用 <em>annotations</em>，它提供了 context 的数学定义和关键性操作符，可以帮助我们自动进行推理。</p><p>某些 annotations 是对特定领域的 context 进行建模，比如 Temporal RDF 对时间段建模：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220215095655.png" alt></p><p>表示 M. Bachelet 在 2006-2010 年期间担任 Chile 总统。Fuzzy RDF 对可信度进行建模：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220215100604.png" alt></p><p>表示 Santiago 有八成的可能性是属于 Semi-Arid 气候。</p><p>其他形式的 annotations 与领域无关，比如 Annotated RDF 允许将不同形式的 context 建模成 semi-rings：由定义域值（比如时间段）和 meet、join 两个操作符组成的代数结构：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220214175533.png" alt></p><p>如上图所示，$G$ 表示简化的时效值集合，（1-365）表示一年的 365 天，其中形如 $\{[150,152]\}$ 表示 $\{150,151,152\}$。$Q$ 表示从 Santiago 到活动举办地的航班，返回符合时效性的答案。为了推导出答案，我们需要以下几个步骤：</p><ul><li>使用 meet 操作符找到同时满足 city 和 flight 的边。比如 Santiago 和 Punta Arenas 的时效性条件分别是 $\{[150,152]\}$ 和 $\{[1,120],[220,365]\}$，两个条件的交集为空，那么这个答案就会被过滤掉。</li><li>使用 join 操作符，将所有符合条件的结果求并集。比如我们可以在 $\{[123, 125]\}$ 参加 EID16，在 $\{[276,279]\}$ 参加 EID17。最后将两组结果合并得到最终的 context。</li></ul><h3 id="3-3-5-Other-Contextual-framework"><a href="#3-3-5-Other-Contextual-framework" class="headerlink" title="3.3.5 Other Contextual framework"></a>3.3.5 Other Contextual framework</h3><p>除了 annotations 以外，还有一些其他的模型也可以用来推理，比如 <em>contextual knowledge repositories</em>，允许在独立的图谱或者子图上计算 context。它不像 命名图那样，而是通过多维建模，每个图或者子图满足一个维度的条件。OnLine Analytic Processing (OLAP) 提出了基于数据块的模型，提出了 “slice-and-dice” 和 “roll-up” 等操作。更详细的内容可以参考 <a href="https://dl.acm.org/doi/10.5555/2773565.2773655" target="_blank" rel="noopener">Contextualized Knowledge Repositories for the Semantic Web</a> 和 <a href="https://www.semanticscholar.org/paper/Building-a-Conference-Recommender-System-Based-on-Iana-Jung/a4ecd48d04aba80c4c21ae5f1d445959a538488b" target="_blank" rel="noopener">Building a Conference Recommender System Based on SciGraph and WikiCFP</a>。</p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KG </tag>
            
            <tag> survey </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知识图谱：综述（二）Data Graphs</title>
      <link href="/2021/05/27/kg-survey-2/"/>
      <url>/2021/05/27/kg-survey-2/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/kgicon.png" alt></p><p>任何知识图谱的基础是首先将图应用于数据，从而产生初始数据图。我们现在讨论一些在实践中常用来表示数据图的图结构数据模型。 然后，我们讨论构成用于查询此类数据图的图查询语言基础的查询语言。</p><a id="more"></a><h1 id="2-Data-Graphs"><a href="#2-Data-Graphs" class="headerlink" title="2.  Data Graphs"></a>2.  Data Graphs</h1><h2 id="2-1-Models"><a href="#2-1-Models" class="headerlink" title="2.1 Models"></a>2.1 Models</h2><p>抛开图数据不谈，假设现在旅游局还没有确定怎样对数据建模。他们首先会考虑的是关系型数据库来展示所需的数据，尽管还不知道具体需要获取什么数据，但是他们可以设计一个初始的数据模式（schema），比如活动表：</p><script type="math/tex; mode=display">\text{Event(name, venue, type, start, end)}</script><p>其中 name 和 start 一起作为主键。当他们开始往表格填充数据的时候会遇到各种各样的问题：</p><ul><li>活动可能有多个名字（多语言）；</li><li>活动可能在多个场地举办；</li><li>活动举办的起始和结束时间未定；</li><li>活动可能包含多种类型等等。</li></ul><p>随着数据变得更加多样化，他们需要增量式的解决建模问题，比如给活动添加 id，调整关系模式：</p><script type="math/tex; mode=display">\text{EventName(id, name)}\\\text{EventStart(id, start)}\\\text{EventEnd(id, end)}\\\text{EventVenue(id, venue)}\\\text{EventType(id, type)}</script><p>通过上面这种模式，旅游局可以给活动 0-n 个名字、场所和类型以及 0-1 个起始和结束时间（不需要在表格中用 null 或者空值表示）。这种方法要求旅游局不断的重新建模，重新加载数据，重建索引以适应新的数据源。</p><p>如果一开始就采用下面这种数据建模方式，我们会发现当有新的数据需要建模的时候，只需要很小的改动就可以实现多关系映射。我们仔细观察上面这种模式，实际上就可以认为是一种图数据结构：id 和 name 是实体，EventName 是关系，下面类似。</p><p>下面我们介绍三种常见的图数据模型。</p><h3 id="2-1-1-Directed-edge-labelled-graphs"><a href="#2-1-1-Directed-edge-labelled-graphs" class="headerlink" title="2.1.1 Directed edge-labelled graphs"></a>2.1.1 Directed edge-labelled graphs</h3><p>有向边标记图（DELG）是一系列节点和连接节点的带标记的有向边组成的图，如下图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220208101829.png" style="zoom:80%;"></p><p>在知识图谱中，节点用来表示实体，边用来表示实体之间的关系。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220208095917.png" alt></p><p>上图是关于 Events 的图数据信息，包含了两个 event：EID5 和 EID6，并分别展示了两个活动的 name，type，venue，start 和 end。如果我们想要添加信息，只需要添加节点和边；对于不完整的信息，只需要忽略特定节点和边即可，比如 EID6 并没有 start 和 end 信息。</p><p>用图对数据进行建模可以使集成新数据源更加流畅，不需要像关系型数据库那样需要提前设计对数据模式，一旦新数据结构不符合之前设计的数据模式又要重新建表。虽然其他的数据结构也可以方便添加新数据，比如树，但是树需要一个层级结构，而且图支持循环表示，如上图中的  Santiago, Arica 和 Viña del Mar 三个节点。</p><p>W3C 制定了一个标准的有向边标记图——RDF（Resource Description Framework）。RDF 定义了不同的节点类型，包括：</p><ul><li>IRIs：国际化资源标识符（Internationalized Resource Identifiers），允许对 Web 上的实体进行全局识别。</li><li>literals：字面量，允许表示字符串和其他数据类型，比如整数，日期等。</li><li>blank nodes：表示未分配标识符的匿名节点。</li></ul><h3 id="2-1-2-Graph-dataset"><a href="#2-1-2-Graph-dataset" class="headerlink" title="2.1.2 Graph dataset"></a>2.1.2 Graph dataset</h3><p>尽管我们可以通过对多个图取并集的方式将多图合并，但是通常管理多个图更符合实际需求，比如更新和细化单一来源的数据；区分可信数据和不可信数据等。一个图数据集由一系列命名图和一个默认图组成，每个命名图包括图本身和其对应的 ID，默认图不需要 ID。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220208111005.png" alt></p><p>上图给出一个例子，Events 和 Routes 表示两个命名图，默认图用来管理命名图中的元数据。需要指出的是命名图的名字可以作为图的节点，节点和边是可重复的，即不同图中的节点表示同一个实体，在进行图合并的时候可以利用这些节点。</p><h3 id="2-1-3-Property-graphs"><a href="#2-1-3-Property-graphs" class="headerlink" title="2.1.3  Property graphs"></a>2.1.3  Property graphs</h3><p>当我们对更加复杂的关系进行建模的时候可以引入属性图。比如如果我们的数据中包含了哪些公司提供了哪些航班的票价信息，可以使我们更好的了解城市之间的交通信息。这种情况下，我们就不能直接用：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220208112222.png" alt></p><p>而是需要添加新的航班节点，如下所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220208112416.png" alt></p><p>但是这样的话，我们就需要对原始的图进行大改。另一种方法就是将不同公司的航班信息用命名图的方式表示出来，但是如果命名图已经被用作他途，这种方式也会变得很麻烦。</p><p>属性图简单来说就是，节点和边都可以带标签，如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220208145030.png" alt></p><p>属性图在常见的图数据库中是最常用的，比如 Neo4j。属性图可以在没有任何信息损失的情况下与有向边标记图和图数据集相互转化。</p><blockquote><ul><li>有向边标记图是更小巧的图数据模型；</li><li>属性图是更加灵活的图数据模型。</li></ul></blockquote><h3 id="2-1-4-Other-graph-data-models"><a href="#2-1-4-Other-graph-data-models" class="headerlink" title="2.1.4 Other graph data models"></a>2.1.4 Other graph data models</h3><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220208151840.png" alt></p><p>前面我们介绍了最常见的三种图数据模型，还有一些更加复杂的模型，比如有些复杂节点包含孤立边或者嵌套图等。还有一些复杂边连接的是一个集合而不是节点对。在我们看来，知识图谱可以应用任意的图数据模型：通常情况下数据可以从一个模型转化为另一个模型，比如上面两图所示的例子。</p><p>近些年来，最流行的图数据模型就是前面介绍的三种模型，后面我们会详细介绍。</p><h2 id="2-2-Querying"><a href="#2-2-Querying" class="headerlink" title="2.2 Querying"></a>2.2 Querying</h2><p>就像关系型数据库需要 SQL 语言对数据进行检索一样，图数据同样需要检索语言。目前的图数据语言有很多，比如 SPAEQL 用于检索 RDF 数据，Cypher、Gremlin、G-CORE 等语言用于检索属性图。虽然不同语言的具体语法不尽相同，但是却存在一些共通性，包括（基本的）图模式（graph patterns）、关系运算符（relational operators）、路径表达式（path expressions）等等。</p><h3 id="2-2-1-graph-patterns"><a href="#2-2-1-graph-patterns" class="headerlink" title="2.2.1 graph patterns"></a>2.2.1 graph patterns</h3><p>每一种图数据的结构化查询语言核心都是图模式，它遵循与被查询的图数据相同的模型，另外还支持变量项。因此图数据模式中的术语分成两类：常量和变量。常量如 “Arica”，变量通常使用问号做前缀，比如 “?event”。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220208163800.png" alt></p><p>上图展示的是变量术语情况的例子。现在我们要查找 Food Festival 的举办地点，图模式通过将变量映射到图数据库中的常量，上图右侧展示可能的映射。</p><p>注意上图右表最后两行，同一个 event 对应两个相同的 venue，这种情况在有些应用中是可取的，但是有些情况确实不可取的。因此，为了评估图模式，人们提出一些语义标准，其中最重要的有两个：</p><ul><li>基于同态的语义（homomorphism-based semantics），即不同的变量可以映射到相同的常量上。</li><li>基于同构的语义（isomorphism based semantics），即要求变量映射的节点或者边必须是唯一项。</li></ul><p>不同的查询语言使用不同的语义标准，比如 SPARQL 基于同态语义，而 Cypher 在边上则基于同构语义。 </p><h3 id="2-2-2-Complex-graph-patterns"><a href="#2-2-2-Complex-graph-patterns" class="headerlink" title="2.2.2 Complex graph patterns"></a>2.2.2 Complex graph patterns</h3><p>图模式是将输入的图转化成表格，如上面的例子所示。因此，我们可以考虑使用关系代数实现这种转化，这样可以形成更复杂的检索。</p><blockquote><p>关系代数由一元操作符和二元操作符组成，一元操作符接收一个输入表，二元操作符接收两个输入表。</p><p>一元操作符包括：</p><ul><li>映射（$\pi$）：输出表格的列；</li><li>筛选（$\sigma$）：根据匹配条件输出行；</li><li>重命名列（$\rho$）。</li></ul><p>二元操作符包括：</p><ul><li>并集（$\cup$）：合并两个表的行成一个一个表；</li><li>差集（$-$）：从第一个表中移除出现在第二个表中的行；</li><li>合集（$\Join$）：将其他表中满足联合条件的行扩充到第一个表中；</li></ul><p>筛选和合集条件基本包含：等于（$=$）、小于等于（$\le$）、非（$\neg$）、析取（$\lor$）等。</p><p>根据这些操作符，我们可以进一步定义一些操作符：</p><ul><li>交集（$\cap$）：输出两张表；</li><li>反合集（$\rhd$，即不存在）：输出第一个表中不满足与第二个表的联合条件的行；</li><li>左联合（$\lhd$，即可选）：输出合集但是保持第一个表中与第二个表没有冲突的行。</li></ul></blockquote><p>假设 $G(s, p,o)$ 表示一个图，那么上图的检索可以用关系代数表示：</p><script type="math/tex; mode=display">\pi_{ev,vn1,vn2}(\sigma(\rho_{s\rightarrow ev}(G \Join \rho_{p \rightarrow p1, o \rightarrow vn1}(G) \Join \rho_{p \rightarrow p2, o \rightarrow vn2}(G)))|\text{condition})</script><p>其中 $\text{conditipon}= [p=\text{type},o=\text{Food Festival},p1=p2=\text{venue}]$。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220209105531.png" alt></p><p>上图中，我们给出一个例子，其中加粗字体表示我们想要的最终结果，这个检索相当于连词查询。从右表中我们可以看到，前两行的结果是重复的，说明复杂图模式也会给出重复结果。针对这个问题，查询语言提供了两种语义：</p><ul><li>bag semantic：根据地层映射的多重性来保留副本；</li><li>set semantic：移除结果中的重复项（DISTINCT）。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220209105546.png" alt></p><p>另一个例子，查找不在 Santiago 举办的 Food Festival 或者 Drinks Festival，若存在则返回它们的 name 和 start date。这条语句相当于一个一阶查询（first-order queries）。</p><h3 id="2-2-3-Navigational-graph-patterns"><a href="#2-2-3-Navigational-graph-patterns" class="headerlink" title="2.2.3 Navigational graph patterns"></a>2.2.3 Navigational graph patterns</h3><p>区分不同图查询语言的一个关键特性是：在查询语句中包含路径表达式的能力。路径表达式就是两个节点之间任意长度的路径。$\text{query}(x,r,y)$，其中 $x,y$ 表示变量或者常量（甚至可以是相同的项），$r$ 表示路径。基础的路径表达式中 $r$ 是常量（边的标签），如果 $r$ 是一个表达式，那么 $r^-$（反转），$r^\star$（0-$\infty$）等等同样也是路径表达式。最后，如果 $r_1$ 和 $r_2$ 都是表达式，那么 $r_1 | r_2$ （析取） ，$r_1\cdot r_2$（拼接）也是路径表达式。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220209113336.png" alt></p><p>通常情况下，我们么可以直接在查询语句中使用路径表达式，比如（Arica, bus*, ?city）。但是由于图是可循环的，所以路径也有可能是循环的，这就会造成有无数种路径的可能性。所以，通常只会返回最短路径或者没有重复节点或边的路径。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220209113433.png" alt></p><p>上图的例子表示，从 Arica 出发通过 bus 或者 flight 能够到达的举办 food festival 的城市。</p><p>当查询语句中包含多个路径时，可以使用我们在关系代数中介绍的那些操作符。</p><h3 id="2-2-4-Other-features"><a href="#2-2-4-Other-features" class="headerlink" title="2.2.4 Other features"></a>2.2.4 Other features</h3><p>到目前为止，我们介绍了一些查询语言的实践或者理论基础。但是在实际生产中，特定的查询语言还支持其他特性，比如聚合（GROUP BY, COUNT 等），更复杂的过滤器、数据类型操作符、远程查询、图数据更新、语义约束机制等等。</p><h3 id="2-3-小结"><a href="#2-3-小结" class="headerlink" title="2.3 小结"></a>2.3 小结</h3><ul><li>图数据模型：有向边标记图、图数据集、属性图以及复杂图模型等；</li><li>图数据查询语言特性：图模式、复杂图模式、可导航图模式以及其他特性。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KG </tag>
            
            <tag> survey </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知识图谱：综述（一）前言</title>
      <link href="/2021/05/25/kg-survey-1/"/>
      <url>/2021/05/25/kg-survey-1/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/kgicon.png" alt></p><p>目前，知识图谱在学术界和工业界都引起了重视。本人目前也开始负责知识图谱项目，因此从本文开始对知识图谱进行系统性的介绍。首先从综述入手可以使我们对知识图谱有一个整体的概念，然后对其中的每个细节进行深入介绍。本文来自论文<a href="https://arxiv.org/abs/2003.02320v3" target="_blank" rel="noopener">《Knowledge Graphs》</a>，是一篇长达 132 页（558篇引用）的综述，可谓干货满满，所以以这篇综述作为切入点。因为内容过长，所以我们将论文的每一章作为一篇博文，一共大概会有十几篇博文。</p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>尽管“知识图谱”首次出现在文献中是在1972年，但是现代意义上的知识图谱起源于2012年谷歌发布的“谷歌知识图谱”（Google Knowledge Graph），随后更进一步的知识图谱项目如雨后春笋般涌现出来。随着知识图谱在工业界的兴起，学术界也开始重视：越来越多的关于知识图谱的文献和书籍开始出现。</p><p>知识图谱的核心就是用图（graph）来表示数据，通常还会用一些明确表示知识的方法加以增强。使用基于图的知识表示方法相比于其他方法有很大的优势，比如关系型数据库或者非关系型数据库：</p><ul><li>图可以为各领域提供简洁直观的抽象化概念，其中图的节点用实体表示，边则表示实体与实体之间的内在关系。</li><li>图允许维护人员推迟定义schema，允许数据以更灵活的方式发展，特别是在捕获不完整知识方面。（开放世界假设）</li><li>特定的图查询语言不仅支持鸟准的关系操作符（join，union，projection 等），还有导航操作符，通过递归查找任意路径长度的实体。</li><li>标准的知识表示形式，比如本体和规则，还可以用来进行知识的定义和语义推理。</li><li>大规模的图分析框架帮助我们进行各种图计算以获得各领域的新的见解。</li><li>各种知识表示技术的发展支持将机器学习技术直接应用于图数据上。</li></ul><p>总之，知识图谱打开了一扇从不同数据源集成和抽取知识的大门。但是到目前为止，还没有一个统一的方法来描述知识图谱如何被使用，用到了什么技术，以及它们与现有的数据管理之间有什么联系。本文的目的就是全面介绍知识图谱：</p><ul><li>数据模型基础以及如何对数据进行检索；</li><li>讨论知识表示方法（schema，identity 和 context） ；</li><li>讨论归纳和演绎的方法，使得知识更加明确；</li><li>介绍不同的用于创建和增强图结构数据的技术；</li><li>介绍知识图谱的质量评估以及如何进行改进；</li><li>讨论知识图谱发布的标准以及最佳实践；</li><li>提供一些已有的知识图谱项目的梗概。</li></ul><h2 id="1-1-知识图谱"><a href="#1-1-知识图谱" class="headerlink" title="1.1 知识图谱"></a>1.1 知识图谱</h2><p>知识图谱的定义目前仍然存在争议，现有的定义从具体的技术性定义到更具包容性的一般性定义。本文采用如下定义：</p><blockquote><p>  <em>a knowledge graph as a graph of data intended to accumulate and convey knowledge of the real world, whose nodes represent entities of interest and whose edges represent relations between these entities</em>. </p><p>  <em>知识图谱是一个数据图旨在积累和表达真实世界的知识，其中图的节点表示实体，边表示实体之间的关系。</em></p></blockquote><p>知识指的是已知的东西，它可以是从外部源积累的或者抽取出来的，也可以是知识图谱本身提取出来的。知识可以由简单的陈述（simple statements）表示，比如“圣地亚哥是智利的首都”，也可以是量化描述（quantified statements），比如“所有的首都都是城市”。简单描述可以由知识图谱中的边来表示，但是要想实现量化描述就需要借助更加复杂的方法来表示知识，比如本体（ontology）或者规则（rules）。演绎方法可以用来推演更深层次的知识，比如“圣地亚哥是一个城市”。归纳方法可以用来积累和抽取知识。</p><blockquote><p>  <em>归纳</em> 其实就是知识图谱的构建；</p><p>  <em>演绎</em> 其实就是知识图谱的推理。</p></blockquote><p>知识图谱的数据来源通常是不同的，这就造成数据源的结构和粒度的不同。为了解决这些差异，知识模式（schema），知识一致性（identity）和上下文（context）的表示就至关重要。知识模式指的是知识图谱的高级结构，知识一致性指的是同一个知识的不同描述，上下文指的是在某一单元内知识的正确性。如上文所说，知识图谱的构建与优化需要：<em>extraction</em>，<em>enrichment</em>，<em>quality assessment</em>，<em>refinement</em>。</p><h2 id="1-2-实践"><a href="#1-2-实践" class="headerlink" title="1.2 实践"></a>1.2 实践</h2><p>知识图谱是作为一个组织或领域中不断发展的知识共享基础，我们将知识图谱分成两类：开放知识图谱（ open knowledge graphs）和企业知识图谱（enterprise knowledge graphs）。开放知识图谱是发布在网络上，可供公开获取的知识图谱，企业知识图谱指的是面向企业用户的商业化知识图谱。</p><ul><li><p>开放知识图谱</p><p>DBpedia, Freebase, Wikidata, YAGO 等。</p></li><li><p>企业知识图谱</p><p> Bing, Google, Airbnb, Amazon, eBay, Uber, Facebook, LinkedIn, Accenture, Banca d’Italia, Bloomberg , Capital One, Wells Fargo 等。</p><p>企业知识图谱通常用于搜索，推荐，个人助理，广告，商业分析，风险评估，自动化等方面。</p></li></ul><h2 id="1-3-运行实例"><a href="#1-3-运行实例" class="headerlink" title="1.3 运行实例"></a>1.3 运行实例</h2><p>为了更清晰的对知识图谱进行介绍，本文采用一个智利旅游相关的知识图谱作为实例进行讲解。这个知识图谱包括旅游景点，文化事件，服务以及商业介绍等内容，应用场景包括：</p><ul><li>创建一个门户网站，允许用户搜索景点、即将到来的活动、以及相关服务等（多语言）;</li><li>深入了解关于季节、国籍等旅游人口统计数据；</li><li>分析游客对各种景点和活动的情感，包括正面的评价、对活动和服务的投诉总结、犯罪报告等;</li><li>了解旅游轨迹：游客参观的景点、活动等的顺序；</li><li>航班/巴士信息，以建议新的游览路线；</li><li>提供个性化的旅游地推荐等等。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KG </tag>
            
            <tag> survey </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构与算法：分治算法</title>
      <link href="/2021/05/22/ds-divide-and-conquer/"/>
      <url>/2021/05/22/ds-divide-and-conquer/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png" alt></p><p>分治算法（<em>divide and conquer</em>）是一种解决大问题的策略，通过：</p><ol><li>将一个大问题分解成小问题</li><li>解决小问题</li><li>蒋小问题的解合并在一起得到想要的答案</li></ol><a id="more"></a><p>分治思想通常应用在递归函数上。下面我们以一个数组的排序问题进行介绍。</p><ol><li><p>给定一个数组</p><p><img width="300" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/divide-and-conquer-0.png"></p></li><li><p>将数组分解</p><p><img width="400" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/divide-and-conquer-1.png"></p><p>将子问题继续分解，直到每个分支只有一个元素</p><p><img width="500" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/divide-and-conquer-2.png"></p></li><li><p>对分解后的元素进行排序，然后合并排序结果。这里我们边排序边合并。</p><p><img width="500" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/divide-and-conquer-3.png"></p></li></ol><ul><li><p><strong>时间复杂度</strong></p><p>以合并排序算法为例，根据<a href="https://rogerspy.gitee.io/2021/04/22/ds-time-complexity/">主定理</a>：</p><blockquote><p>$T(n)=aT(n/b)+f(n)=2T(n/2)+O(n)$</p><ul><li>$a=2$：每次将问题分解成两个子问题；</li><li>$b=2$：每个子问题的规模是输入数据的一半；</li><li>$f(n)=n$：分解和合并子问题的复杂度是线性增加的；</li><li>$\log_ba=1 \Rightarrow f(n)=n^1=n$;</li><li>由主定理可得：$T(n)=O(n\log n)$</li></ul></blockquote></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.programiz.com/dsa/divide-and-conquer" target="_blank" rel="noopener">Divide and Conquer Algorithm</a> </p>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> divide-conquer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>24种二分类模型的评估方法</title>
      <link href="/2021/04/28/24-binary-class-evaluateion-metrics/"/>
      <url>/2021/04/28/24-binary-class-evaluateion-metrics/</url>
      
        <content type="html"><![CDATA[<p><img width="75%" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/4f4d15426607b3fd4051791fa9224979.jpg"></p><p>评估一个模型的好坏有很多指标，每个指标都有其优缺点。如何针对不同场合选取合适的评估指标是一个非常重要的工作。本文将会介绍一些用于分类模型的评估指标，然后介绍我们该如何选取。</p><a id="more"></a><h1 id="1-混淆矩阵（Confusion-Matrix）"><a href="#1-混淆矩阵（Confusion-Matrix）" class="headerlink" title="1. 混淆矩阵（Confusion Matrix）"></a>1. 混淆矩阵（Confusion Matrix）</h1><p>混淆矩阵（混淆表）是一个用来评估分类模型的 $N\times N$ 矩阵，其中 $N$ 表示类别数量。混淆矩阵通过对比真实的类别标签和模型预测的类别标签从整体上对模型进行评估。</p><h2 id="1-1-二分类混淆矩阵"><a href="#1-1-二分类混淆矩阵" class="headerlink" title="1.1 二分类混淆矩阵"></a>1.1 二分类混淆矩阵</h2><p>对于二分类问题，混淆矩阵是一个 $2 \times 2$ 的矩阵，如下所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210508163046.png" alt></p><ul><li>目标标签有两个类别：<strong>Positive</strong> 和 <strong>Negative</strong></li><li>每一列表示真实的标签类别（actual values）</li><li>每一行表示模型预测的标签类别（predicted values）</li></ul><p>矩阵中的 <strong>TP</strong>、<strong>TN</strong>、<strong>FP</strong>、<strong>FN</strong> 分别表示什么呢？</p><p><strong>True Positive（TP）</strong></p><ul><li>模型预测的标签和真实的标签相同</li><li>真实的标签是 <strong>Positive</strong>，模型预测的标签也是 <strong>Positive</strong></li></ul><p><strong>True Negative（TN）</strong></p><ul><li>模型预测的标签与真实的标签相同</li><li>真实的标签是 <strong>Negative</strong>，模型预测的标签也是 <strong>Negative</strong></li></ul><p><strong>False Positive（FP）</strong></p><ul><li>模型预测的结果与真实的标签不一致</li><li>真实的标签是 <strong>Negative</strong>，但模型预测的是 <strong>Positive</strong></li><li>这种错误称之为 “第一类错误”（<em>Type-I error</em>）</li></ul><p><strong>False Negative（FN）</strong></p><ul><li>模型预测的结果与真实的标签不一致</li><li>真实的标签是 <strong>Positive</strong>，但模型预测的是 <strong>Negative</strong></li><li>这种错误称之为 “第二类错误”（<em>Type-II error</em>）</li></ul><p>举例说明：假设有 1000 个样本，分类模型在这些样本上得到了下面这个混淆矩阵：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210508163200.png" alt></p><p>矩阵中不同的值表示：</p><ul><li>True Positive (TP) = 560，有 560个 正样本被模型正确预测了；</li><li>True Negative (TN) = 330，有 330 个负样本被正确预测了；</li><li>False Positive (FP) = 60，有 60 负样本被模型预测成了正样本；</li><li>False Negative (FN) = 50，有 50 个正样本被模型预测成了负样本。</li></ul><p>从混淆矩阵中可以看出，绝大多数的正样本和负样本可以被模型准确识别出来，说明这是一个还不错的分类模型。</p><h2 id="1-2-多分类混淆矩阵"><a href="#1-2-多分类混淆矩阵" class="headerlink" title="1.2 多分类混淆矩阵"></a>1.2 多分类混淆矩阵</h2><p>有了二分类的混淆矩阵，我们可以把它扩展到多分类问题上。假设有三个类别：A,B,C。那么混淆矩阵应该是一个 $3 \times 3$ 的矩阵：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210508164822.png" alt></p><p>对于每个类别的 TP、TN、FP、FN 的计算方式如下：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{split}A:\\\\& TP=Cell_1 \\\\& TN=Cell_5+Cell_6+Cell_8+Cell_9 \\\\& FP=Cell_2+Cell_3 \\\\& FN=Cell_4+Cell_7 \\\\B:\\\\& TP=Cell_5 \\\\& TN=Cell_1+Cell_3+Cell_7+Cell_9 \\\\& FP=Cell_4+Cell_6 \\\\& FN=Cell_2+Cell_8 \\\\C:\\\\& TP=Cell_9 \\\\& TN=Cell_1+Cell_2+Cell_4+Cell_5 \\\\& FP=Cell_7+Cell_8 \\\\& FN=Cell_3+Cell_6 \\\\\end{split}\end{equation}</script><h2 id="1-3-用-scikit-learn-计算混淆矩阵"><a href="#1-3-用-scikit-learn-计算混淆矩阵" class="headerlink" title="1.3 用 scikit-learn 计算混淆矩阵"></a>1.3 用 scikit-learn 计算混淆矩阵</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br><span class="line">predict_class = y_pred_pos &gt; threshold</span><br><span class="line">confusion = metrics.confusion_matrix(true_class, predict_class)</span><br><span class="line">print(confusion)</span><br></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">330</span>, <span class="number">60</span>]  </span><br><span class="line">[<span class="number">50</span>, <span class="number">560</span>]]</span><br></pre></td></tr></table></figure><p>需要注意的是，<code>scikit-learn</code> 的混淆矩阵<code>(0, 0)</code> 位置是 TN，<code>(1,1)</code> 位置是 TP。</p><h2 id="1-4-什么时候用？"><a href="#1-4-什么时候用？" class="headerlink" title="1.4 什么时候用？"></a>1.4 什么时候用？</h2><p>几乎在所有的分类问题上都可以使用，尤其是在了解具体数量而非归一化的比例的时候（通常是类别不平衡）。</p><h1 id="2-准确率（Accuracy）"><a href="#2-准确率（Accuracy）" class="headerlink" title="2. 准确率（Accuracy）"></a>2. 准确率（Accuracy）</h1><h2 id="2-1-准确率定义"><a href="#2-1-准确率定义" class="headerlink" title="2.1 准确率定义"></a>2.1 准确率定义</h2><p>准确率评估的是模型对样本正确分类的比例，计算方法如下：</p><script type="math/tex; mode=display">\mathrm{accuracy}=\frac{TP+TN}{TP+TN+FP+FN}</script><h2 id="2-2-用-scikit-learn-计算准确率"><a href="#2-2-用-scikit-learn-计算准确率" class="headerlink" title="2.2 用 scikit-learn 计算准确率"></a>2.2 用 scikit-learn 计算准确率</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, accuracy_score </span><br><span class="line"></span><br><span class="line">y_pred_class = y_pred_pos &gt; threshold </span><br><span class="line">tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel() </span><br><span class="line">accuracy = (tp + tn) / (tp + fp + fn + tn) </span><br><span class="line"></span><br><span class="line"><span class="comment"># or simply </span></span><br><span class="line">accuracy_score(y_true, y_pred_class)</span><br></pre></td></tr></table></figure><h2 id="2-3-准确率与阈值的关系"><a href="#2-3-准确率与阈值的关系" class="headerlink" title="2.3 准确率与阈值的关系"></a>2.3 准确率与阈值的关系</h2><p><img src="https://i2.wp.com/neptune.ai/wp-content/uploads/acc_by_thres.png?fit=1024%2C768&amp;ssl=1" alt></p><p>分类任务中，模型输出的是每个类别对应的概率。比如二分类，当正类别概率大于 50% 的时候，我们认为该样本是正样本，其中 50% 就是分类的阈值。阈值是可以人为设定的，比如可以规定当概率大于 70% 的时候才认为是正样本。</p><p>对于二分类模型，通常选择 0.5 作为阈值。阈值过大会造成 FN 过大，从而降低准确率。阈值太小会造成 FP 多大，同样会造成准确率过低。</p><h2 id="2-4-什么时候用？"><a href="#2-4-什么时候用？" class="headerlink" title="2.4 什么时候用？"></a>2.4 什么时候用？</h2><ul><li>各类别比较平衡</li><li>每个类别对我们来说同等重要</li></ul><h2 id="2-5-什么时候不能用？"><a href="#2-5-什么时候不能用？" class="headerlink" title="2.5 什么时候不能用？"></a>2.5 什么时候不能用？</h2><p>考虑一个场景：假设每 100 个人中就有 1 个人生病了，我们用一个分类模型对生病的人和没有生病的人进行分类。即使模型所有的输出都是没有生病那准确率也有 99%，但是这个模型却是很糟糕的一个模型。</p><p>仔细观察一下上面的数据分布，很容易发现问题：数据类别不平衡。也就是说，在类别不平衡的数据上评估分类模型的好坏是不可以使用准确率的。</p><h1 id="3-精准度（Precision）"><a href="#3-精准度（Precision）" class="headerlink" title="3. 精准度（Precision）"></a>3. 精准度（Precision）</h1><h2 id="3-1-精准度定义"><a href="#3-1-精准度定义" class="headerlink" title="3.1 精准度定义"></a>3.1 精准度定义</h2><p>精准度表示在模型预测为正样本的数据中，有多少是真正的正样本。比如用渔网捞鱼，这一网捞上来的有鱼有虾，其中是鱼的比例就是精准度。计算公式如下：</p><script type="math/tex; mode=display">\mathrm{Precision} = \frac{TP}{TP+FP}</script><h2 id="3-2-用-scikit-learn-计算精准度"><a href="#3-2-用-scikit-learn-计算精准度" class="headerlink" title="3.2 用 scikit-learn 计算精准度"></a>3.2 用 scikit-learn 计算精准度</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, precision_score</span><br><span class="line"></span><br><span class="line">y_pred_class = y_pred_pos &gt; threshold</span><br><span class="line">tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()</span><br><span class="line">positive_predictive_value = tp/ (tp + fp)</span><br><span class="line"></span><br><span class="line"><span class="comment"># or simply</span></span><br><span class="line">precision_score(y_true, y_pred_class)</span><br></pre></td></tr></table></figure><h2 id="3-3-精准度与阈值的关系"><a href="#3-3-精准度与阈值的关系" class="headerlink" title="3.3 精准度与阈值的关系"></a>3.3 精准度与阈值的关系</h2><p><img src="https://i0.wp.com/neptune.ai/wp-content/uploads/ppv_by_thres.png?fit=1024%2C768&amp;ssl=1" alt></p><p>从这个解释中我们可以看出，阈值越高说明概率越大。从直觉上可以判断，概率越大说明可信度越高。那么样本被正确分类的可能性就越高。回到精准度的角度，精准度表示真正的正样本比例。如果阈值设定较高的话，正样本分类的正确率也会越高，精准度也会越高。极端情况下，把阈值设定成 100%，精准度也会达到最大。</p><h2 id="3-4-什么时候用？"><a href="#3-4-什么时候用？" class="headerlink" title="3.4 什么时候用？"></a>3.4 什么时候用？</h2><ul><li>单独使用精准度没有什么意义，通常会配合其他指标一起使用</li><li>当错误警报成本过高，或者当你认为每个预测为正样本的样例都值得一看的时候，可以针对精准度进行调整</li></ul><h1 id="4-召回率（Recall）"><a href="#4-召回率（Recall）" class="headerlink" title="4. 召回率（Recall）"></a>4. 召回率（Recall）</h1><h2 id="4-1-召回率定义"><a href="#4-1-召回率定义" class="headerlink" title="4.1 召回率定义"></a>4.1 召回率定义</h2><p>召回率又叫真阳性率，表示有多少是真正的正样本被模型正确识别出来了。我们经常会听到某某产品出现了质量问题，厂家紧急召回的新闻。召回率就是说，市面上所有的问题产品，厂家召回了多少。另外一个例子，目前新冠肆虐，新冠的检测是通过咽拭子。召回率表示，通过咽拭子找到了多少新冠患者。</p><p>通过这两个例子我们可以对准确率，精确度和召回率加以区分。准确率关注的是所有类别的分类正确率，精确度是正样本的准确率，而召回率表示找到的正样本占总正样本的比例。</p><p>用公式表示如下：</p><script type="math/tex; mode=display">\mathrm{recall} = \frac{TP}{TP+FN}</script><h2 id="4-2-用-scikit-learn-计算召回率"><a href="#4-2-用-scikit-learn-计算召回率" class="headerlink" title="4.2 用 scikit-learn 计算召回率"></a>4.2 用 scikit-learn 计算召回率</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, recall_score</span><br><span class="line"></span><br><span class="line">y_pred_class = y_pred_pos &gt; threshold</span><br><span class="line">tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()</span><br><span class="line">true_positive_rate = tp / (tp + fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># or simply</span></span><br><span class="line"></span><br><span class="line">recall_score(y_true, y_pred_class)</span><br></pre></td></tr></table></figure><h2 id="4-3-召回率与阈值的关系"><a href="#4-3-召回率与阈值的关系" class="headerlink" title="4.3 召回率与阈值的关系"></a>4.3 召回率与阈值的关系</h2><p><img src="https://i1.wp.com/neptune.ai/wp-content/uploads/tpr_by_thres.png?fit=1024%2C768&amp;ssl=1" alt></p><p>阈值设定越低，模型预测为正样本的门槛就越低，就越容易把所有的正样本找出来。所以召回率与阈值是一个负相关的关系。</p><h2 id="4-4-什么时候用？"><a href="#4-4-什么时候用？" class="headerlink" title="4.4 什么时候用？"></a>4.4 什么时候用？</h2><ul><li>单独使用召回率没有什么意义，通常会配合其他指标一起使用</li><li>但是有些情况，比如灾难预警、欺诈性交易等，即使收到一些错误预警，我们也必须谨慎对待。即在宁可信其有不可信其无的场景下，适当调整召回率是有必要的</li></ul><h1 id="5-F1-得分（F1-score）"><a href="#5-F1-得分（F1-score）" class="headerlink" title="5. F1 得分（F1-score）"></a>5. F1 得分（F1-score）</h1><h2 id="5-1-F1-得分定义"><a href="#5-1-F1-得分定义" class="headerlink" title="5.1 F1 得分定义"></a>5.1 F1 得分定义</h2><p>通常情况下，我们想提高精准度就需要牺牲召回率，要想提高召回率就要牺牲精准度。从之前介绍的精准度、召回率和阈值的关系中我们就可以看出一些端倪。当然，一个理想的分类模型是精准度和召回率都可以达到很高，但是实际上却是比较困难。</p><p>为了综合评估精准度和召回率，我们可以使用 F1 得分：</p><script type="math/tex; mode=display">F1 = \frac{1}{\frac{1}{\mathrm{Recall}}+\frac{1}{\mathrm{Precision}}} = \frac{2\times \mathrm{Precision} \times \mathrm{Recall}}{\mathrm{Precision}+\mathrm{Recall}}</script><p>从定义上看，我们可以认为 F1 得分是精准度和召回率的一个平均。</p><h2 id="5-2-用-scikit-learn-计算-F1得分"><a href="#5-2-用-scikit-learn-计算-F1得分" class="headerlink" title="5.2 用 scikit-learn 计算 F1得分"></a>5.2 用 scikit-learn 计算 F1得分</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"></span><br><span class="line">y_pred_class = y_pred_pos &gt; threshold</span><br><span class="line">f1_score(y_true, y_pred_class)</span><br></pre></td></tr></table></figure><p>在实际情况中，精准度、召回率和 F1 得分都不会单独使用，而是综合一起来评估模型的好坏：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line">y_pred_class = y_pred_pos &gt; threshold</span><br><span class="line">classification_report(y_true, y_pred_class)</span><br></pre></td></tr></table></figure><p>我们会得到一个类似如下的结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           <span class="number">1</span>       <span class="number">1.00</span>      <span class="number">0.67</span>      <span class="number">0.80</span>         <span class="number">3</span></span><br><span class="line">           <span class="number">2</span>       <span class="number">0.00</span>      <span class="number">0.00</span>      <span class="number">0.00</span>         <span class="number">0</span></span><br><span class="line"></span><br><span class="line">   micro avg       <span class="number">1.00</span>      <span class="number">0.67</span>      <span class="number">0.80</span>         <span class="number">3</span></span><br><span class="line">   macro avg       <span class="number">0.33</span>      <span class="number">0.22</span>      <span class="number">0.27</span>         <span class="number">3</span></span><br><span class="line">weighted avg       <span class="number">1.00</span>      <span class="number">0.67</span>      <span class="number">0.80</span>         <span class="number">3</span></span><br></pre></td></tr></table></figure><p>其中 <code>support</code> 是参与评估的总样本数，<code>1,2,3</code> 分别是类别标签。<code>mirco avg</code>，<code>marco avg</code> 和 <code>weighted avg</code> 的计算方式分别如下：</p><p><code>micro avg</code>:</p><script type="math/tex; mode=display">\begin{equation}\nonumber\begin{split}\mathrm{micro\ avg\ Precision} &= \frac{TP1+TP2}{TP1+TP2+FP1+FP2} = \frac{\sum TP_i}{\sum(TP_i+FP_i)} \\\\\mathrm{micro\ avg\ Recall} &= \frac{TP1+TP2}{TP1+TP2+FN1+FN2} = \frac{\sum TP_i}{\sum(TP_i+FN_i)} \\\\\mathrm{micro\ avg\ F1} &= \frac{2\times \mathrm{micro\ avg\ Precision} \times \mathrm{micro\ avg\ Recall}}{\mathrm{micro\ avg\ Precision} + \mathrm{micro\ avg\ Recall}}\end{split}\end{equation}</script><p><code>macro avg</code>:</p><script type="math/tex; mode=display">\begin{equation}\nonumber\begin{split}\mathrm{macro\ avg\ Precision} &= \frac{1}{n} \sum \mathrm{Precision}_i \\\\\mathrm{macro\ avg\ Recall} &= \frac{1}{n} \sum \mathrm{Recall}_i \\\\\mathrm{macro\ avg\ F1} &= \frac{1}{n} \sum \mathrm{F1}_i\end{split}\end{equation}</script><p><code>weighted avg</code>:</p><p>假设类别 1 有 4 个，类别 2 有 10 个。</p><script type="math/tex; mode=display">\begin{equation}\nonumber\begin{split}\mathrm{weighted\ avg\ Precision} &= \frac{4 \times \mathrm{Precision}_{1}+10 \times \mathrm{Precision}_{2}}{14} &= \frac{\sum(n_i\times \mathrm{Precision}_{i})}{\sum n_i} \\\\\mathrm{weighted\ avg\ Recall} &= \frac{4 \times \mathrm{Recall}_{1}+10 \times \mathrm{Recall}_{2}}{14} &= \frac{\sum(n_i\times \mathrm{Recall}_{i})}{\sum n_i} \\\\\mathrm{weighted\ avg\ F1} &= \frac{4 \times F1_{1}+10 \times F1_{2}}{14} &= \frac{\sum(n_i\times F1_{i})}{\sum n_i}\end{split}\end{equation}</script><h2 id="5-3-F1-得分与阈值的关系"><a href="#5-3-F1-得分与阈值的关系" class="headerlink" title="5.3 F1 得分与阈值的关系"></a>5.3 F1 得分与阈值的关系</h2><p><img src="https://i2.wp.com/neptune.ai/wp-content/uploads/f1_by_thres.png?fit=1024%2C768&amp;ssl=1" alt></p><p>精准度与阈值的关系是正相关，召回率与阈值的关系是负相关，F1 是精准度和召回率的综合平均值，所以当阈值过大或过小的时候都会对 F1 造成损失，所以要保证较高的 F1 得分，阈值必须在一个合理的范围内。</p><h2 id="5-4-什么时候用？"><a href="#5-4-什么时候用？" class="headerlink" title="5.4 什么时候用？"></a>5.4 什么时候用？</h2><ul><li>F1 得分是常规分类问题的首选评估指标，但是通常也会配合准确率，精准度和召回率</li></ul><h1 id="6-F2-得分（F2-score）"><a href="#6-F2-得分（F2-score）" class="headerlink" title="6. F2 得分（F2-score）"></a>6. F2 得分（F2-score）</h1><h2 id="6-1-F2-得分定义"><a href="#6-1-F2-得分定义" class="headerlink" title="6.1 F2 得分定义"></a>6.1 F2 得分定义</h2><p>F2 得分表示精准度和召回率的综合评价，与 F1 不同的是，F2 着重强调召回率：</p><script type="math/tex; mode=display">F2 = \frac{5 \times \mathrm{Precision}\times \mathrm{Recall}}{4\times  \mathrm{Precision+Recall}}</script><h2 id="6-2-用-scikit-learn-计算-F2-得分"><a href="#6-2-用-scikit-learn-计算-F2-得分" class="headerlink" title="6.2 用 scikit-learn 计算 F2 得分"></a>6.2 用 scikit-learn 计算 F2 得分</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> fbeta_score</span><br><span class="line"></span><br><span class="line">y_pred_class = y_pred_pos &gt; threshold</span><br><span class="line">fbeta_score(y_true, y_pred_class, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h2 id="6-3-F2-得分与阈值的关系"><a href="#6-3-F2-得分与阈值的关系" class="headerlink" title="6.3 F2 得分与阈值的关系"></a>6.3 F2 得分与阈值的关系</h2><p><img src="https://i0.wp.com/neptune.ai/wp-content/uploads/f2_by_thres.png?fit=1024%2C768&amp;ssl=1" alt></p><p>由于 F2 得分更强调召回率的作用，所以 F2 的性质也与召回率的性质相似，随着阈值的提高 F2 得分会有一个库快速的上升，然后短暂达到平衡，然后随着阈值的升高 F2 得分逐渐下降。</p><h2 id="6-4-什么时候用？"><a href="#6-4-什么时候用？" class="headerlink" title="6.4 什么时候用？"></a>6.4 什么时候用？</h2><ul><li>在注重召回率的场景下都可以使用</li></ul><h1 id="7-F-beta-得分（F-beta-score）"><a href="#7-F-beta-得分（F-beta-score）" class="headerlink" title="7. F-beta 得分（F-beta score）"></a>7. F-beta 得分（F-beta score）</h1><h2 id="7-1-F-beta-定义"><a href="#7-1-F-beta-定义" class="headerlink" title="7.1 F-beta 定义"></a>7.1 F-beta 定义</h2><p>既然有 F1 得分，有 F2 得分，那么我顶定义一个 $\beta$ ，当 $\beta=1$ 时，即为 F1 得分，当 $\beta=2$ 时，即为 F2 得分。计算方法如下：</p><script type="math/tex; mode=display">F_{beta} = (1+\beta^2)\frac{\mathrm{Precision}\times \mathrm{Recall}}{\beta^2 \times \mathrm{Precision}+\mathrm{Recall}}</script><p>我肯可以通过调整 $\beta$ 值来确定召回率在我们的评估指标中占有的比重。</p><h2 id="7-2-用-scikit-learn-计算-F-beta-得分"><a href="#7-2-用-scikit-learn-计算-F-beta-得分" class="headerlink" title="7.2 用 scikit-learn 计算 F-beta 得分"></a>7.2 用 scikit-learn 计算 F-beta 得分</h2><p>在上面计算 F2 得分的时候，我们就可以发现，用到了 <code>fbeta_score</code> 函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> fbeta_score</span><br><span class="line"></span><br><span class="line">y_pred_class = y_pred_pos &gt; threshold</span><br><span class="line">fbeta_score(y_true, y_pred_class, beta)</span><br></pre></td></tr></table></figure><h2 id="7-3-F-beta-得分与阈值的关系"><a href="#7-3-F-beta-得分与阈值的关系" class="headerlink" title="7.3 F-beta 得分与阈值的关系"></a>7.3 F-beta 得分与阈值的关系</h2><p><img src="https://i0.wp.com/neptune.ai/wp-content/uploads/f_by_beta.png?fit=933%2C518&amp;ssl=1" alt></p><p>上图展示了不同 $\beta$ 值时， F-beta 与阈值的关系。</p><h1 id="8-假阳性率（Type-I-error）"><a href="#8-假阳性率（Type-I-error）" class="headerlink" title="8. 假阳性率（Type-I error）"></a>8. 假阳性率（Type-I error）</h1><h2 id="8-1-假阳性率定义"><a href="#8-1-假阳性率定义" class="headerlink" title="8.1 假阳性率定义"></a>8.1 假阳性率定义</h2><p>假阳性率表示，我们预测的某事但没有发生。因此，假阳性率又可以叫做误报率。比如，本来没有大雨，但是天气预报却预报说有雨，说明天气预报误报了。我们可以将其视为模型发出的错误报警。</p><script type="math/tex; mode=display">FPR = \frac{\mathrm{FP}}{FP+TN}</script><h2 id="8-2-用-scikit-learn-计算假阳性率"><a href="#8-2-用-scikit-learn-计算假阳性率" class="headerlink" title="8.2 用 scikit-learn 计算假阳性率"></a>8.2 用 scikit-learn 计算假阳性率</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br><span class="line">y_pred_class = y_pred_pos &gt; threshold</span><br><span class="line">tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()</span><br><span class="line">false_positive_rate = fp / (fp + tn)</span><br></pre></td></tr></table></figure><h2 id="8-3-假阳性率与阈值的关系"><a href="#8-3-假阳性率与阈值的关系" class="headerlink" title="8.3 假阳性率与阈值的关系"></a>8.3 假阳性率与阈值的关系</h2><p>通常一个好的模型假阳性率都比较低，但是我们还可以通过调节阈值来进一步降低假阳性率。因为在分母中包含真负样本（$TN$），当我们的数据不平衡时，假阳性率通常会很低。</p><p><img src="https://i2.wp.com/neptune.ai/wp-content/uploads/fpr_by_thres.png?fit=1024%2C768&amp;ssl=1" alt></p><p>显然，随着阈值的增大，假阳性率在降低。</p><h2 id="8-4-什么时候用？"><a href="#8-4-什么时候用？" class="headerlink" title="8.4 什么时候用？"></a>8.4 什么时候用？</h2><ul><li>很少单独使用假阳性率，通常是和其他指标一起使用；</li><li>如果误报会导致较严重的后果，可以通过调节阈值来降低。</li></ul><h1 id="9-假阴性率（Type-II-error）"><a href="#9-假阴性率（Type-II-error）" class="headerlink" title="9. 假阴性率（Type-II error）"></a>9. 假阴性率（Type-II error）</h1><h2 id="9-1-假阴性率定义"><a href="#9-1-假阴性率定义" class="headerlink" title="9.1 假阴性率定义"></a>9.1 假阴性率定义</h2><p>假阴性率表示，当我们没有预测的事情却发生了。因此，假阴性率又可以叫做漏报率。比如，本来有一场大雨，但是天气预报没有预报，说明天气预报对这次大雨漏报了。</p><script type="math/tex; mode=display">FNR = \frac{FN}{TP+FN}</script><h2 id="9-2-用-scikit-learn-计算假阴性率"><a href="#9-2-用-scikit-learn-计算假阴性率" class="headerlink" title="9.2 用 scikit-learn 计算假阴性率"></a>9.2 用 scikit-learn 计算假阴性率</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br><span class="line">y_pred_class = y_pred_pos &gt; threshold</span><br><span class="line">tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()</span><br><span class="line">false_negative_rate = fn / (tp + fn)</span><br></pre></td></tr></table></figure><h2 id="9-3-假阴性率与阈值的关系"><a href="#9-3-假阴性率与阈值的关系" class="headerlink" title="9.3 假阴性率与阈值的关系"></a>9.3 假阴性率与阈值的关系</h2><p><img src="https://i2.wp.com/neptune.ai/wp-content/uploads/fnr_by_thres.png?fit=1024%2C768&amp;ssl=1" alt></p><p>当我们提高阈值的时候，假阴性率也会随之升高。</p><h2 id="9-4-什么时候用？"><a href="#9-4-什么时候用？" class="headerlink" title="9.4 什么时候用？"></a>9.4 什么时候用？</h2><ul><li>通常与其他指标一起使用；</li><li>如果漏报的代价比较大的时候，就需要关注这个指标了。</li></ul><h1 id="10-真阴性率（True-negative-rate）"><a href="#10-真阴性率（True-negative-rate）" class="headerlink" title="10. 真阴性率（True negative rate）"></a>10. 真阴性率（True negative rate）</h1><h2 id="10-1-真阴性率定义"><a href="#10-1-真阴性率定义" class="headerlink" title="10.1 真阴性率定义"></a>10.1 真阴性率定义</h2><p>真阴性率表示，在所有的负样本中有多少负样本被检测出来。</p><script type="math/tex; mode=display">TNR = \frac{TN}{TN+FP}</script><h2 id="10-2-用-scikit-learn-计算真阴性率"><a href="#10-2-用-scikit-learn-计算真阴性率" class="headerlink" title="10.2 用 scikit-learn 计算真阴性率"></a>10.2 用 scikit-learn 计算真阴性率</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br><span class="line">y_pred_class = y_pred_pos &gt; threshold</span><br><span class="line">tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()</span><br><span class="line">true_negative_rate = tn / (tn + fp)</span><br></pre></td></tr></table></figure><h2 id="10-3-真阴性率与阈值的关系"><a href="#10-3-真阴性率与阈值的关系" class="headerlink" title="10.3 真阴性率与阈值的关系"></a>10.3 真阴性率与阈值的关系</h2><p><img src="https://i0.wp.com/neptune.ai/wp-content/uploads/tnr_by_thres.png?fit=1024%2C768&amp;ssl=1" alt></p><p>阈值越高，真阴性率越高。</p><h2 id="10-4-什么时候用？"><a href="#10-4-什么时候用？" class="headerlink" title="10.4 什么时候用？"></a>10.4 什么时候用？</h2><ul><li>通常与其他指标一起用；</li><li>当你确实希望确保你所说的每一句都是正确的时候，可以考虑该指标。比如，当一个医生对病人说 “你很健康”  的时候。</li></ul><h1 id="11-负样本预测值（Negative-Predictive-Value）"><a href="#11-负样本预测值（Negative-Predictive-Value）" class="headerlink" title="11. 负样本预测值（Negative Predictive Value）"></a>11. 负样本预测值（Negative Predictive Value）</h1><h2 id="11-1-负样本预测值定义"><a href="#11-1-负样本预测值定义" class="headerlink" title="11.1 负样本预测值定义"></a>11.1 负样本预测值定义</h2><p>负样本预测值表示，模型预测的负样本有多少是真正的负样本，我们可以认为它是负类别的准确率。</p><script type="math/tex; mode=display">NPV = \frac{TN}{TN+FN}</script><h2 id="11-2-用-scikit-learn-计算负样本预测值"><a href="#11-2-用-scikit-learn-计算负样本预测值" class="headerlink" title="11.2 用 scikit-learn 计算负样本预测值"></a>11.2 用 scikit-learn 计算负样本预测值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br><span class="line">y_pred_class = y_pred_pos &gt; threshold</span><br><span class="line">tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()</span><br><span class="line">negative_predictive_value = tn/ (tn + fn)</span><br></pre></td></tr></table></figure><h2 id="11-3-负样本预测值与阈值的关系"><a href="#11-3-负样本预测值与阈值的关系" class="headerlink" title="11.3 负样本预测值与阈值的关系"></a>11.3 负样本预测值与阈值的关系</h2><p><img src="https://i2.wp.com/neptune.ai/wp-content/uploads/npv_by_thres.png?fit=1024%2C768&amp;ssl=1" alt></p><p>阈值越高就会有越多的样本被预测为负样本，被误分类成负样本的几率就越高。但是对于非平衡数据集来说，一个较高的阈值通常负样本预测值表现也还不错。</p><h2 id="11-4-什么时候用？"><a href="#11-4-什么时候用？" class="headerlink" title="11.4 什么时候用？"></a>11.4 什么时候用？</h2><ul><li>当我们更加关注负样本的预测准确率时，可以考虑使用这一评估指标。</li></ul><h1 id="12-假发现率（False-Discovery-Rate）"><a href="#12-假发现率（False-Discovery-Rate）" class="headerlink" title="12. 假发现率（False Discovery Rate）"></a>12. 假发现率（False Discovery Rate）</h1><h2 id="12-1-假发现率定义"><a href="#12-1-假发现率定义" class="headerlink" title="12.1 假发现率定义"></a>12.1 假发现率定义</h2><p>假发现率表示，所有预测为正样本的数据中有多少是真正的正样本。</p><script type="math/tex; mode=display">FDR = \frac{TP}{TP+FP}</script><h2 id="12-2-用-scikit-learn-计算假发现率"><a href="#12-2-用-scikit-learn-计算假发现率" class="headerlink" title="12.2 用 scikit-learn 计算假发现率"></a>12.2 用 scikit-learn 计算假发现率</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br><span class="line">y_pred_class = y_pred_pos &gt; threshold</span><br><span class="line">tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()</span><br><span class="line">false_discovery_rate = fp/ (tp + fp)</span><br></pre></td></tr></table></figure><h2 id="12-3-假发现率与阈值的关系"><a href="#12-3-假发现率与阈值的关系" class="headerlink" title="12.3 假发现率与阈值的关系"></a>12.3 假发现率与阈值的关系</h2><p><img src="https://i2.wp.com/neptune.ai/wp-content/uploads/fdr_by_thres.png?fit=1024%2C768&amp;ssl=1" alt></p><p>阈值越高，假发现率越低。</p><h2 id="12-4-什么时候用？"><a href="#12-4-什么时候用？" class="headerlink" title="12.4 什么时候用？"></a>12.4 什么时候用？</h2><ul><li>通常和其他指标一起使用；</li><li>如果误报的代价过高，或者当你希望所有预测为正样本的数据都值得一看的时候，可以考虑该指标。</li></ul><h1 id="13-Cohen-Kappa-Metric"><a href="#13-Cohen-Kappa-Metric" class="headerlink" title="13. Cohen Kappa Metric"></a>13. Cohen Kappa Metric</h1><h2 id="13-1-Cohen-Kappa-定义"><a href="#13-1-Cohen-Kappa-定义" class="headerlink" title="13.1 Cohen Kappa 定义"></a>13.1 Cohen Kappa 定义</h2><p>简单来说，<em>Cohen Kappa</em> 指的是你的模型比一个随机分类器好多少。</p><script type="math/tex; mode=display">\kappa = \frac{p_0-p_e}{1-p_e}</script><ul><li>$p_0$ 表示模型预测结果，通常为准确率；</li><li>$p_e$ 表示随机预测结果，通常为随机模型的准确率。</li></ul><h2 id="13-2-用-scikit-learn-计算-Cohen-Kappa"><a href="#13-2-用-scikit-learn-计算-Cohen-Kappa" class="headerlink" title="13.2 用 scikit-learn 计算 Cohen Kappa"></a>13.2 用 scikit-learn 计算 Cohen Kappa</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> cohen_kappa_score</span><br><span class="line"></span><br><span class="line">cohen_kappa_score(y_true, y_pred_class)</span><br></pre></td></tr></table></figure><h2 id="13-3-Cohen-Kappa-与阈值的关系"><a href="#13-3-Cohen-Kappa-与阈值的关系" class="headerlink" title="13.3 Cohen Kappa 与阈值的关系"></a>13.3 Cohen Kappa 与阈值的关系</h2><p><img src="https://i0.wp.com/neptune.ai/wp-content/uploads/kappa_by_thres.png?fit=1024%2C768&amp;ssl=1" alt></p><h2 id="13-4-什么时候用？"><a href="#13-4-什么时候用？" class="headerlink" title="13.4 什么时候用？"></a>13.4 什么时候用？</h2><ul><li>Cohen Kappa 通常不会用在一般的文本分类上，而是在非平衡数据的分类模型上。</li></ul><h1 id="14-Matthews-Correlation-Coefficient-（MCC）"><a href="#14-Matthews-Correlation-Coefficient-（MCC）" class="headerlink" title="14. Matthews Correlation Coefficient （MCC）"></a>14. Matthews Correlation Coefficient （MCC）</h1><p>$MCC$ 表示真实标签和预测标签的相关性。</p><h2 id="14-1-MCC-定义"><a href="#14-1-MCC-定义" class="headerlink" title="14.1 MCC 定义"></a>14.1 MCC 定义</h2><script type="math/tex; mode=display">MCC = \frac{TP\times TN-FP\times FN}{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}</script><h2 id="14-2-scikit-learn-计算-MCC"><a href="#14-2-scikit-learn-计算-MCC" class="headerlink" title="14.2 scikit-learn 计算 MCC"></a>14.2 scikit-learn 计算 MCC</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> matthews_corrcoef</span><br><span class="line"></span><br><span class="line">y_pred_class = y_pred_pos &gt; threshold</span><br><span class="line">matthews_corrcoef(y_true, y_pred_class)</span><br></pre></td></tr></table></figure><h2 id="14-3-MCC-与阈值的关系"><a href="#14-3-MCC-与阈值的关系" class="headerlink" title="14.3 MCC 与阈值的关系"></a>14.3 MCC 与阈值的关系</h2><p><img src="https://i0.wp.com/neptune.ai/wp-content/uploads/mcc_by_thres.png?fit=1024%2C768&amp;ssl=1" alt></p><h2 id="14-4-什么时候用？"><a href="#14-4-什么时候用？" class="headerlink" title="14.4 什么时候用？"></a>14.4 什么时候用？</h2><ul><li>不平衡数据集</li><li>希望预测结果有更强的可解释性的</li></ul><h1 id="15-ROC-曲线"><a href="#15-ROC-曲线" class="headerlink" title="15. ROC 曲线"></a>15. ROC 曲线</h1><p>ROC 曲线是一个图表，用于展示真阳性率（$TPR$）和假阳性率（$FPR$）之间的权衡。基本上，对于每个阈值，我们计算 $TPR$ 和 $FPR$ 并将其绘制在一张图表上。它代表的是分类器以多大的置信度将样本分类为正样本。</p><p>可以在 <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.10.9777&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Tom Fawcett</a> 的这篇文章中找到对 ROC 曲线和 ROC AUC 分数的广泛详细的讨论。</p><h2 id="15-1-用-scikit-learn-计算-ROC"><a href="#15-1-用-scikit-learn-计算-ROC" class="headerlink" title="15.1 用 scikit-learn 计算 ROC"></a>15.1 用 scikit-learn 计算 ROC</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scikitplot.metrics <span class="keyword">import</span> plot_roc</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">plot_roc(y_true, y_pred, ax=ax)</span><br></pre></td></tr></table></figure><h2 id="15-2-曲线图是什么样的？"><a href="#15-2-曲线图是什么样的？" class="headerlink" title="15.2 曲线图是什么样的？"></a>15.2 曲线图是什么样的？</h2><p><img src="https://i1.wp.com/neptune.ai/wp-content/uploads/roc_auc_curve.png?fit=1024%2C768&amp;ssl=1" alt></p><p>每个不同的阈值对应曲线上不同的点（即不同的混淆矩阵）。对于每个阈值，较高的 $TPR$ 和较低的 $FPR$ 越好，因此具有更多左上角曲线的分类器更好。从上图可以看出，在大约（0.15， 0.85）左右的位置（左上角黑色实线和黑色虚线焦点）二者取得平衡。因此该位置对应的阈值应该是最佳的分类阈值。</p><h1 id="16-ROC-AUC-得分"><a href="#16-ROC-AUC-得分" class="headerlink" title="16. ROC-AUC 得分"></a>16. ROC-AUC 得分</h1><p>为了从 ROC 曲线上得到一个量化的指标，我们可以计算 ROC-AUC（<em>Area Under the ROC Curve</em>） 得分。</p><h2 id="16-1-用-scikit-learn-计算-ROC-AUC"><a href="#16-1-用-scikit-learn-计算-ROC-AUC" class="headerlink" title="16.1 用 scikit-learn 计算 ROC-AUC"></a>16.1 用 scikit-learn 计算 ROC-AUC</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"></span><br><span class="line">roc_auc = roc_auc_score(y_true, y_pred_pos)</span><br></pre></td></tr></table></figure><h2 id="16-2-什么时候用？"><a href="#16-2-什么时候用？" class="headerlink" title="16.2 什么时候用？"></a>16.2 什么时候用？</h2><ul><li>当你非常关心排序预测的时候，应该使用 ROC-AUC 得分而没有必要关注<a href="https://machinelearningmastery.com/calibrated-classification-model-in-scikit-learn/" target="_blank" rel="noopener">概率修正</a>。</li><li>当你的数据严重不平衡的时候，不应该使用 ROC-AUC 作为评估指标。直观上来讲，当数据严重类别不平衡的时候， $FPR$ 会被严重拉低，因为大量的数据是 <em>True Negative</em> 的。</li><li>当正负样本的类别平衡的时候，可以使用 ROC-AUC 作为评估指标。</li></ul><h1 id="17-Precision-Recall-Curve"><a href="#17-Precision-Recall-Curve" class="headerlink" title="17. Precision-Recall Curve"></a>17. Precision-Recall Curve</h1><p>PRC 是一条融合了精准度和召回率的可视化曲线。对于每个阈值，计算相应的精准度和召回率，然后画在图上即可。Y 轴对应的值越高，则模型表现越好。</p><h2 id="17-1-用-scikit-learn-计算-PRC"><a href="#17-1-用-scikit-learn-计算-PRC" class="headerlink" title="17.1 用 scikit-learn 计算 PRC"></a>17.1 用 scikit-learn 计算 PRC</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scikitplot.metrics <span class="keyword">import</span> plot_precision_recall</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">plot_precision_recall(y_true, y_pred, ax=ax)</span><br></pre></td></tr></table></figure><h2 id="17-2-曲线长什么样？"><a href="#17-2-曲线长什么样？" class="headerlink" title="17.2 曲线长什么样？"></a>17.2 曲线长什么样？</h2><p><img src="https://i1.wp.com/neptune.ai/wp-content/uploads/prec_rec_curve.png?fit=1024%2C768&amp;ssl=1" alt></p><h1 id="18-PR-AUC-得分-平均精准度"><a href="#18-PR-AUC-得分-平均精准度" class="headerlink" title="18. PR AUC 得分 | 平均精准度"></a>18. PR AUC 得分 | 平均精准度</h1><p>与 ROC-AUC 类似，我们也可以计算 <strong>A</strong>rea <strong>U</strong>nder the Precision-Recall <strong>C</strong>urve 以获得评估模型的量化指标。</p><h2 id="18-1-用-sickit-learn计算-PR-AUC"><a href="#18-1-用-sickit-learn计算-PR-AUC" class="headerlink" title="18.1 用 sickit-learn计算 PR AUC"></a>18.1 用 sickit-learn计算 PR AUC</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> average_precision_score</span><br><span class="line"></span><br><span class="line">average_precision_score(y_true, y_pred_pos)</span><br></pre></td></tr></table></figure><h2 id="18-2-什么时候用？"><a href="#18-2-什么时候用？" class="headerlink" title="18.2 什么时候用？"></a>18.2 什么时候用？</h2><ul><li>当你要在精准度和召回率之间做取舍的时候</li><li>当你要选择一个合适的阈值符合实际情况的时候</li><li>当你的数据严重不平衡的时候。就像之前讨论的那样，由于 PR AUC 主要关注点是正样本的类别，很少关注到负样本。所以在类别严重不平衡的时候可以使用 PR AUC 作为模型的评估指标。</li><li>当你更关注正样本而非负样本的时候，可以使用 PR AUC 作为模型的评估指标。</li></ul><h1 id="19-Log-loss"><a href="#19-Log-loss" class="headerlink" title="19. Log loss"></a>19. Log loss</h1><p>对数损失函数经常用来优化机器学习模型的参数。然后实际上它也可以作为模型的评估指标。</p><h2 id="19-1-定义对数损失"><a href="#19-1-定义对数损失" class="headerlink" title="19.1 定义对数损失"></a>19.1 定义对数损失</h2><p>对数损失用来计算真实标签与预测标签之间的差别：</p><script type="math/tex; mode=display">\mathrm{Logloss} = -(y_{\mathrm{true}}\times\log(y_{\mathrm{pred}})) + (1-y_{\mathrm{true}})\times\log(1-y_{\mathrm{pred}})</script><p>观测到的正样本置信度越高，那么它与真实的正样本之间的差距就越小。但是这并不是一个线性关系，真实的关系如下图：</p><p><img src="https://i1.wp.com/neptune.ai/wp-content/uploads/log_los_chart.png?fit=724%2C496&amp;ssl=1" alt></p><h2 id="19-2-用-scikit-learn-计算对数损失"><a href="#19-2-用-scikit-learn-计算对数损失" class="headerlink" title="19.2 用 scikit-learn 计算对数损失"></a>19.2 用 scikit-learn 计算对数损失</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> log_loss</span><br><span class="line"></span><br><span class="line">log_loss(y_true, y_pred)</span><br></pre></td></tr></table></figure><h2 id="19-3-什么时候用？"><a href="#19-3-什么时候用？" class="headerlink" title="19.3 什么时候用？"></a>19.3 什么时候用？</h2><ul><li>几乎总是有一个性能指标可以更好地匹配我们的业务问题。 因此，我们可以使用对数损失作为模型的目标，并使用其他一些指标来评估性能。</li></ul><h1 id="20-Brier-得分"><a href="#20-Brier-得分" class="headerlink" title="20. Brier 得分"></a>20. Brier 得分</h1><h2 id="20-1-Brier-得分定义"><a href="#20-1-Brier-得分定义" class="headerlink" title="20.1 Brier 得分定义"></a>20.1 Brier 得分定义</h2><script type="math/tex; mode=display">\mathrm{Brierloss} = (y_{\mathrm{pred}}-y_{\mathrm{true}})^2</script><h2 id="20-2-用-scikit-learn-计算-Brier-得分"><a href="#20-2-用-scikit-learn-计算-Brier-得分" class="headerlink" title="20.2 用 scikit-learn 计算 Brier 得分"></a>20.2 用 scikit-learn 计算 Brier 得分</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> brier_score_loss</span><br><span class="line"></span><br><span class="line">brier_score_loss(y_true, y_pred_pos)</span><br></pre></td></tr></table></figure><h2 id="20-3-什么时候用？"><a href="#20-3-什么时候用？" class="headerlink" title="20.3 什么时候用？"></a>20.3 什么时候用？</h2><ul><li>当你关心修正概率的时候</li></ul><h1 id="21-累积收益表"><a href="#21-累积收益表" class="headerlink" title="21. 累积收益表"></a>21. 累积收益表</h1><h2 id="21-1-定义累积收益表"><a href="#21-1-定义累积收益表" class="headerlink" title="21.1 定义累积收益表"></a>21.1 定义累积收益表</h2><p>简单来说，累积收益表（Cumulative gains chart）可以帮助我们判断使用当前模型的收益超过一个随机模型多少。</p><ul><li>先对预测结果从高到低进行排序</li><li>对于每个百分数，我们计算大于这个百分数的真阳性样本比例。</li></ul><h2 id="21-2-用-scikit-learn-计算-CGC"><a href="#21-2-用-scikit-learn-计算-CGC" class="headerlink" title="21.2 用 scikit-learn 计算 CGC"></a>21.2 用 scikit-learn 计算 CGC</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scikitplot.metrics <span class="keyword">import</span> plot_cumulative_gain</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">plot_cumulative_gain(y_true, y_pred, ax=ax)</span><br></pre></td></tr></table></figure><h2 id="21-3-CGC-看起来是什么样的？"><a href="#21-3-CGC-看起来是什么样的？" class="headerlink" title="21.3 CGC 看起来是什么样的？"></a>21.3 CGC 看起来是什么样的？</h2><p><img src="https://i0.wp.com/neptune.ai/wp-content/uploads/cum_gain_chart.png?fit=1024%2C768&amp;ssl=1" alt></p><h2 id="21-4-什么时候用？"><a href="#21-4-什么时候用？" class="headerlink" title="21.4 什么时候用？"></a>21.4 什么时候用？</h2><ul><li>当你想选择最有希望与你进行交易的客户的时候，可以使用 CGC 作为评估指标。</li><li>它可以作为 ROC-AUC 指标的一个很好的额外补充。</li></ul><h1 id="22-Lift-curve-lift-chart"><a href="#22-Lift-curve-lift-chart" class="headerlink" title="22. Lift curve | lift chart"></a>22. Lift curve | lift chart</h1><h2 id="22-1-定义-lift-curve"><a href="#22-1-定义-lift-curve" class="headerlink" title="22.1 定义 lift curve"></a>22.1 定义 lift curve</h2><p>Lift curve 基本上只是 CGC 的另一种表示形式：</p><ul><li>首先对预测结果由高到低进行排序；</li><li>对于每个预测值，计算训练好的模型和随机模型达到该百分比概率的真阳性比例</li><li>计算上述比例，然后画图</li></ul><p>它能告诉我们对于给定最大预测值，它比一个随机模型好多少。</p><h2 id="22-2-用-scikit-learn-计算-lift-curve"><a href="#22-2-用-scikit-learn-计算-lift-curve" class="headerlink" title="22.2 用 scikit-learn 计算 lift curve"></a>22.2 用 scikit-learn 计算 lift curve</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scikitplot.metrics <span class="keyword">import</span> plot_lift_curve </span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots() plot_lift_curve(y_true, y_pred, ax=ax)</span><br></pre></td></tr></table></figure><p><img src="https://i2.wp.com/neptune.ai/wp-content/uploads/lift_curve_chart.png?fit=1024%2C768&amp;ssl=1" alt></p><h2 id="22-3-什么时候用？"><a href="#22-3-什么时候用？" class="headerlink" title="22.3 什么时候用？"></a>22.3 什么时候用？</h2><ul><li>当你想选择最有希望与你进行交易的客户的时候，可以使用 CGC 作为评估指标。</li><li>它可以作为 ROC-AUC 指标的一个很好的额外补充。</li></ul><h1 id="23-Kolmogorov-Smirnov-plot"><a href="#23-Kolmogorov-Smirnov-plot" class="headerlink" title="23. Kolmogorov-Smirnov plot"></a>23. Kolmogorov-Smirnov plot</h1><h2 id="23-1-定义-KS-plot"><a href="#23-1-定义-KS-plot" class="headerlink" title="23.1 定义 KS plot"></a>23.1 定义 KS plot</h2><p>KS plot 帮助我们从预测结果中获得独立的正样本分布和负样本分布。</p><ul><li>根据预测得分进行排序</li><li>对 [0.0, 1.0] 之间的每个截点计算相邻截点（depth）之间的数据中的真阳性和真阴性比例</li><li>画出计算出来的比例，y 轴表示 $positive(depth)/positive(all)$，$negative(depth)/negative(all)$，x 轴表示 depth</li></ul><p>KS plot有点类似于 CGC，但是CGC 只关注正样本，而 KS plot同时关注正负样本。</p><h2 id="23-2-用-scikit-learn-计算-KS-plot"><a href="#23-2-用-scikit-learn-计算-KS-plot" class="headerlink" title="23.2 用 scikit-learn 计算 KS plot"></a>23.2 用 scikit-learn 计算 KS plot</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scikitplot.metrics <span class="keyword">import</span> plot_ks_statistic</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">plot_ks_statistic(y_true, y_pred, ax=ax)</span><br></pre></td></tr></table></figure><p><img src="https://i0.wp.com/neptune.ai/wp-content/uploads/ks_plot.png?fit=1024%2C768&amp;ssl=1" alt></p><h1 id="24-Kolmogorov-Smirnov-statistic"><a href="#24-Kolmogorov-Smirnov-statistic" class="headerlink" title="24. Kolmogorov-Smirnov statistic"></a>24. Kolmogorov-Smirnov statistic</h1><h2 id="24-1-定义-KS-statistic"><a href="#24-1-定义-KS-statistic" class="headerlink" title="24.1 定义 KS statistic"></a>24.1 定义 KS statistic</h2><p>如果我们想从 KS plot 中选择一个值作为指标，那么我们可以查看所有 KS plot 中所有阈值，然后找到正负样本分布距离最远的点。</p><p>如果有一个阈值，所有观测到的上方样本都是真阳性，而所有下方的样本都是真阴性，那么我们就找到了一个完美的 KS statistic 值：1.0</p><h2 id="24-2-用-scikit-learn-计算-KS-statistic"><a href="#24-2-用-scikit-learn-计算-KS-statistic" class="headerlink" title="24.2 用 scikit-learn 计算 KS statistic"></a>24.2 用 scikit-learn 计算 KS statistic</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scikitplot.helpers <span class="keyword">import</span> binary_ks_curve</span><br><span class="line"></span><br><span class="line">res = binary_ks_curve(y_true, y_pred_pos)</span><br><span class="line">ks_stat = res[<span class="number">3</span>]</span><br></pre></td></tr></table></figure><h2 id="24-3-什么时候用？"><a href="#24-3-什么时候用？" class="headerlink" title="24.3 什么时候用？"></a>24.3 什么时候用？</h2><ul><li>当你面对的是排序问题且你对正负样本都很关心的时候</li><li>可以作为 ROC-AUC  的补充指标</li></ul><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="https://neptune.ai/blog/evaluation-metrics-binary-classification" target="_blank" rel="noopener">24 Evaluation Metrics for Binary Classification (And When to Use Them)</a></li><li><a href="https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/" target="_blank" rel="noopener">Everything you Should Know about Confusion Matrix for Machine Learning</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 博客转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 评估方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构与算法：时间复杂度</title>
      <link href="/2021/04/22/ds-time-complexity/"/>
      <url>/2021/04/22/ds-time-complexity/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png" alt></p><p>就像做菜有好吃和不好吃一样，算法也有好的算法和不好的算法。那么我们怎么评价一个算法的好坏呢？</p><h1 id="1-时间复杂度"><a href="#1-时间复杂度" class="headerlink" title="1. 时间复杂度"></a>1. 时间复杂度</h1><p>时间复杂度是用来估算算法需要执行的时间的。但是我们并不是直接用时间来估计，而是用一个<strong>函数</strong>。</p><blockquote><p>时间复杂度不是算法需要执行多久，而是算法需要执行多少步。</p></blockquote><a id="more"></a><p>这种评估方法看起来很奇怪，为什么不直接用时间来评估呢？很显然，一个需要 10 秒的的算法肯定是不如只需要 1 秒的算法啊。事实并非如此，比如同一个排序算法，对 100 个元素进行排序和对 1000 个元素进行排序所用时间肯定是不同的，能说算法变坏了吗？算法运算的时间依赖于输入参数的大小。</p><p>如果一个排序算法的输入本来就已经排好了序，那么它的运算时间肯定要比乱序输入更快。即使输入参数大小相同，算法的运算时间也会有差异。</p><p>另外，CPU 运算波动，磁盘读写效率等等都会影响算法的运算时间。</p><p>所以，即使是同一个算法，它的运算时间也会有：</p><ul><li>最长运算时间</li><li>最短运算时间</li><li>平均运算时间</li></ul><p>而我们通常关心的是一个算法的最长运算时间（抱最好的希望，做最坏的打算）。所以我们对算法性能的分析要包含两方面的考虑：</p><ol><li>忽略掉依赖于机器的因素；</li><li>关注运行时间的增长，而不是去检查真正的运行时间.</li></ol><h2 id="1-1-计算时间复杂度"><a href="#1-1-计算时间复杂度" class="headerlink" title="1.1 计算时间复杂度"></a>1.1 计算时间复杂度</h2><p>举个例子：找到一个数组中的最小数字。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">第一步：开始</span><br><span class="line">第二步：声明变量 min</span><br><span class="line">第三步：从输入数组中循环取值</span><br><span class="line">      3.1 对比从数组中取到的值 n 和 min 的大小</span><br><span class="line">      3.2 如果 n &lt; min</span><br><span class="line">      3.3 令 min = n</span><br><span class="line">第四步：返回 min 的值</span><br></pre></td></tr></table></figure><p>假设 cpu 的运算是平稳没有波动的，每一步所需的时间相同。我们来看下上面的过程：</p><ul><li>第一步：1 个操作；</li><li>第二步：1 个操作</li><li>第三步：这一步需要执行 $m$ （数组中有 $m$ 个元素）次，假设每一小步是 1 个操作，每一次循环就需要 4 个操作，那么这个循环就相当于需要 $4m$ 个操作</li><li>第四步： 1 个操作</li></ul><p>总共需要 $4m+3$ 个操作。但是这个表达式太过于具体了，不能对比不同的算法。我们需要进一步简化时间复杂度的计算。</p><h2 id="1-2-渐进分析"><a href="#1-2-渐进分析" class="headerlink" title="1.2 渐进分析"></a>1.2 渐进分析</h2><h3 id="1-2-1-符号表示"><a href="#1-2-1-符号表示" class="headerlink" title="1.2.1 符号表示"></a>1.2.1 符号表示</h3><p>为了更方便表示时间复杂度，我们使用渐进符号来表示。主要有三种符号：</p><ul><li><p>$O$ 符号：表示算法运算的上限，即表现最差的运算时间。</p><p><img width="300" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/big0.png"></p><script type="math/tex; mode=display">O(g(n)) = \{f(n) ~|~  \exists c>0, ~ n \ge n_0,~ 0\le f(n)\le cg(n) \}</script><p>$O(g(n))$ 是函数 $f(n)$ 的集合。$f(n)$ 满足以下条件：存在一个正实数 $c$，使得当 $n$ 足够大时（大于一个阈值 $n_0$） ，所有的 $f(n)$ 都小于 $cg(n)$。</p></li><li><p>$\Omega$ 符号：表示算法运算下限，即表现最好的运算时间。</p><p><img width="300" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/omega.png"></p><script type="math/tex; mode=display">\Omega(g(n)) = \{f(n)~ \vert ~ \exists c>0,~ n\ge n_0,~ 0 \le f(n) \le cg(n) \}</script><p>$\Omega(g(n))$ 是函数  $f(n)$ 的集合。$f(n)$ 满足以下条件：存在一个正实数 $c$，使得当 $n$ 足够大时（大于一个阈值 $n_0$），所有的 $f(n)$ 都大于 $cg(n)$。</p></li><li><p>$\Theta$ 符号：表示算法运算时间的平均水平，即平均运算时间。</p><p><img width="300" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/theta.png"></p><script type="math/tex; mode=display">\Theta(g(n)) = \{f(n)~ \vert ~ \exists c_1,c_2>0,~ n \gt n_0,~ 0 \le c_1g(n) \le f(n) \le c_2g(n)\}</script><p>$\Theta(g(n))$ 是函数 $f(n)$ 的集合。$f(n)$ 满足以下条件：存在正实数 $c_1,c_2$，使得当 $n$ 足够大时（大于一个阈值 $n_0$），所有的 $f(n)$ 都介于 $c_1g(n)$ 和 $c_2g(n)$ 之间。</p></li></ul><h3 id="1-2-2-渐进分析"><a href="#1-2-2-渐进分析" class="headerlink" title="1.2.2 渐进分析"></a>1.2.2 渐进分析</h3><p>渐进分析遵循三个原则：</p><ul><li>分析最坏的情况，即 $O(g(n))$。</li><li>不关心系数和低阶项。从上面我们对 $O(g(n))$ 的定义可以看出，系数相当于 $c$，而我们关心的是 $g(n)$。如前所说，对时间复杂度的分析更关注的是运行时间的增长。当 $n$ 足够大的时候，低阶项对运行时间的增长影响力越来越低，所以也不是我们关心的点。</li><li>分析当 $n$  足够大的时候的运行时间。</li></ul><p>渐进分析是将时间复杂度函数做无穷近似。比如上面的例子中 $4m+3$ 可以泛化成 $km+c$。忽略掉系数 $k$ 和低阶项 $c$，所以上面的算法时间复杂度为 $O(m)$。</p><p>$O$ 表示方程的阶（<em>order</em>）。常见的时间复杂度及其对比：</p><div class="table-container"><table><thead><tr><th>$n$</th><th>$O(1)$</th><th>$O(\log n)$</th><th>$O(n)$</th><th>$O(n \log n)$</th><th>$O(n^2)$</th><th>$O(2^n)$</th></tr></thead><tbody><tr><td>1</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td></tr><tr><td>10</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td></tr><tr><td>100</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>40170 trillion years</td></tr><tr><td>1,000</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>&gt; vigintillion years</td></tr><tr><td>10,000</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>2 min</td><td>&gt; centillion years</td></tr><tr><td>100,000</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>1 sec</td><td>3 hours</td><td>&gt; centillion years</td></tr><tr><td>1,000,000</td><td>&lt; 1 sec</td><td>&lt; 1 sec</td><td>1 sec</td><td>20 sec</td><td>12 days</td><td>&gt; centillion years</td></tr></tbody></table></div><h2 id="1-3-非递归算法"><a href="#1-3-非递归算法" class="headerlink" title="1.3 非递归算法"></a>1.3 非递归算法</h2><p>非递归算法时间复杂度计算步骤如下：</p><ol><li>找到问题的输入数据规模，比如 2.1 节中的 $m$；</li><li>找出算法的基本操作，比如 2.1 节中的循环操作；</li><li>建立算法中涉及到的操作数求和表达式；</li><li>利用 $O$ 准则进行简化。</li></ol><p>具体例子可以看 2.1 节的例子。</p><h2 id="1-4-递归算法"><a href="#1-4-递归算法" class="headerlink" title="1.4 递归算法"></a>1.4 递归算法</h2><p>递归算法的时间复杂度有两种方法进行计算：</p><ul><li>迭代法：每次运算只是将数据规模减一，比如斐波那契数列；</li><li>主定理：利用分治的思想，将问题拆解成几份，分别求解。</li></ul><h3 id="1-4-1-迭代法"><a href="#1-4-1-迭代法" class="headerlink" title="1.4.1 迭代法"></a>1.4.1 迭代法</h3><p>以斐波那契数列为例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">第一步：定义函数 fab(n)</span><br><span class="line">第二步：判断 n 是否为 0</span><br><span class="line">第三步：如果 n == 0，返回 1</span><br><span class="line">第四步：如果 n != 0, 返回 fab(n-1) * n</span><br></pre></td></tr></table></figure><p>假设 <code>fab(n)</code> 需要执行 $C(n)$ 次，$C(n)$ 的计算公式为：</p><script type="math/tex; mode=display">C(n) = C(n-1)+1</script><p>$C(n-1)$ 为 <code>fab(n-1)</code> 的运算次数，当 $n=0$ 时，<code>fab(0)</code> 直接返回值，所以只需要运算 1 次，即 $C(0)=1$。</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}C(n) &= C(n-1)+1\\\\     &= [C(n-2)+1]+1\\\\     &= [C(n-3)+1]+2\\\\     & \dots \\\\     &= C(n-i)+i \\\\     & \dots \\\\     &= C(n-n)+n \\\\     &= n\end{aligned}\end{equation}</script><p>所以这个算法的时间复杂度为 $O(n)$。</p><h3 id="1-4-2-主定理"><a href="#1-4-2-主定理" class="headerlink" title="1.4.2 主定理"></a>1.4.2 主定理</h3><p>当我们使用分治思想求解递归问题的时候，就可以用“主定理”（<em>master theorem</em>）的方法来计算时间复杂度。具体来说，当递归函数的时间复杂度计算公式满足：</p><blockquote><script type="math/tex; mode=display">T(n)=aT(\frac{n}{b})+f(n)</script><p>其中：</p><ul><li>$n$：输入数据规模</li><li>$a$：将递归问题分解成子问题的个数</li><li>$n/b$：每个子问题的规模，</li><li>$f(n)$：递归操作之外所需要的操作次数，包括问题分解和结果合并</li></ul><p>$a$ 和 $b$ 是大于 $0$ 的常数，$f(n)$ 是渐进函数，即当 $n \to \infty$ 时 $f(n)&gt;0$。</p></blockquote><p>上面的公式可以理解为：对于一个规模为 $n$ 的问题，我们把它分解成 $a$ 个子问题，每个子问题规模为 $n/b$ 指的是 $\lfloor{n/b}\rfloor$ 或者 $\lceil{n/b}\rceil$（$\lfloor \cdot \rfloor$ 表示是向下取整，$\lceil \cdot\rceil$ 表示向上取整），然后将问题的解通过 $f(n)$次操作整合到一起。</p><p><img width="600" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210822131342.png"></p><p>首先构建一棵递归任务分解树，观察每一层的变化：</p><blockquote><p>第一层：</p><ul><li>子问题的数目：$a^0$</li><li><p>每个子问题的规模：$\frac{n}{b^0}$</p></li><li><p>合并子问题需要花费的操作次数：$f(n)$</p></li></ul><p>第二层：</p><ul><li>子问题的数目：$a^1$</li><li>每个子问题的规模：$\frac{n}{b^1}$</li><li>合并子问题需要花费的操作次数：$af(n/b)$</li></ul><p>第三层：</p><ul><li>子问题的数目：$a^2$</li><li>每个子问题的规模：$\frac{n}{b^2}$</li><li>合并子问题需要花费的操作次数：$a^2f(n/b^2)$</li></ul><p>…</p><p>最后一层：</p><ul><li>子问题的数目：$a^2$</li><li>每个子问题的规模：$\frac{n}{b^h}$</li><li>合并子问题需要花费的操作次数：$a^hf(n/b^h)$</li></ul></blockquote><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • NOTE            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            第一层，“合并子问题需要花费的操作次数”，实际上指的是要解决一个和原问题等规模的问题所需要的操作次数。因为第一层还没有对问题进行分解，也就谈不上合并子问题，上面的说法只是为了和下面保持一致。        </div>        </div>    </div><ul><li><p>树的高度 $h$</p><p>对于分治递归方法，最后一层的每个子问题规模为 1，即 $\frac{n}{b^h}=1$，由此可得 </p><script type="math/tex; mode=display">h = \log_b n</script></li><li><p>叶子结点个数</p><p>叶子结点的个数即为最后一层子问题的数目 $a^h$。由上一步 $h = \log_b n$ 可得，叶子结点的个数为 $a^{\log_b n}$。根据换底公式 $x^{\log_ny}=y^{\log_nx}$ 可以将上式改写成  $n^{\log_ba}$。注意每个子问题的划分都是 $n/b$ 的均匀划分，所以时间复杂度也应该用 $\Theta$ 表示，即</p><script type="math/tex; mode=display">\Theta(n^{\log_ba})</script></li><li><p>合并子问题需要花费的操作次数总和</p><p>根据我们对每层递归树的分析可以发现，每层合并子问题需要的操作次数为 $a^if\Big(\frac{n}{b^i}\Big)$，只需要将每层次数相加即可得到总次数：</p><script type="math/tex; mode=display">\sum_{i=0}^{\log_bn-1}a^if\Big(\frac{n}{b^i}\Big)</script></li></ul><p>有了递归操作总次数和分解合并操作总次数以后，根据递归函数时间复杂度公式可得</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}T(n) &= aT(\frac{n}{b})+f(n) \\\\     &= \Theta(n^{\log_ba})+\sum_{i=0}^{\log_bn-1}a^if\Big(\frac{n}{b^i}\Big) \\\\     &= \Theta(n^{\log_ba})+g(n)\end{aligned}\end{equation}</script><p>从中我们可以看出，整个递归的时间复杂度取决于 $f(n)$，分三种情况讨论：</p><ol><li><p>如果右边第一项的阶数比第二项高， $T(n)$ 主要由第一项决定，这意味着递归树的时间主要消耗在子问题的递归上。根据时间复杂度分析“忽略低阶项”的原则：</p><script type="math/tex; mode=display">T(n) = \Theta(n^{\log_ba})</script></li><li><p>如果右边第二项的阶数比第一项高， $T(n)$ 主要由第二项决定，这意味着递归树的时间主要消耗在对问题的分解和解的合并上，根据时间复杂度分析“忽略低阶项”的原则：</p><script type="math/tex; mode=display">T(n) = \Theta(f(n))</script></li><li><p>如果两部分的阶数相等，意味着递归树的总时间分布是均匀的，由两部分共同决定：</p><script type="math/tex; mode=display">T(n)=\Theta(n^{\log_b a} \cdot \log n)</script></li></ol><h3 id="1-4-3-主定理证明"><a href="#1-4-3-主定理证明" class="headerlink" title="1.4.3 主定理证明"></a>1.4.3 主定理证明</h3><p>为了方便写证明过程，将以上三种情况用数学语言进行描述：</p><ol><li><p>$\exists \epsilon&gt;0 s.t. f(n)=O(n^{\log_ba-\epsilon})$，则 $T(n)=\Theta(n^{\log_ba})$。</p><p>证明：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}f(n) &= O(n^{(\log_ba)-\epsilon}) \\\\\Rightarrow f(\frac{n}{b^i}) &= O((\frac{n}{b^i})^{(\log_ba)-\epsilon}) \\\\                 &\le c(\frac{n}{b^i})^{(\log_ba)-\epsilon} \\\\\Rightarrow a^if(\frac{n}{b^i}) &\le c a^i(\frac{n}{b^i})^{(\log_ba)-\epsilon} \\\\                    &= cn^{(\log_ba)-\epsilon} \cdot (\frac{a}{b^{(\log_ba)-\epsilon}})^i\\\\                    &= cn^{(\log_ba)-\epsilon} \cdot (\frac{a}{a^{(\log_bb)}\cdot b^ {-\epsilon}})^i\\\\                    &= cn^{(\log_ba)-\epsilon} \cdot (b^\epsilon)^i \\\\\Rightarrow \sum_{i=0}^{\log_bn-1}a^if\Big(\frac{n}{b^i}\Big) &\le \sum_{i=0}^{\log_bn-1} cn^{(\log_ba)-\epsilon} \cdot (b^\epsilon)^i \\\\                     &= cn^{(\log_ba)-\epsilon} \cdot \sum_{i=0}^{\log_bn-1}(b^\epsilon)^i\\\\                    &= cn^{(\log_ba)-\varepsilon} \cdot \frac{(b^\varepsilon)^{\log_bn}-1}{b^\varepsilon-1} \\\\                    &= cn^{(\log_ba)-\varepsilon} \cdot \frac{n^\epsilon-1}{b^\epsilon-1} \\\\                    &= \frac{c}{b^\epsilon-1} \left[ n^{\log_ba}-n^{(\log_ba)-\epsilon} \right] \\\\                    &= O(n^{\log_ba})\end{aligned}\end{equation}</script><p>由此可得</p><script type="math/tex; mode=display">T(n) =\Theta(n^{\log_ba}) +O(n^{\log_ba})</script><p>由于上式右侧两部分的函数增长率相同，所以</p><script type="math/tex; mode=display">T(n) = \Theta(n^{\log_ba})</script><blockquote><p>个人理解是，$\Theta$ 表示的是平均水平，即它可能有更高的上限和更低的下限。而 $O$ 已经是上限了，当 $\Theta$ 取上限的时候，阶数是要比 $O$ 更高的。所以，当 $\Theta$ 和 $O$ 的增长率是相同的时候，复杂度取 $\Theta$。</p></blockquote></li><li><p>$\exists \epsilon&gt;0 s.t. f(n)=\Omega(n^{\log_ba+\epsilon})$，且 $\exists c&lt;1$。当 $n \to \infty$， $a\, f(\frac{n}{b})\le c\, f(n)$。此时 $T(n)=\Theta(f(n))$。</p><p>证明：</p></li></ol><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}af(\frac{n}{b}) &\le cf(n) \\\\\Rightarrow f\Big(\frac{n}{b}\Big) &\le \frac{c}{a}f(n)\\\\\Rightarrow f\Big(\frac{n}{b^2}\Big) &\le \frac{c}{a}f\Big(\frac nb\Big)\le\Big(\frac{c}{a}\Big)^2f(n)\\\\&\vdots\\\\f\Big(\frac{n}{b^i}\Big) &\le\Big(\frac{c}{a}\Big)^if(n)\\\\\Rightarrow a^i\, f\Big(\frac{n}{b^i}\Big) &\le c^i\, f(n)\\\\\Rightarrow g(n) &\le f(n)\sum_{i=1}^{\log_bn-1}\\\\     &\le f(n)\sum_{i=1}^\infty c^i\\\\     &= \frac{1}{1-c}f(n)\\\\\Rightarrow g(n) &= O(f(n))\\\\g(n) &= f(n) + af(\frac{n}{b})+ \dots + a^{\log_bn-1}f(\frac{n}{b^{\log_bn-1}}) \\\\     &\ge f(n)\\\\\Rightarrow g(n) &= \Omega(f(n))\end{aligned}\end{equation}</script><p>由 $g(n)=O(f(n))$ 和 $g(n)=\Omega(f(n))$ 可得：</p><script type="math/tex; mode=display">g(n) = \Theta(f(n))=\Theta(n^{\log_ba+\epsilon})</script><p>此时</p><script type="math/tex; mode=display">T(n) = \Theta(n^{\log_ba})+\Theta(n^{\log_ba+\epsilon})</script><p>后者的阶数更高，所以</p><script type="math/tex; mode=display">T(n) = \Theta(f(n))</script><ol><li>如果 $f(n)=\Theta(n^{\log_ba})$，则 $T(n)=\Theta(n{\log_ba}\cdot\log n)$。</li></ol><p>证明：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}f(n) &= \Theta(n^{\log_ba}) \\\\\Rightarrow f(\frac{n}{b^i}) &= \Theta((\frac{n}{b^i})^{\log_ba}) \\\\\Rightarrow c_1(\frac{n}{b^i})^{\log_ba} &\le f(\frac{n}{b^i}) \le c_2(\frac{n}{b^i})^{\log_ba}\\\\\Rightarrow a^i \cdot c_1(\frac{n}{b^i})^{\log_ba} &\le a^i \cdot f(\frac{n}{b^i}) \le a^i\cdot c_2(\frac{n}{b^i})^{\log_ba}\\\\\Rightarrow c_1n^{\log_ba}(\frac{a}{b^{\log_ba}})^i &\le a^if(\frac{n}{b^i})\le c_2n^{\log_ba}(\frac{a}{b^{\log_ba}})^i\\\\c_1n^{\log_ba}(1)^i &\le a^if(\frac{n}{b^i})\le c_2n^{\log_ba}(1)^i\\\\\sum_{i=0}^{\log_bn-1} c_1n^{\log_ba}(1)^i &\le \sum_{i=0}^{\log_bn-1}a^if(\frac{n}{b^i})\le \sum_{i=0}^{\log_bn-1} c_2n^{\log_ba}(1)^i\\\\c_1n^{\log_ba}\cdot \log_bn &\le g(n) \le c_2n^{\log_ba} \\\\c_1n^{\log_ba}\frac{\log n}{\log b} &\le g(n) \le c_2 n^{\log_ba} \frac{\log n}{\log b}\\\\\frac{c_1}{\log b} \cdot n^{\log_ba}\cdot \log n &\le g(n) \le \frac{c_2}{\log b} n^{\log_ba} \cdot \log n \\\\\Rightarrow g(n) &= \Theta(n^{\log_ba}\cdot \log n) \\\\\Rightarrow T(n) &= \Theta(n^{\log_ba}) +\Theta(n^{\log_ba}\cdot \log b)\end{aligned}\end{equation}</script><p>由于 $\Theta(n^{\log_ba}\cdot \log n)$  是高阶项，所以</p><script type="math/tex; mode=display">T(n) = \Theta(n^{\log_ba}\cdot \log n)</script><h3 id="1-4-4-主定律应用"><a href="#1-4-4-主定律应用" class="headerlink" title="1.4.4 主定律应用"></a>1.4.4 主定律应用</h3><script type="math/tex; mode=display">T(n)=aT(\frac{n}{b})+f(n) = \begin{cases}\Theta(n^{\log_ba}), & \text{if}\quad f(n)=O(n^{\log_ba-\epsilon}) \\\\\Theta(n^{\log_ba}\cdot \log n), &\text{if}\quad f(n)=\Theta(n^{\log_ba})\\\\\Theta(f(n)) , & \text{if} \quad f(n) = \Omega(n^{\log_ba+\epsilon})\end{cases}</script><blockquote><p>$T(n)=3T(n/2)+n^2$</p><ol><li>$a=3, b=2$，$f(n)=n^2$</li><li><p>$\log_ba=\log_23 \approx 1.58 &lt; 2$</p></li><li><p>即 $f(n)&lt;n^{\log_23+\epsilon}$</p></li><li>$T(n)=\Theta (f(n))=\Theta(n^2)$</li></ol></blockquote><h3 id="1-4-5-主定律的局限性"><a href="#1-4-5-主定律的局限性" class="headerlink" title="1.4.5 主定律的局限性"></a>1.4.5 主定律的局限性</h3><p>主定律在以下情况下不可用：</p><ul><li>$T(n)$ 是非单调函数，比如 $T(n)=\sin(n)$;</li><li>$ f(n)$ 是非多项式，比如 $f(n)=2^n$;</li><li>$a$ 不是常数，比如 $a=2n$；</li><li>$a&lt;1$。</li></ul><h2 id="1-5-关于时间复杂度的讨论"><a href="#1-5-关于时间复杂度的讨论" class="headerlink" title="1.5 关于时间复杂度的讨论"></a>1.5 关于时间复杂度的讨论</h2><h3 id="1-5-1-渐进分析有什么缺点？"><a href="#1-5-1-渐进分析有什么缺点？" class="headerlink" title="1.5.1 渐进分析有什么缺点？"></a>1.5.1 渐进分析有什么缺点？</h3><ul><li>由于 渐近分析是假设 $n\to \infty$ 时才成立的，通常情况下 ，算法需要解决的问题规模不会这么大。此时，估算结果会与实际情况有所偏差。</li><li>由于渐近分析考虑的是时间增长率，忽略掉了低阶项和系数。所以无法区分增长率相同而系数不同的情况。比如 $f(n)=100n$ 和 $g(n)=n\log_2n$ ，按照渐近分析，$f(n)$ 的复杂度要优于 $g(n)$。然而只有当问题规模达到宇宙原子总数量级的时候，这种情况才成立。而我们实际应用中问题规模通常是远小于这个量级。</li></ul><h3 id="1-5-2-为什么通常关心的是-O-f-n-？"><a href="#1-5-2-为什么通常关心的是-O-f-n-？" class="headerlink" title="1.5.2 为什么通常关心的是 $O(f(n))$？"></a>1.5.2 为什么通常关心的是 $O(f(n))$？</h3><p>首先， $\Omega(f(n))$ 对各种条件要求比较苛刻，所以我们主要讨论 $\Theta(f(n))$ 和 $O(f(n))$。</p><ul><li><p>当我们讨论 “平均” 情况的时候，意味着要对输入数据的分布作出假设。在做这种假设的时候需要大量的数据支持。这就意味着分析结果是不普适的，因为不同的数据有不同的分布。所以通常情况下，“平均” 分析结果并不准确。</p></li><li><p>“最坏” 的情况得到的结论容易组合，但“平均”不行，比如：</p><ol><li>算法 1 在最坏的情况下执行 $n$ 次过程 1；</li><li>过程 1 在最坏的情况下执行 $m$ 次过程 2；</li><li>过程 2 执行若干次基本操作。</li></ol><p>此时，我们就可以说算法 1 的最坏复杂程度为 $O(n\times m)$。但是如果算法 1 是在平均情况下执行 $n$ 次过程 1，和 $m$ 次过程 2，我们却不能说算法 1 的复杂度是 $\Theta(n\times m)$。因为过程 1 的数据分布我们不清楚。</p></li></ul><h3 id="1-5-3-如何选择-O-f-n-，-Theta-f-n-，-Omega-f-n-？"><a href="#1-5-3-如何选择-O-f-n-，-Theta-f-n-，-Omega-f-n-？" class="headerlink" title="1.5.3 如何选择 $O(f(n))，\Theta(f(n))，\Omega(f(n))$？"></a>1.5.3 如何选择 $O(f(n))，\Theta(f(n))，\Omega(f(n))$？</h3><p>视情况而定。</p><ul><li>对实时性要求不高的时候，可以考虑平均复杂度。</li><li>对实时性要求非常高的情况，就必须考虑最坏的情况了。比如汽车抱死系统，人命关天，时间就是生命。如果用平均复杂度，意味着系统平均反应速度很快，但是偶尔会比较慢。而这个“偶尔”就可能造成无法挽回的损失。</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p><a href="https://www.programiz.com/dsa" target="_blank" rel="noopener">DAS Introduction</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/146490404" target="_blank" rel="noopener">算法复杂度分析的那些事</a>, <em>跟小新一起玩编程</em></p></li><li><p><a href="https://adrianmejia.com/categories/coding/data-structures-and-algorithms-dsa/" target="_blank" rel="noopener">Data Structures and Algorithms (DSA)</a></p></li><li><p><a href="https://www.zhihu.com/question/28713446/answer/423755676" target="_blank" rel="noopener">算法分析中，为什么分析最坏情况而不是平均情况？</a></p></li><li><p><a href="https://my.oschina.net/u/240275/blog/232763" target="_blank" rel="noopener">关于&lt;&lt;算法导论&gt;&gt;上的主定理（Master Theorem）的证明</a> </p></li><li><p><a href="https://blog.csdn.net/caozhk/article/details/24734371?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-4.control&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-4.control" target="_blank" rel="noopener">主定理的证明及应用举例</a></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
            <tag> 时间复杂度 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构与算法：数据结构简介</title>
      <link href="/2021/04/16/ds-data-structure-introduce/"/>
      <url>/2021/04/16/ds-data-structure-introduce/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png" alt></p><h1 id="1-什么是数据结构"><a href="#1-什么是数据结构" class="headerlink" title="1. 什么是数据结构"></a>1. 什么是数据结构</h1><p>数据结构是用来存储和组织数据的仓库，是一种计算机高效获取和更新数据的方式。根据你的项目需求，找到合适的数据结构至关重要。比如，你想存储序列数据，那么你可以将数据存储在 <code>Array</code> 数据结构中：</p><a id="more"></a><p><img width="500" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/array_dsa.png"></p><h1 id="2-数据结构类型"><a href="#2-数据结构类型" class="headerlink" title="2. 数据结构类型"></a>2. 数据结构类型</h1><p>基本上，数据结构有两种类型：</p><ul><li>线性数据结构</li><li>非线性数据结构</li></ul><h2 id="2-1-线性数据结构"><a href="#2-1-线性数据结构" class="headerlink" title="2.1 线性数据结构"></a>2.1 线性数据结构</h2><p>线性数据结构中，数据是以一个数字接一个数字排列的形式组织起来的。典型的线性数据结构有：</p><h3 id="2-1-1-数组-Array"><a href="#2-1-1-数组-Array" class="headerlink" title="2.1.1 数组 Array"></a>2.1.1 数组 Array</h3><p>数组数据结构中，数据排列在连续内存中，所有的元素都有相同的数据类型。而数据的具体形式由不同的编程语言决定，比如 <code>Python</code> 中，可以用 <code>list</code> 和 <code>array</code> 来实现。</p><p><img width="400" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/array_.png"></p><h3 id="2-1-2-堆栈-Stack"><a href="#2-1-2-堆栈-Stack" class="headerlink" title="2.1.2 堆栈 Stack"></a>2.1.2 堆栈 Stack</h3><p>在堆栈数据结构中，数据以 <em>LIFO</em> 的原则进行存储，即后存进去的数据会先被删除。就像堆盘子，最后放上去的盘子会首先被拿下来。</p><p><img width="250" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/stack_dsa.png"></p><h3 id="2-1-3-队列-Queue"><a href="#2-1-3-队列-Queue" class="headerlink" title="2.1.3 队列 Queue"></a>2.1.3 队列 Queue</h3><p>与堆栈数据结构相反，队列数据结构是以 <em>FIFO</em> 原则进行数据存储，即先进先出。就像排队，先排队的人先取到票。</p><p><img width="350" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/queue_dsa.png"></p><h3 id="2-1-4-链表-Linked-List"><a href="#2-1-4-链表-Linked-List" class="headerlink" title="2.1.4 链表 Linked List"></a>2.1.4 链表 Linked List</h3><p>链表中的每个元素都可以通过一系列的节点与其他元素相连，并且每个节点都包含元素本身以及下一个节点的地址。</p><p><img width="700" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/linked-list_dsa.png"></p><h2 id="2-2-非线性数据结构"><a href="#2-2-非线性数据结构" class="headerlink" title="2.2 非线性数据结构"></a>2.2 非线性数据结构</h2><p>非线性数据结构中的数据是无序的，且一个元素可以与其他元素相连。非线性数据结构主要有两种：</p><ul><li>图数据结构</li><li>树数据结构</li></ul><h3 id="2-2-1-图数据结构"><a href="#2-2-1-图数据结构" class="headerlink" title="2.2.1 图数据结构"></a>2.2.1 图数据结构</h3><p>图数据结构中，每个节点称为“顶点”（<em>vertex</em>），顶点与顶点通过“边”相连。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/graph_dsa.png" alt="graph_dsa.png" style="zoom:50%;"></p><p>常见的图数据结构包括：</p><ul><li>Spanning Tree and Minimum Spanning Tree</li><li>Strongly Connected Components</li><li>Adjacency Matrix</li><li>Adjacency List</li></ul><h3 id="2-2-2-树数据结构"><a href="#2-2-2-树数据结构" class="headerlink" title="2.2.2 树数据结构"></a>2.2.2 树数据结构</h3><p>与图类似，树也是由顶点和边将数据组合在一起的。但是在树结构中，两个顶点只能有一条边。</p><p><img width="350" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/tree_dsa.png"></p><p>常见的树数据结构有：</p><ul><li>Binary Tree</li><li>Binary Search Tree</li><li>AVL Tree</li><li>B-Tree</li><li>B+ Tree</li><li>Red-Black Tree</li></ul><h2 id="2-3-线性数据结构-vs-非线性数据结构"><a href="#2-3-线性数据结构-vs-非线性数据结构" class="headerlink" title="2.3 线性数据结构 vs 非线性数据结构"></a>2.3 线性数据结构 vs 非线性数据结构</h2><div class="table-container"><table><thead><tr><th>线性数据结构</th><th>非线性数据结构</th></tr></thead><tbody><tr><td>数据按照一定顺序组织起来</td><td>数据是无序的</td></tr><tr><td>数据只有一层</td><td>数据有多个层级</td></tr><tr><td>只有一条路径可以遍历所有数据</td><td>通常需要多条路径遍历所有数据</td></tr><tr><td>内存使用率不高</td><td>不同的结构有不同的内存使用率</td></tr><tr><td>时间复杂度随数据量线性增加</td><td>时间复杂度保持不变</td></tr></tbody></table></div><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.programiz.com/dsa/data-structure-types" target="_blank" rel="noopener">Data Structure and Types</a> </p>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
            <tag> queue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NL2SQL 综述</title>
      <link href="/2021/04/14/nl2sql-survey/"/>
      <url>/2021/04/14/nl2sql-survey/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210414172835.png" alt></p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>当今社会，人们越来越多地开始和数据打交道，其中一个核心的问题是数据的存储。为了更好的存储和管理数据，通常会把数据存储在数据库中。对于数据库管理系统的用户来说，必须具备两方面的能力：</p><ol><li>对于数据库的结构非常了解，比如表名、列名、实体关系等等；</li><li>熟悉数据库查询语言，比如 SQL。</li></ol><p>虽然 SQL 语言并不复杂，但是对于非技术人员来说，这仍然是阻碍他们与数据进行交互的门槛。就像封面中的例子：“<em>what ‘s the total number of songs originally performed by anna nalick？</em>”</p><ul><li>首先，我怎么知道有哪些数据可用？也就是说，我怎么知道我要去哪张表里查我想要的数据？</li><li>其次，我怎么知道要去 <code>Song choice</code> 和 <code>Original artist</code> 这两列中去找？</li><li>然后，如果我得到了 <em>anna nalick</em> 有多少首歌以后，我还想知道她和 <em>Macy Gray</em> 谁的歌更多怎么办？</li><li>再然后，如果我想知道的不是 <em>anna nalick</em> 有多少首歌，而是每个歌手分别有多少首歌又怎么办？或者有哪些歌手的歌比 <em>anna nalick</em> 多？</li><li>$\cdots$</li></ul><p>如果我们能让计算机（数据库）理解人类的语言，只需要像和人类一样交流就可以从数据库中得到我们想要的数据岂不是美哉？这就是 NL2SQL 想要达到的目标。NL2SQL（Natural Language to SQL）， 顾名思义是将自然语言转为 SQL 语句。它可以充当数据库的智能接口，让不熟悉数据库的用户能够快速地找到自己想要的数据。</p><h2 id="1-1-NL2SQL-的一些实际应用场景"><a href="#1-1-NL2SQL-的一些实际应用场景" class="headerlink" title="1.1 NL2SQL 的一些实际应用场景"></a>1.1 NL2SQL 的一些实际应用场景</h2><p>通常数据交互的场景有三种：</p><ol><li>企业内部数据管理。几乎所有企业都有大量数据需要存储和管理，多数企业会选择使用数据库，而对于这些企业来说，通常会雇佣专业技术人员来管理数据库。因此，企业内部数据管理对于 NL2SQL 的需求并不强烈；</li><li>个人用户数据管理。对于个人用户而言，很少有用户的数据量会多到需要用到数据库。因此，这种场景下似乎 NL2SQL 的作用也并不明显。</li><li>普通用户与企业数据之间的交互。比如订餐、订票、查天气、查公交地铁等等。在这些场景下，现在的技术手段是企业通过一些界面的选项引导用户一步一步查询到自己想要的数据。对于用户来说，这些操作繁复冗杂，而且需要一定的门槛。正是由于引导式操作的繁杂给社会带来了一些尴尬局面——移动互联网为年轻人提供了便利，但却给老年人带来了麻烦。同时，对于企业来说，如何引导设计查询页面，如何编写查询程序同样也是必须面对的问题。如果能够实现专用户直接使用自然语言进行信息查询，以上问题都会迎刃而解，这也是 NL2SQL 最大的意义所在。</li></ol><h1 id="2-NL2SQL-发展简史"><a href="#2-NL2SQL-发展简史" class="headerlink" title="2. NL2SQL 发展简史"></a>2. NL2SQL 发展简史</h1><p>NL2SQL 虽然是最近才火起来的，但是实际上它的研究历史还是比较长的。早在上世纪六七十年代人们就提出了 NLIDB （<em>Natural Language Interface to Database</em>）概念并做了一些研究。但是受限于数据量和计算机的算力，当时主要的技术手段是模式匹配、语法树和语义语法等。</p><div class="timeline"><div class="timenode"><div class="meta"><p></p><p>1960-70 年代</p><p></p></div><div class="body"><p>最早出现的 NLIDB 系统是有两个：</p><ul><li><strong>BASEBALL</strong>，主要面向当时没搞过国内的棒球联赛；</li><li><strong>LUNAR</strong>，用来回答有关从月球带回来的岩石样本的问题。 它能够准确地回答 80％ 的查询，没有任何错误。</li></ul></div></div><div class="timenode"><div class="meta"><p></p><p>1970 年代</p><p></p></div><div class="body"><p>到了上世纪 70 年代末期，陆续出现了一些 NLIDB 系统：</p><ul><li><strong>PLANES</strong>，该系统甚至能够响应不连贯或模糊的用户请求；</li><li><strong>LIFER/LADDER</strong>，通过语义语法系统分析用户请求；</li><li><strong>RENDEZVOUS</strong>，它是 IBM 实验室的 <em>San José</em> 开发的，可以帮助用户在模棱两可的情况下提出自己的请求</li></ul></div></div><div class="timenode"><div class="meta"><p></p><p>1980 年代</p><p></p></div><div class="body"><ul><li><strong>CHAT-80</strong>，采用语义语法技术处理自然语句，使得 CHAT-80 达到了当时最好的效果。它最大的问题仍然是只对特定领域的数据集有效。</li><li><strong>MASQUE</strong>， <strong>DIALOGIC</strong>，都是对 CHAT-80 的改进型系统，在处理效率上有了大幅的提升。</li></ul></div></div><div class="timenode"><div class="meta"><p></p><p>1990 年代</p><p></p></div><div class="body"><p>从 90 年代开始，人们的研究开始聚焦关系型数据库。也就是说 NLIDB 系统开始向 NL2SQL 方向聚焦，当然此时的方法仍然是只针对对特定领域的数据。</p><ul><li><strong>MASQUE/SQL</strong>，就是此时开发出来用于处理商业数据库的系统。</li><li><strong>California Restaurant Query</strong>，<strong>Expedia Hotels</strong>，<strong>GeoQuery</strong>，<strong>Hollywood</strong>，<strong>JobQuery</strong>，<strong>SQ-HAL</strong>，<strong>SystemX</strong> 等等一系列系统如雨后春笋般相继出现。</li></ul></div></div><div class="timenode"><div class="meta"><p></p><p>2000 年后</p><p></p></div><div class="body"><p>如果说 70 年代是 NLIDB 系统的诞生的话，那么进入 2000 年以后可以算是该系统的第一次进化。一些新方法的提出使得 NLIDB 不再只对特定领域的数据有效（需要指出的是，此时的主要技术手段仍然是基于规则的）。</p><ul><li><strong>PRECISE</strong>，是第一个采用插件式的 NLIDB 系统。它通过结合语法技术和数学方法，使得语义分析器完全独立于数据库的配置信息。但是它只能处理简单的句子，不能处理嵌套语句，主要是因为它假设自言语句中的词和数据库表格中的数据存在一一对应的关系。</li><li><strong>NALIX</strong>，用于处理 XML 数据库。</li></ul></div></div><div class="timenode"><div class="meta"><p></p><p>2010 至今</p><p></p></div><div class="body"><p>随着机器学习技术的发展，机器学习也开始应用于 NLIDB 系统。</p><ul><li><strong>NaLIR</strong>，通过与用户的交互使得它能处理复杂的自然语句。</li><li><strong>ATHENA</strong>，通过特定领域的本体处理更加丰富的语义信息。</li><li><strong>SQLizer</strong>，将自然语句转成逻辑表达，然后通过迭代优化逻辑表达式。</li><li><strong>Templar</strong>，是一种使用查询日志进行映射和联接路径生成的优化技术。</li></ul></div></div>以上的这些方法或者系统都是严重依赖人工设计规则。而近些年深度学习的发展为这一领域也带来了新的景象，尤其是最近两三年，基于深度学习的 NL2SQL 方法不断刷新着记录。本文主要是对基于深度学习的 NL2SQL 技术进行一个总结，并探索 NL2SQL 技术在 Excel 领域应用。<div class="timenode"><div class="meta"><p></p><p>基于深度学习的 NL2SQL</p><p></p></div><div class="body"><p><strong>Seq2sql</strong>，<strong>SQLNet</strong>，<strong>TypeSQL</strong>，<strong>Coarse-to-Fine</strong>，<strong>IncSQL</strong>，<strong>X-SQL</strong>，<strong>STAMP</strong>，<strong>IRNet</strong>，<strong>SQLova</strong>，<strong>TRANX</strong>，<strong>SyntaxSQL</strong> …</p></div></div></div><h1 id="3-数据集"><a href="#3-数据集" class="headerlink" title="3. 数据集"></a>3. 数据集</h1><div class="table-container"><table><thead><tr><th style="text-align:center">数据集</th><th style="text-align:center">总语句</th><th style="text-align:center">训练语句</th><th style="text-align:center">验证语句</th><th style="text-align:center">测试语句</th><th style="text-align:center">总表数</th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://github.com/salesforce/WikiSQL" target="_blank" rel="noopener">WiKiSQL</a></td><td style="text-align:center">80654</td><td style="text-align:center">56355</td><td style="text-align:center">8421</td><td style="text-align:center">15875</td><td style="text-align:center">26531</td></tr><tr><td style="text-align:center"><a href="https://www.kaggle.com/siddhadev/ms-cntk-atis" target="_blank" rel="noopener">ATIS</a></td><td style="text-align:center">5317</td><td style="text-align:center">4379</td><td style="text-align:center">491</td><td style="text-align:center">447</td><td style="text-align:center">25</td></tr><tr><td style="text-align:center"><a href="https://github.com/jkkummerfeld/text2sql-data" target="_blank" rel="noopener">Advising</a><br>(query split)</td><td style="text-align:center">4387</td><td style="text-align:center">2040</td><td style="text-align:center">515</td><td style="text-align:center">1832</td><td style="text-align:center">15</td></tr><tr><td style="text-align:center"><a href="https://github.com/jkkummerfeld/text2sql-data" target="_blank" rel="noopener">Advising</a><br>(question split)</td><td style="text-align:center">4387</td><td style="text-align:center">3585</td><td style="text-align:center">229</td><td style="text-align:center">573</td><td style="text-align:center">15</td></tr><tr><td style="text-align:center"><a href="https://github.com/jkkummerfeld/text2sql-data" target="_blank" rel="noopener">GeoQuery</a></td><td style="text-align:center">880</td><td style="text-align:center">550</td><td style="text-align:center">50</td><td style="text-align:center">280</td><td style="text-align:center">7</td></tr><tr><td style="text-align:center"><a href="https://github.com/jkkummerfeld/text2sql-data" target="_blank" rel="noopener">Scholar</a></td><td style="text-align:center">816</td><td style="text-align:center">498</td><td style="text-align:center">100</td><td style="text-align:center">218</td><td style="text-align:center">10</td></tr><tr><td style="text-align:center"><a href="https://github.com/nkapetanas/dbpal" target="_blank" rel="noopener">Patients</a></td><td style="text-align:center">342</td><td style="text-align:center">214</td><td style="text-align:center">19</td><td style="text-align:center">109</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center"><a href="https://github.com/jkkummerfeld/text2sql-data" target="_blank" rel="noopener">Restaurant</a></td><td style="text-align:center">251</td><td style="text-align:center">157</td><td style="text-align:center">14</td><td style="text-align:center">82</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center"><a href="https://academic.microsoft.com/home" target="_blank" rel="noopener">MAS</a></td><td style="text-align:center">196</td><td style="text-align:center">123</td><td style="text-align:center">11</td><td style="text-align:center">62</td><td style="text-align:center">17</td></tr><tr><td style="text-align:center"><a href="https://github.com/jkkummerfeld/text2sql-data" target="_blank" rel="noopener">IMDB</a></td><td style="text-align:center">131</td><td style="text-align:center">82</td><td style="text-align:center">7</td><td style="text-align:center">42</td><td style="text-align:center">16</td></tr><tr><td style="text-align:center"><a href="https://github.com/jkkummerfeld/text2sql-data" target="_blank" rel="noopener">YELP</a></td><td style="text-align:center">128</td><td style="text-align:center">80</td><td style="text-align:center">7</td><td style="text-align:center">41</td><td style="text-align:center">7</td></tr><tr><td style="text-align:center"><a href="https://yale-lily.github.io/spider" target="_blank" rel="noopener">Spider</a></td><td style="text-align:center">9693</td><td style="text-align:center">8659</td><td style="text-align:center">1034</td><td style="text-align:center">-</td><td style="text-align:center">873</td></tr><tr><td style="text-align:center"><a href="https://github.com/ppasupat/WikiTableQuestions/releases" target="_blank" rel="noopener">WTQ</a></td><td style="text-align:center">9287</td><td style="text-align:center">5804</td><td style="text-align:center">528</td><td style="text-align:center">2955</td><td style="text-align:center">2102</td></tr><tr><td style="text-align:center"><a href="https://nlp.stanford.edu/software/sempre/wikitable/" target="_blank" rel="noopener">WikiTableQuestions</a></td><td style="text-align:center">22033</td><td style="text-align:center">14152</td><td style="text-align:center">3537</td><td style="text-align:center">4344</td><td style="text-align:center">2100</td></tr><tr><td style="text-align:center"><a href="https://taolusi.github.io/CSpider-explorer/" target="_blank" rel="noopener"><font color="blue">CSpider</font></a></td><td style="text-align:center">8832</td><td style="text-align:center">6831</td><td style="text-align:center">95</td><td style="text-align:center">1906</td><td style="text-align:center">876</td></tr><tr><td style="text-align:center"><a href="https://github.com/PaddlePaddle/Research/tree/master/NLP/DuSQL-Baseline" target="_blank" rel="noopener"><font color="blue">DuSQL</font></a></td><td style="text-align:center">28763</td><td style="text-align:center">22521</td><td style="text-align:center">2483</td><td style="text-align:center">3759</td><td style="text-align:center">813</td></tr><tr><td style="text-align:center"><a href="https://github.com/ZhuiyiTechnology/TableQA" target="_blank" rel="noopener"><font color="blue">TableQA</font></a></td><td style="text-align:center">54059</td><td style="text-align:center">41522</td><td style="text-align:center">4396</td><td style="text-align:center">8141</td><td style="text-align:center">5291</td></tr></tbody></table></div><p>上表中列出了目前主要的一些数据及其统计信息，其中蓝色字体对应的数据为中文数据集，其他数据都是英文数据集。下面我们挑选几个应用最广泛的几个英文数据集和中文数据集进行简单的介绍。</p><ul><li><p><strong>ATIS</strong> (<em>The Air Travel Information System</em>)：ATIS 是一个年代较为久远的经典数据集，由德克萨斯仪器公司在1990 年提出。该数据集获取自关系型数据库 Official Airline Guide (OAG, 1990)，包含 25 张表以及 5000 次的问询，每次问询平均7轮，93% 的情况下需要联合 3 张以上的表才能得到答案，问询的内容涵盖了航班、费用、城市、地面服务等信息。总的来说， ATIS 数据集存在数据量少，标注简单，领域单一等问题。</p></li><li><p><strong>WikiSQL</strong>：该数据集是 <em>Salesforce</em> 在 2017 年提出的大型标注 NL2SQL 数据集，也是目前规模最大的 NL2SQL 数据集。它是 <em>Victor Zhong</em> 等研究人员基于维基百科标注的数据。这个数据集一经推出就引起了学术界极大的关注。因为他对模型提出了新的挑战，研究人员也在此数据集的基础上研究出了大量的优秀模型。目前学术界的预测准确率可达 91.8%。但是 WiKiSQL 数据集是一个单表无嵌套的数据集，总的来说相对于实际场景还是偏于简单。</p></li><li><strong>Spider</strong>：Spider 数据集是耶鲁大学于 2018 年新提出的一个较大规模的 NL2SQL 数据集。Spider 数据集虽然在数据数量上不如 WikiSQL，但 Spider 引入了更多的 SQL 用法，例如 <em>Group By</em>、<em>Order By</em>、<em>Having</em>、<em>UNION</em>、<em>EXCEPT</em>、<em>INTERSECT</em>、<em>LIMIT</em> 等高阶操作，甚至需要 Join 不同表，更贴近真实场景，所以 Spider 是目前最复杂的数据集。作者根据 SQL 语句的复杂程度（关键字个数、嵌套程度）分成 4 种难度，而以 Spider 的标准划分的话， WiKiSQL 只有 easy 难度。目前准确率最高只有 54.7%。</li><li><strong>WikiTableQuestions</strong>：该数据集是斯坦福大学于 2015 年提出的一个针对维基百科中那些半结构化表格问答的数据集，表格中的数据是真实且没有经过归一化的，一个 <em>cell</em> 内可能包含多个实体或含义，比如「Beijing, China」或「200 km」；同时，为了很好地泛化到其它领域的数据，该数据集测试集中的表格主题和实体之间的关系都是在训练集中没有见到过的。</li><li><strong>CSpider</strong>：CSpider 是西湖大学利用 Spider 作为源数据集进行问题翻译，并利用 SytaxSQLNet 作为基线进行评测的 NL2SQL 数据集。CSpider 只翻译了问题，而数据库仍然是英文的，因此它在传统的 NL2SQL 上增加了额外的一些挑战，比如中文问题和英文数据库的对应问题(question-to-DB mapping)、中文的分词问题以及一些其他的语言现象。</li><li><strong>DuSQL</strong>：该数据集是百度基于现有数据集问题和实际应用需求构建的多领域、多表数据集，覆盖了更多的问题类型。</li><li><strong>TableQA</strong>：TableQA 是一个大规模，跨领域的中文 NL2SQL 数据集。与现有的 NL2SQL 数据集不同，TableQA 不仅要很好地概括为不同问题和表模式的 SQL 框架，而且还要概括为条件值的各种表达式。实验结果表明，在 WikiSQL 上具有 95.1％ 的条件值精度的最新模型在 TableQA 上仅获得 46.8％ 的条件值精度和 43.0％ 的逻辑形式精度，这表明所提出的数据集具有挑战性且需要处理。</li></ul><h1 id="4-技术路线"><a href="#4-技术路线" class="headerlink" title="4. 技术路线"></a>4. 技术路线</h1><script src="https://cdn.jsdelivr.net/npm/echarts@4.8.0/dist/echarts.min.js"></script><div id="main" style="width: 100%;height:400px;"></div><script type="text/javascript">  // 基于准备好的dom，初始化echarts实例  var myChart = echarts.init(document.getElementById('main'));  var data = {    "name": "NL2SQL",    "children": [        {            "name": "输入",            "children": [                {"name": "自然语言问句处理"},                {"name": "数据库的用法"},                {"name": "附加信息的输入"}            ]        },        {            "name": "技术方法",            "children": [                {                    "name": "输入增强技术",                     "children": [                         {"name": "标记、链接和匿名化"},                         {"name": "数据库表头或数据"}                     ]                },                {                    "name": "翻译技术",                    "children": [                        {"name": "基于规则、基于深度学习"},                        {"name": "处理数据库表头的方法"},                        {"name": "生成 SQL 语句的方法"}                    ]                },                {                    "name": "后处理技术",                    "children": [                        {"name": "处理模型生成的 SQL 语句"}                    ]                },                {                    "name": "训练",                    "children": [                        {"name": "模型优化算法"}                    ]                }           ]        },        {            "name": "输出",            "children": [                {"name": "SQL 语法合成"}            ]        },    ]  };  // 指定图表的配置项和数据  var option = {     tooltip: {        trigger: 'item',        triggerOn: 'mousemove'    },    series:[        {            type: 'tree',            data: [data],            top: '1%',            left: '10%',            bottom: '1%',            right: '25%',            symbolSize: 7,            label: {                position: 'left',                verticalAlign: 'middle',                align: 'right',                fontSize: 14            },            leaves: {                label: {                    position: 'right',                    verticalAlign: 'middle',                    align: 'left'                }            },            emphasis: {                focus: 'descendant'            },            expandAndCollapse: false,            animationDuration: 550,            animationDurationUpdate: 750        }    ]  };  // 使用刚指定的配置项和数据显示图表。  myChart.setOption(option);  // 刷新调整  window.onresize = function () {    myChart.resize();  }</script><p>为了系统的介绍现在的 NL2SQL 方法，我们将模型分成三部分：输入、技术方法、输出。然后总结出不同模型在这三部分的处理方法。</p><h2 id="4-1-输入"><a href="#4-1-输入" class="headerlink" title="4.1 输入"></a>4.1 输入</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/nl2sql_input.png" alt="图片"></p><p>模型的输入一般从三个方面考虑：</p><ul><li>自然语言问句 $q_{nl}$ 的预处理；</li><li>附加信息的输入；</li><li>数据库的处理。</li></ul><p>现有的所有方法中，模型输入都会包含两部分：自然语言问句 $q_{nl}$ 和数据库 $D$，其中 $S_D$ 表示数据库中的表头，$V_D$ 表示表中每一列的数据集合。</p><h3 id="4-1-1-自然语言问句预处理"><a href="#4-1-1-自然语言问句预处理" class="headerlink" title="4.1.1 自然语言问句预处理"></a>4.1.1 自然语言问句预处理</h3><p>自然语言问句的预处理有不同的方法：</p><ul><li>基于深度学习的模型，将（$q_{nl}$，$S_D$，$V_D$）转化成词向量是必备的步骤；</li><li>基于规则的模型，包含以下几种预处理方法：<ol><li>将自然语言问句解析成句法树，比如 NaLIR；</li><li>将自然语言问句转化成逻辑表达式，比如 SQLizer；</li><li>单纯地只进行分词，比如 ATHENA；</li></ol></li></ul><h3 id="4-1-2-数据库的处理"><a href="#4-1-2-数据库的处理" class="headerlink" title="4.1.2 数据库的处理"></a>4.1.2 数据库的处理</h3><p>模型对数据库的处理包含以下几种方法：</p><ul><li><p>将 $D$ 作为模型的输入进行处理，这是最常见的做法；</p></li><li><p>只用 $D$ 来构建词表；</p></li><li><p>有些模型假设数据库只包含一张表；</p></li><li><p>通过字典将自然语句中的一些实体映射到本体上，然后通过本体关系找到可能需要 <em>JOIN</em> 的表；</p><blockquote><p>”本体（<em>ontology</em>）“原本是一个哲学概念，后来被应用于计算机领域。这里不对这个概念做过多的介绍，简单举几个例子说明什么是本体，大家自行体会。</p><p>比如：</p><p>“阿里巴巴”、“金山办公”、“姚明”等等的本体是“名字”；</p><p>“2012年”、“周五”、“3月份”等的本体是“时间”；</p><p>…</p><p>诸如此类的用来描述事物本质的概念称为“本体”。</p></blockquote></li><li><p>只用 $S_D$ 或者使用 $S_D + V_D$。</p></li></ul><h3 id="4-1-3-附加信息输入"><a href="#4-1-3-附加信息输入" class="headerlink" title="4.1.3 附加信息输入"></a>4.1.3 附加信息输入</h3><p>通常使用的附加信息包括：</p><ul><li>开放领域的知识库，比如 Freebase；</li><li>使用专业领域的字典；</li><li>提前构建的 $q_{nl}$ 中的词语与 SQL 关键词之间的映射字典；</li><li>WordNet；</li><li>用于计算词相似度的预训练词向量矩阵。</li></ul><h2 id="4-2-技术方法：输入增强"><a href="#4-2-技术方法：输入增强" class="headerlink" title="4.2 技术方法：输入增强"></a>4.2 技术方法：输入增强</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/nl2sql_input_enrich.png" alt="nl2sql_input_enrich"></p><p>输入增强有三种方法：</p><ul><li><p>标签化（Tagging）</p><ol><li>首先，找到 $q_{nl}$ 中的一些特殊词，比如 TypeSQL 通过字符串匹配找到 $q_{nl}$ 中包含在数据库或知识库中的实体；PT-MAML 是利用词相似度匹配的方法在 $q_{nl}$ 中找到包含在数据库中的 $V_D$。</li><li>然后，将这些词和数据库中实体之间建立联系；</li><li>然后，给 $q_{nl}$ 中的找到的词打上实体标签或者直接将这些词规范化到 $V_D$；</li><li>将每个词的标签转化成词向量拼接到每个词的词向量后面，或者将每个实体的类型加到 $q_{nl}$ 对应实体的位置之前；</li><li>将处理后的词向量序列输入到编码器中。</li></ol><blockquote><p>其实一句话总结就是，标签化增强技术是利用数据库中的实体对 $q_{nl}$ 中的实体进行增强。这样可以使模型更好的捕捉到 $q_{nl}$ 实体和数据库实体的对应关系，从而提高准确率。但是这严重依赖 $q_{nl}$ 实体与数据库实体的匹配技术（无论是字符串匹配还是词相似度匹配），理论上讲即使没有这一步模型也是可以自动学习到，但是这种做法可以提高训练效率，在小数据量上效果应该不错，但是上到大数据量上之后过拟合的风险会大大提高。（当然，这只是个人推断没有实验证据）</p></blockquote></li><li><p>词链接（Linking）</p><p>所谓词链接技术就是计算 $q_{nl}$ 中每个词与数据库中的实体之间的相似度。词链接有两种方法：</p><ol><li>使用词向量计算 $q_{nl}$ 中的词与数据库中的实体之间的相似度；</li><li>使用神经网络计算词与词之间的相似度。</li></ol><p>词链接的相似度同时输入给 $q_{nl}$ 编码器和 $S_D$ 编码器。词链接技术与标签化技术的不同之处在于计算相似度的过程是可以跟随整个模型一起进行训练的。</p><blockquote><p>词链接的技术要比标签化的方法更加合理，但这也意味着模型需要训练的参数量有所上升，训练和推理效率会受到影响。</p></blockquote></li><li><p>匿名化（Anonymizing）</p><p>匿名化方法就是将 $q_{nl}$ 和 $q_{sql}$ 中的常数值替换成一些特殊符号，这样可以降低词表大小。比如 “city.state_name=’New York’” 替换成 “city.state_name=’@STATE’”。用 “@STATE” 来代替具体的城市。</p></li></ul><h2 id="4-3-技术方法：翻译技术"><a href="#4-3-技术方法：翻译技术" class="headerlink" title="4.3 技术方法：翻译技术"></a>4.3 技术方法：翻译技术</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/nl2sql_tech.png" alt="nl2sql_tech"></p><h3 id="4-3-1-基于规则的方法"><a href="#4-3-1-基于规则的方法" class="headerlink" title="4.3.1 基于规则的方法"></a>4.3.1 基于规则的方法</h3><p>现在的基于规则的方法通常是将 $q_{nl}$ 解析成一个树结构的中间表达，不同的模型生成的树不一样。</p><ul><li><p>NaLIR 是将输入的依存句法树转化成另一种分析树。通过一个简单的算法来移动任意初始句法树的子树，然后使用一系列节点插入规则最终实现树结构的变换。</p></li><li><p>ATHENA 是构建一棵解释树（<em>Interpretation tree</em>），其中节点对应概念或者属性，边表示在本体库中概念或属性之间的关系。</p></li><li><p>SQLizer 是将经过预处理的自然语言问句转化成逻辑表达式。</p></li></ul><h3 id="4-3-2-基于深度学习的方法"><a href="#4-3-2-基于深度学习的方法" class="headerlink" title="4.3.2 基于深度学习的方法"></a>4.3.2 基于深度学习的方法</h3><p>基于深度学习的方法接班架构都是“编码器—解码器”（<em>encoder-decoder</em>）结构的。2019 年之前的工作，通常都是使用 RNN 作为编码器对 $q_{nl}$ 进行编码。但是随着 Transformer 的特征提取能力逐渐被人们发掘，尤其是以 BERT 为代表的预训练语言模型技术被提出以后，使用预训练语言模型作为编码器逐渐成为主流。</p><p>下面我们从两个方面介绍基于深度学习的 $q_{nl} \rightarrow q_{sql}$ 映射方法。</p><ul><li><p><strong>如何处理数据库 $S_D$</strong></p><p>在深度学习框架下，数据库通常有两个作用：作为输出词表的一部分（SaV）和作为输入（SaI）。</p><ol><li>在 SaV 方法中，将数据库中所有的表名和列名添加到输出词表中。在解码过程中，解码器从输出词表中选择需要解码的词。</li><li>在 SaI 方法中，数据库中所有表名和列名都以输入的方式传递给模型。在解码过程中，解码器利用指针机制从输入中选择需要解码的词。</li></ol></li><li><p><strong>如何生成 $q_{sql}$</strong></p><p>在生成 $q_{sql}$ 方面，已有的深度学习模型可以分成三种：</p><ol><li>sequence-to-sequence：输入一个序列，输出一个序列，类似于机器翻译；</li><li>sequence-to-tree：输入一个序列，输出一棵树；</li><li>slot filling：所谓的槽填充就是，把 SQL 语句看成是一系列的槽，通过解码器对一个一个的槽进行填充。比如，我们预先设定一个 SQL 语句：“SELECT * FROM * WHERE *”，其中 “ * ” 就是我们要填充的内容。</li></ol></li></ul><h2 id="4-4-技术方法：后处理"><a href="#4-4-技术方法：后处理" class="headerlink" title="4.4 技术方法：后处理"></a>4.4 技术方法：后处理</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/nl2sql_postprocess.png" alt="nl2sql_postprocess"></p><p>后处理通常有四种：</p><ul><li>将之前匿名化的值进行还原；</li><li>对输出是树结构的模型来说，需要后处理将树结构的结果解析成 $q_{sql}$；</li><li><em>JOIN</em> 推理；</li><li>使用用户反馈对结果进行修正。</li></ul><h2 id="4-5-技术方法：训练"><a href="#4-5-技术方法：训练" class="headerlink" title="4.5 技术方法：训练"></a>4.5 技术方法：训练</h2><p>NL2SQL 模型的训练方法是根据它采用的 AI 算法确定的，目前来说有以下几种：</p><ul><li>最常见的深度学习模型采用监督学习训练算法；</li><li>NSP 和 DBPal 模型通过预先设定的模板和释义技术（<em>paraphrasing techniques</em>）记性训练；</li><li>Seq2SQL 和 STAMP 模型采用强化学习方法进行训练；</li><li>PT-MAML 采用元学习方法训练；</li></ul><h2 id="4-6-输出"><a href="#4-6-输出" class="headerlink" title="4.6 输出"></a>4.6 输出</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/nl2sql_output.png" alt="nl2sql_output"></p><p>目前 NL2SQL 技术存在四个方面的缺陷：</p><ul><li>预定义的语法模式或者槽类型（实体类型）；</li><li>启发式翻译规则；</li><li>语法覆盖能力有限的中间表达；</li><li>有限的训练数据。</li></ul><p>所以目前的 NL2SQL 输出的 SQL 语句是有限制的。根据限制能力的不同可以分成：</p><ol><li>不受限制的，这种模型通常是数据驱动类型的。缺点就是需要大量的训练数据，这也是目前几乎所有 AI 任务的痛点；</li><li>低限制的，这样的模型通常是基于规则的模型， 限制通常来源于连表查询和嵌套查询等；</li><li>非常受限的，这样的模型通常需要预定义语法模式和实体类型，一旦发现问题要进行改动的影响面会非常大，甚至可能需要重新设计模型结构，重新标注数据等等。</li></ol><p>所有模型输出结果的方式是先生成一系列候选语句，然后对候选语句进行排序，选择排名最高的 SQL 语句。基于规则的模型会设计不同的排序算法，而基于深度学习的模型通常采用的是用 <em>softmax</em> 函数来计算得分，根据每一个候选语句的得分进行排序。</p><h1 id="5-评估方法"><a href="#5-评估方法" class="headerlink" title="5. 评估方法"></a>5. 评估方法</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/nl2sql_evaluation.png" alt="nl2sql_evaluation"></p><p>对于 NL2SQL 模型来说，一个非常重要的点就是，我们如何评估一个模型的好坏？目前的评估方法有四种：</p><ul><li>字符串匹配，通过对比模型生成的 SQL 语句和给定的标准 SQL 语句，判断生成 SQL 语句的正确性。这种方法可能导致误判，比如 WHERE 条件中，$a \mathrm{and} b$ 和 $b \mathrm{and} a$ 是等价的，但是用字符串匹配的方式就会得到错误的评估。</li><li>解析树匹配，通过对比模型生成的 SQL 和标准 SQL 的解析树，判断生成 SQL 的正确性。这种方法相比字符串匹配的方式，误判率会低一些，但是仍然是有一定的限制。比如，一个嵌套 SQL 语句和一个非嵌套语句实际上是等效的，但是他们的解析树是不一样的，这样解析树匹配的时候就会造成误判。</li><li>结果匹配，通过对比两条 SQL 语句在数据库中的执行结果来判断生成的 SQL 是否正确。这种方式看起来比较合理，但是仍然不能排除两条不同的 SQL 会得到相同结果的可能性。</li><li><p>人工验证，人工验证的误判率是最低的，但是却是成本最高的方法，几乎无法应用于实际生产中。</p></li><li><p>多级验证（<em>Multi-level</em>），用于验证两条 SQL 语句是否等价，如下图所示：</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/nl2sql_multi_level.png" alt="nl2sql_multi_level"></p><p>该方法的核心思想就是：给定两条 SQL 语句，如果它们在任何数据库中的执行结果总是相同的，那么他们就是等价的，一旦出现一次不同的结果，那么他们就不是等价的。</p><ol><li><p>在给定的数据库上面执行两条 SQL 语句，如果结果不同则直接认定模型生成的 SQL 有误。如果结果相同，则继续下一步。</p></li><li><p>如果第一部中的数据库比较小，数据比较少，两条不等价的 SQL 语句可能会产生相同结果。所以，作者提出使用<a href="https://dl.acm.org/doi/10.1145/3180155.3180202" target="_blank" rel="noopener">数据库测试技术</a>对比两条 SQL 语句的执行结果。</p><blockquote><p>所谓数据库测试技术，实际上是为了检测数据库引擎是否存在bug的技术。具体做法就是生成大量的数据库实例，然后跑 SQL 语句。用在这里主要是通过生成大量的数据库实例来验证两条 SQL 语句是否有相同的执行结果。</p></blockquote></li><li><p>用一个现成的验证器，验证两条语句是否等价，比如 Cosette。</p></li><li><p>如果验证器无法验证两条语句是否等价，那么使用 SQL语句重写技术对两条 SQL 语句进行重写。然后对比重写后的 SQL 语法结构。</p></li><li><p>如果重写后的 SQL 语法结构不等价，再使用人工验证。</p></li></ol><p>在本文的实验中，数据库实例生成器用的是 EvoSQL，SQL 语句等价验证器用的是 Cosette，SQL 语句重写用的是 IBM DB2。</p><h1 id="6-实验结果"><a href="#6-实验结果" class="headerlink" title="6. 实验结果"></a>6. 实验结果</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210426113709.png" alt></p><p>为了方便我们对实验结果进行分析，这里专门挑选出一些比较典型的错误案例。</p><h2 id="6-1-不同评估方法的对比"><a href="#6-1-不同评估方法的对比" class="headerlink" title="6.1 不同评估方法的对比"></a>6.1 不同评估方法的对比</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210426095839.png" alt></p><p>首先定义一个表格中的变量。</p><ul><li>$acc_{sem}$，表示利用语义等价评估方法得到的结果；</li><li>$acc_{ex}$，表示利用 SQL 执行结果进行评估的方法的得到的结果；</li><li>$acc_{str}$，表示利用字符串匹配方法得到的评估结果；</li><li>$acc_{syn}$，表示利用解析树匹配的方法得到的评估结果。</li></ul><p>这个实验的目的是对比同一个模型在相同的数据集上用不同的评估方法得到的评估结果。这里选取的模型是 NSP 模型，因为它能生成复杂的 SQL 语句。从表中我们可以看出，不同的评估方法得到的结果差别很大，如果考虑复杂的 SQL 语句的话，这个差别将会更大。</p><h2 id="6-2-简单-SQL-语句实验结果"><a href="#6-2-简单-SQL-语句实验结果" class="headerlink" title="6.2 简单 SQL 语句实验结果"></a>6.2 简单 SQL 语句实验结果</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210426112041.png" alt></p><p>上表是用于验证简单 SQL 语句使用的数据集的统计数据。在所有数据集中，只有 WikiSQL、Patients和 WTQ 三个数据集包含了简单的 SQL 语句，将这三个数据集中的简单语句筛选出来形成新的数据集（加上 -s 后缀以示区别）。</p><table><tr>    <td><div align="center"><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210426104630.png"></div></td>    <td><div align="center"><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210426101828.png"></div></td></tr></table>            <p>上表中，左边的表表示不同模型的总的表现，右侧表示每个单项的准确率，其中 $acc_{sel}$ 表示 SELECT 列的准确率，$acc_{agg}$ 表示聚合函数的准确率，$acc_{wh, col},acc_{wh, op}, acc_{wh,val}$ 分别表示 WHERE 条件中的列，操作符和值的准确率。</p><h3 id="6-2-1-模型泛化性"><a href="#6-2-1-模型泛化性" class="headerlink" title="6.2.1 模型泛化性"></a>6.2.1 模型泛化性</h3><p>从表中我们可以看出，基于规则的模型准确率都比较低。主要还是基于规则的方法有很大的限制性，比如 NaLIR，如果映射表中没有将 “longest” 映射成 $MAX$ 函数的方法，那么当自然问句中包含 “longest” 时就会出错。解决的办法就是尽可能的扩充映射表，但是这需要大量的人工工作。</p><p>NSP 作为基于深度学习的模型，其表现出来的水平较之其他深度学习模型差了很多。主要原因是它将 $S_D$ 作为输出词表的一部分，这样会有三个问题：</p><ol><li>模型无法捕捉自然语言问句中的实体与数据表中实体之间的关系；</li><li>输出词表会很大，对解码器选择正确的输出造成困扰；</li><li>如果用于验证的语句中包含训练集中没有见过的实体，模型的处理效果就会很差。</li></ol><p>而其他深度学习模型通常是将 $S_D$ 作为输入输送给模型进行处理，然后使用指针机制进行解码。这样就可以避免以上三个问题，最终的效果会好很多。</p><h3 id="6-2-2-小数据集的模型鲁棒性"><a href="#6-2-2-小数据集的模型鲁棒性" class="headerlink" title="6.2.2 小数据集的模型鲁棒性"></a>6.2.2 小数据集的模型鲁棒性</h3><p>另一方面，我们会发现 NSP 在 Patients 数据集上的效果意外得好。通过分析发现， Patients-s 数据集的训练样本是最少的，NSP 的效果好是否跟它的数据增强技术有关呢？为了验证这个猜测，我们将 Patient-s 数据集利用数据增强技术扩展到 500。对比所有深度学习模型的表现，我们发现所有模型表现都有所提升，但是 NSP 的提升还是尤为明显。通过分析我们发现可能有以下几个原因：</p><ul><li>由于其他深度模型是通过指针机制从自然问句中找出实体，如果自然问句中没有数据表中对应的实体，那么这些模型就不能生成正确的 SQL 语句了。比如错误案例中 （B）所展示的，数据表中是 “flu”，而自然问句中是 “influenza”，通过指针机制无法从自然问句中找到 “flu” 这个实体，自然就无法生成正确的语句了。而 NSP 模型是将数据表中的实体作为输出词表的一部分进行解码的，这样就可以正确找到 “flu” 这个实体了。</li><li>另外与 NSP 类似的将数据表实体作为输出词表的一部分的模型，比如 SyntaxSQLNet, GNN 和 IRNet 在小数据集上的效果也不如 NSP，主要原因是 NSP 的释义技术的应用。比如错误案例（A）中 NSP 可以准确的将 “hospitalization period” 映射到 “length_of_stay”，而另外几个模型就不能做到。</li></ul><h3 id="6-2-3-自然语言复杂性"><a href="#6-2-3-自然语言复杂性" class="headerlink" title="6.2.3 自然语言复杂性"></a>6.2.3 自然语言复杂性</h3><p>自然语言的复杂性来源于多个方面：</p><ul><li>语法多样性</li><li>领域多样性</li><li>目标 SQL 的多样性</li><li>句子数量</li></ul><p>这里我们先讨论语法多样性和领域多样性。仔细看实验结果我们会发现，几乎所有的模型在 WTQ-s 数据集上的效果都比较差。主要是因为 WTQ-s 的数据集中自然语言问句比较多样，比如错误示例（C），不仅包含两个独立的条件，而且还有否定条件。这样的句子几乎现在所有的模型都不能处理。另外，如果数据表中的列名或者常数值比较复杂，也会导致问句比较复杂。比如错误示例（D）中列名是 “<em>number of autos da fe</em>”，这里面包含了非英文词汇，而这些词在词表中属于 OOV（<em>Out of Vocabulary</em>），导致模型无法准确辨别列名。</p><h3 id="6-2-4-q-nl-与-S-D-的实体对齐"><a href="#6-2-4-q-nl-与-S-D-的实体对齐" class="headerlink" title="6.2.4 $q_{nl}$ 与 $S_D$ 的实体对齐"></a>6.2.4 $q_{nl}$ 与 $S_D$ 的实体对齐</h3><p>$q_{nl}$ 与 $S_D$ 的实体对齐对准确率的影响非常之大。在 WikiSQL 数据集上 GNN 的总体准确率是最高的，而在单项预测上 GNN 在预测列名的准确率也是最高的，因为在 GNN 模型中使用了实体链接技术。而使用标签化技术的 TypeSQL-C 和 IRNet 在列名的预测上，准确率仅次于 GNN，他们的总体准确率也是如此。</p><p>在 WTQ-s 数据集上，IRNet 的总体准确率是最高的，在单项上 $acc_{wh, op}$ 和 $acc_{agg}$ 与其他模型的准确率相比相差不多，但是在 $acc_{wh, col}$ 和 $acc_{sel}$ 上的准确率高出其他模型一大截。细致的分析我们会发现， IRNet 在长列名和表中有相似列名的时候表现尤为突出。比如错误示例（D）中，IRNet 是唯一预测正确的模型。</p><p>从以上的分析中我们不难发现，列名的准确性在很大程度上影响了模型的准确性。要提高模型预测列名的准确性，实体对齐是一个很有效的方法。而实体链接和标签化各有优势。</p><h3 id="6-2-5-学习算法的有效性"><a href="#6-2-5-学习算法的有效性" class="headerlink" title="6.2.5 学习算法的有效性"></a>6.2.5 学习算法的有效性</h3><p>在以上所有评估模型中，Seq2SQL 和 PT-MAML 分别采用了强化学习和元学习的方法。我们将这两个学习算法都改成监督学习以后发现，两个模型的准确率都有所下降。说明他们各自的学习算法都有利于发挥他们的能力，但是总体而言，强化学习和元学习在准确率上还是比其他深度学习模型准确率差一些。</p><h2 id="6-3-复杂-SQL-语句实验结果"><a href="#6-3-复杂-SQL-语句实验结果" class="headerlink" title="6.3 复杂 SQL 语句实验结果"></a>6.3 复杂 SQL 语句实验结果</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210426113745.png" alt></p><p>复杂 SQL 语句的实验只在上表中的六个模型上做的，因为其他模型都不支持 复杂 SQL 语句的生成。数据集排除了 WikiSQL，因为该数据集全部是简单 SQL 语句。总的来说，这些模型的准确率都很低。</p><h3 id="6-3-1-基于规则的模型"><a href="#6-3-1-基于规则的模型" class="headerlink" title="6.3.1 基于规则的模型"></a>6.3.1 基于规则的模型</h3><p>基于规则的模型准确率低，主要的原因和前面简单 SQL 的问题一样。但是 Templar 比较有意思，它的准确率在除 MAS 数据集之外的所有数据集上的表现和 NaLIR 相差不大。经过分析我们发现，主要是在 MAS 数据集上 Templar 总是无法正确进行实体映射。给定两个实体 $qf_1$ 和 $qf_2$，如果 $qf_2$ 在检索日志中出现的频率更高的话，Templar 选择 $qf_2$ 的概率更高，即使可能 $qf_1$ 的语义更符合 $q_{nl}$。比如错误案例（E），“area” 与 “domain.name” 的语义相似度比 “publication.title” 更高，但是 Templar 选择了后者。 </p><h3 id="6-3-2-泛化性"><a href="#6-3-2-泛化性" class="headerlink" title="6.3.2 泛化性"></a>6.3.2 泛化性</h3><p>SaI 方法总体效果强于 SaV 方法，主要原因还是输出词表的 OOV 问题。使用 SaI 方法的模型中， SyntaxSQLNet 的准确率比 GNN 和 IRNet 低很多，主要原因为后者使用了实体对齐技术。</p><h3 id="6-3-3-小数据集上的模型鲁棒性"><a href="#6-3-3-小数据集上的模型鲁棒性" class="headerlink" title="6.3.3 小数据集上的模型鲁棒性"></a>6.3.3 小数据集上的模型鲁棒性</h3><p>在小数据集上的表现和简单的 SQL 的情况类似，但是对于复杂 SQL 下，迷行表现更加糟糕。基于 SaI 的深度学习模型甚至无法训练处一个有意义的模型出来。这也从侧面说明，要想训练一个能处理复杂语句的深度学习模型需要大量的训练数据。</p><h3 id="6-3-4-生成-SQL-的覆盖面"><a href="#6-3-4-生成-SQL-的覆盖面" class="headerlink" title="6.3.4 生成 SQL 的覆盖面"></a>6.3.4 生成 SQL 的覆盖面</h3><p>SyntaxSQLNet, GNN 和 IRNet 只能支持有限的 SQL，因为：</p><ul><li>SyntaxSQLNet 采用槽填充的方法，而槽的类型是预先定义的。预定义的槽类型严重限制了 SQL 语句的覆盖面。</li><li>GNN 和 IRNet 是基于语法的模型，他们的表现比 SyntaxSQLNet 要好一些，但是也只能支持部分 SQL 语法。比如，不支持 LIMITE、ORDER BY、GROUP BY 等等。</li></ul><p>NSP 也面临着类似的问题，随着复杂度和多样性的提升 NSP 生成的 SQL 的错误率也随之提升。通过分析发现，NSP 的主要错误来源于表名和列名的预测错误，而这些错误中主要集中在需要连表查询的时候。</p><h3 id="6-3-5-自然语言问句复杂度"><a href="#6-3-5-自然语言问句复杂度" class="headerlink" title="6.3.5 自然语言问句复杂度"></a>6.3.5 自然语言问句复杂度</h3><p>$q_{sql}$  的复杂度随着 $q_{nl}$ 增加。目前所有的模型在面对复杂语句的时候准确率都非常低。</p><h3 id="6-3-6-常数值的匿名化"><a href="#6-3-6-常数值的匿名化" class="headerlink" title="6.3.6 常数值的匿名化"></a>6.3.6 常数值的匿名化</h3><p>在 Advising（question split）数据集上 NSP 模型 $acc_{sem}$ 和 $acc_{val}$ 准确率相差近 35%，这个差距非常大。而 35% 非常接近 NSP 在 WikiSQL 上对常数值的预测准确率。看下错误示例（F）我们会发现， NSP 的错误点在于将 “751” 预测错误。说明 NSP 没有正确地将 “751”匿名化。这给我们两点启示：</p><ul><li>NSP 的匿名化技术还有待提升；</li><li>精准的映射常数值也是提升模型准确率的有效手段。</li></ul><h1 id="7-NL2SQL-未来的发展"><a href="#7-NL2SQL-未来的发展" class="headerlink" title="7. NL2SQL 未来的发展"></a>7. NL2SQL 未来的发展</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210426113821.png" alt></p><p>上表总结了目前的模型能力，红色倒三角表示模型表现很差，“-” 表示一般，绿色正三角表示表现很好。从上表中我们可以看出，没有一个模型能在所有问题上达到哪怕一般般的水平。因为每个模型都这或这或那的局限性。</p><p>对于跨领域的模型适应能力，需要将数据表实体当成模型输入而不是输出，但是在特定领域下，SaV 的效果会比 SaI 好。对于数据表实体的编码研究会是一个有意思的课题。（ps：将数据表实体既当输入也当输出？）</p><p>$q_{nl}$ 和 $S_D$ 实体对齐能有效提升准确率。但是目前的对齐方法还比较基础。字符串匹配的方式比词向量相似度的方法更加可靠，因为 $S_D$ 实体中可能包含很多词向量中没有的词。但是字符串匹配的方式容易造成过拟合，从实验结果中也可以看出来，IRNet 即使使用字符串匹配的方式，在 WTQ-s 数据集上的表现仍然很低，在更复杂的语句上准确率就更低了。</p><p>如何生成常数值也是一个具有挑战性的问题。即使使用匿名化技术，NSP 在 $V_D$ 比较多的情况下，表现也不尽如人意。</p><h1 id="8-总结"><a href="#8-总结" class="headerlink" title="8. 总结"></a>8. 总结</h1><p>本文总结了 2014年-2019 年之间的 NL2SQL SOTA 模型。并综合性的做了对比实验，从实验中我们发现了一些规律：</p><ul><li>表名，列名的预测准确性很大程度上影响了生成 SQL 语句的准确性；</li><li>实体对齐技术能有效提升表名\列名的准确性；</li><li>数据表实体作为输入和输出都有其优劣性，目前还没有同时作为输入输出的模型；理论上应该会有一定的提升；</li><li>匿名化技术对预测常数值有一定的帮助。</li></ul><h1 id="9-参考资料"><a href="#9-参考资料" class="headerlink" title="9. 参考资料"></a>9. 参考资料</h1><ol><li><a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45791.pdf" target="_blank" rel="noopener">Analyza: Exploring Data with Conversation</a>，<em>Kedar Dhamdhere, Kevin S. McCurley, Ralfi Nahmias, Mukund Sundararajan, Qiqi Yan</em></li><li><a href="http://www.vldb.org/pvldb/vol13/p1737-kim.pdf" target="_blank" rel="noopener">Natural language to SQL: Where are we today?</a>，<em>Hyeonji Kim, Byeong-Hoon So, Wook-Shin Han, Hongrae Lee</em></li><li><a href="https://www.aclweb.org/anthology/P18-1034.pdf" target="_blank" rel="noopener">Semantic Parsing with Syntax- and Table-Aware SQL Generation</a>，<em>Yibo Sun, Duyu Tang, Nan Duan, Jianshu Ji, Guihong Cao, Xiaocheng Feng, Bing Qin, Ting Liu, Ming Zhou</em></li><li><a href="https://ui.adsabs.harvard.edu/abs/2020E3SWC.22901039M/abstract" target="_blank" rel="noopener">The history and recent advances of Natural Language Interfaces for Databases Querying</a>，<em>Majhadi, Khadija; Machkour, Mustapha</em></li><li><a href="http://blog.csuldw.com/2019/10/20/2019-10-20-nl2sql-introduction/" target="_blank" rel="noopener">NL2SQL概述：一文了解NL2SQL</a></li><li><a href="https://zhuanlan.zhihu.com/p/93132638" target="_blank" rel="noopener">试试中国人自己的Spider吧！</a></li><li><a href="http://jkk.name/text2sql-data/" target="_blank" rel="noopener">Text-to-SQL Datasets and Baselines</a></li><li><a href="https://github.com/salesforce/WikiSQL" target="_blank" rel="noopener">WIKISQL</a></li><li><a href="https://yale-lily.github.io/spider" target="_blank" rel="noopener">Spider</a></li><li><a href="https://taolusi.github.io/CSpider-explorer/" target="_blank" rel="noopener">CSpider</a></li><li><a href="https://www.aclweb.org/anthology/P18-1033.pdf" target="_blank" rel="noopener">Improving Text-to-SQL Evaluation Methodology</a>，<em>Catherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, Dragomir Radev</em></li><li><a href="http://dbgroup.eecs.umich.edu/files/SIGMOD14LFb.pdf" target="_blank" rel="noopener">NaLIR: An Interactive Natural Language Interface for Querying Relational Databases</a>，<em>Fei Li，H. V. Jagadish</em> </li><li><a href="https://dl.acm.org/doi/10.1145/3180155.3180202" target="_blank" rel="noopener">Search-based test data generation for SQL queries</a>，<em>Jeroen Castelein, Maurício Aniche, Mozhan Soltani, Annibale Panichella, Arie van Deursen</em> </li></ol>]]></content>
      
      
      <categories>
          
          <category> NL2SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nl2sql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐系统发展里程碑</title>
      <link href="/2021/04/13/recsys-milestone/"/>
      <url>/2021/04/13/recsys-milestone/</url>
      
        <content type="html"><![CDATA[<blockquote><p>出处见水印</p></blockquote><a id="more"></a><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/6d1b7657gy1gg9ddr1pm8j20u04f6wwm.jpg" alt></p>]]></content>
      
      
      <categories>
          
          <category> 博客转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 推荐系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构与算法：算法简介</title>
      <link href="/2021/04/12/ds-introduction/"/>
      <url>/2021/04/12/ds-introduction/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png" alt></p><p>在开始学习算法之前先说一些废话。</p><h2 id="1-一个算法拯救无数生命"><a href="#1-一个算法拯救无数生命" class="headerlink" title="1. 一个算法拯救无数生命"></a>1. 一个算法拯救无数生命</h2><p>第二次世界大战期间，德军使用 <a href="https://en.wikipedia.org/wiki/AM_broadcasting" target="_blank" rel="noopener">AM</a> 进行信息交流，任何掌握对应 AM 频率和摩斯码的人都可以对信号进行解码得到信息。但是由于信息是被加密的，所以需要对信息进行解密。有时候人们很幸运能够猜对，但是很快德军又换了密码。</p><a id="more"></a><p>阿兰·图灵加入英军，帮助他们对德军密码进行破译。最初他们建造了一台机器进行计算，但是由于计算过于耗时，所以最开始这台计算机并没有多大用处。后来，图灵改变了原来的算法使得解码速度大幅提升。在后来的战争中，这台机器帮助英军破译了大量的德军密码，大大加速了战争的结束。</p><p><em>同一台机器从一个没什么用的废品，摇身一变成为拯救万民于水火的救世主，这就是算法的力量。</em></p><p>另一个很有说服力的例子是 “<em>PageRank</em>” 算法。<code>PageRank</code> 帮助谷歌从一众搜索引擎中脱颖而出，成为行业领军者，因为 <code>PageRank</code> 能够得到更好的搜索结果。</p><h2 id="2-什么是算法？"><a href="#2-什么是算法？" class="headerlink" title="2. 什么是算法？"></a>2. 什么是算法？</h2><p>算法就是计算机在完成任务过程中执行的步骤。就像人做菜，按照菜谱的步骤一步一步把各种调料和菜品烹饪成一道可口的菜，这个菜谱就是人在炒菜的时候的算法。对于计算机来说，算法就是一系列用于完成特定任务的指令。算法接收一系列输入，然后得到我们想要的输出。比如：</p><blockquote><p>两个数字相加算法：</p><ul><li>输入两个数字</li><li>使用 <code>+</code> 运算符对两个数字进行相加</li><li>输出结果</li></ul></blockquote><p>算法也有“好的”算法和“不好的”算法，就像菜有好吃和不好吃一样。好的算法速度更快，不好的算法更慢。如果算法运算太慢意味着更高的成本，甚至在一些任务上是无法完成运算的（相对人的生命）。</p><h1 id="3-为什么学习算法？"><a href="#3-为什么学习算法？" class="headerlink" title="3. 为什么学习算法？"></a>3. 为什么学习算法？</h1><p>计算机程序最宝贵的两种资源是时间和空间（内存）。</p><h2 id="3-1-时间和空间（内存）都是有限的"><a href="#3-1-时间和空间（内存）都是有限的" class="headerlink" title="3.1 时间和空间（内存）都是有限的"></a>3.1 时间和空间（内存）都是有限的</h2><p>程序运行时间用下式计算：</p><script type="math/tex; mode=display">t = t_e \times n</script><p>其中 $n$ 表示指令数目，$t_e$ 表示运行每条指令需要的时间。$n$ 依赖于你编写的程序，而 $t_e$ 依赖于计算机硬件。</p><p>假设我们要计算 $1-10^{11}$ 的自然数之和。如果我们逐个数字相加，则需要 $10^{11}$ 次取值和 $10^{11}$ 次相加，即 $2\times 10^{11}$ 次计算。假设计算机每秒计算 $10^8$ 次，要计算完这个程序，仍需 16 分钟。</p><p>有没有什么办法帮助提高运算效率？我们中学就学过一个等差数列求和公式：</p><script type="math/tex; mode=display">\text{sum} = \frac{n\times (n+1)}{2}</script><p>利用这个公式，我们只需要执行一次指令就可完成运算。</p><p>同样计算机内存并不是无限的，当你需要处理或者存储大量数据的时候，处理算法就需要考虑如何节省内存的使用。</p><h2 id="3-2-可扩展性"><a href="#3-2-可扩展性" class="headerlink" title="3.2 可扩展性"></a>3.2 可扩展性</h2><p>可扩展性意味着算法或系统处理更大规模的问题的能力。</p><p>假设我们现在要建一间 50 人的教室，最简单的方法是订一个房间，放上一个黑板，几支粉笔，几张桌子和椅子就好了。如果学生是 200 人呢？我们还可以用上面的方法，只是需要用到更大的房间，更多的桌椅。但是如果学生是 1000 人呢？我们就找不到足够大的房间和足够多的的桌椅了。这个时候原来的方法就失效了。</p><p>就像前面的例子，最初我们计算实数和的方法就是不可扩展的，因为随着数据量增大，需要消耗的时间已经超出了我们所能忍受的范围。</p><p>而第二种方法就是可扩展的，因为无论数据量有多大，我们都能在一次指令下完成计算。</p><h1 id="4-结语"><a href="#4-结语" class="headerlink" title="4. 结语"></a>4. 结语</h1><p>软件系统每天都会诞生许多新的方法和技术，而算法更像是其中的灵魂。当一个系统遇到瓶颈的时候，很可能通过优化其中的算法使整个系统性能得到提升。</p><p>但是同时也要注意，改善算法提升系统的可扩展性并不是唯一的方法。比如我们还可以通过分布式计算来实现。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.programiz.com/dsa/why-algorithms" target="_blank" rel="noopener">Why Learn Data Structures and Algorithms?</a></p>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>预训练语言模型-神经网络语言模型：CNNLM</title>
      <link href="/2021/04/11/neural-language-model-cnn/"/>
      <url>/2021/04/11/neural-language-model-cnn/</url>
      
        <content type="html"><![CDATA[<p>神经网络三大神器：DNN、CNN、RNN。其中 DNN 和 RNN 都已经被用来构建语言模型了，而 CNN 一直在图像领域大展神威，它是否也可以用来构建语言模型呢？如果要用 CNN 构建语言模型应该怎么做？接下来我们从四篇论文看 CNN 构建语言模型的三种方法。</p><a id="more"></a><h1 id="1-CNN-语言模型"><a href="#1-CNN-语言模型" class="headerlink" title="1. CNN 语言模型"></a>1. CNN 语言模型</h1><p>简单的 CNN 语言建模思路如下：</p><ol><li>首先输入的词经过 embedding 层，将每个词转化成句子矩阵 $x_{1:n} \in \mathbb{R}^{n \times k}$，其中 $n$ 表示句子长度，$k$ 表示词向量维度；</li><li>得到句子矩阵以后，用 $W \in \mathbb{R}^{w\times k}$ 的卷积核对句子矩阵进行卷积运算，一共有 $k$ 个卷积核；</li><li>经过卷积之后得到一个新的矩阵，然后在新的矩阵上使用 <code>ReLu​</code> 激活函数；</li><li>接下来使用 <em>batch normalization</em>，为了解决内部协方差飘移问题；</li><li>然后经过一个 DNN 层进行降维之后直接使用 <em>softmax</em> 层输出，而不经过最大池化层，因为最大池化层会使句子丢失位置信息。</li></ol><p>以上就是最基本的 CNN 语言模型的结构。除此之外，还有几个变种：</p><ul><li><p><strong>MLPConv</strong></p><p>标准的 CNN 卷积操作是利用卷积核和特征矩阵进行线性变换。而 <em>MLPConv</em> 是在标准的卷积核后面接一个多层的 DNN 将原来的线性变换转化成非线性变换。需要注意的是，<em>MLPConv</em> 同样没有加入最大池化层。</p></li><li><p><strong>Multi-Layer CNN</strong></p><p>将多个同尺寸卷积核的卷积叠加在一起，如下图左侧。</p></li><li><p><strong>COM</strong></p><p>将不同尺寸的卷积核的卷积拼接在一起，如下图右侧。</p></li></ul><table><tr>    <td><div align="center"><img width="400" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210411162823.png"></div></td>    <td><div align="center"><img width="400" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210411162837.png"></div></td></tr></table>            <h1 id="2-Gated-CNN-语言模型"><a href="#2-Gated-CNN-语言模型" class="headerlink" title="2. Gated-CNN 语言模型"></a>2. Gated-CNN 语言模型</h1><h2 id="2-1-gen-CNN"><a href="#2-1-gen-CNN" class="headerlink" title="2.1 $gen$CNN"></a>2.1 $gen$CNN</h2><p>下图展示了 $gen$CNN 模型的大致结构：</p><ul><li>$\beta$CNN 用来记忆序列中较早之前的信息；</li><li>$\alpha$CNN 用来处理距离需要预测的词最近的部分序列。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210411163002.png" width="600"></p><p>相对于传统 CNN，$gen$CNN 有两点不同：</p><ol><li>部分权重共享策略；</li><li>用门控网络代替池化层。</li></ol><p>接下来我们详细介绍一下模型的每一个部分。</p><h3 id="2-1-1-alpha-CNN-卷积"><a href="#2-1-1-alpha-CNN-卷积" class="headerlink" title="2.1.1 $\alpha$CNN: 卷积"></a>2.1.1 $\alpha$CNN: 卷积</h3><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210411163024.png" width="350"></p><p>$\alpha$CNN 是部分共享权重的，在卷积单元中存在两种特征：<code>Time-Flow</code> 和 <code>Time-Arrow</code>，分别对应上图中的空心节点和实心节点。</p><ul><li><code>Time-Flow</code>：就是传统的 CNN，用来理解句子的整体时态结构；</li><li><code>Time-Arrow</code>：更像是基于 <em>FFNN</em> 的语言模型，用来理解句子的顺序。</li></ul><p>假设输入句子 $\pmb{x}=(x_1, \cdots, x_T)$，第 $l$ 层的特征为：</p><script type="math/tex; mode=display">z_i^{(l, f)}(\pmb{x}) = \begin{cases}\sigma(W_{TF}^{(l, f)} \pmb{\hat{z}}_i^{(l-1)} + b_{TF}^{(l, f)}) & f\in \mathrm{Time-Flow}\\\\\sigma(W_{TF}^{(l, f, i)} \pmb{\hat{z}}_i^{(l-1)} + b_{TF}^{(l, f, i)}) & f \in \mathrm{Time-Arrow}\end{cases}</script><p>其中：</p><ul><li>$z_i^{(l, f)}$ 表示第 $l$ 层网络第 $i$ 个位置的特征输出；</li><li>$\sigma(\cdot)$ 表示 <code>sigmoid</code> 函数或者 <code>Relu</code> 函数；</li><li>$W_{TF}^{(l, f)}$ 表示第 $l$  层网络，$f\in \mathrm{Time-Flow}$ 的参数；$W_{TA}^{(l, f, i)}$ 表示第 $l$ 层网络第 $i$ 个位置，$f\in \mathrm{Time-Arrow}$ 的参数；</li><li>$\pmb{\hat{z}}_i^{(l-1)}$ 表示第 $l-1$ 层第 $i$ 个位置的特征，令 $\pmb{\hat{z}}_i^{(0)}=[x_i^T, x_{i+1}^T, \cdots,x_{i+k-1}^T]$，$k$ 表示窗口大小。</li></ul><h3 id="2-1-2-门控网络"><a href="#2-1-2-门控网络" class="headerlink" title="2.1.2 门控网络"></a>2.1.2 门控网络</h3><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210411163036.png" width="400"></p><p>假设有一个第 $l$ 层网络的特征矩阵，然后再第 $l+1$ 层上加门控（窗口大小为 2）。门控网络定义是一个二分类逻辑回归器：</p><script type="math/tex; mode=display">g_j^{(l+1, f)} = \begin{cases}\frac{1}{1+\exp(-W_{gate}^{(l, f, j)}\cdot\bar{\pmb{z}}_j^{(l)})} & f \in \mathrm{Time-Arrow} \\\\\frac{1}{1+\exp(-W_{gate}^{(l, f)}\cdot\bar{\pmb{z}}_j^{(l)})} & f \in \mathrm{Time-Flow} \end{cases}</script><p>其中 $\bar{\pmb{z}}_j$ 表示第 $j$ 个窗口中将 $\hat{\pmb{z}}_{2j-1}^{(l)}$ 和 $\hat{\pmb{z}}_{2j}^{(l)}$ 融合在一起的矩阵。</p><p>然后将得到的值与特征矩阵进行加权求和：</p><script type="math/tex; mode=display">z_j^{(l+1, f)} = g_j^{(l+1, f)}\cdot z_{2j-1}^{(l, f)} + (1-g_j^{(l+1, f)}) \cdot z_{2j}^{(j, f)}</script><p>这样就得到了 $l+1$ 层的特征矩阵。</p><blockquote><ol><li>窗口为 2，是因为网络包含两种特征 <code>Time-Flow</code> 和 <code>Time-Arrow</code>，每个窗口位置 $j$ 都包含两个窗口，所以用 $2j-1$ 和 $2j$  表示；</li><li>门控网络实际上是代替池化层做特征筛选，将卷积后的特征矩阵利用门控网络筛选出第 $j$ 个位置的特征到底是更偏向于结构信息还是更偏向于位置信息。</li></ol></blockquote><h3 id="2-1-3-循环结构"><a href="#2-1-3-循环结构" class="headerlink" title="2.1.3 循环结构"></a>2.1.3 循环结构</h3><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210411163052.png" width="400"></p><p>就像之前介绍的那样，我们处理使用 $\alpha$CNN 来获取当前信息以外，还会使用 $\beta$CNN 来记录历史信息。$\beta$CNN 只包含 <code>Time-Flow</code> 特征，得到 $\beta$CNN 特征之后将它作为 $\alpha$CNN 输入的第一个词输入给 $\alpha$CNN。所有的 $\beta$CNN 都是相同且循环对齐的，使得 $gen$CNN 能处理任意长度的句子。每个 $\beta$CNN 后面加一个特殊的开关，当没有历史信息的时候就会把开关关掉。</p><p>实验中 $\alpha$CNN 处理的句子长度为 $L_\alpha=30\sim40$，也就是说，如果句子长度短于 $L_\alpha$ 的话，那么模型就只包含了 $\alpha$CNN。实验中发现，90% 以上的句子可以只用 $\alpha$CNN 就可以， 超过 99% 的句子只需要一个 $\beta$CNN 就可以。实际上作者发现一个更大更深的 $\alpha$CNN 比 更小的 $\alpha$CNN 更多的 $\beta$CNN 结构表现更好。</p><h2 id="2-2-Gated-CNN"><a href="#2-2-Gated-CNN" class="headerlink" title="2.2 Gated-CNN"></a>2.2 Gated-CNN</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210411162913.png" width="400"></p><p>上图是 Gated CNN 模型的基本结构，大体上和传统的 CNN 语言模型区别不大，只是在 卷积之后加了一个门控机制：</p><script type="math/tex; mode=display">h_l(\pmb{x}) = (\pmb{x}\cdot W +b) \otimes \sigma(\pmb{x} \cdot V + c)</script><p>其中 $\pmb{x} \in \mathbb{R}^{N\times m}$ 表示词向量组成的句子矩阵或者上一层网络的输出，$W \in \mathbb{R}^{k\times m\times n}$,  $b \in \mathbb{R}^n$, $V \in \mathbb{R}^{k\times m\times n}$, $c \in \mathbb{R}^{n}$ 表示模型参数，$\sigma$ 为 <em>sigmoid</em> 函数，$\otimes$ 表示元素级相乘。</p><p>其实这就相当于决定每个特征值有多少信息可以进入下一层网络，和 LSTM 中的遗忘门作用相同。</p><h2 id="2-3-小结"><a href="#2-3-小结" class="headerlink" title="2.3 小结"></a>2.3 小结</h2><ul><li>从 LSTM 出发到门控 CNN，我们会发现所谓门控其实就是通过 <em>sigmoid</em> 函数控制特征的传递。</li><li>CNN 语言建模的关键在于不要使用池化层，因为池化层会丢失位置信息。</li></ul><h1 id="3-TCN-语言模型"><a href="#3-TCN-语言模型" class="headerlink" title="3. TCN 语言模型"></a>3. TCN 语言模型</h1><p>TCN 模型即所谓的 <em>Temporal Convolution Networks</em>，该模型基于两个基本原则：</p><ul><li>网络输出一个和输入序列等长度的序列；</li><li>不会发生从未来到过去的信息泄露。</li></ul><p>为了达到第一点原则，TCN 使用 1D 全连接卷积网络：每一个隐层的长度和输入层相同，并且使用零填充保证后续的隐层也能保持和之前序列长度相同。</p><p>为了达到第二点目的，TCN 使用因果卷积（<em>causal convolutions</em>, 如下左图），即 $t$ 时刻的输出仅为 $t$ 时刻及之前的序列的卷积结果。</p><script type="math/tex; mode=display">\mathrm{TCN} = 1D\ \mathrm{FCN} + \mathrm{causal\ convolutions}</script><h2 id="3-1-Dilated-causal-convolution"><a href="#3-1-Dilated-causal-convolution" class="headerlink" title="3.1 Dilated causal convolution"></a>3.1 Dilated causal convolution</h2><p>为了使模型能够足够深，且能处理足够长的序列，TCN 没有使用标准的卷积网络，而是使用的空洞卷积（<em>dilated causal convolution</em>，如下右图）。</p><p><table><tr>    <td><div align="center"><img width="400" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210411163423.png"></div></td>    <td><div align="center"><img width="600" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210411163437.png"></div></td></tr></table><br>1D 空洞卷积的定义如下：</p><p>输入序列 $\pmb{x} \in \mathbb{R}^{n}$，卷积核 $f:\{0, \cdots, k-1\} \rightarrow \mathbb{R}$，在序列元素 $s$ 上的空洞卷积操作 $F$ 定义如下：</p><script type="math/tex; mode=display">F(s) = \sum_{i=0}^{k-1}f(i)\cdot \pmb{x}_s +d\cdot i</script><p>其中 $d$ 是空洞步长（或者空洞卷积的扩张率），$k$ 是卷积核尺寸。</p><p>使用空洞卷积可以使得上层的节点感受野更大，这样也就可以引入更多的历史信息。一般 $d$ 随着层数的指数增加，即 $d=O(2^i)$，其中 $i$ 为层数。每层计算卷积时相隔 $d-1$ 个位置。</p><p>TCN 的视野取决于网络深度，卷积核大小和空洞卷积的步长。比如，如果我们需要依赖前 $2^{12}$ 个历史信息来预测下一个词的话，就需要至少 12 层网络。为了防止梯度消失，TCN 还使用了残差网路。</p><h2 id="3-2-Residual-connection"><a href="#3-2-Residual-connection" class="headerlink" title="3.2 Residual connection"></a>3.2 Residual connection</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210411163452.png" width="600"></p><p>如上图所示，残差网络是训练深度模型常用的技巧，它使得网络可以以跨层的方式传递信息。TCN 构建了一个残差块来代替一层的卷积。一个残差包含两层的卷积和非线性映射，在每层中还加入了 <code>WeightNorm</code> 和 <code>Dropout</code> 来正则化网络。</p><h2 id="3-3-TCN-的优点"><a href="#3-3-TCN-的优点" class="headerlink" title="3.3 TCN 的优点"></a>3.3 TCN 的优点</h2><ol><li><p>并行性。当给定一个句子时，TCN可以将句子并行的处理，而不需要像RNN那样顺序的处理。</p></li><li><p>灵活的感受野。TCN的感受野的大小受层数、卷积核大小、扩张系数等决定。可以根据不同的任务不同的特性灵活定制。</p></li><li><p>稳定的梯度。RNN经常存在梯度消失和梯度爆炸的问题，这主要是由不同时间段上共用参数导致的，和传统卷积神经网络一样，TCN不太存在梯度消失和爆炸问题。</p></li><li><p>内存更低。RNN在使用时需要将每步的信息都保存下来，这会占据大量的内存，TCN在一层里面卷积核是共享的，内存使用更低。</p></li></ol><h2 id="3-4-TCN-的缺点"><a href="#3-4-TCN-的缺点" class="headerlink" title="3.4 TCN 的缺点"></a>3.4 TCN 的缺点</h2><ol><li>TCN 在迁移学习方面可能没有那么强的适应能力。这是因为在不同的领域，模型预测所需要的历史信息量可能是不同的。因此，在将一个模型从一个对记忆信息需求量少的问题迁移到一个需要更长记忆的问题上时，TCN 可能会表现得很差，因为其感受野不够大。</li><li>论文中描述的TCN还是一种单向的结构，在语音识别和语音合成等任务上，纯单向的结构还是相当有用的。但是在文本中大多使用双向的结构，当然将TCN也很容易扩展成双向的结构，不使用因果卷积，使用传统的卷积结构即可。</li><li>TCN毕竟是卷积神经网络的变种，虽然使用扩展卷积可以扩大感受野，但是仍然受到限制，相比 Transformer那种可以任意长度的相关信息都可以抓取到的特性还是差了点。TCN在文本中的应用还有待检验。</li></ol><h1 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h1><ul><li>用 CNN 进行序列建模不要使用池化层，因为池化层会丢失位置信息；</li><li>门控 CNN 通常是用 <em>sigmoid</em> 函数进行信息控制；</li><li>空洞卷积可以扩展 CNN 的视野；</li><li>残差连接可以训练更深的网络。</li></ul><h1 id="5-Reference"><a href="#5-Reference" class="headerlink" title="5. Reference"></a>5. Reference</h1><ol><li><p><a href="https://www.aclweb.org/anthology/D16-1123.pdf" target="_blank" rel="noopener">Convolutional Neural Network Language Models</a>, <em>Ngoc-Quan Pham and German Kruszewski and Gemma Boleda</em></p></li><li><p><a href="http://www.hangli-hl.com/uploads/3/4/4/6/34465961/wang_et_al._acl_2015.pdf" target="_blank" rel="noopener">A Convolutional Architecture for Word Sequence Prediction</a>, <em>Mingxuan Wang, Zhengdong Lu, Hang Li, Wenbin Jiang, Qun Liu</em></p></li><li><p><a href="https://arxiv.org/pdf/1612.08083v1.pdf" target="_blank" rel="noopener">Language Modeling with Gated Convolutional Networks</a>, <em>Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier</em></p></li><li><p><a href="https://openreview.net/pdf?id=rk8wKk-R-" target="_blank" rel="noopener">Convolutional Sequence Modelling Revisited</a>, <em>Shaojie Bai, J. Zico Kolter, Vladlen Koltun</em></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 语言模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Language Model </tag>
            
            <tag> CNNLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Text-to-Viz：根据语言描述自动创建信息图表</title>
      <link href="/2021/04/09/text2viz/"/>
      <url>/2021/04/09/text2viz/</url>
      
        <content type="html"><![CDATA[<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>市面上的专业创建信息图表的工具虽然在努力平衡易用性和功能强大，但是这些工具通常是面向高级用户的，比如设计师、数据科学家等等。对于普通用户非常不友好。</p><a id="more"></a><p>普通用户通常有以下几个特点：</p><ol><li>他们偶尔才会创建信息图表，对各种图表工具并不熟悉；</li><li>他们的目标不是设计美观、复杂、令人印象深刻的图表，而是需要一个专业、高效、能够准确说明问题的图表；</li><li>他们几乎没有设计经验，不知道如何从头开始设计图表。但是，给他们一些好的模板，他们能快速挑选符合心意的图表。</li></ol><p>为了解决这类用户的需求，我们希望能够从自然语言描述中自动创建图表。为了达成这一目标，我们需要解决两个主要问题：</p><ol><li>准确理解给定的语句并从中提取适当的信息；</li><li>利用提取出的信息创建图表。</li></ol><p>为了解决以上两个问题，我们使用的技术方案是：</p><ul><li><p>信息抽取解决方案</p><ol><li>收集大量的真实用户数据；</li><li>对收集到的数据进行人工标注（序列标注）；</li><li>利用标注数据训练一个基于 CRF 算法的模型（NER 模型）。</li></ol></li><li><p>图表设计解决方案</p><p>利用从互联网上收集到的图表模板，分析其设计空间，提出了一套系统的设计方案。</p></li></ul><p>本文提出的方法使用范围：适用于比例相关的语言描述，比如 “中国 2020 年 GDP 涨幅为 6%。”</p><h1 id="2-图表综述"><a href="#2-图表综述" class="headerlink" title="2. 图表综述"></a>2. 图表综述</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210318104323320.png" alt></p><p>一个合法图表单元应该具备以下四个特征：</p><ul><li>至少能传达一条信息；</li><li>至少包含一个图形元素；</li><li>在视觉和语义上保持完整性和连贯性；</li><li>无法被拆分成更小的满足上面三个条件的单元。</li></ul><p>上图中每个蓝色方框中表示一个图表单元。</p><p>根据从互联网上收集到的图表，我们将图表分成四类：</p><ul><li>基于统计的图表：这是最主要的图表类型，这类图表通常包含，水平/垂直的柱状图、饼状图、甜甜圈图等；</li><li>基于时间线的图表：这类图表用来表示事件发展信息，通常包含，时间线、表格等；</li><li>基于过程的图表：这类图表用来告诉读者如何一步一步达到特定的目标，这类图表通常用在食谱或者操作手册上；</li><li>基于位置的图表：这类图表通常包含一个地图，地图上有一些标志、箭头、图例等信息。</li></ul><p>在进一步分析基于统计的图表时，我们又将基于统计的图表细分成四个字类：</p><ul><li><p>比例图表：用于表示某一部分占据总量的比例。通常在文本描述中会包含 “$n\%$”、“$m/n$”、“$m$ 分之 $n$”、“百分之 $m$” 等表述。这类图通常时柱状图、饼状图、甜甜圈图等。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210318101045374.png" alt></p></li><li><p>数量图表：用于表示总量，比如收入、人口、速度等。这类图表通常不会使用饼状图、甜甜圈图，而是使用横向/纵向柱状图、象形图（pictographs chart）等;</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210318100525489.png" alt></p></li><li><p>变化图表：虽然变化图表通常可以用比例图标或者数量图表来表示，但是实际上变化图表和之前两种图表并不相同。变化图表的文本描述中通常会包含 “增长”、“下降” 等词汇，而图表通常会有不同的颜色、图形等的一一对比；</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210318100507951.png" alt></p></li><li><p>排名图表：这类图表中通常会包含星星、奖牌、奖杯等图像，而文本描述中通常会出现序数词，“第一名”、“前十名”等等。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210318100547350.png" alt></p></li></ul><p>对于基于统计的图表，通常一个图表只包含一个信息，但是有些图表在一个单元内包含多个信息。根据包含的不同信息，我们将图表分成四类：</p><ul><li>单实例图表：这种是最简单的</li><li>合成图表：这类图标指的是用多个子图表形成一个完整的图表。比如文字描述 “中国2013年的总人口是14亿，较 2012 年的 13.5 亿增长了 10%。” 这段文字中的粟裕偶数字都是用来描述中国总人口的。</li><li>对比图表：多个实例进行对比，通常会使用颜色、图标、尺寸、形状等进行区分；</li><li>累积图表：这种图表有点类似于对比图表，但是不同的是，不同的主体会形成一个较大的整体。比如 “中国生产了美国大选所需的所有美国国旗，其中义乌生产了 70%，上海生产了 30%。”</li></ul><h1 id="3-比例相关的图表"><a href="#3-比例相关的图表" class="headerlink" title="3. 比例相关的图表"></a>3. 比例相关的图表</h1><h2 id="3-1-文字描述"><a href="#3-1-文字描述" class="headerlink" title="3.1 文字描述"></a>3.1 文字描述</h2><ul><li>从网上爬取了 10 万 PPT 数据；</li><li>利用规则从 10万 PPT 数据中抽取 5562 条描述语句；</li><li>主要规则包含： “$n\%$”、“$m/n$”、“$m$ 分之 $n$”、“百分之 $m$” 等。需要注意的是，包含这些关键词的句子并不一定是比例图标，比如 “公司营收增长了 50%。”显然应该属于变化图表；</li><li>从 5562 条规则抽取的文本中，经过人工筛选得到 800 条比例图表相关语句；</li><li>我们对比了 PPT 里面的语句和文章当中的语句，我们发现PPT里面的句子更加精炼，因此，没有在训练数据中添加文章句子。</li></ul><h2 id="3-2-图表可视化"><a href="#3-2-图表可视化" class="headerlink" title="3.2 图表可视化"></a>3.2 图表可视化</h2><p>图表可视化设计的四要素：<strong>布局、描述、图形、颜色</strong>。</p><ul><li><p><strong>布局</strong></p><p>图表布局分两种：</p><ol><li><p>单实例（最常见）。单实例的图表通常包含一个图形和一段描述性的语言。这样的图表通常采用的布局是网格布局，便于图形和描述性文字的对齐。</p></li><li><p>多实例。将多个实例组合成一个图表通常采用四种策略：</p><ul><li>并排（side-by-side）。为了表示对比、层级或者其他逻辑关系，多个实例通常采用网格布局，包括平行、并列、循环、层级和堆积等（下图a）。</li><li>共享轴（share-axes）。这种布局通常是将多个实例进行对齐，然后放到一个通用的坐标系统下。通常用在带坐标轴的统计图表里，比如条形图、散点图等（下图b）。</li><li>共享中心（share-center）。这种布局将多个数字实例以同一个圆心排列成圆形或者扇形。通常用在饼状图、夜莺玫瑰走势图（Nightingale rose charts）等上（下图c）。</li><li>共享文本（share-context）。这种布局将图表上不同的区域使用相同的注释加以说明。通常用在注释图表中，将注释放在公共空白区域（下图a）。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210318100626046.png" alt></p></li></ol></li><li><p><strong>文本描述</strong></p><p>前面我们对比例图表的定义是，用来表示某一部分占据总数的比例。那么在比例图表的文本描述中就应该包含三个关键信息：<strong>数值、局部、整体</strong>，实际上图表设计师通常会在数值前面加上一个修饰词，比如 “大于”、“小于” 等。除了这四种类型（包括修饰词）的描述，我们还发现了下面一些通用形式的描述：</p><ol><li>完整描述。</li><li>在描述中数字被省略了。比如，“40 percent of USA fresh water is used for agriculture” 中的 “40 percent” 被省略。（ps: 我没有找到合适的中文例子）</li><li>局部是一个动词词组。比如，“65% 的咖啡是在早餐中消费掉的”， 其中 “消费掉” 作为局部，是一个动词词组。</li><li>数值-整体组合。比如，“65% 的咖啡是在早餐中消费掉的” 中的 “65% 的咖啡” 就是一个数值-整体组合。</li><li>文本描述以数值为界被分割开了。比如，“中国只有”，“10%”，“的人有本科学历”（ps: 同样没找到比较合适的中文例子）。</li></ol></li><li><p><strong>图形</strong></p><p>图形部分涉及两点：图形的选择和组合。图形设计主要考虑两点：</p><ol><li>图形应该与原始的文本描述具有语义相关性；</li><li>图形的不同元素应该具有不同的作用。（ps: 不同的元素表示相同的信息的话容易造成混淆）</li></ol><p>根据已有的比例图表，我们将最常使用的图形分成七大类：象形图（下图a）、装饰图（下图d）、圈图（下图c）、饼状图（下图o）、条形图（下图i）、填充图标（下图e）、缩放图标（下图g）。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210318104638047.png" alt></p></li><li><p><strong>颜色</strong></p><p>颜色设计的通用规则是：</p><ol><li>前景和背景颜色：背景颜色占据绝大多数区域；前景颜色更加多样；不同文本框的文本描述使用相同的颜色；图形元素可以使用一种或者多种颜色，视情况而定。比如，象形图、饼状图等至少需要两种颜色，而装饰图标只需要一种颜色。</li><li>数字高亮：常见的高亮方法包括尺寸、颜色和字体。</li></ol></li></ul><h1 id="4-Text-to-Viz-技术实现方案"><a href="#4-Text-to-Viz-技术实现方案" class="headerlink" title="4. Text-to-Viz 技术实现方案"></a>4. Text-to-Viz 技术实现方案</h1><p>系统包括两个主要模块：文本分析模块和可视化生成器模块。首先用户输入一段文本，然后文本分析模块对文本进行分析，从中抽取 <strong>修饰词、数字、局部、整体</strong> 等信息。然后将原始文本和从文本中抽取出的信息一同输入给可视化生成器模块。可视化生成器模块生成或者选取合适的 <strong>布局、描述、图形、颜色</strong>，然后将这些元素组合成一系列可用的图表。对生成的图表进行评估和打分，根据打分的高低进行排序，呈现给用户，让用户进行选择。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210318111739190.png" alt></p><h2 id="4-1-文本分析模块"><a href="#4-1-文本分析模块" class="headerlink" title="4.1 文本分析模块"></a>4.1 文本分析模块</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210318112525492.png" alt></p><p>预定义四种实体：修饰词（<strong>M</strong>odifier）、数字（<strong>N</strong>umber）、局部（<strong>P</strong>art）、整体（<strong>W</strong>hole）（ps: B-M、I-M、B-N、I-N、B-P、I-P、B-W、I-W、O一共需要九个输出）。利用命名实体识别模型对这四种实体进行识别抽取。本文使用的命名实体识别模型是 CNN+CRF 模型。主要包括一下三步：</p><ul><li>分词：“中国 GDP 上涨 6%。” -&gt; [”中国“，”GPD“，”上涨“，”6%“，”。“]</li><li>特征化：将分词后的结果映射成词向量序列</li><li>CNN+CRF：一维 CNN，$kernel_size=(9\times m)$，本文中 $m=59$。</li></ul><p>训练结果：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210318113057290.png" alt></p><h2 id="4-2-可视化生成模块"><a href="#4-2-可视化生成模块" class="headerlink" title="4.2 可视化生成模块"></a>4.2 可视化生成模块</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210318114310942.png" alt></p><h3 id="4-2-1-布局"><a href="#4-2-1-布局" class="headerlink" title="4.2.1 布局"></a>4.2.1 布局</h3><p>布局设计需要解决两个问题：</p><ol><li>如何分割区域？</li><li>每个区域需要填充什么样的信息？</li></ol><p>以上图为例，我们可以通过人为设计规则来解决以上两个问题：</p><ol><li>如果用户的文本描述是以数值开头的，那么我们将区域分成上图 $2\times 1$ 的形式；</li><li>区域1 填充象形图、图标、饼状图等；</li><li>区域2 填充文本描述中的数字信息；</li><li>区域3 填充删除数字信息后的用户输入文本</li></ol><p>我们通过分析收集到的数据，预先定义了 20 套布局模板作为项目启动数据。然后针对每一套布局模板都制定了相关的规则。</p><h3 id="4-2-2-描述"><a href="#4-2-2-描述" class="headerlink" title="4.2.2 描述"></a>4.2.2 描述</h3><p>基本的描述信息通过文本分析模块已经得到了，剩下一些补充描述我们利用 Stanford Parser 对用户文本进行语法分析得到一个语法树。然后根据不同的布局，从语法树中抽取不同的内容。</p><h3 id="4-2-3-图形"><a href="#4-2-3-图形" class="headerlink" title="4.2.3 图形"></a>4.2.3 图形</h3><p>在我们的实现中，我们构建了一个图形库，包含 100 个图形，每个图形都人为添加一个或多个关键词用来表示该图形的语义信息。然后将局部和整体的内容进行行分词、删除停用词，然后用 word2vec 进行词义匹配，找个最合适图形。</p><h3 id="4-2-4-颜色"><a href="#4-2-4-颜色" class="headerlink" title="4.2.4 颜色"></a>4.2.4 颜色</h3><p>颜色生成考虑两方面：</p><ol><li>所有选用的颜色组合起来应该是协调的；</li><li>如果搭配的颜色与文本描述的内容相搭就更好了，比如文本描述保护环境，我们选择绿色和蓝色。</li></ol><p>在我们的颜色生成模块中，同样是人为构建了一套颜色系统，给每个颜色系统打上标签，包括前景、背景、高亮等。颜色系统设计可参考 <a href="https://color.adobe.com" target="_blank" rel="noopener">Adobe 颜色</a> 和 <a href="https://coolors.co/browser" target="_blank" rel="noopener">Coolors</a>。然后根据标签设计规则，只当每种颜色系统的使用条件。</p><h3 id="4-2-5-合成"><a href="#4-2-5-合成" class="headerlink" title="4.2.5 合成"></a>4.2.5 合成</h3><p>遍历所有生成的布局、描述、图形、颜色，将这四元素组合。比如，可视化生成模块生成结果如下：</p><ul><li>布局 = <code>[layout1, layout2]</code></li><li>描述 = <code>[description1, description2, description3]</code></li><li>图形 = <code>[graphic1, graphic2]</code></li><li>颜色 = <code>[color1]</code></li></ul><p>将上面的元素组合起来得到一个候选集：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">候选集 = [</span><br><span class="line">    [layout1, description1, graphic1, color1],</span><br><span class="line">    [layout1, description1, graphic2, color1],</span><br><span class="line">    [layout1, description2, graphic1, color1],</span><br><span class="line">    [layout1, description2, graphic2, color1],</span><br><span class="line">    [layout1, description3, graphic1, color1],</span><br><span class="line">    [layout1, description3, graphic2, color1],</span><br><span class="line">    [layout2, description1, graphic1, color1],</span><br><span class="line">    [layout2, description1, graphic1, color1],</span><br><span class="line">    [layout2, description1, graphic2, color1],</span><br><span class="line">    [layout2, description2, graphic1, color1],</span><br><span class="line">    [layout2, description2, graphic2, color1],</span><br><span class="line">    [layout2, description3, graphic1, color1],</span><br><span class="line">    [layout2, description3, graphic2, color1]</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>得到候选集以后根据以下规则进一步处理：</p><ol><li>用户的输入文本不满足布局模板所需要的元素。比如，某个布局模板需要修饰词（比如 ”大于“），但是在用户文本中没有出现修饰词，那么我们就将这个布局模板的组合删掉。比如将候选集中所有包含 layout1 的组合删掉。</li><li>抽出描述和图形，计算他们的横纵比，将他们缩放到一个合适的尺寸，放到布局模板中。这个问题可以通过 <a href="https://constraints.cs.washington.edu/solvers/cassowary-tochi.pdf" target="_blank" rel="noopener">Badros</a> 等人提出的方法去解决。</li><li>挑选合适的字体，然后将所有元素进行对齐。</li></ol><h3 id="4-2-6-重排"><a href="#4-2-6-重排" class="headerlink" title="4.2.6 重排"></a>4.2.6 重排</h3><p>综合考虑三种评分机制：语义得分、可视化得分和信息得分。</p><ul><li><p>语义得分：由于图形和颜色的选取是通过标签语义匹配得到的，因此，得分越高说明匹配度越高。word2vec 计算词义距离 $\alpha_s \in [0,1]$，0 分说明语义和图形或者颜色毫无关系，1 分说明非常契合。</p></li><li><p>可视化得分：由于图形和描述的尺寸与布局不一定相称，所以可视化得分也是我们考虑的因素之一。我们通过计算可视化元素中空白区域的占比来衡量可视化得分：</p><script type="math/tex; mode=display">\alpha_v = \frac{所有非空白区域面积}{画布区域总面积}</script></li><li><p>信息得分：通过信息得分来衡量图表是否包含了用户描述的所有信息。定义如下：</p><script type="math/tex; mode=display">\alpha_i = \sum_{w\in S} \frac{I(w)}{|S|}</script><p>其中 $S$ 表示用户描述文本中的所有非停用词，$I(w)$ 的定义如下：</p><script type="math/tex; mode=display">I(w) = \begin{cases}1 & 如果 w 作为图标或者词出现在了生成的图表中 \\0 & 不满足上面的条件\end{cases}</script></li></ul><p>有了上面三种评分方式，候选集中每种组合方式最后的总得分为：</p><script type="math/tex; mode=display">\alpha = w_s\alpha_s + w_v\alpha_v+w_i\alpha_i</script><p>默认情况下，令 $w_s=0.25,w_v=0.5, w_i=0.25$。</p><p>最后我们根据每种组合最后的得分对候选集中的图表进行重排，然后推荐给用户。</p><p>需要注意的是，我们要支持用户对图表进行编辑，改变布局、描述、图形、颜色等。</p><h1 id="5-Text-to-Viz-实际效果"><a href="#5-Text-to-Viz-实际效果" class="headerlink" title="5. Text-to-Viz 实际效果"></a>5. Text-to-Viz 实际效果</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210318162444126.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> text2viz </tag>
            
            <tag> nl2infographic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>随机梯度下降中隐式正则化的起源</title>
      <link href="/2021/04/06/Implicit_Regularization/"/>
      <url>/2021/04/06/Implicit_Regularization/</url>
      
        <content type="html"><![CDATA[<p>首先推荐两篇论文：</p><ul><li><em>Samuel L Smith, Benoit Dherin, David Barrett, Soham De (2021)</em> <a href="https://openreview.net/forum?id=rq_Qr0c1Hyo" target="_blank" rel="noopener">On the Origin of Implicit Regularization in Stochastic Gradient Descent</a></li><li><em>David G.T. Barrett, Benoit Dherin (2021)</em> <a href="https://arxiv.org/abs/2009.11162" target="_blank" rel="noopener">Implicit Gradient Regularization</a></li></ul><a id="more"></a><h1 id="1-深度学习为什么起作用？"><a href="#1-深度学习为什么起作用？" class="headerlink" title="1. 深度学习为什么起作用？"></a>1. 深度学习为什么起作用？</h1><p>为了理解为什么深度学习会如此有效，仅对损失函数或模型进行分析是不够的，这是经典泛化理论所关注的。 相反，我们用来寻找极小值的算法（即，随机梯度下降）似乎起着重要作用。 在许多任务中，强大的神经网络能够内插（interpolate）训练数据，即达到接近0的训练损失。 实际上，存在一些训练损失的最小值，它们在训练数据上几乎没有区别。 在这些最小值中，有一些也可以很好地泛化（即导致较低的测试误差），而另一些则可以任意严重地过度拟合。 </p><p>那么似乎重要的不是优化算法是否迅速收敛到局部最小值，而是它希望达到那个可用的“虚拟全局”最小值。 似乎是我们用于训练深度神经网络的优化算法比其他算法更喜欢一些最小值，并且这种偏好会导致更好的泛化性能。 优化算法优先收敛到特定的最小值而避免其他最小值的情况称为隐式正则化。</p><h1 id="2-有限步长的影响分析"><a href="#2-有限步长的影响分析" class="headerlink" title="2. 有限步长的影响分析"></a>2. 有限步长的影响分析</h1><p>帮助我们想象深度学习模型训练过程中发生的事情的新理论之一是<a href="https://www.inference.vc/neural-tangent-kernels-some-intuition-for-kernel-gradient-descent/" target="_blank" rel="noopener">神经正切核（neural tangent kernels）</a>。在这个框架下，我们研究了在无限宽层（infinitely wide layers）、全批次（full batch）和无限小学习率（ infinitesimally small learning rate）的限制下的神经网络训练。尽管这个理论有用且具有吸引力。但是使用全批、无限小学习率进行模型训练是不切实际的。实际上太小的学习率并不是总是有用的，minibatch-SGD 中梯度更新的随机性似乎也很重要。</p><p><em>Smith et al. (2021)</em> 等人在的做法不同的是，他们尝试针对小型（但不是无限小的）学习率来研究minibatch-SGD，这更接近实际。 允许他们研究这种情况的工具是从微分方程的研究中借来的，称为<strong>后向误差分析</strong>（backward error analysis），无下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/Backward-error-analysis.png" alt></p><p>假设有一个微分方程 $\dot{\omega} = f(\omega)$，上图中黑色的曲线表示该微分方程 $\omega_t$ 的运动轨迹，初始条件为 $\omega_0$。我们通常没有办法直接进行求解，而是使用欧拉法对该微分方程进行近似：</p><script type="math/tex; mode=display">\omega_{k+1} = \omega_k + \epsilon f(\omega_k)</script><p>这个近似是离散的，如上图绿色线所示。由于离散化带来的误差，对于有限的步长 $\epsilon$ ，离散路径可能不完全位于连续的黑色路径所在的位置。误差随着时间积累，如图所示。后向误差分析的目标是找到一个不同的微分方程 $\dot{\omega}=\widetilde f(\omega)$，使得我们找到的近似离散路径位于新的微分方程路径附近。我们的目标是对 $\widetilde{f}$ 进行反向工程使得离散化迭代能很好的用微分方程进行建模。</p><p>这个方法为什么有效？因为 $\widetilde{f}$ 采用的形式可以揭示离散化算法行为的偏好，尤其是如果它对进入不同空间有隐含偏好的话。</p><p>损失函数 $C$ 的梯度下降中，原始的微分方程是 $f(\omega)=-\nabla C(\omega)$。修正后的微分方程：</p><script type="math/tex; mode=display">\dot{\omega} = - \nabla \widetilde{C}_{GD}(\omega) \\\\\widetilde{C}_{GD}(\omega) = C(\omega)+\frac{\epsilon}{4}\lVert \nabla C(\omega)\rVert^2</script><p>因此，具有有限步长 $\epsilon$ 的梯度下降就像运行梯度流一样，但是增加了用来惩罚损失函数梯度的惩罚项。第二项就是所谓的隐式梯度正则化。</p><h1 id="3-随机梯度"><a href="#3-随机梯度" class="headerlink" title="3. 随机梯度"></a>3. 随机梯度</h1><p>在这个框架下分析 SGD 有点困难，因为随机梯度下降的轨迹是随机的。 因此，没有一个单一的要优化的离散轨迹，而是有一个不同轨迹的分布，如果要随机重排数据，则要遍历这些轨迹。如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/Backward-error-analysis--1-.png" alt></p><p>从起始点 $\omega_0$ 开始，我们有多条路径。这些路径对应于不同的数据重排的方式（论文中假设 mini-batch 中的数据是固定的，而随机性来源于 mini-batch 的处理顺序）。路径最终在一个随机位置处结束，绿色点显示了轨迹可能最终在其处的其他随机端点。 绿色星号代表了随机轨迹终点分布的平均值。</p><p><em>Smith et al. (2021)</em> 的目标是对微分方程反向工程使得图中橙色的轨迹靠近绿色线的平均轨迹：</p><script type="math/tex; mode=display">\dot{\omega} = - \nabla \widetilde{C}_{SGD}(\omega) \\\\\widetilde{C}_{SGD}(\omega) = C(\omega) + \frac{\epsilon}{4m} \sum_{k=1}^{m} \lVert \nabla\hat{C}_k(\omega) \rVert^2</script><p>其中 $\hat{C}_k$ 表示第 $k$ 个 mini-batch 的损失函数，总共有 $m$ 个mini-batch。注意，这里与我们平时见到的梯度下降很类似，但是这里使用的是 mini-batch 梯度的平均数。另一个有趣的视角是看看 GD 和 SGD 的不同之处：</p><script type="math/tex; mode=display">\widetilde{C}_{SGD} = \widetilde{C}_{GD} + \frac{\epsilon}{4m} \sum_{k=1}^m \rVert \nabla\hat{C}_k(\omega)-C(\omega) \rVert^2</script><p>其中额外的正则项 $\frac{\epsilon}{4m} \sum_{k=1}^m \rVert \nabla\hat{C}_k(\omega)-C(\omega) \rVert^2$，有点像 mini-batch 的总方差。直观来说，此正则化项将避免参数空间中在不同 mini-batch 上计算出的梯度变化太大。</p><p>重要的是 $C_{GD}$ 与 $C$ 有相同的最小值，但 $C_{SGD}$ 就不一定了。这就意味着，SGD 不仅与 full-batch GD 有不同的轨迹，而且可能会收敛到完全不同的解。</p><h1 id="4-与泛化的关系"><a href="#4-与泛化的关系" class="headerlink" title="4. 与泛化的关系"></a>4. 与泛化的关系</h1><p>为什么隐式正则化效果可以避免 mini-batch 梯度方差过大？ 考虑下面两个局部极小值的插图：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/xRYHk0m.png" alt></p><p>就平均损失 $C$ 来说，左右两边是相同的：最小值相同，宽度相同。但是，在左侧情况下，最小值是几个 mini-batch 损失的平均值，这些损失看起来都一样，而它们本身也相对较宽。 在右边的最小值中，宽泛的平均损失最小值是许多尖峰小批量损失的平均值，所有这些都无法确定最小值的确切位置。</p><p>可以合理地预期左边最小值可以更好地泛化，因为损失函数似乎对我们正在评估的任何特定 mini-batch 不那么敏感。这样，损失函数也可能对数据点是在训练集中还是在测试集中不太敏感。</p><h1 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h1><p>总而言之，本文是对随机梯度下降的非常有趣的分析。 尽管有其局限性（作者并没有试图在本文中进行透明地隐藏和讨论），但它还是为分析有限步长优化算法提供了一种非常有趣的新技术。 论文写得很好，清楚地阐明了分析中一些乏味的细节。 </p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 隐式正则化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>预训练语言模型-神经网络语言模型：LSTMLM</title>
      <link href="/2021/03/31/neural-language-model-lstm/"/>
      <url>/2021/03/31/neural-language-model-lstm/</url>
      
        <content type="html"><![CDATA[<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p><em>Mikolov</em> 等人提出的 <em>RNN</em> 语言模型解决了前馈神经网络语言模型的语序问题。但是由于 <em>RNN</em> 神经网络本身存在着长程依赖问题，导致 <em>RNN</em> 语言模型很难学到距离较远的信息。</p><a id="more"></a><p>比如：“我的家乡是广东，广东有很多好吃的，我最喜欢的是海鲜，我们的方言是粤语…” 假设有这样一个句子，我们想通过前文去预测 “粤语” 这个词，显然它是和 “广东” 相关联的信息。但是我们会发现 “广东” 在句子中距离 “粤语” 很远。<em>RNN</em> 很难学到这样远距离的信息，关于为什么会出现这样的情况可以参考 <a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" target="_blank" rel="noopener">Hochreiter &amp; German (1991)</a> 和 <a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="noopener">Bengio, et al. (1994)</a> 两篇文章，简单来说就是因为 <em>RNN</em> 循环过程中时间步之间是连乘的关系，一旦出现较大或者较小的值，经过连乘就会发生梯度爆炸或者梯度消失的情况。出现这种情况以后模型就学不到什么东西了。</p><p>为了解决这个问题，<a href="http://www.bioinf.jku.at/publications/older/2604.pdf" target="_blank" rel="noopener">Hochreiter &amp; Schmidhuber (1997)</a> 提出了长短期记忆网络（<em>Long Short Term Memory networks</em>，即所谓的 <em>LSTM</em> 网络）。使用 <em>LSTM</em> 来构建语言模型可以避免由 <em>RNN</em> 带来的长程依赖问题。</p><h1 id="2-LSTM-语言模型"><a href="#2-LSTM-语言模型" class="headerlink" title="2. LSTM 语言模型"></a>2. LSTM 语言模型</h1><h2 id="2-1-LSTM-神经网络简介"><a href="#2-1-LSTM-神经网络简介" class="headerlink" title="2.1 LSTM 神经网络简介"></a>2.1 LSTM 神经网络简介</h2><p>长短期记忆网络（Long Short Term Memory networks），通常简称为“LSTM”，是一种特殊的RNN，它能够规避掉长期依赖学习问题。它是由 <a href="http://www.bioinf.jku.at/publications/older/2604.pdf" target="_blank" rel="noopener">Hochreiter &amp; Schmidhuber (1997)</a> 提出的，并且经过很多人的改进。</p><p>LSTM被设计出来用以解决长期依赖问题。<strong>长时间记住信息实际上是他们的默认行为，而不是他们努力学习的东西！</strong>(<em>Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!</em>)</p><p>所有的循环神经网络都有着重复模块的链式神经网络结构。标准的RNN的重复模块有非常简单的结构，比如单个 <em>tanh</em> 层。LSTM 也有这种类似的链式结构，但是重复模块却有着不同的结构，不同于单一网络层结构，LSTM 的基本结构如下图。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/lSTM.png" height="170" width="450"></p><p>对比 RNN 的结构，我们会发现，LSTM 要复杂得多。简单来说，LSTM 通过三个门控来调节当前神经元中学习到的信息，避免梯度消失或者爆炸。</p><blockquote><p>图中 $\sigma$ 表示 <em>sigmoid</em> 函数：</p><script type="math/tex; mode=display">\sigma(z) = \frac{\exp(z)}{\sum_i \exp(z)} \in (0, 1)</script><p>$\tanh$ 表示 <em>tanh</em> 函数：</p><script type="math/tex; mode=display">\tanh(x) = \frac{\exp(x)-\exp(-x)}{\exp(x)+\exp(-x)} \in [-1, 1]</script></blockquote><ul><li><p><em>cell</em> 状态：LSTM 最重要的就是 <em>cell</em> 状态 $\vec{C}_t$，表示当前时间神经网络学习到的信息；</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/cellstate.png" width="450" height="170"></p></li><li><p>遗忘门：控制上一个 <em>cell</em> 中有多少信息会进入到当前的 <em>cell</em>；</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/forget.png" width="450" height="170"></p></li><li><p>输入门：控制输入层有多少信息会进入到当前 <em>cell</em> 中；</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/input.png" width="450" height="170"></p></li><li><p>输出门：控制当前 <em>cell</em> 有多少信息可以用于输出。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/output.png" width="450" height="170"></p></li></ul><h2 id="2-2-LSTM-语言模型"><a href="#2-2-LSTM-语言模型" class="headerlink" title="2.2 LSTM 语言模型"></a>2.2 LSTM 语言模型</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210408163912.png" alt></p><p>从之前的神经网络语言模型中，我们会发现一些规律：</p><ul><li>输入词是通过 <em>1-of-K</em> 编码的，其中 $K$ 是词表大小；</li><li>输出层通过 <em>softmax</em> 得到一个归一化的概率分布；</li><li>训练过程使用交叉熵损失函数，等价于最大似然估计。</li></ul><p><em>Sundermeyer</em> 等人也使用了相同的方法，用来你构建 LSTM 语言模型。首先将输入层的词经过一个投影层，转化成词嵌入（实际上就是 <em>Embedding</em> 过程），然后传递给 LSTM，最后经过 <em>softmax</em> 进行输出。</p><p>对于大规模的语言模型训练来说，<em>softmax</em> 层的计算消耗了大量的时间：</p><script type="math/tex; mode=display">a_i = \sum_{j=1}^J \omega_{ij} b_j</script><p>其中 $J$ 表示 LSTM 隐层节点数，$\omega_{ij}$ 表示 LSTM 层与输出层的权重，$i=1,…,V$ ，其中 $V$ 表示词表大小。</p><p>为了降低计算时间，<em>Morin &amp;  Bengio</em> 、<em>Goodman</em> 提出将词进行分类，然后预测下一个词所在的类别，然后再预测具体的词：</p><script type="math/tex; mode=display">p(w_m|w_{1:m-1}) = p(w_m|c(w_m),w_{1:m-1})p(c(w_m)|w_{1:m-1})</script><p>其中 $w_m \in c(w_m)$，$c(w_m)$ 表示 $w_m$  所在的类别。</p><h2 id="2-3-AWD-LSTM-语言模型"><a href="#2-3-AWD-LSTM-语言模型" class="headerlink" title="2.3 AWD-LSTM 语言模型"></a>2.3 AWD-LSTM 语言模型</h2><p>LSTM 作为 RNN 最优秀的变种之一，在进行语言建模的时候也有着相当优秀的表现。但是 作为神经网络，LSTM 也存在着泛化性问题。通常为了提高神经网络的泛化性，人们提出了各种各样的正则化策略。</p><p>AWD-LSTM 提出了一些正则化和优化策略，这些策略不仅高效，而且可以在不改变 LSTM 结构的条件下实现。它在语言建模上的优异表现使得它一度成为最优秀的语言模型。下面我们就介绍一下这个模型。</p><blockquote><p>LSTM 数学公式：</p><script type="math/tex; mode=display">f_t=\sigma(W_f\cdot [h_{t-1}, x_t]+b_f) \\\\i_t = \sigma(W_i\cdot[h_{t-1}, x_t]+b_i) \\\\\widetilde{C}_t = \tanh(W_C\cdot [h_{t-1}]+b_C) \\\\o_t = \sigma(W_o \cdot [h_{h-1}, x_t] +b_o) \\\\C_t = i_t * \widetilde{C}_t + f_t * \widetilde{C}_{t-1} \\\\h_t = o_t * \tanh(C_t)</script></blockquote><h3 id="2-3-1-weight-dropped-LSTM"><a href="#2-3-1-weight-dropped-LSTM" class="headerlink" title="2.3.1 weight-dropped LSTM"></a>2.3.1 weight-dropped LSTM</h3><p><em>Dropout</em> 是神经网络中常用的防止过拟合的方法，但是用在 <em>RNN</em> 型的网络中通常效果不佳。这与 <em>Dropout</em> 的原理有关，见下图中间：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/nn_do1.jpg" alt></p><p><em>Dropout</em> 会随机丢掉一些神经元，即将神经元节点置为零。这样 $h_t$ 接收到的 $h_{t-1}$ 就不完整了，会干扰 <em>RNN</em>  的长程依赖能力。为了解决这一问题，<a href="http://proceedings.mlr.press/v28/wan13.pdf" target="_blank" rel="noopener"><em>Wan</em></a> 等人提出 <em>DropConnect</em> 技术，如上图右侧。不同于 <em>Dropout</em> 的丢掉神经元，<em>DropConnect</em> 是随机丢掉一些权重，完整的保留了神经元。用伪代码来说明如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Dropout</span></span><br><span class="line">h_1 = RNNCell(x)</span><br><span class="line">h_2 = Dropout(h_1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># DropConnect</span></span><br><span class="line">h_1 = RNNCell(x)</span><br><span class="line">h_2 = Dropout(h_1.weights)</span><br></pre></td></tr></table></figure><p>这样就不会影响到 <em>RNN</em> 的长程依赖能力了。</p><p>LSTM 的权重参数包括 $[W_f, W_i, W_C, W_o, U_f, U_i, U_C, U_o]$，其中 $W^{*}$ 是与输入 $x_t$ 相关的， $U^{*}$ 是与隐状态相关的。LSTM 的梯度问题通常与隐状态有关（循环连乘带来的梯度消失或者爆炸），因此将 <em>DropConnect</em> 应用于 $U^{*}$ 上效果更好（当然，$W^{*}$ 和 $U^{*}$ 都用也行，只是考虑到以牺牲效率为代价换来的效果提升并不明显）。</p><h3 id="2-3-2-Non-monotonically-Triggered-ASGD"><a href="#2-3-2-Non-monotonically-Triggered-ASGD" class="headerlink" title="2.3.2 Non-monotonically Triggered ASGD"></a>2.3.2 Non-monotonically Triggered ASGD</h3><p>对于语言建模任务来说，传统的 SGD 优化算法比带动量的 SGD 变体效果更好。因此，作者在调研了一些传统 SGD 算法之后选定了 ASGD 算法。</p><p>所谓 ASGD 算法指的是 Averaged SGD 算法，它是 <a href="https://epubs.siam.org/doi/abs/10.1137/0330046?journalCode=sjcodc" target="_blank" rel="noopener">Polyak &amp; Juditsky</a> 等人 1992 年提出的一种优化算法，经过了二十多年的研究发展，ASGD 已经非常成熟，无论是理论研究还是实际表现都非常出色。</p><p>ASGD 采取和 SGD 相同的更新步骤 ，不同的是传统 SGD 在更新权重的时候只考虑当前的轮次，而 ASGD 不仅考虑当前的的轮次还考虑之前的轮次，然后计算平均值。用伪代码来表示如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 传统 SGD</span><br><span class="line">w_t= w_t_1 - lr * grad(w_t_1)</span><br><span class="line"></span><br><span class="line"># ASGD</span><br><span class="line">avg_fact = 1 / max(t - K, 1)</span><br><span class="line"></span><br><span class="line">if avg_fact != 1:</span><br><span class="line">    w_t = avg_fact * (sum(w_t_1) + (w_t_1 - lr_t * grad(w_t_1)))</span><br><span class="line">else:</span><br><span class="line">    w_t = w_t_1 - lr_t * grad(w_t_1)</span><br></pre></td></tr></table></figure><p>其中 $K$ 表示在计算权重平均值之前权重更新的迭代的次数，也就是说，前 $K$ 轮的 ASGD 与 SGD 是完全相同的。</p><p>但是作者认为这种方法有两处不足：</p><ol><li>学习率的调整原则不明确；</li><li>参数 $K$ 作为超参，其取值原则也不明确。$K$ 值太小会对效果产生负面影响；取值太大可能需要更多的迭代才能收敛。</li></ol><p>因此，作者提出了 ASGD 的一种变体—— NT-ASGD，即非单调触发 ASGD（<em>Non-monotonically Triggered ASGD</em>），算法如下：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210409194127.png" alt></p><ul><li>当模型评估指标多轮训练（$n$）后都没有提升的时候 ASGD 就会触发，实验发现 $n=5$ 的效果最好；</li><li>整个实验使用恒定的学习率。</li></ul><h3 id="2-3-3-其他正则化方法"><a href="#2-3-3-其他正则化方法" class="headerlink" title="2.3.3 其他正则化方法"></a>2.3.3 其他正则化方法</h3><p>除了上面讨论到的两种技术，论文作者还使用了其他预防过拟合、提升数据效率的正则化技术。</p><h4 id="2-3-3-1-可变长度反向传播序列"><a href="#2-3-3-1-可变长度反向传播序列" class="headerlink" title="2.3.3.1 可变长度反向传播序列"></a>2.3.3.1 可变长度反向传播序列</h4><p>一般在训练语言模型的时候，将整个语料看成一个连续的超长的句子，在预处理的时候会将句子截断成固定长度的 <em>batch size</em> 个序列。这样由于句子被截断，在后向传播的过程中神经网络学到的信息就不玩完整了。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">原始语料：“我是中国人。我爱北京天安门。”</span><br><span class="line">预处理后：[</span><br><span class="line">    &quot;我是中国人。我&quot;,</span><br><span class="line">    &quot;爱北京天安门。&quot;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>“我爱北京天安门。”这句话中的 “我” 就无法学到任何信息，因为它后面的内容被截断了。</p><p>为了解决这个问题，作者提出了使用可变长度的反向传播序列。首先以概率 $p$ 选取长度为 $bptt$ 的序列，然后以概率 $1-p$ 选取长度度为 $bptt/2$ 的序列。($p$ 是个超参数，实验中作者选用的 $p=0.95$)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">base_bptt = bptt <span class="keyword">if</span> np.random.random() &lt; <span class="number">0.95</span> <span class="keyword">else</span> bptt / <span class="number">2</span></span><br></pre></td></tr></table></figure><p>然后根据 $N(base\_bptt, s)$ 得到序列长度，其中 $s$ 表示标准差，$N$ 表示正态分布。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">seq_len = max(<span class="number">5</span>, int(np.random.normal(base_bptt, <span class="number">5</span>)))</span><br></pre></td></tr></table></figure><p>然后再根据 <code>seq_len</code>  改变学习率。因为当学习速率固定时，会更倾向于对短序列，所以需要进行缩放。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lr2 = lr * seq_len / bptt</span><br></pre></td></tr></table></figure><blockquote><p>作者的这种做法其实还是引入了很多超参。其实还有一种更好的方法，可以在固定长度的 BPTT 下，不影响效果。</p><p>上面的例子中，“我是中国人。我爱北京天安门。”被分成了 [“我是中国人。我”,  “爱北京天安门。”]。这是通常的做法。我们还可以用下面的这种方法：</p><p>原始语料：“我是中国人。我爱北京天安门。”<br>预处理后：[</p><p>​    “我是中国人。我”,</p><p>​    “是中国人。我爱”,</p><p>​    “中国人。我爱北”,</p><p>​    …</p><p>​    “爱北京天安门。”</p><p>]</p></blockquote><h4 id="2-3-3-2-变分-Dropout"><a href="#2-3-3-2-变分-Dropout" class="headerlink" title="2.3.3.2 变分 Dropout"></a>2.3.3.2 变分 Dropout</h4><p>通常情况下，每次调用 <em>Dropout</em> 时取样一个新的 <em>dropout mask</em>。但是在 LSTM 中参数是共享的，作者希望在不同的时刻共享的参数也共享同一套 <em>dropout mask</em>，这就是 <em>variational dropout</em>，在隐层作者使用了共享<em>mask</em> 的 <em>dropConnect</em>，而在输入和输出中，作者使用共享 <em>mask</em> 的 <em>variational dropout</em>。但是请注意在不同的 <em>mini-batch</em> 中，<em>mask</em> 是不共享的，所以 <em>mask</em> 的共享和参数共享还是有区别的，<em>dropout mask</em> 的共享是在每一个迭代中发生的，不同的迭代输入的数据不同，为了体现数据的差异性，要保证 <em>dropout mask</em> 不一致。</p><h4 id="2-3-3-3-嵌入-Dropout"><a href="#2-3-3-3-嵌入-Dropout" class="headerlink" title="2.3.3.3 嵌入 Dropout"></a>2.3.3.3 嵌入 Dropout</h4><p>对嵌入层引入 <em>Dropout</em>，实际上是在词级上操作的，即随机将一些词给去掉，这些被去掉的词的向量值就全为 0，并且在前向和反向传播中都保持这样的操作。对其余没有丢掉的词，用 $\frac{1}{1-p_e}$ 缩放其向量值，$p_e$ 为 <em>Dropout</em> 的比例。</p><h4 id="2-3-3-4-权重绑定"><a href="#2-3-3-4-权重绑定" class="headerlink" title="2.3.3.4 权重绑定"></a>2.3.3.4 权重绑定</h4><p>共享 <em>embedding</em> 层和 <em>softmax</em> 层，可以降低模型总参数。语言模型的最后输出也是 $|\mathcal{V}|$ 维，是预测词表中每个词的概率，从模型设计上来看嵌入层和最后输出层的参数矩阵的维度是很容易保证一致的，而从语言模型的特性上来看两个矩阵之间也是有一定的联系，所以作者选择共享嵌入层和输出层的权重矩阵，这种方法在 <em>seq2seq</em> 中也经常用到。</p><h4 id="2-3-3-5-减小嵌入尺寸"><a href="#2-3-3-5-减小嵌入尺寸" class="headerlink" title="2.3.3.5 减小嵌入尺寸"></a>2.3.3.5 减小嵌入尺寸</h4><p>减少语言模型的总参数量的最简单方法是降低词向量的尺寸，尽管这无助于缓解过拟合。论文作者修改了第一个和最后一个 LSTM 层的输入和输出维度，以和降低了的嵌入尺寸保持一致。</p><h4 id="2-3-3-6-激活正则化和时域激活正则化"><a href="#2-3-3-6-激活正则化和时域激活正则化" class="headerlink" title="2.3.3.6 激活正则化和时域激活正则化"></a>2.3.3.6 激活正则化和时域激活正则化</h4><p>常见的正则化技术除了 <em>Dropout</em> 以外还有 $L_2$ 正则化。坐着在模型中不仅用了 <em>Dropout</em> 还用了 $L_2$ 正则化，$L_2$ 正则化分成两部分：</p><ul><li><p>对每个单独的 $h_t$ ，用于惩罚明显过大的值。这部分称之为 <em>Activation Regularization</em>：</p><script type="math/tex; mode=display">\alpha L_2(m \odot h_t)</script><p>其中 $m$ 为 <em>dropout mask</em>，$\alpha$ 为缩放系数。</p></li><li><p>对 $h_t$ 和 $h_{t+1}$ 之间的差值，用于惩罚隐状态变动过大，称之为 <em>Temporal Activation Regularization</em>。这一步很容易理解，$h_{t}$ 包含了之前的所有信息，$h_{t+1}$ 不仅包含了之前的所有信息，还包含了当前信息。一个通顺的句子包含的信息应该是平滑的，不会因为某个词的出现大规模改变隐状态。如果两个连续的隐状态之间出现了较大的差别很可能是训练过程出现了问题，所以通过 $L_2$ 正则化进行修正：</p><script type="math/tex; mode=display">\beta L_2(h_t-h_{h+1})</script></li></ul><h1 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h1><p>本文介绍了 LSTM 语言模型，尤其着重介绍了 AWD-LSTM 语言模型。LSTM 作为在 <em>Transformer</em> 出现之前最优秀的序列建模的模型一直是 NLP 中的王者，实际上即使是 <em>Transformer</em> 在众多任务中的表现强于 LSTM，但是 LSTM  在序列位置捕捉能力上还是强于 <em>Transformer</em>。本文不仅包含了 LSTM 语言建模的思路 ，也介绍了多种非常有用的序列建模的优化方法。 </p><h1 id="4-Reference"><a href="#4-Reference" class="headerlink" title="4. Reference"></a>4. Reference</h1><ol><li><p><em>Morin, F., Bengio, Y.</em>，<a href="http://www-labs.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf" target="_blank" rel="noopener">Hierarchical Probabilistic Neural Network Language Model</a></p></li><li><p><em>Goodman, J.,</em> <a href="https://ieeexplore.ieee.org/document/940893" target="_blank" rel="noopener">Classes for fast maximum entropy training</a></p></li><li><p><em>Martin Sundermeyer, Ralf Schluter, and Hermann Ney</em>，<a href="http://www-i6.informatik.rwth-aachen.de/publications/download/820/Sundermeyer-2012.pdf" target="_blank" rel="noopener">LSTM Neural Networks for Language Modeling</a> </p></li><li><p><em>Stephen Merity, Nitish Shirish Keskar, Richard Socher</em>，<a href="https://arxiv.org/pdf/1708.02182.pdf" target="_blank" rel="noopener">Regularizing and Optimizing LSTM Language Models</a></p></li><li><p><em>Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, Rob Fergus</em>，<a href="http://proceedings.mlr.press/v28/wan13.pdf" target="_blank" rel="noopener">Regularization of Neural Networks using DropConnect</a></p></li><li><p><em>Yashu Seth</em>, <a href="https://yashuseth.blog/2018/09/12/awd-lstm-explanation-understanding-language-model/" target="_blank" rel="noopener">What makes the AWD-LSTM great?</a></p></li><li><p><a href="https://www.cnblogs.com/jiangxinyang/p/13125519.html" target="_blank" rel="noopener">语言模型系列（一）——AWD-LSTM</a></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 语言模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> LSTMLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>预训练语言模型-神经网络语言模型：RNNLM</title>
      <link href="/2021/03/24/neural-language-model-rnn/"/>
      <url>/2021/03/24/neural-language-model-rnn/</url>
      
        <content type="html"><![CDATA[<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p><em>Bengio</em> 等人使用前馈神经网络构建语言模型，解决了两个问题：参数维度爆炸和词与词之间的语义关系的问题。然我们看到使用神经网络构建语言模型存在的巨大潜力。但是前馈神经网络构建的语言模型同样也存在问题：他只能输入特定长度的上下文（窗口 $n$）。也就是说，它只能用固定长度内的信息来预测下一个词，这与 <em>n-gram</em> 模型有相同的问题。</p><a id="more"></a><p>循环神经网络之前是专门用来处理序列化数据的，它对于输入长度没有限制。因此，<em>Mikolov</em> 等人于 2010 年提出基于 <em>RNN</em> 的语言模型。</p><h1 id="2-模型"><a href="#2-模型" class="headerlink" title="2. 模型"></a>2. 模型</h1><h2 id="2-1-RNN-神经网络简介"><a href="#2-1-RNN-神经网络简介" class="headerlink" title="2.1 RNN 神经网络简介"></a>2.1 RNN 神经网络简介</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/rnnloops.png" height="150" width="100"></p><p>上图是一个 <em>RNN</em> 神经网络结构图，$A$ 是神经网络的一部分，给定输入 $x_t$，输出 $h_t$。上一步的信息通过循环传递给下一步。</p><p>循环神经网络可以看做同一个网络的多次重复，每次传递一个信息给下一级。考虑以下，我们把它展开是什么样的：</p><table><tr>    <td><div align="center"><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/unrollrnn.png" height="100" width="300"></div></td>    <td><div align="center"><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/tanhrnn.png" height="100" width="300"></div></td></tr></table>    <p>这个链式特性说明循环神经网络与序列和列表有很大关系。它们是用于这种序列化数据的一种很自然的网络结构。</p><h2 id="2-2-RNN-语言模型"><a href="#2-2-RNN-语言模型" class="headerlink" title="2.2 RNN 语言模型"></a>2.2 RNN 语言模型</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210324172307.png" alt></p><p>上图为模型的基本结构。该模型采用最简单的循环神经网络结构。网络分成三部分：输入层、隐藏层和输出层。</p><ul><li><p>$t$ 时刻的输入层为：$t$ 时刻的词向量 $C(w_t) \in \mathbb{R}^{1\times m}$ 与 $t-1$ 时刻的隐状态 $s(t-1) \in \mathbb{R}^{1\times n}$ 向量拼接在一起的向量，即 $x(t)=[C(w_t);s(t-1)]\in \mathbb{R}^{1\times (m+n)}$；</p></li><li><p>$t$ 时刻的隐藏层为：</p><script type="math/tex; mode=display">s(t) = f\left(x(t) \cdot U\right) \\\\f(x) = \frac{1}{1+e^{-x}}</script><p>其中 $U\in \mathbb{R}^{(m+n)\times n}$ 表示一个权重矩阵。对于 $t=1$ 时，没有前一时刻的隐状态输出，所以需要对 $s(0)$ 进行初始化。这里采用的方法是令 $s(0)=[0.1,\cdots,0.1]^{1\times n}$，实际上只要赋值一个小量向量即可，在后续的模型更新优化过程中，初始化的向量重要性不高。</p></li><li><p>$t$ 时刻的输出层为：</p><script type="math/tex; mode=display">y(t) = g(s(t)\cdot W) \\\\g(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}</script><p>其中 $W\in \mathbb{R}^{n\times |\mathcal{V}|}$ 为权重矩阵。</p></li></ul><blockquote><p>  假设句子是：$\langle 我，爱，北京，天安门\rangle$</p><p>  我们可以将模型理解为下面的过程：</p><ul><li>$t=1$ 时，模型输入第一个字 $w_1=我$，将它和初始化隐向量 $s(0)$ 拼在一起传递给隐藏层。然后隐藏层对输入的信息进行处理得到 $s(1)$，然后将 $s(1)$ 传递给输出层，输出层来预测句子中第 2 个词 $w_2=爱$；</li><li>$t=2$ 时，模型输入第二个字 $w_2=爱$，将它和上一步得到的隐向量 $s(1)$ 拼在一起传递给隐藏层。然后隐藏层对输入的信息进行处理得到 $s(2)$，然后将 $s(2)$ 传递给输出层，输出层来预测句子中第 3 个词 $w_3=北京$；</li><li><p>$t=3$ 时，模型输入第二个字 $w_3=北京$，将它和上一步得到的隐向量 $s(2 )$ 拼在一起传递给隐藏层。然后隐藏层对输入的信息进行处理得到 $s(3)$，然后将 $s(3)$ 传递给输出层，输出层来预测句子中第 4 个词 $w_4=天安门$；</p><p>模型的每一步都将前面接受到的所有信息存储在隐藏状态里了，输出层是利用之前所有序列产生的隐藏状态来预测下一个词。这样就避免了前馈神经网络那样只能使用固定长度内的信息来预测下一个词。</p></li></ul></blockquote><p>损失函数定义为：</p><script type="math/tex; mode=display">\mathrm{error}(t) = d(t) -y(t)</script><p>其中 $d(t)$ 表示第 $t$ 步真实的词，$y(t)$ 表示模型输出的词。</p><blockquote><p>   作者还提出一个动态模型的概念：在测试阶段也要更新模型。作者认为在训练阶段同一批数据会多次更新参数，而经过多轮训练以后，再进行一次测试集的参数更新对模型本身影响不大，但是可能会提升某些不常见的句子的表现。比如训练集中经常出现 ”狗“ 的句子，但是很少出现 ”猫“，而在测试集中有 “猫”的类似的句子，这种情况下，使用动态模型可能会提升模型应对不常见句子的能力。</p></blockquote><h1 id="3-模型优化"><a href="#3-模型优化" class="headerlink" title="3. 模型优化"></a>3. 模型优化</h1><p>为了提升模型的表现，将所有出现频率低于某一阈值的词全部替换成统一的编码，比如 $<rare>$。包括罕见词的概率计算如下：</rare></p><script type="math/tex; mode=display">p(w_i(t+1)|w(t), s(t-1)) = \begin{cases}\frac{y_{<RARE>}(t)}{C_{<RARE>}} & 如果 w_i(t+1) 属于 <RARE> \\\\y_i(t)\end{cases}</script><p>其中 $C_{<rare>}$ 表示训练集中出现罕见词的总次数。</rare></p><h1 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4. 实验结果"></a>4. 实验结果</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210324173109.png" alt></p><h1 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h1><p>基于 <em>RNN</em> 的语言模型解决了前馈神经网络语言模型的视野太短问题，但是由于 <em>RNN</em> 本身的梯度消失问题导致实际上语言模型能够建模的序列长度也是有限的。</p><h1 id="6-Reference"><a href="#6-Reference" class="headerlink" title="6. Reference"></a>6. Reference</h1><p><a href="https://www.isca-speech.org/archive/archive_papers/interspeech_2010/i10_1045.pdf" target="_blank" rel="noopener">Recurrent neural network based language model</a>. <em>Tomas Mikolov , Martin Karafiat , Lukas Burget , Jan Honza Cernocky , Sanjeev Khudanpur， 2010</em></p>]]></content>
      
      
      <categories>
          
          <category> 语言模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> RNNLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>预训练语言模型-神经网络语言模型：FFNNLM</title>
      <link href="/2021/03/21/neural-language-model-ffnnlm/"/>
      <url>/2021/03/21/neural-language-model-ffnnlm/</url>
      
        <content type="html"><![CDATA[<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>统计语言模型中，无论是 <em>n-gram</em> 还是对数线性语言模型都面临一个非常严重的问题——维度爆炸。为了解决维度爆炸问题，<a href="http://www.iro.umontreal.ca/~lisa/pointeurs/jdm.pdf" target="_blank" rel="noopener"><em>Bengio &amp; Bengio</em></a> 2000 年提出了一种使用分布式词特征表示的方法，也就是后来所说的词向量。</p><a id="more"></a><p>在他们的方法中是将每个词利用神经网络映射到一个连续的向量空间中得到词向量。其实这种方法与对数线性语言模型使用不同特征表示词有异曲同工之妙，只是对数线性语言模型中的特征是人工构建，而且是离散化的。词特征的连续分布式表示相对离散化表示有两个显著的优点：</p><ol><li>可以将向量维度压缩到很低；</li><li>可以计算词与词之间的距离，用以表示语义相似度。</li></ol><blockquote><p>  随着技术的发展，原来的词向量表示维度还是太高。为了降维，人们又发展出了离散化词向量表示技术，但是这种离散化表示技术与对数线性语言模型中的词特征表示法已经不是同一个东西了。</p></blockquote><p>比如，一个 2-d 的向量，离散化向量最多只能表示 4 个词：$[0, 0],[0,1],[1,0],[1,1]$，而连续分布向量则可以表示无数个词：$[0.1,0.2],[0.1, 0.3], [0.11,0.22],…$。所以要想表达更多的词，离散化向量就需要更多的维度，而连续分布向量就不需要。另外，我肯可以看到，离散化向量两两都是正交的，这就意味着两两之间的距离是相等的，也就是丢失了词义。再看连续分布向量，明显两两之间的距离并不相同，这样我们就可以利用这种差异来表示词与词之间的语义相似度。</p><p>其实这种将符号用一个连续向量表示的方法在很早之前就出现了，上世纪 80 年代的联结主义就曾使用过。到了 2000 年左右的时候，神经网络技术日渐成熟，人们就利用神经网络将不同的符号表示成连续向量来表示不同的符号之间的关系。</p><p>而使用神经网络直接对语言进行建模也是很早就出现了，当时的人们更加关注于每一个词在语言中扮演的角色。利用神经网络对条件概率进行建模的方式首先出现在对词的建模上，即给定前面的字母来预测后一个字母。</p><p>最早使用神经网络对语言模型进行条件概率建模的是徐伟，在其 2000 年的论文《<a href="https://www.isca-speech.org/archive/archive_papers/icslp_2000/i00_1202.pdf" target="_blank" rel="noopener">Can Artificial Neural Networks Learn Language Models?</a>》提出了一种构建 2-gram 语言模型的方法，但是由于没有隐藏层，且只是二元语言模型，因此限制了其模型泛化能力和上下文语义捕捉能力。</p><p>第一个具有现代意义上的真正神经网络语言模型是  <a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="noopener"><em>Bengio</em></a> 等人提出的 <em>Feedforward Neural Network Language Model</em>, （<em>FFNNLM</em>）。接下来我们就从这篇文章开始，介绍几个具有代表性的神经网络语言模型。</p><h1 id="2-FFNN-语言模型"><a href="#2-FFNN-语言模型" class="headerlink" title="2. FFNN 语言模型"></a>2. FFNN 语言模型</h1><h2 id="2-1-神经模型"><a href="#2-1-神经模型" class="headerlink" title="2.1 神经模型"></a>2.1 神经模型</h2><p>假设有一个训练集 $X$，训练集中每一句话 $x_i \in X$ 是由词序 $\langle w_1, w_2, …, w_T \rangle$ 组成，其中 $w_t \in \mathcal{V}$，$\mathcal{V}$ 表示词表。词表通常很大，但是是有限的。我们的目标是构建一个模型：</p><script type="math/tex; mode=display">f(w_t, ..., w_{t-n+1}) = p(w_t|w_{1:t-1})</script><p>使得训练集中的句子具有较高的概率分布。另外，模型必须满足一个限制条件：</p><script type="math/tex; mode=display">\sum_{i=1}^{|\mathcal{V}|} f(i, w_{t-1}, \cdots,w_{t-n+1})=1</script><p>有了以上限制条件以后，<em>Bengio</em> 等人就设计出下面一个神经网络模型：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20201019145634.png" alt></p><p>模型可以分成两部分：</p><ol><li>一个映射 $C \in \mathbb{R}^{|\mathcal{V}| \times m}$，将词表中的词映射成分布式特征向量，$C(w_i) \in \mathbb{R}^m$。$C$ 是一个自由参数矩阵。</li><li>一个概率函数 $g$ 用来计算特征向量序列 $\langle C(w_{t-n+1}), \cdots, C(w_{t-1}) \rangle$ 的条件概率。$g$ 的输出是句子中第 $t$ 个词出现的概率。既然是神经网络语言模型，那么 $g$ 肯定就是一个神经网络啦。</li></ol><p>假设有一个句子 $\langle w_1, w_2, …, w_T \rangle$，整个模型的建模过程如下：</p><ol><li><p>选取滑动窗口大小 $n$，对特征向量序列进行滑动切片，得到 $\mathrm{ngrams}=[(w_1, w_2, \cdots,w_n), (w_2, w_3, \cdots, w_{n+1}), (w_3, w_4, \cdots, w_{n+2}),\cdots, (w_{T-n}, \cdots, w_{T})]$。比如 $\langle 我，爱， 北京，天安门 \rangle, n=3$，滑动切片以后得到 $[(我，爱， 北京),(爱，北京，天安门)]$。</p></li><li><p>$\langle w_{t-n+1}, \cdots, w_{t-1} \rangle$ 表示 $\mathrm{ngrams}$ 中第 $t$ 项的前 $n-1$ 个词，比如 $t=1$，则 $\langle w_{t-n+1}, \cdots, w_{t-1} \rangle$ 表示 $(我,爱)$。将其中每个词 $w_i$ 使用 $C$ 映射成特征向量，得到 $\langle C(w_{t-n+1}), \cdots, C(w_{t-1}) \rangle$。比如 $(我，爱，北京)$，$C(我)=[0.1, 0.3, 0.12], C(爱)=[0.2, 0.23, 0.4]$，那么可以得到特征向量序列（其实就是个特征矩阵）：</p><script type="math/tex; mode=display">x_{\mathrm{in}}=\left[\begin{matrix}0.1 & 0.2 \\\\0.3 & 0.23 \\\\0.12 & 0.4\end{matrix}\right]</script></li><li><p>特征向量序列，就相当于神经网络的输入层。输入层的特征向量序列传递到隐藏层，在隐藏层经过 $\tanh$ 变换：</p><script type="math/tex; mode=display">x_{\mathrm{hidden}} = \tanh(d+Hx_{\mathrm{in}})</script><p>其中 $H$ 和 $d$ 分别表示隐藏层的权重和偏置。</p></li><li><p>经过隐藏层变换之后，传入输出层，在输出层经过线性变换：</p><script type="math/tex; mode=display">x_{\mathrm{out}} = b+Wx_{\mathrm{in}} + Ux_{\mathrm{hidden}}</script><p>其中 $b,W,U$ 也是神经网络的参数。经过输出层的变换我们就可以得到一个 $x_{\mathrm{out}} \in \mathbb{R}^{|\mathcal{V}| \times m}$ 的输出矩阵。矩阵的每一列都代表 $w_t$ 可能的分布。</p></li><li><p>神经网络输出一个 $C(w_t)$ 的候选集，要从这些候选集中选取正确的 $C(w_t)$，就需要将这些候选集转化成概率分布，选择概率最高的那一列对应的向量。最常见的方法是使用 $\mathrm{softmax}$ 函数：</p><script type="math/tex; mode=display">\mathrm{softmax}(x_{\mathrm{out}}) = \frac{\exp(x_{\mathrm{out}})}{\sum_{w_i}\exp(x_{\mathrm{out}})}</script><p>这样其实就是对 $(我，爱，北京)$ 的建模过程：输入 $(我，爱)$，输出 $(我，爱)$ 后面所有可能出现的词的概率分布。</p></li></ol><p>有了这样一个过程以后，就可以对整个 $\mathrm{ngrams}$ 进行相同的建模过程。我们的目标是模型每次预测出来的词都是正确的，即我们希望找到一套参数 $\theta$，最大化对数似然函数：</p><script type="math/tex; mode=display">L=\frac{1}{T} \sum_t \log f(w_t, _{t-1}, \cdots, w_{t-n+1}; \theta) + R(\theta)</script><p>其中 $R(\theta)$ 表示正则项（只对权重做正则，不对偏置）。</p><p>要找到一套合适的参数，神经网络通常采用的方法是随机梯度下降方法：</p><script type="math/tex; mode=display">\theta \leftarrow \theta + \epsilon\frac{\partial \log p(w_t|w_{t-n+1}, \cdots, w_{t-1})}{\partial\theta}</script><p>其中 $\epsilon$ 表示学习率。</p><h2 id="2-2-模型参数量分析"><a href="#2-2-模型参数量分析" class="headerlink" title="2.2 模型参数量分析"></a>2.2 模型参数量分析</h2><p>模型参数来源于两部分：</p><ul><li>$C \in \mathbb{R}^{|\mathcal{V}| \times m}$ 是一个矩阵，$C(w_i)$ 表示矩阵第 $i$ 行对应的向量，即为 $w_i$ 对应的特征向量；</li><li>$g$ 是一个神经网络，$\pmb{\omega}$ 表示神经网络的参数，从上面我们可以确定 $\pmb \omega = (b,d,W,U,H)$。</li></ul><p>所以，整个模型的参数为 $\theta = (C, b,d,W,U,H)$。</p><ul><li>$C$ 是一个 $\mathbb{R}^{|\mathcal{V}|\times m}$ 的矩阵，所以参数量为：$|\mathcal{V}|\times m$;</li><li>假设有 $h$ 个神经元，隐藏层的权重 $H$ 参数量为：$h \times (n-1)m$；</li><li>从隐藏层到输出层的权重 $U$ 参数量为：$|\mathcal{V}| \times h$；</li><li>从输入层到输出层的权重 $W$ 参数量为：$|\mathcal{V}| \times (n-1)m$；</li><li>输出层偏置 $b$ 的参数量为：$|\mathcal{V}|$；</li><li>隐藏层偏置 $d$ 的参数量为：$h$。</li></ul><p>那么总的参数量为以上所有参数的数量相加得到：$|\mathcal{V}|(1+mn+h)+h(1+(n-1)m)$。可以看到整个模型的参数随着词表 $\mathcal{V}$ 线性增长，不像统计语言模型那样是指数型增长的。另外，模型参数量也与滑动窗口大小 $n$ 成正比。 </p><h1 id="3-实验结果"><a href="#3-实验结果" class="headerlink" title="3. 实验结果"></a>3. 实验结果</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210323154829.png" alt></p><h1 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h1><p><em>Bengio</em> 等人提出一种使用神经网络训练 <em>n-gram</em> 模型的方法，该方法具有以下几个优点：</p><ol><li>相比传统的统计语言模型，Ngram的N增加只带来线性提升，而非指数复杂的提升；</li><li>通过高维空间连续稠密的词向量解决统计语言模型中解决稀疏的问题，不用进行平滑等操作；</li><li>另外词向量的引入解决统计语言模型部分相似性的问题，为后续词向量时代的发展做铺垫；</li><li>相比传统的统计语言模型，神经网络的非线性能力获得更好的泛化能力，perplexity（困惑度下降）</li></ol><p>但是不可避免的也存在一些缺点：</p><ol><li>不能处理变长句子序列。由于输入层的神经元是固定的，因此模型必须输入固定长度的序列。</li><li>由于输入序列长度是固定的，因此，模型只能对固定长度的上下文进行建模，而不是对整段序列进行建模。</li><li>序列中的词没有包含位置信息，而实际上我们知道，对于相同的几个词，放在不同的位置整句话就会表达不同的意思。</li><li>尽管全接连神经网络需要学习的参数量远小于 <em>n-gram</em> 但是相比于其他结构的神经网络，其参数量还是过大。</li></ol><p>语言模型的发展随着这篇文章的诞生进入了一个崭新的时代。</p><h1 id="5-Reference"><a href="#5-Reference" class="headerlink" title="5. Reference"></a>5. Reference</h1><p><a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="noopener">A Neural Probabilistic Language Model</a>. <em>Yoshua Bengio， Réjean Ducharme，Pascal Vincent，Christian Jauvin，2003</em></p>]]></content>
      
      
      <categories>
          
          <category> 语言模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Language Model </tag>
            
            <tag> FFNNLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>预训练语言模型-对数线性语言模型</title>
      <link href="/2021/03/16/ptm-log-linear-language-model/"/>
      <url>/2021/03/16/ptm-log-linear-language-model/</url>
      
        <content type="html"><![CDATA[<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><p>回想语言模型问题，我们的任务是在给定前 $j-1$ 个词的情况下，预测第 $j$ 个词：</p><script type="math/tex; mode=display">p(W_j=w_j|W_1=w_1, W_2=w_2, ..., W_{j-1}=w_{j-1}) =  p(w_j|w_1, w_2, ..., w_{j-1})</script><p>在马尔科夫假设条件下：</p><script type="math/tex; mode=display">p(w_j|w_1, w_2, ..., w_{j-1}) \approx  p(w_j|w_{j-n+1:j-1})</script><a id="more"></a><p>在进行参数估计的时候，为了避免因数据稀疏化造成的零词频问题，引入了各种平滑技术。在使用平滑技术的时候，有诸多限制，比如稍有不慎可能造成概率和不为 1 的情况，比如对于线性插值法来说，一旦 n-gram 过多引入的 $\pmb{\alpha}$ 超参数的搜索空间也会变大，找到一个合适的 $\pmb{\alpha}$ 会变得很困难，另外参数估计的复杂度也会变得很高；对于加性平滑和减值平滑来说，对于零词频词的概率分配引入过多的人为假设。</p><p>下面我们介绍一种新的解决数据稀疏带来的参数估计困难的方法——对数线性语言模型（log-linear language model）。实际上对数线性模型在自然语言处理领域有着广泛的应用，这里只介绍利用对数线性模型对语言进行建模，即所谓的对数线性语言模型。</p><h1 id="2-对数线性模型（Log-Linear-Model）"><a href="#2-对数线性模型（Log-Linear-Model）" class="headerlink" title="2. 对数线性模型（Log-Linear Model）"></a>2. 对数线性模型（Log-Linear Model）</h1><p>假设有一个模型可能的输入集 $\mathcal{X}$ ，一个模型可能的有限输出集 $\mathcal{Y}$，一个函数 $f:\mathcal{X}\times\mathcal{Y}\rightarrow\mathbb{R}^d$ 可以将 $(x,y)$ 映射到一个特征向量，其中 $d$ 为向量的维度，另外还有一个参数向量 $v\in\mathbb{R}^d$。对于任意的 $\{(x, y)|x\in \mathcal{X}, y\in \mathcal{Y}\}$，我们定义：</p><script type="math/tex; mode=display">p(y|x; v) = \frac{\exp(v\cdot f(x, y))}{\sum_{y' \in \mathcal{Y}}\exp(v \cdot f(x, y'))}</script><p>其中 $v \cdot f(x,y) = \sum_{k=1}^d v_k \cdot f_k(x, y)$ 表示 $v$ 和 $f(x, y)$ 的内积。上式的意义是在模型参数为 $v$ 时，在 $x$ 的条件下，出现 $y$ 的概率。</p><p>既然模型的名字是线性对数模型，那么显然，我们要对上式求对数：</p><script type="math/tex; mode=display">\log p(y|x;v) = v \cdot f(x,y) -\log \sum_{y' \in \mathcal{Y}} \exp(v \cdot f(x, y'))</script><p>令：</p><script type="math/tex; mode=display">g(x) = \log \sum_{y' \in \mathcal{Y}} \exp(v \cdot f(x, y'))</script><p>上式得：</p><script type="math/tex; mode=display">\log p(y|x;v) = v\cdot f(x,y) -g(x)</script><p>从上式可以看出，只要 $x$ 固定，那么 $\log p(y|x, v)$ 就是一个线性方程，所以模型的名称是对数线性模型。</p><p>好了，我们已经给出了对数线性模型的定义了，那这样一个模型和语言模型有什么关系呢？假设我们有下面一段话：</p><blockquote><p>  我是中国少年先锋队队员，我在队旗下宣誓，我决心遵守队章，在中国共产党和共青团的领导下，做个好队员。好好学习，好好工作，好好劳动，准备着为共产主义和祖国的伟大事业，贡献出一切（       ）！</p></blockquote><p>我们首先令 $\pmb{h}$ 表示 n-gram，$y$ 表示需要填入括号中的词，计算 $v \cdot f(\pmb{h}, y)$ 内积，可以得到：</p><div class="table-container"><table><thead><tr><th style="text-align:center">$v \cdot f(\pmb{h}, y=力量)$</th><th style="text-align:center">$v \cdot f(\pmb{h}, y=小明)$</th><th style="text-align:center">$v \cdot f(\pmb{h}, y=什么)$</th><th style="text-align:center">$\cdots$</th><th style="text-align:center">$v \cdot f(\pmb{h}, y=这样)$</th></tr></thead><tbody><tr><td style="text-align:center">$8.1$</td><td style="text-align:center">$0.5$</td><td style="text-align:center">$-2.1$</td><td style="text-align:center">$\cdots$</td><td style="text-align:center">$0.01$</td></tr></tbody></table></div><p>要计算表格中的数据，首先要确定 $v$ 和 $f(\cdot)$，这个我们在后面的内容里详细介绍，现在先假定这两个是确定的，然后我们可以根据他们在给定 $\pmb{h}$ 和 $y$ 的情况下计算出相应的内积值。我们计算出来的内积可以是任意实数，包括正数、负数等等。</p><p>然后将计算出来的内积取 $e$ 指数：$\exp(v \cdot f(x,y))$，这样就保证了结果是非负数的了。</p><p>然后我们对结果求和：$\sum_{y’ \in \mathcal{Y}} \exp(v \cdot f(x, y’))$，再利用求和结果对计算出的每一项进行归一化，这样我们就得到了一个对数线性模型。而该模型表示模型参数 $v$ 确定后，给定历史词 $\pmb{h}$ 的条件下，$y$ 出现的概率。</p><p>这样一个结果不就回到了 n-gram 语言模型 $p_{\theta}(w_j|\pmb{h})$，在确定参数 $\theta$ 的情况下，给定历史词 $\pmb{h}$ 的条件下，输出 $y$ 的概率吗？</p><h1 id="3-对数线性语言模型（Log-Linear-Language-Model）"><a href="#3-对数线性语言模型（Log-Linear-Language-Model）" class="headerlink" title="3. 对数线性语言模型（Log-Linear Language Model）"></a>3. 对数线性语言模型（Log-Linear Language Model）</h1><p>为了和 n-gram 语言模型的符号保持一致，我们这里重写对数线性模型在语言模型上的定义：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}p_\theta(\pmb{X}=\pmb{x}) &= \prod_{j=1}^l p_{\theta}(W_j=w_j|W_{0:j-1}=w_{0:j-1}) \\\\                          &= \prod_{j=1}^l \frac{\exp(\theta \cdot f(w_{0:j-1}, w_j))}{\sum_{w_j \in \mathcal{V}}\exp(\theta \cdot f(w_{0:j-1}, w_j))} \\\\                          &= \prod_{j=1}^l \frac{\exp(\theta \cdot f(w_{j-n+1:j-1}, w_j))}{\sum_{w_j \in \mathcal{V}}\exp(\theta \cdot f(w_{j-n+1:j-1}, w_j))} (马尔科夫假设)\\\\                          &= \prod_{j=1}^l \frac{\exp(\theta \cdot f(\pmb{h}, w_j))}{\sum_{w_j \in \mathcal{V}}\exp(\theta \cdot f(\pmb{h}, w_j))} \\\\\end{aligned}\end{equation}</script><p>其中 $\theta$ 为模型参数，$f(\cdot)$ 表示特征投影。$\theta$ 和 $f(\cdot)$ 都是 $d$ 维向量。</p><h2 id="3-1-特征"><a href="#3-1-特征" class="headerlink" title="3.1 特征"></a>3.1 特征</h2><p>对于任意 $\{(x, y)|x\in \mathcal{X}, y\in \mathcal{Y}\}$，$f(x, y) \in \mathbb{R}^d$ 就是一个特征向量，其中 $f_k(x, y), k=[1,2,…,d]$，表示特征。每一维上的特征是可以任意定义的，只要是可以只通过 $\pmb{h}$ 和 $w_j$ 计算的。通常二元法进行定义，比如：</p><ul><li>n-gram 特征。比如对于三元组特征 “$\langle 我,爱,北京 \rangle$”，如果 $\pmb{h} = \langle 我, 爱 \rangle，w_j=北京$，令 $f_k(\pmb{h}, w_j)=1$，否则 $f_k(\pmb{h},w_j)=0$；</li><li>空洞 n-gram 特征。比如 “$\langle 我,爱,北京 \rangle$”、“$\langle我,讨厌,北京\rangle$”、“$\langle 我,在,北京 \rangle$” 等等，只要 $\pmb{h} $ 的倒数第二个词是 “我”，并且 $w_j=北京$ ，那么就令 $f_k(\pmb{h}, w_j)=1$，否则 $f_k(\pmb{h}, w_j)=0$；</li><li>拼写特征，通常在英文语言模型上比较常见。比如判断一个词是否以大写字母开头，是否在词中包含数字，是否以元音字母开头等等，如果答案是 “是”，那么就令 $f_k(\pmb{h}, w_j)=1$，否则 $f_k(\pmb{h}, w_j)=0$；</li><li>类别特征，使用外部资源判断一个词是不是某种类别。比如是不是地名、是不是组织机构名、是不是名词、是不是形容词等，如果答案是 “是”，那么就令 $f_k(\pmb{h}, w_j)=1$，否则 $f_k(\pmb{h}, w_j)=0$；</li><li>$\cdots$</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">n-gram 特征</th><th style="text-align:center">空洞 n-gram特征</th><th style="text-align:center">拼写特征</th><th style="text-align:center">类别特征</th><th>…</th><th style="text-align:center">$\times \times$ 特征</th></tr></thead><tbody><tr><td style="text-align:center">$f_1=1$</td><td style="text-align:center">$f_2=1$</td><td style="text-align:center">$f_3=0$<br>（$w_j=北京$ 不包含数字）</td><td style="text-align:center">$f_3=1$ <br>（$w_j=北京$ 是地名）</td><td>…</td><td style="text-align:center">$f_d(\pmb{h}, w_j)=0$</td></tr></tbody></table></div><p>这样，我们对 “$\langle 我，爱，北京 \rangle$” 构建了一个 $f(x, y)=[1,1,0,1,…,0]$ 的特征向量。</p><p>总结一下我们构建某种特征的方式：</p><script type="math/tex; mode=display">f(\pmb{h}, w_j) = \begin{cases}1 & 如果 \pmb{h}，w_j 满足某种条件 \\\\0 & 否则\end{cases}</script><p>在特征构建的时候，$d$ 作为超参数出现，特征过多容易造成过拟合，特征不足会造成欠拟合。在构造特征的时候有一个重要的原则是必须遵守的，那就是区分性。也就是说任意两个不同的 $(\pmb{h}_i,w_i)$ 和 $(\pmb{h}_j, w_j)$ 都必须有不同的特征向量。所以通常 $d$ 的选取不应该小于 $V^{n}$ ，$n$ 表示 n-gram。</p><h2 id="3-2-特征稀疏"><a href="#3-2-特征稀疏" class="headerlink" title="3.2 特征稀疏"></a>3.2 特征稀疏</h2><p>上述方法构建特征向量有一个很严重的问题——特征稀疏化。通常自然语言处理任务用到的 $d$ 都会非常大。以 3-gram 为例，假设我们给每一个 $(w_{j-2}, w_{j-1}, w_j)$ 分配一个特征值（one-hot），那么 $d=V^3$。如此庞大的特征量，怎样使模型更有效率就成了一个问题。</p><p>对于任意给定的 $(\pmb{h},w_j)$ ，定义特征向量 $[f_1(\pmb{h}, w_j), f_2(\pmb{h}, w_j), …, f_d(\pmb{h}, w_j)]$ 中 $f_k(\pmb{h}, w_j)=1$ 的个数：</p><script type="math/tex; mode=display">N(\pmb{h}, w_j) = \sum_{f(\pmb{h}, w_j)=1}f_k(\pmb{h}, w_j)</script><p>通过实际观察我们会发现，通常 $N(\pmb{h}, w_j) \ll d$。最极端的情况，考虑 one-hot 特征向量，只有一个 1，其余全部是 0。</p><p>为了解决特征稀疏的问题，我们回过头去看对数线性语言模型的定义：</p><script type="math/tex; mode=display">p_\theta(\pmb{X}=\pmb{x}) = \prod_{j=1}^l \frac{\exp(\theta \cdot f(\pmb{h}, w_j))}{\sum_{w_j \in \mathcal{V}}\exp(\theta \cdot f(\pmb{h}, w_j))}</script><p>通过观察我们会发现，上式的核心是 $\theta \cdot f(\pmb{h},w_j)$ 的计算：</p><script type="math/tex; mode=display">\theta \cdot f(\pmb{h}, w_j) = \sum_{k=1}^d \theta_k\cdot f_k(\pmb{h}, w_j)</script><p>当 $f_k(\pmb{h},w_j)=0$ 时，$\theta_k \cdot f_k(\pmb{h}, w_j)=0$，所以内积最终的结果取决于 $f_k(\pmb{h}, w_j) \ne 0$ 的那些特征。</p><p>有了这样一个发现，我们就可以通过一些函数（比如哈希表）找到特征向量中的非零特征对应的索引：</p><script type="math/tex; mode=display">Z(\pmb{h}, w_j) = \{k: f_k(\pmb{h},w_j)=1\}</script><p>有了 $Z(\pmb{h},w_j)$ 以后，可以得到：</p><script type="math/tex; mode=display">\sum_{k=1}^d \theta_k\cdot f_k(\pmb{h}, w_j) = \sum_{k\in Z(\pmb{h}, w_j)} \theta_k</script><p>计算复杂度从 $\mathcal{O}(d)$ 降到 $\mathcal{O}(|Z(\pmb{h}, w_j)|)$。</p><h2 id="3-3-Softmax"><a href="#3-3-Softmax" class="headerlink" title="3.3 Softmax"></a>3.3 Softmax</h2><p>得到特征向量以后，我们可以通过 softmax 将特征向量的分数映射到概率分布上:</p><script type="math/tex; mode=display">\mathrm{softmax}([f_1, f_2, ..., f_d]) = \left[\frac{\exp(f_1)}{\sum_{k=1}^V\exp(f_k)}, \frac{\exp(f_2)}{\sum_{k=1}^V\exp(f_k)}, ..., \frac{\exp(f_d)}{\sum_{k=1}^V\exp(f_k)} \right]</script><p>之所以使用 softmax 进行映射，是因为：</p><ol><li>保持了原来向量的单调性，即原来的特征 $f_a&gt;f_b$，经过 softmax 映射以后仍然是 $f_a’&gt;f_b’$;</li><li>softmax 将原来的特征向量映射到了概率分布上，即向量所有元素和为 1；</li><li>向量中的每个元素代表其对应的历史词 $\pmb{h}$ 出现的概率。</li></ol><h2 id="3-4-系数-权重"><a href="#3-4-系数-权重" class="headerlink" title="3.4 系数/权重"></a>3.4 系数/权重</h2><p>一旦我们将特征映射到特征向量以后，就可以计算 $\theta \cdot f(\pmb{h}, w_j)$ 了，其中 $\theta$ 我们称之为特征权重或者特征系数。我们可以将这样一个线性映射看成是 n-gram 的得分，特征权重 $\theta$ 中的每一维决定了特征向量中的每一个向量对 n-gram 最终得分的影响力。假设 $f_k(\pmb{h},w_j)=1$：</p><ul><li>当 $\theta_k &gt; 0$ 时，表明 $f_k(\pmb{h}, w_j)$ 在 n-gram 中起到了正向作用，即 $(\pmb{h},w_j)$ 出现的概率更高了；</li><li>当 $\theta_k&lt;0$ 时，表明 $f_k(\pmb{h}, w_j)$ 在 n-gram 中起到了反向作用，即 $(\pmb{h},w_j)$ 出现的概率更低了；</li><li>当 $\theta_k=0$ 时，表明 $f_k(\pmb{h}, w_j)$ 在 n-gram 中没有任何影响，即 $(\pmb{h},w_j)$ 出现的概率不变；</li></ul><p>知道了 $\theta$ 的意义，那么我们该怎么得到权重系数向量呢？还是靠验证集一个一个试吗？显然不靠谱，接下来就介绍一下 $\theta$ 的估计方法。</p><h3 id="3-4-1-对数似然函数"><a href="#3-4-1-对数似然函数" class="headerlink" title="3.4.1 对数似然函数"></a>3.4.1 对数似然函数</h3><p>之前我们定义了对数线性语言模型，每一个 n-gram 的对数概率是：</p><script type="math/tex; mode=display">\log p(w_j|\pmb{h},\theta)</script><p>那么每个句子的对数概率为：</p><script type="math/tex; mode=display">L_\theta(\pmb{X}=\pmb{x}) = \sum_{j=1}^l \log p(w_j|\pmb{h},\theta)</script><p>如果我们将 $\theta$ 作为变量，即：</p><script type="math/tex; mode=display">L_{\pmb{x}}(\theta) = \sum_{j=1}^l \log p(w_j|\pmb{h},\theta)</script><p>可以将此式理解为：参数 $\theta$ 在多大程度上使语言模型接近训练数据。我们的目标是找到一套参数，使得语言模型最大程度接近训练数据：</p><script type="math/tex; mode=display">\theta = \arg \max_{\theta \in \mathbb{R}^d} L_{\pmb{x}}(\theta)</script><p>在考虑如何对上式求解之前，我们思考这样一种情况：假设我们有一个三元组 $s = \langle 我，爱，北京\rangle$ 在训练集中只出现了一次，此时 $\theta$ 应该是什么样的。</p><p>根据概率最大原则，因为 $s$ 只出现一次，没有其他三元组和它分割概率，那么它出现的概率最大应该是 1，也就意味着：</p><script type="math/tex; mode=display">p_\theta(\pmb{X}=\pmb{我爱北京天安门}) = \prod_{j=1}^l \frac{\exp(\theta \cdot f(\pmb{h}, w_j))}{\sum_{w_j \in \mathcal{V}}\exp(\theta \cdot f(\pmb{h}, w_j))}=1</script><p>上式分子为（假设特征向量只有$f_k=1$，其余全部为零）：</p><script type="math/tex; mode=display">\exp(\theta_k)</script><p>分母为：</p><script type="math/tex; mode=display">\sum_{w_j \in \mathcal{V}}\exp(\theta_k)</script><p>当且仅当 $\theta_k \rightarrow \infty$ 时</p><script type="math/tex; mode=display">\lim_{\theta_k \rightarrow \infty} \frac{\exp(\theta_k)}{\sum \exp(\theta_k)} = 1</script><p>也就是说，这种情况会导致特征权重接近无穷，这显然是不对的。为了解决这个问题，通常采用的方法是加入正则化。</p><h3 id="3-4-2-正则化"><a href="#3-4-2-正则化" class="headerlink" title="3.4.2 正则化"></a>3.4.2 正则化</h3><p>正则化的方法有很多，比如 L1 正则化、L2 正则化等等。常用的正则化是 L2 正则化， 定义如下：</p><script type="math/tex; mode=display">||\theta||^2 = \sum_k \theta_k^2</script><p>其中 $||\theta||$ 表示向量 $\theta$ 的欧拉长度，即 $||\theta|| = \sqrt{\sum_k \theta_k^2}$。经过正则化修正的目标函数为：</p><script type="math/tex; mode=display">L'(\theta)=\sum_{j=1}^l \log p(w_j|\pmb{h};\theta) - \frac{\lambda}{2} \sum_k\theta_k^2</script><p>其中 $\lambda&gt;0$ 是一个超参数。</p><p>使用正则化避免出现参数过大的原理是这样的：上式第一项的意义是参数 $\theta$ 能使语言模型在多大程度上接近训练数据；第二项的意义是对参数过大的惩罚项：它希望参数尽可能接近零，因为欧拉距离是大于等于零的，$\lambda$ 也是一个大于零的数，也就是说第二项是一个大于零的数，要想使得 $L’(\theta)$ 尽可能大，就需要上式中第一项尽可能大，而第二项尽可能小。</p><h3 id="3-4-3-梯度下降"><a href="#3-4-3-梯度下降" class="headerlink" title="3.4.3 梯度下降"></a>3.4.3 梯度下降</h3><p>有了目标函数以后，我们可以通过梯度下降算法对参数进行更新，直到参数收敛。</p><p>参数更新过程如下：</p><blockquote><ul><li>初始化 $\theta=\pmb{0}$ （元素全部是 0 的向量）</li><li>重复下面的过程：<ol><li>计算 $\delta_k=\frac{dL’(\theta)}{d\theta}, k=[1,2,…,d]$;</li><li>计算 $\beta^*=\arg \max_{\beta \in \mathbb{R}}L’(\theta+\beta \delta)$，</li><li>更新 $\theta \leftarrow \theta + \beta^*\delta$</li></ol></li></ul></blockquote><p>其中</p><script type="math/tex; mode=display">\frac{dL'(\theta)}{d_{\theta_k}} = \sum_{i=1}^n f_k(\pmb{h}^{(i)}, w_j^{(i)})-\sum_{i=1}^n\sum_{w_j\in\mathcal{Y}} p(w_j|\pmb{h}^{(i)};\theta)f_k(\pmb{h}_{(i)}, w_j)-\lambda\theta_k</script><h1 id="4-小结"><a href="#4-小结" class="headerlink" title="4. 小结"></a>4. 小结</h1><p>对数线性模型的出现为我们打开了一条通往神经网络语言模型的道路，我们发现对数线性语言模型中蕴含的思想已经具备了神经网络语言模型的雏形。尤其是将 n-gram 扩展到了特征这一概念，为后来的特征向量分布式表示奠定了基础。这里使用的特征构建方法虽然简单，但是与分布式特征向量表示相比也存在着优点，那就是每一维上的特征都具有具体的含义，这就意味着，该模型是具有可解释性的。神经网络语言模型对特征的自动构建也有很多对数线性语言模型特征向量不具备的优势，比如无需人工参与、可以大幅度缩减参数维度、可计算词与词之间的语义关系等。总而言之，对数线性语言模型简约而不简单。</p>]]></content>
      
      
      <categories>
          
          <category> 语言模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Log-Linear Language Model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>预训练语言模型-统计语言模型</title>
      <link href="/2021/03/16/ptm_probabilistic_language_model/"/>
      <url>/2021/03/16/ptm_probabilistic_language_model/</url>
      
        <content type="html"><![CDATA[<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><h2 id="1-1-词表-语料"><a href="#1-1-词表-语料" class="headerlink" title="1.1 词表/语料"></a>1.1 词表/语料</h2><p>词表表示语言中包含的所有的词，比如对于中文来说：</p><a id="more"></a><script type="math/tex; mode=display">\mathcal{V} = \{我, 爱, 北京, 天安门, ...\}</script><blockquote><p>  ps: 实际上对于中文来说，词表可以分成两类：字表和词表。字表就是所有的汉字，词表指的是汉字组成的词。由于中文句子中不存在英文那种使用空格作为词与词之间的分割符的标志，因此怎样准确地进行分词也是中文自然语言处理的非常重要的工作。使用字作为句子的基本单位不存在分词的问题但是会丢失部分以词作为基本单位的语义信息。因此，以字和词作为句子的基本单位各有优劣。我们在这里讨论语言模型的时候以词作为基本单位。</p></blockquote><p>令 $x = w_1w_2…w_n$ 表示一个句子，其中 $w_i \in \mathcal{V}$ 以及 $i \in \{ 1, …, (n-1)\}, n\ge 1$。令 $V$ 表示词表 $\mathcal{V}$ 的大小，写作 $|\mathcal{V}|$。</p><p>我们假设 $w_n$ 是一个特殊符号—— $STOP$ ，这个符号包含在词表中（在后面的内容中我们可以看到为什么要定义这样一个符号了），比如：</p><blockquote><p>  我爱北京天安门STOP</p><p>  我爱吃苹果STOP</p><p>  小狗好可爱STOP</p><p>  …</p></blockquote><p>令 $\mathcal{V}^+ = \{x_1, x_2, …\}$ 表示所有由词表中的词组成的句子集合。$\mathcal{V}^+$ 是一个无限集，因为句子的长度是可以任意的（$\ge 1$）。</p><p>一个理想的词表应该包含索要处理的语言的所有词汇，但实际上这是不可能的，因为：</p><ol><li>每种语言（被广泛使用的）随时随刻都在产生新的词汇，有些是从别的语言借鉴而来，有些是随着新生事务的产生而产生的；</li><li>通常在构建词表的时候是通过训练语料，训练语料的数量是有限了，不可能包含该种语言的所有句子；</li><li>一个新词可以通过一些规则排列组合，而这样的组合可以产生的新词数量是无穷无尽的；</li><li>即使是同一个字/词在不同的时代都会有不同的拼写方式。</li></ol><p>由于以上原因的限制，我们所用到的词表通常是有限的。而一个不能包含所有词汇的词表通常会遇到一个问题，当我们处理的句子中包含一个不在词表中的词时，我们称这个问题为 OOV（Out Of Vocabulary）问题，而这个不在词表中的词称之为“未登录词”。未登录词的出现会给自然语言处理任务带来很大的困扰，对于传统的统计方法来说，未登录词就意味着我们没有关于这个词的统计信息，无法将它纳入模型中；对于神经网络来说，我们没有办法对它进行向量化，也会对后续的模型训练和推理造成问题。</p><p>面对 OOV 问题通常有三种解决方式：</p><ol><li>在词表中添加一个特殊符号，比如 “<unk>” ，用来表示所有未登录词。在训练数据中，将一些出现频率较低的词替换成 “<unk>”。</unk></unk></li><li>对于形态学上比较丰富的语言，可以训练一个模型将一个词分割成几个子串可以覆盖更多的词汇，比如对于英语来说，可以把 “starting” 分成 “start” 和 “ing” 两部分，这样即使词表中没有 “starting” 这个词，也不会遇到 OOV 问题。</li><li>使用字，而不是使用词来处理，或者字和词相互结合的方式。对于任何一种语言来说，它的最基本元素是字，而字的个数通常是有限的，任何一个词都可以使用字的组合来实现。比如英文只有26个字母，中文常用字也不过万。</li></ol><p>最常用，成本最低的方式是将未登录词替换成 <unk> ，因此这里我们以这种方式为例介绍语言模型。</unk></p><h2 id="1-2-语言模型的定义"><a href="#1-2-语言模型的定义" class="headerlink" title="1.2 语言模型的定义"></a>1.2 语言模型的定义</h2><p>语言模型是一个随机变量的概率分布，随机变量的值来源于 $\mathcal{V}^+$ 中。因此语言模型可以定义为 $p:\mathcal{V}^+ \rightarrow \mathbb{R}$：</p><script type="math/tex; mode=display">\forall x \in \mathcal{V}^+, p(x) \ge 0, \\\\\sum p(X=x) = 1</script><p>建立一个语言模型可以分成以下几步：</p><ol><li>收集训练语料 $x_{1:n}=\{x_1, x_2, …, x_n\}, x_i \in \mathcal{V}^+$；</li><li>从语料中构建一个词表 $\mathcal{V}$；</li><li>定义概率分布 $p$ 的参数；</li><li>利用训练语料进行参数估计。</li></ol><p>举例说明我们如何从训练语料中学习一个语言模型：</p><p>令 $c(x_i)$ 为训练语料中句子 $x_i$ 出现的次数，$N$ 为句子总数。我们可以令：</p><script type="math/tex; mode=display">p(x_i) = \frac{c(x_i)}{N}</script><p>这就是一个最简单的语言模型（ps：我们给这个模型取个名字叫 <em>Creepy model</em> 吧，其实这个名字是 <em>Noah A. Smith</em> 取的），他表示每个句子出现的概率。当然，这是一个非常简单的例子，因为对于没有出现在训练语料中的句子他会认为概率为 0，因此这个语言模型不能泛化到训练语料中不存在的句子上。而在接下来要介绍的统计语言模型就是为了尽可能的提升这种泛化性。</p><h2 id="1-3-我们为什么要进行语言建模？"><a href="#1-3-我们为什么要进行语言建模？" class="headerlink" title="1.3 我们为什么要进行语言建模？"></a>1.3 我们为什么要进行语言建模？</h2><p>从上面的介绍我们可以大致了解到，所谓语言模型其实就是判断一个句子出现的概率。一个经常被人们使用的句子出现的概率就会高，被人们使用频率较低的句子出现概率就会低，人们几乎不使用的句子出现的概率几乎为零。这实际上就是在判断一个句子是一个正常的为人所用的句子的概率，简而言之就是判断一句话是不是人话，比如，$p(我爱北京天安门) \gt p(但是风格的歌)$” 说明前一句比后一句更像人话。</p><p>比如早期的语言建模动机来源于 “噪声通道范式” （noisy channel paradigm），该模型广泛应用于语音识别和机器翻译领域。比如语音识别先利用声学模型产生多个候选结果，然后利用语言模型判断哪一句话更像人话进行结果重排。</p><blockquote><p>   所谓的噪声通道是指，假设我们有两个随机变量，$O$ 和 $D$，$O$ 是系统输入（可观测量），$D$ 是我们希望系统的输出。我们可以认为 $O$ 的值是密文， $D$ 的值是 $O$ 解密后的源文本。解密过程需要：</p><script type="math/tex; mode=display">   \begin{equation} \nonumber   \begin{aligned}   \pmb{d}^* &= \arg \max_{\pmb{d}} p(\pmb{d}|\pmb{o}) \\\\            &= \arg \max_{\pmb{d}} \frac{p(\pmb{o}|\pmb{d})\cdotp(\pmb{d})}{p(\pmb{o})} \\\\            &= \arg \max_{\pmb{d}} \underbrace{p(\pmb{o}|\pmb{d})}_{\mathrm{channel\ model}} \cdot \underbrace{p(\pmb{d})}_{\mathrm{source\ model}}   \end{aligned}   \end{equation}</script><p>   从上式可以看出，我们可以通过先验的明文（$p(\pmb{d})$）和明文对应的密文（$p(\pmb{o}|\pmb{d})$）建模对密文进行解密。</p><script type="math/tex; mode=display">   \mathrm{source} \rightarrow D \rightarrow \mathrm{channel} \rightarrow O</script><p>   首先源分布随机生成一个明文值对应 $D$，然后通过在该明文中添加噪声（channel）生成观测值 $O$ 。而解码的过程就是将这个过程反过来，利用贝叶斯公式在给定观测值 $O=o$ 时得到最有可能的 $D$。</p></blockquote><p>另一个很重要的原始是在语言建模过程中使用到的参数估计方法也可以被广泛应用于其他领域。</p><h1 id="2-语言模型"><a href="#2-语言模型" class="headerlink" title="2. 语言模型"></a>2. 语言模型</h1><p>前面我们举例介绍了一个非常简单的语言模型，但是这种语言模型几乎没有任何泛化性，为了使得我们的语言模型具有一定的泛化性，接下来介绍统计语言模型。</p><p>考虑一个随机变量序列 $ \langle W_1, W_2, …, W_l \rangle $，序列中每个随机变量可以从 $\mathcal{V}$ 中取值。我们的目标是对于任意序列 $\langle w_1, w_2, …, w_l \rangle$ 对他们的联合概率进行建模，其中 $n \ge 1, w_i \in \mathcal{V} (i=1, …, l)$：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}p(W_1=w_1, ..., W_l=w_l) &= \left(\begin{matrix}p(W_1=w_1|W_0=w_0) \times \\\\p(W_2=w_2|W_0=w_0, W_1=w_1) \times \\\\p(W_3=w_3|W_0=w_0, W_1=w_1, W_2=w_2) \times \\\\\cdots \\\\p(W_n=w_n|W_0=w_0, W_1=w_1, W_2=w_2, ...W_{l-1}=w_{l-1})\end{matrix}\right) \\\\&= \left(\begin{matrix}p(W_1=w_1|W_0=w_0) \times \\\\p(W_2=w_2|W_{0:1}=w_{0:1}) \times \\\\p(W_3=w_3|W_{0:2}=w_{0:2}) \times \\\\\cdots \\\\p(W_n=w_n|W_{0:l-1}=w_{0:l-1})\end{matrix}\right) \\\\&= \prod_{j=1}^l p(W_j=w_j|W_{0:j-1}=w_{0:j-1})\end{aligned}\end{equation}</script><p>我们令 $W_0=w_0=START$ 表示任意句子的起始符。</p><p>上式的 $l$ 并不是一个定值，任何语言中的句子都有不同的长度，在建模过程中我们怎么确定每个句子的长度呢？这个时候我们在介绍词表的时候介绍了一个特殊的符号——$STOP$ 就发挥作用了，当我们建模过程中遇到这个符号的时候就表明这个句子到此为止了，此时句子的长度就是 $l$。</p><p>以上的建模过程没有涉及任何假设，我们可以看到序列中每个词的产生都依赖于在它之前出现的所有的词。上式中的每一部分都是一个条件概率，对于条件概率我们可以用简单的统计方法进行估计：</p><script type="math/tex; mode=display">p(W_j=w_j|W_{0:j-1}=w_{0:j-1}) = \frac{c(w_{0:j})}{c(w_{0:j-1})}</script><p>其中 $c(\cdot)$ 表示序列出现在语料中的次数。例如：序列 $\langle 我， 爱， 北京， 天安门 \rangle$ </p><script type="math/tex; mode=display">p(北京|我，爱) = \frac{c(我，爱，北京)}{c(我，爱)}</script><p>由此可得</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}p(W_1=w_1, ..., W_l=w_l) &= \prod_{j=1}^n \frac{c(w_{0:j})}{c(w_{0:j-1})} \\\\                                  &= \frac{\prod_{j=1}^n c(w_{0:j})}{\prod_{j=1}^n c(w_{0:j-1})} \\\\                                  &= \frac{ \qquad \qquad c(w_{0:1})\times c(w_{0:2}) \times ... \times c(w_{0:l-1}) \times c(w_{0:l})}{c(w_0) \times c(w_{0:1})\times c(w_{0:2}) \times ... \times c(w_{0:l-1})  \qquad  \qquad} \\\\                                  &= \frac{c(w_{0:l})}{c(w_0)} \\\\                                  &= \frac{c(\pmb{x})}{n}\end{aligned}\end{equation}</script><p>其中 $n$ 表示训练语料的句子数， $\pmb{x}$ 表示由 $\langle w_0, w_1, …, w_n \rangle$ 构成的句子。 </p><p>到了这一步我们会遗憾的发现，这样构建起来的语言模型又回到了原点，一切都未曾改变！</p><h2 id="2-1-Uni-Gram-模型"><a href="#2-1-Uni-Gram-模型" class="headerlink" title="2.1 Uni-Gram 模型"></a>2.1 Uni-Gram 模型</h2><p>上述建模过程我们没有涉及到任何假设，认为句子中每个词都依赖于在它之前的所有词，造成一个很尴尬的局面。现在我们走向另一个极端，给模型加入一个非常强的假设看看模型会变成什么样子：句子中的每个词都是相互独立互不依赖的。在这个假设下：</p><script type="math/tex; mode=display">p(W_j=w_j|W_{0:j-1}=w_{0:j-1}) = p_{\theta}(W_j=w_j)</script><p>那么</p><script type="math/tex; mode=display">p_{\theta} (\pmb{W}=\pmb{w})= \prod_{j=1}^l p_{\theta}(W_j=w_j)</script><p>此时我们需要统计句子中每个词在语料中出现的概率：</p><script type="math/tex; mode=display">\theta_w = \frac{c(w)}{N}</script><p>其中 $N$ 表示语料中所有的词的数量。注意在计算频率的时候 $START$ 符号是不进行计算的，因为它作为句子的起始符并不是由模型生成的；但是需要计算 $STOP$ 符号，因为他是作为模型生成的结果出现的，它的概率通常是 $n/N$。Uni-Gram 模型的参数量为 $V$，对应每个 $w \in \mathcal{V}$。</p><p>通常把 Uni-Gram 模型称之为“词袋模型”，因为它并不关注句子的顺序，只关注句子中每个词出现的概率。也就是说，对于相同的词组成的句子的概率是相同的。比如 $p(START我爱北京天安门STOP) = p(START天安门我爱北京STOP)$。</p><p>Uni-Gram 模型非常简单易于理解，建模也非常容易。虽然存在一些问题，但是在一些任务中却非常实用（比如信息检索）。</p><h2 id="2-2-Bi-Gram-模型"><a href="#2-2-Bi-Gram-模型" class="headerlink" title="2.2 Bi-Gram 模型"></a>2.2 Bi-Gram 模型</h2><p>前面两种语言模型走了两个极端：一个过于精细导致模型不能泛化，另一个过于粗放导致模型缺乏准确。下面我们取个折中的假设：假设句子中每个词出现的概率只依赖于它前面的那个词。</p><script type="math/tex; mode=display">p(W_j=w_j|W_{0:j-1}=w_{0:j-1}) = p_{\theta}(W_j=w_j|W_{j-1}=w_{j-1}) \\\\p_{\theta}(\pmb{X}=\pmb{x}) = \prod_{j=1}^l p_{\theta} (W_j=w_j|W_{j-1}=w_{j-1})</script><p>Bi-Gram 模型又叫一阶马尔科夫模型，因为这个假设是一阶马尔科夫假设。</p><p>经过前面的讨论，不难得出：</p><script type="math/tex; mode=display">\theta_{w_j} = \frac{c(w_{j-1}, w_j)}{c(w_{j-1})}</script><p>Bi-Gram 模型参数量为词表中任意两个词的随机组合，即 $\approx V^2$。</p><h2 id="2-3-n-Gram-模型"><a href="#2-3-n-Gram-模型" class="headerlink" title="2.3 n-Gram 模型"></a>2.3 n-Gram 模型</h2><p>既然可以有一阶马尔科夫假设，那也可以有二阶马尔科夫假设，三阶马尔科夫假设等等。这些依赖马尔科夫假设的模型我们统称为 “n-Gram” 语言模型。其中 $n$ 表示当前词出现的概率依赖于前 $n-1$ 个词。</p><p>n-Gram 模型的通用形式是：</p><script type="math/tex; mode=display">p(W_j=w_j|W_{0:j-1}=w_{0:j-1}) = p_{\theta}(W_j=w_j|W_{j-n+1:j-1}=w_{j-n+1:j-1}) \\\\p_{\theta}(\pmb{X}=\pmb{x}) = \prod_{j=1}^l p_{\theta}(W_j=w_j|W_{j-n+1:j-1}=w_{j-n+1:j-1})</script><p>为了简化公式符号，我们令 $\pmb{h}$ 为 $w_j$ 的历史词汇，比如对于 Bi-Gram 语言模型，$\langle 我，爱，北京，天安门 \rangle$ 中”北京“ 的历史词汇是”爱“，对于 Tri-Gram 语言模型来说，”北京“的历史词汇就是”我，爱“。此时，我们可以得到模型的参数：</p><script type="math/tex; mode=display">\theta_{w_j|\pmb{h}} = \frac{c(\pmb{h}, w_j)}{\pmb{h}}</script><p>n-Gram 模型中的 $n$ 值越大，模型越接近 Creepy model，$n$ 值越小模型越接近 Uni-Gram 模型。通常 $n$ 值的选取取决于训练数据，这里我们可以通过一些简单的步骤进行 $n$ 值的选取：</p><ol><li>在构建训练数据的时候留出一部分作为验证集；</li><li>令 $\mathcal{N}$ 为你期望的 $n$ 的取值范围集合，比如 $\mathcal{N} = \{1,2,3,4,5\}$；</li><li>对于任意 $g \in \mathcal{N}$：（a）在训练数据集上构建 $g$-Gram 语言模型；（b）在验证集上计算 $g$-Gram 模型的困惑度（Perplexity，用来评估语言模型好坏的指标，后面会介绍）；</li><li>选择困惑度最低的 $g$-Gram 模型。</li></ol><h2 id="2-4-语言模型评估：困惑度（Perplexity-ppl）"><a href="#2-4-语言模型评估：困惑度（Perplexity-ppl）" class="headerlink" title="2.4 语言模型评估：困惑度（Perplexity, ppl）"></a>2.4 语言模型评估：困惑度（Perplexity, ppl）</h2><p>现在我们知道怎么样构建一个语言模型了，但我们应该怎样对我们构建的语言模型进行评估呢？目前通用的指标是困惑度（Perplexity），定义如下：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}\mathrm{perplexity}(p;\bar{\pmb{x}}) &= 2^{-\left(\frac{1}{M}\log_2\prod_{i=1}^m p(\bar{\pmb{x}}_i)\right)} \\\\                                     &= 2^{-\left(\frac{1}{M}\sum_{i=1}^m -\log_2 p(\bar{\pmb{x}}_i)\right)}\end{aligned}\end{equation}</script><p>其中 $\bar{\pmb{x}}$ 是未参与训练的验证集（<strong>验证集不能参与训练这一点非常重要</strong>），$M$ 是验证集中词的数量。这个公式看起来好吓人哦，但是别着急，我们拆开了，掰碎了一点一点来看，这到底是个什么东西。</p><p>假设我们在训练数据集之外有一个验证数据集，$\langle \bar{\pmb{x}}_1, \bar{\pmb{x}}_2, …, \bar{\pmb{x}}_m \rangle$，其中每个 $\bar{\pmb{x}}_i, i \in \{1, 2, …, m\}$ 都是一个句子，每个句子都是由 $\langle w_{i1}, w_{i2}, …, w_{in} \rangle$ 组成的序列，$w_{ij}, j \in \{1,2,…,n\}$ 表示句子 $\bar{\pmb{x}}_i$ 中的第 $j$ 个词。 </p><p>对于验证集中的每一句话我们都可以计算它的概率 $p(\bar{\pmb{x}}_i)$，那么对于整个验证集的所有句子出现的概率为：</p><script type="math/tex; mode=display">p(\bar{\pmb{x}})=\prod_{i=1}^m p(\bar{\pmb{x}}_i)</script><p>直观上来讲，语言模型的质量越高，那么 $p(\bar{\pmb{x}})$ 的值应该越大。困惑度指标就是基于这一基本假设提出的。</p><p>假设我们的验证集中包含 $N$ 个词，我们定义：</p><script type="math/tex; mode=display">\frac{1}{N}\log_2\prod_{i=1}^mp(\bar{\pmb{x}}_i) = \frac{1}{N} \sum_{i=1}^m \log_2p(\bar{\pmb{x}}_i)</script><blockquote><p>  这里先回答两个问题：</p><ol><li><p>为什么要取对数？</p><p>① 语言模型作为一种概率分布，对于每一个句子出现的概率一定是满足 $0&lt;p(\pmb{x})&lt;1$ 的，即使每个句子出现的概率都能达到 0.99，当我们的验证语料包含 1 万个句子的时候，$\prod_i p(\pmb{x}_i) = 0.99^{10000} \approx 2.2e^{-44}$。所以我们看到，我们直接相乘的时候很容易造成极小的数字，最后精度溢出。而如果我们取对数，最后的结果会是 $\log_2(0.99^{10000})\approx -145$，这样就不会造成精度溢出问题了 。</p><p>② 对于计算机来说，乘法的效率要低于加法，因此，通过取对数将计算过程从乘法变成加法，提高了困惑度的计算效率。</p></li><li><p>为什么以 2 为底？</p><p>首先，其实以几为底并不重要，因为通过下面的介绍我们会发现，最后还是会通过取 2 的指数来进行 “还原”，2 的作用相当于被抵消了。其次，以 2 为底可以使计算曲线更加平滑，不会造成大起大落的效果。</p><p>实际上对于这两个问题以上的解释并没有触及问题的本质，要真正理解这两个问题需要从信息熵的角度出发，具体可以看<a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf" target="_blank" rel="noopener">这里</a>。</p></li></ol></blockquote><p>有了以上定义，我们就可以定义困惑度了：</p><script type="math/tex; mode=display">\mathrm{perplexity} = 2^{-l} \\\\l = \frac{1}{N} \sum_{i=1}^m \log_2p(\bar{\pmb{x}}_i)</script><p>困惑度取了负指数，说明困惑度值越低，语言模型越好。为什么要这样定义呢？我们以最简单的情形加以说明。</p><p>假设我们有一个词表 $\mathcal{V}$ ，词表大小为 $V$，语言模型服从均匀分布，即对于语言模型来说词表中的每个词出现的概率是一样的，比如对于 Uni-Gram 来说 $p(w_i) = 1/V$，对于Bi-Gram 来说 $p(w_i|w_{i-1})=1/V$ 等等。我们将这样一个分布代入到上面的困惑度计算公式中会发现困惑度为 $V$，也就是词表的大小。</p><p>因此，我们可以认为，<strong>困惑度其实相当于一个等效词表，困惑度的值表明对于模型来说预测下一个词有多少种选择</strong>。比如困惑度为 120 时，当模型遇到“我爱”的时候需要从 120 个选择中准确找到“北京”这个词。（选择越多越困惑）</p><p>接下来我们一步一步解析困惑度的计算过程：</p><ol><li>计算验证集中每个句子的概率：$p(\bar{\pmb{x}}_i), i \in \{1, 2, …m\}$；</li><li>对每个句子的概率取对数：$\log_2 p(\bar{\pmb{x}}_i), i \in \{1,2,…,m\}$；</li><li>计算每个词出现的平均概率：$\frac{1}{M} \sum_{i=1}^m \log_2p(\bar{\pmb{x}}_i), i \in \{1,2,…m\}$；</li><li>将平均概率作为指数计算：$2^{-\frac{1}{M} \sum_{i=1}^m \log_2p(\bar{\pmb{x}}_i)}, i \in \{1,2,…m\}$。</li></ol><p>需要注意的是困惑度并不是一个完美的指标，困惑度降低并不一定意味着语言模型质量的提升，影响困惑度的因素主要有以下几个：</p><ol><li><p>训练数据集越大，困惑度会下降得更低，十亿数据量的语料和十万数据量的语料训练效果是很不一样的；</p></li><li><p>数据中的标点会对模型的困惑度产生很大影响，一个句号能让困惑度波动几十，标点的预测总是不稳定；</p></li><li><p>预测语句中的“的，了”等无意义词（通用词）也对困惑度有很大影响，可能“我借你的书”比“我借你书”的指标值小几十，但从语义上分析有没有这些停用词并不能完全代表句子生成的好坏。</p></li></ol><p>因此，在对比两个 模型的困惑度的时候要记住两个原则：</p><ol><li>使用不同词表的语言模型对比困惑度没有意义；</li><li>概率和不等于 1 的语言模型困惑度没有意义。</li></ol><p>所以，<strong>评估语言模型时可以用困惑度大致估计训练效果</strong>，作出判断和分析，但它不是完全意义上的标准，具体问题还是要具体分析。</p><h1 id="3-语言模型参数估计中的稀疏化问题"><a href="#3-语言模型参数估计中的稀疏化问题" class="headerlink" title="3. 语言模型参数估计中的稀疏化问题"></a>3. 语言模型参数估计中的稀疏化问题</h1><p>前面在介绍语言模型参数估计的时候是通过统计相对频率的方式，这种方式我们称之为最大似然估计。在实际情景中，我们直接用最大似然估计进行参数估计的时候会有一个问题——数据稀疏化。</p><p>随着 n-Gram 模型中 $n$ 的增大，模型参数量以 $\approx V^n$ 的指数形式增长，如此大量的参数必然造成参数的稀疏，比如词表大小 $V=10000$，$n=3$ （这也是常见的语言模型），此时模型参数量为 $\approx 10^{12}$。拥有如此大量的参数而训练集有限的情况下，必然有很多计数为零的情况出现：</p><ol><li>当 $c(\pmb{h},w_j)=0$ 时，会导致在训练集中没有出现的句子的概率为 0，这显然是不对的，因为训练集并没有包含所有的句子；</li><li>当 $c(\pmb{h})=0$ 时，会导致参数没有意义，因为作为分母是不能为 0 的。</li></ol><p>为了使语言模型达到可用状态，我们需要使用平滑技术（smoothing）来消除这些统计词频为零的情况。平滑的基本思想就是从高概率的句子中抽取微小的概率分配给零概率的句子，有点类似“劫富济贫”。在使用平滑技术的时候有一点要非常小心，那就是我们必须保证平滑完以后的语言模型概率和仍为 1。这里介绍三种平滑技术：线性插值平滑（Linear Interpolation Smoothing）、加性平滑（Additive Smoothing）和减值平滑（Discounting Smoothing）。</p><h2 id="3-1-线性插值平滑"><a href="#3-1-线性插值平滑" class="headerlink" title="3.1 线性插值平滑"></a>3.1 线性插值平滑</h2><p>前面介绍 Creepy 模型 和 Uni-Gram 模型的时候说这两个模型走了两个极端，然后通过调整 $n$ 来控制语言模型的精细度。由此可见，不同 $n$ 对应的语言模型的优缺点是不同的，比如 Uni-Gram 一定不会出现统计频率为零的情况出现，但忽略了词与词之间的依赖关系，而 Bi-Gram 模型可能出现零词频的情况，但比 Uni-Gram 多了对词之间依赖关系的考虑，如果我们将两个模型结合起来，既可以保留 Bi-Gram 的依赖关系，又可以保留 Uni-Gram 的非零词频的优点岂不是美哉。</p><script type="math/tex; mode=display">\hat{\theta}(w_j|w_{j-1}) = \alpha \theta(w_j|w_{j-1}) + (1-\alpha)\theta(w_j)</script><p>其中 $\alpha \in [0,1]$。利用 Uni-Gram 对 Bi-Gram 进行插值，只要 $\alpha \gt 0$，既保证了 Bi-Gram 不会出现零概率，也保证了 Uni-Gram 的词依赖。这就是线性插值的基本思想：利用低元 n-Gram 模型对高元 n-Gram 模型进行线性插值，取长补短。</p><p>将上面的例子扩展到一般情况：</p><script type="math/tex; mode=display">\hat{\theta}(\pmb{h}, w_j) = \sum_{k=1}^{n-1} \alpha_{k}\theta_k(\pmb{h}, w_j), \\\\\sum_{k=1}^{n-1} \alpha_k = 1, \alpha_k \in \mathbb{R}_{+}^{n-1}</script><p>那么我们如何选择 $\alpha_k$ 的值呢？一个常用的方法和前面介绍 n-Gram 模型时，介绍选取 $n$ 值的方法一样，利用验证集找到最合适的 $\alpha_k$ 值。但是无论怎么选，有一个大的原则：在高元语言模型分母遇到零词频的时候令对应的 $\alpha_k=0$。 比如：Tri-Gram </p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}\hat{\theta}(w_j|w_{j-1}, w_{j-2}) &= \alpha_1\theta(w_j|w_{j-1}, w_{j-2})+\alpha_2\theta(w_j|w_{j-1})+\alpha_3\theta(w_j) \\\\                                   &= \alpha_1 \frac{c(w_{j-2}, w_{j-1}, w_j)}{c(w_{j-2}, w_{j-1})} + \alpha_2\frac{c(w_{j-1}, w_j)}{c(w_{j-1})} + \alpha_3 \frac{c(w_j)}{N}\end{aligned}\end{equation}</script><p>当 $c(w_{j-2}, w_{j-1})=0$ 或 $c(w_{j-1})=0$ 时，相应的令 $\alpha_1=0$ 或 $\alpha_2=0$ 。</p><h2 id="3-2-加性平滑"><a href="#3-2-加性平滑" class="headerlink" title="3.2 加性平滑"></a>3.2 加性平滑</h2><p>加性平滑又称拉普拉斯平滑，想法非常简单：只需要对每个统计单元词频加 $\alpha$，比如：</p><script type="math/tex; mode=display">\hat{\theta}(w_j|w_{j-1}, w_{j-2}) = \frac{\alpha + c(w_{j-2}, w_{j-1}, w_j)}{\alpha \cdot V + c(w_{j-2}, w_{j-1})}</script><p>假设 $c(我，爱)$ 在训练语料中的出现的次数是 100，经过拉普拉斯平滑以后就变成了 $100+\alpha$，这样就一定能避免任意词频出现零的情况。而 $\alpha$ 的选取仍然作为模型的超参数，通过验证集进行选取。在实际的语言模型中通常 $\alpha \lt 1$。</p><p>拉普拉斯平滑存在两个问题：</p><ol><li>由于训练语料中零词频的词数量太多，通常会占据整个概率分布中的一个很大的比例。通过拉普拉斯平滑会给零词频的词分配了太多的概率空间。</li><li>给所有未出现的词的词频加上相同的数，即认为这些词出现的概率是相同的，这种假设是否合理其实也值得商榷。而且，对于非零词频的词都增加同样的频度值，这是否合理，我们并不能给出一个明确的答案。</li></ol><h2 id="3-3-减值平滑"><a href="#3-3-减值平滑" class="headerlink" title="3.3 减值平滑"></a>3.3 减值平滑</h2><p>之所以会产生零词频，是因为那些非零词频的词出现的概率被模型高估了，占用了原本属于零词频词的概率份额，所以我们可以通过直接从非零词频对应的概率上扣除一部分概率还给零词频词，这就是减值平滑的基本思想。</p><p>以 Bi-Gram 为例，$\theta(w_j|w_{j-1}) = c(w_{j-1}, w_j)/c(w_{j-1})$，对于任意 $c(w_{j-1}, w_j)&gt;0$ 的情况，我们都令</p><script type="math/tex; mode=display">c^*(w_{j-1}, w_j) = c(w_{j-1}, w_j) - \beta</script><p>其中 $\beta \in [0,1]$（通常 $\beta=0.5$），这个 $\beta$ 就是我们从非零词频词的概率上抠下来的份额。</p><p>举个例子，假设 $c(w_{j-1}=我)=48$：</p><div class="table-container"><table><thead><tr><th style="text-align:center">$(w_{j-1}, w_j)$</th><th style="text-align:center">$c(w_{j-1}, w_j)$</th><th style="text-align:center">$c^*(w_{j-1}, w_j)$</th><th style="text-align:center">$c^*(w_{j-1})/c(w_{j-1}=我)$</th></tr></thead><tbody><tr><td style="text-align:center">（我，爱）</td><td style="text-align:center">$15$</td><td style="text-align:center">$15-0.5=14.5$</td><td style="text-align:center">$14.5/48$</td></tr><tr><td style="text-align:center">（我，吃）</td><td style="text-align:center">$13$</td><td style="text-align:center">$13-0.5=12.5$</td><td style="text-align:center">$12.5/48$</td></tr><tr><td style="text-align:center">（我，喜欢）</td><td style="text-align:center">$10$</td><td style="text-align:center">$10-0.5=9.5$</td><td style="text-align:center">$9.5/48$</td></tr><tr><td style="text-align:center">（我，在）</td><td style="text-align:center">$10$</td><td style="text-align:center">$10-0.5=9.5$</td><td style="text-align:center">$9.5/48$</td></tr></tbody></table></div><script type="math/tex; mode=display">\sum_{c(w_{j-1}, w_j)} \frac{c^*(w_{j-1}, w_j)}{c(w_{j-1})} = \frac{14.5}{48} + \frac{12.5}{48} + \frac{9.5}{48} +\frac{9.5}{48} = \frac{46}{48}=\frac{23}{24}</script><p>由此可见，我们从非零词频的词身上抠下来 $1/24$ 的概率，我们称抠下来这部分概率为缺失质量（missing mass）。有了这部分缺失质量我们就可以将它分配给零词频的词了，通常采用的分配方法是平均分配。</p><p>以上过程我们用形式化的语言描述为：</p><p>定义 $\beta \in (0, 1)$，任意给定 $\pmb{h}$，我们将训练数据分成两部分：</p><script type="math/tex; mode=display">\mathcal{A}(\pmb{h}) = \{ w_j \in \mathcal{V}:c(\pmb{h}, w_j) \gt 0\};\\\\\mathcal{B}(\pmb{h}) = \{ w_j \in \mathcal{V}:c(\pmb{h}, w_j) = 0\}</script><p>对于 $w_{j} \in \mathcal{A}(\pmb{h})$，定义：</p><script type="math/tex; mode=display">\hat{\theta}(\pmb{h}, w_j) = \frac{c(\pmb{h}, w_j)-\beta}{c(\pmb{h})} \\\\q(\pmb{h}) = 1-\sum_{w_j\in\mathcal{A}(\pmb{h})} \hat{\theta}(\pmb{h}, w_j)</script><p>最后将 $q(\pmb{h})$ 平均分配给所有 $w_j \in \mathcal{B}(\pmb{h})$：</p><script type="math/tex; mode=display">\hat{\theta}(\pmb{h}, w_j) = \frac{q(\pmb{h})}{\sum_{w_j \in \mathcal{B}(\pmb{h})}c(\pmb{h}, w_j)}</script><p>最终我们得到:</p><script type="math/tex; mode=display">\begin{equation} \nonumber\hat{\theta}(\pmb{h}, w_j) = \begin{cases}\frac{c(\pmb{h}, w_j)-\beta}{c(\pmb{h})} & \mathrm{if}: w_j \in \mathcal{A}(\pmb{h})\\\\\frac{q(\pmb{h})}{\sum_{w_j \in \mathcal{B}(\pmb{h})}c(\pmb{h}, w_j)} & \mathrm{if}: w_j \in \mathcal{B}(\pmb{h})\end{cases}\end{equation}</script><p>这种减去一个固定值 $\beta$ 的减值法称为 “绝对减值平滑”，这种方法简单易操作，但缺点也很明显：在每个非零词频词上减去相同的词频，然后平均分配给不同的零词频词，其中蕴含的平权假设过于绝对。因此，针对这个问题人们又提出一个古德-图灵减值平滑，其基本思想是未出现的词的词频和只出现过一次的词的词频相同，然后再去调整只出现过一次的词频使整个概率和为 1。关于这种方法这里就不详细介绍了，有兴趣可以看<a href="https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec11.pdf" target="_blank" rel="noopener">这份资料</a>。</p><p>除了以上三种平滑方法以外，还有很多其他平滑方法，比如 Kneser-Ney Smoothing、Jelinek-Mercer Smoothing 等等，更多资料<a href="https://courses.engr.illinois.edu/cs447/fa2018/Slides/Lecture04.pdf" target="_blank" rel="noopener">看这里</a>。</p><h1 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h1><ol><li>语言模型实际上就是一个概率分布，我们可以从中得到一个句子出现的概率或者可以根据之前出现的词推断下一个可能出现的词；</li><li>构建一个统计语言模型需要马尔科夫假设；</li><li>通常使用最大似然估计对模型参数进行估计；</li><li>在遇到词频为零的情况时可以使用平滑方法来解决；</li><li>使用困惑度对构建好的语言模型进行评估，语言模型质量越好困惑度就会越低。</li></ol><p>统计语言模型存在几个缺点：</p><ol><li>由于存在马尔科夫假设，尤其是通常使用的马尔科夫假设不会超过四阶，也就是通常的统计语言模型不会超过 5-gram，所以很难建立词与词之间的远距离依赖关系；</li><li>无法构建词与词之间的相似性关系。 </li></ol>]]></content>
      
      
      <categories>
          
          <category> 语言模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Probabilistic Language Model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络的正则化</title>
      <link href="/2021/03/15/l1-l2/"/>
      <url>/2021/03/15/l1-l2/</url>
      
        <content type="html"><![CDATA[<h1 id="过拟合（overfitting）"><a href="#过拟合（overfitting）" class="headerlink" title="过拟合（overfitting）"></a>过拟合（overfitting）</h1><p><img src="https://hackernoon.com/hn-images/1*SBUK2QEfCP-zvJmKm14wGQ.png" alt></p><a id="more"></a><p>我们在训练机器学习/深度学习模型的时候，通常会发现，模型在训练数据集上表现非常好，但是在验证集上却非常差，这种现象就是过拟合。我们训练模型的目的不是希望它在训练集上有良好的表现，而是希望在验证集（更准确的说是在实际数据，验证集可以认为是用于模拟真实数据的数据集）上有好的表现。所以过拟合的模型非我所欲也。</p><p>从上面的图中，我们可以直观的看出来，从左至右模型的复杂度越来越高。所以通常造成模型过拟合的原因就是模型复杂度过高，对数据过于敏感（模型稳定性差）。</p><blockquote><p>  这里稍微解释一下，为什么模型复杂度高或者模型不稳定就会造成模型过拟合？</p><ul><li><p>模型复杂度过高</p><p>根据泰勒定理，在一个区间之内我们可以通过泰勒展开式来逼近任意可导函数：</p><script type="math/tex; mode=display">f(x) = f(a) + \frac{f'(a)}{1!}(x-a) + \frac{f''(a)}{2!}(x-a)^2+ \cdots + \frac{f^{(n)}(a)}{n!}(x-a)^n+R_n(x)</script><p>当泰勒展开式的逼近项阶数越高，则越接近原始的函数，但泰勒展开式的表达形式也越复杂：</p><ul><li>当 $n=0$ 时，$f(x)=f(a)$；</li><li>当 $n =1$ 时，$f(x)=f(a)+w_1\cdot (x-a)$;</li><li>当 $n=2$ 时，$f(x)=f(a)+w_1\cdot (x-a)+w_2\cdot (x-a)^2$</li><li>$\cdots$</li></ul><p>这是一目了然的。虽然神经网络不是通过泰勒展开式进行逼近，但是其遵循的规律是相同的，即越是逼近原始函数就需要更复杂的逼近函数。</p><p>如果原始函数是训练数据分布，机器学习/深度学习模型表示逼近函数。那么如果模型出现过拟合现象，一定是一个复杂度过高的模型。</p></li><li><p>模型不稳定</p><p>稳定性代表在小的扰动下，其轨迹不会有太大的变化。放在模型训练过程，我们可以从两个方面理解这句话：</p><ul><li>训练样本中的异常值对模型不会产生太大的影响；</li><li>相对大量的训练样本，小部分样本不会对模型产生太大影响。</li></ul><p>对于一个不够稳定的模型，遇到任意的样本点都会对模型产生较大的影响，使模型也要去拟合那些异常点和小样本点，就会形成上图右侧的复杂曲线。</p><p>训练过度，训练数据不足等也是造成过拟合的重要原因，但是这些都是外因，这里我们只讨论模型本身的原因。</p></li></ul></blockquote><h1 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h1><p>首先考虑模型复杂度过高带来的过拟合问题。我们先定义模型复杂度，假设神经网络 $l$ 层的输出为：</p><script type="math/tex; mode=display">a_l = \sigma(f(X)) = \sigma( b + w_1x_1 + w_2x_2+\cdots+w_nx_n), x_i\in X</script><p>在机器学习/深度学习中，$x_i$ 通常表示特征项，$w_i$ 表示对应特征在任务中所起到的作用。过拟合是因为我们采用了过多无用特征，比如一个成年人，通过（身高，体重，头发长度）等几个特征就可以大致判断其性别，但是如果还有其他额外特征（是否戴眼镜，学历，肤色）等等没这些特征很有可能对我们造成干扰，从而造成误判。从这个例子中我们可以看出，是由于无用特征过多造成的过拟合，那么我们可以定义模型复杂度：</p><blockquote><p>  $f(X)=b + w_1x_1 + w_2x_2+\cdots+w_nx_n$ 中 $n$ 表示模型的复杂度</p></blockquote><p>在实际中，我们无法得知那些特征是有效的，那些是多余。所以直接从特征着手去解决是比较困难的。那么我们只能从权重 $(w_1, w_2, …, w_n)$ 角度考虑，如果我们能令无用特征的权重为 0 就等效于去除无效特征，降低模型复杂度，从而解决过拟合问题。</p><p>从这个角度出发，我们可以在损失函数中引入 L1 正则项：</p><script type="math/tex; mode=display">L = L(y, \hat{y}) + \lambda||w_i||_1</script><p>下面我们看，为什么在原有的损失函数上加上 L1 正则化项之后就能将权重拉向 0？</p><p>我们举一个最简单的例子，假设我们有一组数据 $[(x_1, y_1), (x_2, y_2), …, (x_n, y_n)]$，我们的模型为 $y=w\cdot x$ ，只有一个参数 $w$。我们从训练样本中随机抽取数据，使用 MSE 损失函数对模型进行训练，最后在 $w=0.5$ 时，$loss=0$。</p><table>    <tr>         <td>            <img src="https://github.com/nickyeolk/regularization_visualized/raw/master/resources/original_equation.png" border="0">        </td>         <td>            <img src="https://github.com/nickyeolk/regularization_visualized/raw/master/resources/unregularized_loss.png" border="0">        </td>     </tr></table><p>现在我们单独把 L1 正则项的图像画出来：</p><p><img src="https://github.com/nickyeolk/regularization_visualized/raw/master/resources/L1_loss.png" alt></p><p>当 $w=0$ 时，$L_1=0$。现在我们把 MSE 损失项和 L1 项叠加在一起：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/l1.png" alt></p><p>从图中我们可以很清楚的看到， 加入 L1 正则项的损失函数最小值在 $w=0$ 处。也就是说，在训练过程中权重的更新方向是向 0 靠近的。这样模型在训练过程中可以将无用的特征权重置零，达到减小模型复杂度的目的，从而解决（严格意义上来说是缓解）过拟合的问题。</p><h1 id="L2-正则化"><a href="#L2-正则化" class="headerlink" title="L2 正则化"></a>L2 正则化</h1><p>现在我们来考虑模型不稳定带来的过拟合问题。不稳定的模型本质上是输出相对输入变化率太大，仍然以 $y=w \cdot x$ 为例，$y+\Delta y = w \cdot (x+\Delta x)$，当模型不稳定的时候，一个很小的 $\Delta x$ 就可能带来一个很大的 $\Delta y$。这种相对变化率的评估指标就是导数：</p><script type="math/tex; mode=display">\frac{dy}{dx} = w</script><p>也就是当权重较低的时候，模型更加稳定。所以为了使模型更加稳定，我们需对权重设定一个上限，超过这个上限我们就认为模型有较大的过拟合风险，即我们令 $w_1+w_2+ \cdots w_n &lt; \tau$。</p><p>我们当然可以在训练过程中，每一次权重调整都去验证看看权重是否符合这个限定条件。但是这种方法显然比较麻烦，而且比较消耗算力。我们希望将这种限制直接添加到损失函数，使得每一次权重更新都自动满足这个限定条件。相当于我们要对有限制条件的微分方程求解，很自然地我们会引入拉格朗日乘子法。</p><p>L2 正则化就是假设限制条件是 $\lambda ||w||_2^2$，其中 $||w||_2^2$ 表示：</p><script type="math/tex; mode=display">w_1^2+w_2^2+\cdots+w_n^2</script><p>此时经过 L2 正则化后的损失函数为：</p><script type="math/tex; mode=display">L = L(y, \hat{y}) + \lambda ||w||_2^2</script><blockquote><p>  关于拉格朗日乘子可以参考这篇文章：<a href="http://www.slimy.com/~steuard/teaching/tutorials/Lagrange.html?continueFlag=4bf3f8b1aa0c329ce876360c121939a1" target="_blank" rel="noopener">An Introduction to Lagrange Multipliers</a>。</p></blockquote><p>下面我们来看，为什么 L2 正则化可以起作用？</p><p>仍然以 $y = w \cdot x$ 为例，我们将 MSE 与 L2 项叠加在一起的图像画出来：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/l2.png" alt></p><p>从上图我们可以很清晰的看出，L2 正则项确实可以将权重拉向小量，但是不会到 0。</p><h1 id="lambda-的影响"><a href="#lambda-的影响" class="headerlink" title="$\lambda$ 的影响"></a>$\lambda$ 的影响</h1><p>不同的 $\lambda$ 对损失有什么影响呢？</p><p><img src="https://i.imgur.com/4cGQ7a4.png" alt></p><p>如果 $\lambda=0$ ，相当于沒有做正则化，模型就会过拟合 。 当 $\lambda$ 一直变大，模型就 overfitting → generalization → underfitting。</p><h1 id="L1-VS-L2"><a href="#L1-VS-L2" class="headerlink" title="L1 VS. L2"></a>L1 VS. L2</h1><ul><li>L1 正则项使权重稀疏，因为有些权重会置零。L2 正则项使权重平滑，增加模型稳定性。</li><li>L1 正则不容易求导，计算量大。L2 正则容易求导。</li><li>L1 正则项对异常值更鲁棒</li></ul><h1 id="什么时候用L1-L2"><a href="#什么时候用L1-L2" class="headerlink" title="什么时候用L1/L2?"></a>什么时候用L1/L2?</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220315103056.jpg" alt></p><p>在实际的应用中，L2 正则几乎总能取得比 L1 更好的效果。</p><h1 id="其他的正则化方法"><a href="#其他的正则化方法" class="headerlink" title="其他的正则化方法"></a>其他的正则化方法</h1><h2 id="Elastic-net"><a href="#Elastic-net" class="headerlink" title="Elastic net"></a>Elastic net</h2><script type="math/tex; mode=display">\Omega(\theta) = \lambda_1||w||_1 + \lambda_2||w||_2^2</script><h2 id="Entropy-Regularization"><a href="#Entropy-Regularization" class="headerlink" title="Entropy Regularization"></a>Entropy Regularization</h2><script type="math/tex; mode=display">\Omega(X)=-\sum p(x)\log (p(x))</script><h2 id="Label-smoothing"><a href="#Label-smoothing" class="headerlink" title="Label smoothing"></a>Label smoothing</h2><script type="math/tex; mode=display">\begin{cases}0 \rightarrow \frac{\epsilon}{k-1} \\\\1 \rightarrow 1-\epsilon\end{cases}</script><p>其中 $k$ 表示类别数目。</p><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>随机“切断”一部分神经元的连接。</p><p><img src="https://theaisummer.com/static/b554f90b0976c40825f40b5833bad57a/1b19f/dropout.png" style="zoom:50%;"></p><ul><li><p><strong>Inverted dropout</strong>：为了避免激活函数的输出过大，模型以 $1-p$  的概率对激活函数进行缩放。</p></li><li><p><strong>Gaussian dropout</strong>：不再“切断”神经元，而是在神经元中添加噪声：</p><ol><li>减少测试期间的计算量；</li><li>无需对权重进行缩放；</li><li>训练速度更快。</li></ol></li><li><p><strong>DropConnect</strong>：将神经元的权重随机置零：</p><script type="math/tex; mode=display">r = a((M*W)v)</script><p>其中，$r$ 表示网络层输出，$v$ 表示输入，$W$ 表示权重，$M$ 表示二元矩阵。</p></li><li><p><strong>Variational Dropout</strong>：每一个训练步都是用相同的 dropout 掩码矩阵，通常用于RNN。</p></li><li><p><strong>Attention Dropout</strong>：以 $p$ 概率随机 drop 掉注意力值。</p></li><li><p><strong>Adaptive Dropout</strong>：不同单元的神经元有不同的 drop 概率。</p></li><li><p><strong>Embedding Dropout</strong>：在嵌入矩阵中进行 drop。</p></li><li><p><strong>DropBlock</strong>：在一个连续区域内，drop 掉所有的单元，通常用于 CNN。</p></li></ul><h2 id="Stochastic-Depth"><a href="#Stochastic-Depth" class="headerlink" title="Stochastic Depth"></a>Stochastic Depth</h2><p><img src="https://theaisummer.com/static/913450e16a44a9380ca258f322c0e448/2e195/stochastic-depth.png" alt></p><script type="math/tex; mode=display">H_l=\text{ReLU}(b_lf_l(H_{l-1}) + \text{id}(H_{l-1}))</script><p>随机 drop 掉一些网络层。</p><h2 id="Early-stopping"><a href="#Early-stopping" class="headerlink" title="Early stopping"></a>Early stopping</h2><p><img src="https://theaisummer.com/static/7a6353ed78b045f32e4ac39b0b4d66d2/c1b63/early-stopping.png" style="zoom: 33%;"></p><h2 id="Parameter-sharing"><a href="#Parameter-sharing" class="headerlink" title="Parameter sharing"></a>Parameter sharing</h2><p>不再在权重上施加惩罚项，而是强行令一部分权重相等，通常用于 CNN。</p><p><img src="https://theaisummer.com/static/6d1f10339bd4fd278a426cb3988737d2/29579/parameter-sharing.png" alt></p><h2 id="Batch-normalization"><a href="#Batch-normalization" class="headerlink" title="Batch normalization"></a>Batch normalization</h2><p><img src="https://theaisummer.com/static/d42512016d9b99eabb69a61bb295cd50/2e9f9/normalization.png" style="zoom: 67%;"></p><script type="math/tex; mode=display">\mu \mathcal{B} = \frac{1}{m} \sum_{i=1}^m x_i \\\\\sigma^2\mathcal{B} = \frac{1}{m} \sum_{i=1}^m (x_i -\mu \mathcal{B})^2 \\\\\hat{x}_i = \frac{x_i-\mu \mathcal{B}}{\sqrt{\sigma^2\mu+\epsilon}} \\\\y_i = \gamma \hat{x}_i +\beta = BN_{\gamma, \beta}(x_i)</script><h2 id="Data-augmentation"><a href="#Data-augmentation" class="headerlink" title="Data augmentation"></a>Data augmentation</h2><ul><li><strong>Basic Data Manipulations</strong>：图像剪切，旋转，翻转等。</li><li><strong>Feature Space Augmentation</strong>：比如使用自编码器提取图像隐特征。</li><li><strong>GAN-based Augmentation</strong>：基于生成对抗网络生成数据。</li><li><strong>Meta-Learning</strong>：给 GAN 随机喂一张图片，然后将生成的图片和原始图片同时喂给第二个网络，然后让第二个网络对比两张图片，然后告诉我们哪张图片更好。</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p><a href="https://zhuanlan.zhihu.com/p/349954266" target="_blank" rel="noopener">浅谈L2正则化</a></p></li><li><p><a href="http://www.slimy.com/~steuard/teaching/tutorials/Lagrange.html?continueFlag=4bf3f8b1aa0c329ce876360c121939a1" target="_blank" rel="noopener">Lagrange multipliers in the calculus of variations (often in physics)</a></p></li><li><p><a href="https://aisingapore.org/2020/03/a-better-visualization-of-l1-and-l2-regularization/" target="_blank" rel="noopener">A Better Visualization of L1 and L2 Regularization</a></p></li><li><p><a href="https://liam.page/2017/03/30/L1-and-L2-regularizer/" target="_blank" rel="noopener">谈谈 L1 与 L2-正则项</a></p></li><li><p><a href="https://explained.ai/regularization/" target="_blank" rel="noopener">A visual explanation for regularization of linear models</a></p></li><li><p><a href="https://theaisummer.com/regularization/" target="_blank" rel="noopener">Regularization techniques for training deep neural networks</a></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> L1 </tag>
            
            <tag> L2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>交叉熵与 KL 散度</title>
      <link href="/2021/03/08/cekl/"/>
      <url>/2021/03/08/cekl/</url>
      
        <content type="html"><![CDATA[<p><img src="https://images.squarespace-cdn.com/content/v1/54e50c15e4b058fc6806d068/1494401025139-ODE7CP2043TS1CO9MQSN/biting-worms.jpg?format=500w" alt></p><a id="more"></a><p>首先假设一个场景：假设我们是一个空间科学家，现在正在造访一个遥远的行星，在这颗行星上发现了一种蠕虫。我们发现这些蠕虫的一半有10颗牙齿，但是因为某些原因很多蠕虫的牙齿是有缺失的。我们收集了很多蠕虫统计了他们的牙齿数量</p><p><img src="https://images.squarespace-cdn.com/content/v1/54e50c15e4b058fc6806d068/1494396254926-SWY85XI22T1M4Q1ZEPCO/empirical-distribution-of-data.png?format=750w" style="zoom:50%;"></p><p>我们想把这些数据发送回地球，有一个问题就是距离太远，把这么多的数据发送回去代价太高，我们想把数据压缩到一个只需要几个参数的模型上去。</p><p>第一个模型就是均匀分布：</p><p><img src="https://images.squarespace-cdn.com/content/v1/54e50c15e4b058fc6806d068/1494397124633-IC3E9LB2IML2JXHJQVFQ/uniform-approximation.png?format=750w" style="zoom:50%;"></p><p>很显然，跟我们的实际的牙齿概率分布区别还蛮大的，所以我们想到另一个分布——二项分布：</p><p><img src="https://images.squarespace-cdn.com/content/v1/54e50c15e4b058fc6806d068/1494397201106-RKMWRQ4GUNY1ZTKCM1S0/binomial-approximation.png?format=750w" style="zoom:50%;"></p><p>为了对比这两种分布哪个更符合实际，我们把这些数据画到同一张图上去：</p><p><img src="https://images.squarespace-cdn.com/content/v1/54e50c15e4b058fc6806d068/1494397358518-09MZORGNU1VQK4EBQ1ZL/all-approximations.png?format=750w" style="zoom:50%;"></p><p>这样我们还是没办法直观的看出哪个分布更好一些。我们最初的设想是以最小的信息损失将我们的发现传回地球。这两个模型都可以以非常少的参数完成任务，但是哪一个模型的信息损失更小呢？要探究这个问题，首先我们要知道什么是信息？</p><h1 id="信息量"><a href="#信息量" class="headerlink" title="信息量"></a>信息量</h1><p>我们那需要做的一个基础工作就是量化信息。我们先对信息量做一个定义：</p><blockquote><p>  所谓信息量就是我们对一个事件进行编码所需要的比特数。</p></blockquote><p>直观上来说，越是罕见的事我们要对其进行编码，就需要更多的信息来对它进行编码。也就是说，概率越低的事件，信息量越高。举个例子：</p><blockquote><p>  事件 $I$：人都会死。</p><p>  事件 $II$：我可以开游艇，住别墅。</p></blockquote><p>对于事件 $I$ 来说，我们通常会吐槽 “你说点有用的话！”就是因为这是一件人人都知道，没有任何信息的话。但是如果有人告诉你事件 $II$，这句话透露出来的信息是，这个人很有钱，他过着锦衣玉食的生活。说明事件 $II$ 的信息量比事件 $I$ 的信息量更大。对于事件 $I$ 来说，是必然会发生的，也就是概率为 $1$，而事件 $II$ 却是小概率事件。这样我们可以得到一个结论：</p><ul><li>小概率事件：信息量大</li><li>大概率事件：信息量小</li></ul><p>可以看出概率与信息量是有相关性的，那么我们可以用概率来计算一个事件的信息量：</p><script type="math/tex; mode=display">h(x)=-\log(p(x))</script><h1 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h1><p>我们知道了单一事件的信息量的计算方法，那么对于一个由一系列事件组成的统计分布来说，它的任意随机变量包含的信息称之为信息熵（information entropy）:</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}H(x) &= \mathbb{E}[-\log(p(x))] \\\\     &= - \sum_{x \in X} p(x) \cdot \log(p(x))\end{aligned}\end{equation}</script><p>从它的计算公式可以看出，信息熵的物理意义其实就是“<em>一个统计分布中每个变量所包含的信息量的期望值</em>”，或者更通俗的说法就是一个统计系统中每个事件信息量的平均值。</p><blockquote><p>  注意 $\log$ 函数如果是以 $\log_2$ 的话，其单位为 <em>bits</em>，如果是 $\log_e$ 的话，单位为 <em>nats</em>。</p></blockquote><p>前面我们讨论了单一事件的信息量，对于一个系统来说，不同的概率分布会对其信息熵有什么影响呢？我们这里考虑最简单的情况，只有两个事件的统计系统，比如抛硬币。</p><p>先不说废话，直接用代码画出不同概率分布情况下的信息熵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compare probability distributions vs entropy</span></span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log2</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"> </span><br><span class="line"><span class="comment"># calculate entropy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entropy</span><span class="params">(events, ets=<span class="number">1e-15</span>)</span>:</span></span><br><span class="line"><span class="keyword">return</span> -sum([p * log2(p + ets) <span class="keyword">for</span> p <span class="keyword">in</span> events])</span><br><span class="line"> </span><br><span class="line"><span class="comment"># define probabilities</span></span><br><span class="line">probs = [<span class="number">0.0</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.5</span>]</span><br><span class="line"><span class="comment"># create probability distribution</span></span><br><span class="line">dists = [[p, <span class="number">1.0</span> - p] <span class="keyword">for</span> p <span class="keyword">in</span> probs]</span><br><span class="line"><span class="comment"># calculate entropy for each distribution</span></span><br><span class="line">ents = [entropy(d) <span class="keyword">for</span> d <span class="keyword">in</span> dists]</span><br><span class="line"><span class="comment"># plot probability distribution vs entropy</span></span><br><span class="line">pyplot.plot(probs, ents, marker=<span class="string">'.'</span>)</span><br><span class="line">pyplot.title(<span class="string">'Probability Distribution vs Entropy'</span>)</span><br><span class="line">pyplot.xticks(probs, [str(d) <span class="keyword">for</span> d <span class="keyword">in</span> dists])</span><br><span class="line">pyplot.xlabel(<span class="string">'Probability Distribution'</span>)</span><br><span class="line">pyplot.ylabel(<span class="string">'Entropy (bits)'</span>)</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure><p><img src="https://machinelearningmastery.com/wp-content/uploads/2019/10/Plot-of-Probability-Distribution-vs-Entropy-1024x768.png" style="zoom:50%;"></p><p>我们可以看到，当正反两面的概率相等的时候信息熵最大，而概率越是偏向某一事件的时候，信息熵就越小：</p><ul><li>Skewed Probability Distribution：低信息熵</li><li>Balanced Probability Distribution：高信息熵</li></ul><p>其实这一点与单一事件的信息量的性质也是一致的，因为当概率分布偏向某一事件的时候，说明该系统存在一个主导事件，该事件概率很大，而信息量很低，造成整个概率分布的信息熵较低。</p><p>我们现在已经知道了如何量化一个统计分布的信息熵的方法了，但是我们还是没办法知道哪个模型是更好的模型。我们还需要将两个模型与真实的蠕虫牙齿数量分布进行对于，看看哪个模型的信息损失更低，即我们要计算两个概率分布的差别。这里我们介绍两个工具：交叉熵和 KL 散度。</p><h1 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h1><p>假设有两个概率分布 $p$ 和 $q$，他们之间的差别为：</p><script type="math/tex; mode=display">H(p, q) = - \sum_{x \in X} p(x)\cdot \log(q(x))</script><p>我们将这种计算方法称之为交叉熵。</p><h1 id="KL-散度"><a href="#KL-散度" class="headerlink" title="KL 散度"></a>KL 散度</h1><p>假设有两个概率分布 $p$ 和 $q$，他们之间的差别为：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}D(p||q) &= \sum_{x \in X} p(x) \cdot (\log p(x) - \log q(x)) \\\\        &= \sum_{x \in X} p(x) \cdot \log (\frac{p(x)}{q(x)})\end{aligned}\end{equation}</script><p>到此时，我们就可以计算到底是哪种模型更加符合实际分布了。比如如果我们用 KL 散度的话，得到：</p><script type="math/tex; mode=display">D(\text{observed||Uniform}) = 0.338 \\\\D(\text{observed||Binomial}) = 0.447</script><p>所以使用均匀分布的模型损失的信息更小。</p><h1 id="交叉熵与-KL-散度的应用"><a href="#交叉熵与-KL-散度的应用" class="headerlink" title="交叉熵与 KL 散度的应用"></a>交叉熵与 KL 散度的应用</h1><p>我们在上面通过一个例子简单介绍了交叉熵和 KL 散度，那么他们有什么用呢？交叉熵被广泛用于分类模型的损失函数。</p><ul><li>$p(x)$：模型的预测输出；</li><li>$q(x)$：训练数据的标签。</li></ul><script type="math/tex; mode=display">H(p, q) = - (p(\text{class0}) * \log (q(\text{class0})) + p(\text{class1})* \log(q(\text{class1})))</script><p>例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"></span><br><span class="line">q = [<span class="number">0.8</span>, <span class="number">0.9</span>, <span class="number">0.9</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">0.3</span>]</span><br><span class="line">p = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corss_entropy</span><span class="params">(p, q)</span>:</span></span><br><span class="line">    <span class="comment"># 为了防止 log 定义域为 0 的情况</span></span><br><span class="line">    <span class="comment"># p 是训练数据标签， q 是预测值</span></span><br><span class="line">    <span class="keyword">return</span> -sum([p[i]*log(q[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(p))])</span><br><span class="line"></span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(p)):</span><br><span class="line"><span class="comment"># calculate cross entropy for the two events</span></span><br><span class="line">ce = cross_entropy(p, q)</span><br><span class="line">print(<span class="string">'&gt;[y=%.1f, yhat=%.1f] ce: %.3f nats'</span> % (p[i], q[i], ce))</span><br><span class="line">results.append(ce)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;[y=<span class="number">1.0</span>, yhat=<span class="number">0.8</span>] ce: <span class="number">1.685</span> nats</span><br><span class="line">&gt;[y=<span class="number">1.0</span>, yhat=<span class="number">0.9</span>] ce: <span class="number">1.685</span> nats</span><br><span class="line">&gt;[y=<span class="number">1.0</span>, yhat=<span class="number">0.9</span>] ce: <span class="number">1.685</span> nats</span><br><span class="line">&gt;[y=<span class="number">1.0</span>, yhat=<span class="number">0.6</span>] ce: <span class="number">1.685</span> nats</span><br><span class="line">&gt;[y=<span class="number">1.0</span>, yhat=<span class="number">0.8</span>] ce: <span class="number">1.685</span> nats</span><br><span class="line">&gt;[y=<span class="number">0.0</span>, yhat=<span class="number">0.1</span>] ce: <span class="number">1.685</span> nats</span><br><span class="line">&gt;[y=<span class="number">0.0</span>, yhat=<span class="number">0.4</span>] ce: <span class="number">1.685</span> nats</span><br><span class="line">&gt;[y=<span class="number">0.0</span>, yhat=<span class="number">0.2</span>] ce: <span class="number">1.685</span> nats</span><br><span class="line">&gt;[y=<span class="number">0.0</span>, yhat=<span class="number">0.1</span>] ce: <span class="number">1.685</span> nats</span><br><span class="line">&gt;[y=<span class="number">0.0</span>, yhat=<span class="number">0.3</span>] ce: <span class="number">1.685</span> nats</span><br></pre></td></tr></table></figure><p>同样的 KL 散度也可以用于分类模型的损失函数。</p><script type="math/tex; mode=display">D(p, q) = H(p, q) = - (p(\text{class0}) * \log (\frac{p(\text{class0})}{q(\text{class0})}) + p(\text{class1})* \log(\frac{p(\text{class1})}{q(\text{class1})}))</script><p>例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"></span><br><span class="line">q = [<span class="number">0.8</span>, <span class="number">0.9</span>, <span class="number">0.9</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">0.3</span>]</span><br><span class="line">p = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kl_divergence</span><span class="params">(p, q)</span>:</span></span><br><span class="line">    <span class="comment"># 为了防止 log 定义域为 0 的情况</span></span><br><span class="line">    <span class="comment"># p 是训练数据标签+10e-5， q 是预测值</span></span><br><span class="line">    <span class="keyword">return</span> sum([p[i]*log((p[i]+<span class="number">10e-5</span>)/q[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(p))])</span><br><span class="line"></span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(p)):</span><br><span class="line"><span class="comment"># calculate cross entropy for the two events</span></span><br><span class="line">kl = kl_divergence(p, q)</span><br><span class="line">print(<span class="string">'&gt;[y=%.1f, yhat=%.1f] kl: %.3f nats'</span> % (p[i], q[i], ce))</span><br><span class="line">results.append(kl)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;[y=<span class="number">1.0</span>, yhat=<span class="number">0.8</span>] kl: <span class="number">1.168</span> nats</span><br><span class="line">&gt;[y=<span class="number">1.0</span>, yhat=<span class="number">0.9</span>] kl: <span class="number">1.168</span> nats</span><br><span class="line">&gt;[y=<span class="number">1.0</span>, yhat=<span class="number">0.9</span>] kl: <span class="number">1.168</span> nats</span><br><span class="line">&gt;[y=<span class="number">1.0</span>, yhat=<span class="number">0.6</span>] kl: <span class="number">1.168</span> nats</span><br><span class="line">&gt;[y=<span class="number">1.0</span>, yhat=<span class="number">0.8</span>] kl: <span class="number">1.168</span> nats</span><br><span class="line">&gt;[y=<span class="number">0.0</span>, yhat=<span class="number">0.1</span>] kl: <span class="number">1.168</span> nats</span><br><span class="line">&gt;[y=<span class="number">0.0</span>, yhat=<span class="number">0.4</span>] kl: <span class="number">1.168</span> nats</span><br><span class="line">&gt;[y=<span class="number">0.0</span>, yhat=<span class="number">0.2</span>] kl: <span class="number">1.168</span> nats</span><br><span class="line">&gt;[y=<span class="number">0.0</span>, yhat=<span class="number">0.1</span>] kl: <span class="number">1.168</span> nats</span><br><span class="line">&gt;[y=<span class="number">0.0</span>, yhat=<span class="number">0.3</span>] kl: <span class="number">1.168</span> nats</span><br></pre></td></tr></table></figure><h1 id="交叉熵-VS-KL-散度"><a href="#交叉熵-VS-KL-散度" class="headerlink" title="交叉熵 VS. KL 散度"></a>交叉熵 VS. KL 散度</h1><p>交叉熵：</p><script type="math/tex; mode=display">H(p, q) = - \sum_{x \in X} p(x)\cdot \log(q(x))</script><p>KL 散度：</p><script type="math/tex; mode=display">D(p||q) = \sum_{x \in X} p(x) \cdot \log (\frac{p(x)}{q(x)})</script><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}H(p, q) - D(p || q) &= - \sum_{x \in X} p(x)\cdot \log(q(x)) - \sum_{x \in X} p(x) \cdot \log (\frac{p(x)}{q(x)}) \\\\                    &= - \sum_{x \in X} p(x) \cdot [\log(q(x)) + \log (\frac{p(x)}{q(x)} ) \\\\                    &= - \sum_{x \in X} p(x) \cdot \log(q(x) \cdot \frac{p(x)}{q(x)}) \\\\                    &= - \sum_{x \in X} p(x) \cdot \log(p(x))\end{aligned}\end{equation}</script><p>即</p><script type="math/tex; mode=display">H(p, q) = D(p ||q) + h(p)</script><p>当 $h(p)$ 是常数的时候，交叉熵和 KL 散度是等效的。那么什么时候 $h(p)$ 是常数呢？在分类任务中，我们希望</p><script type="math/tex; mode=display">p(model) \approx p(D) \approx p(truth)</script><p>$p(model)$ 表示模型输出，$p(D)$ 表示数据集的分布，$p(truth)$ 表示真实世界的数据分布。当使用 KL 散度作为损失函数的时候，我们是最小化 $D(p(D)||p(model))$，通常情况下 $p(D)$ 是固定不变的。所以，在通常情况下交叉熵损失函数和 KL 散度损失函数是等效的。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>我们用 keras 在 cifar10 数据集上训练一个 ConvNet 模型进行一下验证：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> cifar10</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Dropout, Flatten</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Conv2D, MaxPooling2D</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"></span><br><span class="line"><span class="comment"># Model configuration</span></span><br><span class="line">img_width, img_height         = <span class="number">32</span>, <span class="number">32</span></span><br><span class="line">batch_size                    = <span class="number">250</span></span><br><span class="line">no_epochs                     = <span class="number">25</span></span><br><span class="line">no_classes                    = <span class="number">10</span></span><br><span class="line">validation_split              = <span class="number">0.2</span></span><br><span class="line">verbosity                     = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load CIFAR10 dataset</span></span><br><span class="line">(input_train, target_train), (input_test, target_test) = cifar10.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reshape data based on channels first / channels last strategy.</span></span><br><span class="line"><span class="comment"># This is dependent on whether you use TF, Theano or CNTK as backend.</span></span><br><span class="line"><span class="comment"># Source: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py</span></span><br><span class="line"><span class="keyword">if</span> K.image_data_format() == <span class="string">'channels_first'</span>:</span><br><span class="line">    input_train = input_train.reshape(input_train.shape[<span class="number">0</span>],<span class="number">3</span>, img_width, img_height)</span><br><span class="line">    input_test = input_test.reshape(input_test.shape[<span class="number">0</span>], <span class="number">3</span>, img_width, img_height)</span><br><span class="line">    input_shape = (<span class="number">3</span>, img_width, img_height)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    input_train = input_train.reshape(input_train.shape[<span class="number">0</span>], img_width, img_height, <span class="number">3</span>)</span><br><span class="line">    input_test = input_test.reshape(input_test.shape[<span class="number">0</span>], img_width, img_height, <span class="number">3</span>)</span><br><span class="line">    input_shape = (img_width  , img_height, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Parse numbers as floats</span></span><br><span class="line">input_train = input_train.astype(<span class="string">'float32'</span>)</span><br><span class="line">input_test = input_test.astype(<span class="string">'float32'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize data.</span></span><br><span class="line">input_train = input_train / <span class="number">255</span></span><br><span class="line">input_test = input_test / <span class="number">255</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert target vectors to categorical targets</span></span><br><span class="line">target_train = keras.utils.to_categorical(target_train, no_classes)</span><br><span class="line">target_test = keras.utils.to_categorical(target_test, no_classes)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the model</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Conv2D(<span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, input_shape=input_shape))</span><br><span class="line">model.add(MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(Dropout(<span class="number">0.50</span>))</span><br><span class="line">model.add(Conv2D(<span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(Dropout(<span class="number">0.50</span>))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dense(no_classes, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">loss = keras.losses.kullback_leibler_divergence</span><br><span class="line"><span class="comment"># loss = keras.losses.categorical_crossentropy</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compile the model</span></span><br><span class="line">model.compile(loss=loss,</span><br><span class="line">              optimizer=keras.optimizers.Adam(),</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit data to model</span></span><br><span class="line">model.fit(input_train, target_train,</span><br><span class="line">          batch_size=batch_size,</span><br><span class="line">          epochs=no_epochs,</span><br><span class="line">          verbose=verbosity,</span><br><span class="line">          validation_split=validation_split</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate generalization metrics</span></span><br><span class="line">score = model.evaluate(input_test, target_test, verbose=<span class="number">0</span>)</span><br><span class="line">print(<span class="string">f'Test loss: <span class="subst">&#123;score[<span class="number">0</span>]&#125;</span> / Test accuracy: <span class="subst">&#123;score[<span class="number">1</span>]&#125;</span>'</span>)</span><br></pre></td></tr></table></figure><p>我们的得到的结果如下：</p><p><img src="https://www.machinecurve.com/wp-content/uploads/2019/12/kld4.png" alt></p><p><img src="https://www.machinecurve.com/wp-content/uploads/2019/12/kld3.png" alt></p><p>实验结果与我们的分析是一致的。</p><h1 id="Not-distance"><a href="#Not-distance" class="headerlink" title="Not distance"></a>Not distance</h1><p>我们介绍交叉熵和 KL 散度时说，这两个量是评估两个分布的差别。通常情况下，我们说两个东西的差别我们通常用距离来表示，比如欧氏距离，余弦距离等等。那么交叉熵和 KL 散度也是两个分布的距离吗？答案是 <strong>No</strong>!</p><p><strong>距离</strong> 是一个没有方向性的概念， A 到 B 的距离是 10，那么 B 到 A 的距离也应该是 10，这叫做对称性。但是交叉熵和 KL 散度是不具有对称性的，即</p><script type="math/tex; mode=display">H(p, q) \ne H(q, p) \\\\D(p||q) \ne D(q||p)</script><p>这一点我们可以用上面的例子就可以证明。所以无论是交叉熵还是 KL 散度都不是距离。</p><h1 id="Jenson-Shannon-散度"><a href="#Jenson-Shannon-散度" class="headerlink" title="Jenson-Shannon 散度"></a>Jenson-Shannon 散度</h1><p>JS 散度是另一张计算两个分布之间差距的方法：</p><script type="math/tex; mode=display">JS(p||q) = \frac{1}{2} D(p||m) + \frac{1}{2} D(q||m)</script><p>其中 $m=1/2(p+q)$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kl_divergence</span><span class="params">(p, q)</span>:</span></span><br><span class="line"><span class="keyword">return</span> sum(p[i] * log2(p[i]/q[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(p)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">js_divergence</span><span class="params">(p, q)</span>:</span></span><br><span class="line">m = <span class="number">0.5</span> * (p + q)</span><br><span class="line"><span class="keyword">return</span> <span class="number">0.5</span> * kl_divergence(p, m) + <span class="number">0.5</span> * kl_divergence(q, m)</span><br></pre></td></tr></table></figure><p>很明显 JS 散度是具有对称性的，所以我们可以将 JS 散度单程两个分布的距离。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained" target="_blank" rel="noopener">Kullback-Leibler Divergence Explained</a>. Count Yayesie. 2017</li><li><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/" target="_blank" rel="noopener">A Gentle Introduction to Cross-Entropy for Machine Learning</a>.  Jason Brownlee. 2019</li><li><a href="https://machinelearningmastery.com/divergence-between-probability-distributions/" target="_blank" rel="noopener">How to Calculate the KL Divergence for Machine Learning</a>.  Jason Brownlee. 2019</li><li><a href="https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-kullback-leibler-divergence-kl-divergence-with-keras.md" target="_blank" rel="noopener">how to use kullback leibler divergence kl divergence with keras</a>. christianversloot. </li></ol>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cross-entropy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模型压缩——网络量化</title>
      <link href="/2021/02/25/quantization/"/>
      <url>/2021/02/25/quantization/</url>
      
        <content type="html"><![CDATA[<p><img src="https://pytorch.org/assets/images/quantization-practice/compare_output_ns.png" alt></p><a id="more"></a><h2 id="1-理论基础"><a href="#1-理论基础" class="headerlink" title="1. 理论基础"></a>1. 理论基础</h2><blockquote><p>  如果有人问你现在几点了，你不会告诉他现在“10:14:34:4301”，而是会告诉他现在“10:15”。</p></blockquote><p>量化就是通过降低网络层权重和/或激活函数的数值精度达到模型压缩的方法。通常神经网络模型的参数都是过量的，将模型进行压缩可以使模型更小，运算速度更快。通常硬件上 8-bit 精度的数据比 32-bit 的数据处理速度更快。</p><h2 id="1-1-映射函数"><a href="#1-1-映射函数" class="headerlink" title="1.1 映射函数"></a>1.1 映射函数</h2><p>这里所说的映射函数就是将浮点数映射到整型数。一个常用的映射函数为：</p><script type="math/tex; mode=display">Q(r) = \text{round}(\frac{r}{S}+Z)</script><p>其中 $r$ 是输入，$S,Z$ 分别是量化参数。通过下式可以将量化后的输入进行浮点数翻转：</p><script type="math/tex; mode=display">\tilde{r} = (Q(r)-Z)\cdot S</script><p>注意 $\tilde{r}\ne r$，两者的差距表示量化误差。</p><h2 id="1-2-量化参数"><a href="#1-2-量化参数" class="headerlink" title="1.2 量化参数"></a>1.2 量化参数</h2><p>上面我们说到 $S,Z$ 表示量化参数，$S$ 可以简单的表示成输入范围和输出范围的比值：</p><script type="math/tex; mode=display">S = \frac{\beta-\alpha}{\beta_q-\alpha_q}</script><p>其中 $[\alpha, \beta]$ 表示输入的范围，即允许输入的上下界；$[\alpha_q, \beta_q]$ 表示量化后的输出的上下界。对于 8-bit 来说，输出的上下界为 $\beta_q -\alpha_q \le 2^8-1$。</p><p>$Z$ 表示偏差，如果输入中有 0，$Z$ 用来保证将 0 也映射到 0：$Z = -\frac{\alpha}{S}-\alpha_q$。</p><h2 id="1-3-校准"><a href="#1-3-校准" class="headerlink" title="1.3 校准"></a>1.3 校准</h2><p>选择输入限定范围的过程称之为<strong>校准</strong>。最简单的方法就是记录输入的最小最大值作为 $\alpha, \beta$。不同的校准方法会带来不同的量化策略。</p><ul><li>$\alpha=r_{min},\beta=r_{max}$ 时，称之为非对称量化，因为 $-\alpha\ne \beta$；</li><li>$\alpha=-\max(|r_{min}|, |r_{max}|), \beta = \max(|r_{min}|, |r_{max}|)$ 时，称之为对称量化， 因为 $-\alpha=\beta$。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220314174421261.png" alt></p><p>在实际应用中，对称量化应用更加广泛，因为其可以令 $Z=0$，这样可以减少推理过程计算成本。而非对称量化比对称量化更加紧凑。非对称量化容易受到异常值的影响，所以可以改进为百分比或者 KL 散度来优化这个问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">act =  torch.distributions.pareto.Pareto(<span class="number">1</span>, <span class="number">10</span>).sample((<span class="number">1</span>,<span class="number">1024</span>))</span><br><span class="line">weights = torch.distributions.normal.Normal(<span class="number">0</span>, <span class="number">0.12</span>).sample((<span class="number">3</span>, <span class="number">64</span>, <span class="number">7</span>, <span class="number">7</span>)).flatten()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_symmetric_range</span><span class="params">(x)</span>:</span></span><br><span class="line">  beta = torch.max(x.max(), x.min().abs())</span><br><span class="line">  <span class="keyword">return</span> -beta.item(), beta.item()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_affine_range</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> x.min().item(), x.max().item()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">(plt, data, scheme)</span>:</span></span><br><span class="line">  boundaries = get_affine_range(data) <span class="keyword">if</span> scheme == <span class="string">'affine'</span> <span class="keyword">else</span> get_symmetric_range(data)</span><br><span class="line">  a, _, _ = plt.hist(data, density=<span class="literal">True</span>, bins=<span class="number">100</span>)</span><br><span class="line">  ymin, ymax = np.quantile(a[a&gt;<span class="number">0</span>], [<span class="number">0.25</span>, <span class="number">0.95</span>])</span><br><span class="line">  plt.vlines(x=boundaries, ls=<span class="string">'--'</span>, colors=<span class="string">'purple'</span>, ymin=ymin, ymax=ymax)</span><br><span class="line"></span><br><span class="line">fig, axs = plt.subplots(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">plot(axs[<span class="number">0</span>, <span class="number">0</span>], act, <span class="string">'affine'</span>)</span><br><span class="line">axs[<span class="number">0</span>, <span class="number">0</span>].set_title(<span class="string">"Activation, Affine-Quantized"</span>)</span><br><span class="line"></span><br><span class="line">plot(axs[<span class="number">0</span>, <span class="number">1</span>], act, <span class="string">'symmetric'</span>)</span><br><span class="line">axs[<span class="number">0</span>, <span class="number">1</span>].set_title(<span class="string">"Activation, Symmetric-Quantized"</span>)</span><br><span class="line"></span><br><span class="line">plot(axs[<span class="number">1</span>, <span class="number">0</span>], weights, <span class="string">'affine'</span>)</span><br><span class="line">axs[<span class="number">1</span>, <span class="number">0</span>].set_title(<span class="string">"Weights, Affine-Quantized"</span>)</span><br><span class="line"></span><br><span class="line">plot(axs[<span class="number">1</span>, <span class="number">1</span>], weights, <span class="string">'symmetric'</span>)</span><br><span class="line">axs[<span class="number">1</span>, <span class="number">1</span>].set_title(<span class="string">"Weights, Symmetric-Quantized"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://pytorch.org/assets/images/quantization-practice/affine-symmetric.png" alt></p><p>我们可以直接用 Pytorch 内置的模块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.quantization.observer <span class="keyword">import</span> MinMaxObserver, MovingAverageMinMaxObserver, HistogramObserver</span><br><span class="line">C, L = <span class="number">3</span>, <span class="number">4</span></span><br><span class="line">normal = torch.distributions.normal.Normal(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">inputs = [normal.sample((C, L)), normal.sample((C, L))]</span><br><span class="line">print(inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># &gt;&gt;&gt;&gt;&gt;</span></span><br><span class="line"><span class="comment"># [tensor([[-0.0590,  1.1674,  0.7119, -1.1270],</span></span><br><span class="line"><span class="comment">#          [-1.3974,  0.5077, -0.5601,  0.0683],</span></span><br><span class="line"><span class="comment">#          [-0.0929,  0.9473,  0.7159, -0.4574]]]),</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[-0.0236, -0.7599,  1.0290,  0.8914],</span></span><br><span class="line"><span class="comment">#          [-1.1727, -1.2556, -0.2271,  0.9568],</span></span><br><span class="line"><span class="comment">#          [-0.2500,  1.4579,  1.4707,  0.4043]])]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> qscheme <span class="keyword">in</span> [torch.per_tensor_affine, torch.per_tensor_symmetric]:</span><br><span class="line">  obs = MovingAverageMinMaxObserver(qscheme=qscheme)</span><br><span class="line">  <span class="keyword">for</span> x <span class="keyword">in</span> inputs:</span><br><span class="line">      obs(x)</span><br><span class="line">  print(<span class="string">f"Qscheme: <span class="subst">&#123;qscheme&#125;</span> | <span class="subst">&#123;obs.calculate_qparams()&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># &gt;&gt;&gt;&gt;&gt;</span></span><br><span class="line"><span class="comment"># Qscheme: torch.per_tensor_affine | (tensor([0.0101]), tensor([139], dtype=torch.int32))</span></span><br><span class="line"><span class="comment"># Qscheme: torch.per_tensor_symmetric | (tensor([0.0109]), tensor([128]))</span></span><br></pre></td></tr></table></figure><h2 id="1-4-Per-Tensor-and-Per-Channel-量化策略"><a href="#1-4-Per-Tensor-and-Per-Channel-量化策略" class="headerlink" title="1.4 Per-Tensor and Per-Channel 量化策略"></a>1.4 Per-Tensor and Per-Channel 量化策略</h2><p>量化参数可以针对整个权重计算，也可以单独计算每个通道。Per-tensor 就是使用相同的量化参数应用于所有通道，而 per-channel 是不同的通道使用不同的量化参数：</p><p><img src="https://pytorch.org/assets/images/quantization-practice/per-channel-tensor.svg" style="zoom: 25%;"></p><p>通常权重的量化方面，对称 per-channel 量化效果更好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.quantization.observer <span class="keyword">import</span> MovingAveragePerChannelMinMaxObserver</span><br><span class="line">obs = MovingAveragePerChannelMinMaxObserver(ch_axis=<span class="number">0</span>)  <span class="comment"># calculate qparams for all `C` channels separately</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> inputs: obs(x)</span><br><span class="line">print(obs.calculate_qparams())</span><br><span class="line"></span><br><span class="line"><span class="comment"># &gt;&gt;&gt;&gt;&gt;</span></span><br><span class="line"><span class="comment"># (tensor([0.0090, 0.0075, 0.0055]), tensor([125, 187,  82], dtype=torch.int32))</span></span><br></pre></td></tr></table></figure><h2 id="1-5-QConfig"><a href="#1-5-QConfig" class="headerlink" title="1.5 QConfig"></a>1.5 QConfig</h2><p>用 <code>QCfing</code> NameTuple 存储 <code>Observers</code> 和量化策略：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">my_qconfig = torch.quantization.QConfig(</span><br><span class="line">  activation=MovingAverageMinMaxObserver.with_args(qscheme=torch.per_tensor_affine),</span><br><span class="line">  weight=MovingAveragePerChannelMinMaxObserver.with_args(qscheme=torch.qint8)</span><br><span class="line">)</span><br><span class="line"><span class="comment"># &gt;&gt;&gt;&gt;&gt;</span></span><br><span class="line"><span class="comment"># QConfig(activation=functools.partial(&lt;class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'&gt;, qscheme=torch.per_tensor_affine)&#123;&#125;, weight=functools.partial(&lt;class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'&gt;, qscheme=torch.qint8)&#123;&#125;)</span></span><br></pre></td></tr></table></figure><h1 id="2-In-Pytorch"><a href="#2-In-Pytorch" class="headerlink" title="2. In Pytorch"></a>2. In Pytorch</h1><h2 id="2-1-Post-Training-Dynamic-Weight-only-Quantization"><a href="#2-1-Post-Training-Dynamic-Weight-only-Quantization" class="headerlink" title="2.1 Post-Training Dynamic/Weight-only Quantization"></a>2.1 Post-Training Dynamic/Weight-only Quantization</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># toy model</span></span><br><span class="line">m = nn.Sequential(</span><br><span class="line">  nn.Conv2d(<span class="number">2</span>, <span class="number">64</span>, (<span class="number">8</span>,)),</span><br><span class="line">  nn.ReLU(),</span><br><span class="line">  nn.Linear(<span class="number">16</span>,<span class="number">10</span>),</span><br><span class="line">  nn.LSTM(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">m.eval()</span><br><span class="line"></span><br><span class="line"><span class="comment">## EAGER MODE</span></span><br><span class="line"><span class="keyword">from</span> torch.quantization <span class="keyword">import</span> quantize_dynamic</span><br><span class="line">model_quantized = quantize_dynamic(</span><br><span class="line">    model=m, qconfig_spec=&#123;nn.LSTM, nn.Linear&#125;, dtype=torch.qint8, inplace=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">## FX MODE</span></span><br><span class="line"><span class="keyword">from</span> torch.quantization <span class="keyword">import</span> quantize_fx</span><br><span class="line">qconfig_dict = &#123;<span class="string">""</span>: torch.quantization.default_dynamic_qconfig&#125;  <span class="comment"># An empty key denotes the default applied to all modules</span></span><br><span class="line">model_prepared = quantize_fx.prepare_fx(m, qconfig_dict)</span><br><span class="line">model_quantized = quantize_fx.convert_fx(model_prepared)</span><br></pre></td></tr></table></figure><ul><li>通常会有更高的准确率；</li><li>对于 LSTM / Transformer 来说，动态量化是首选；</li><li>每一层事实校准和量化可能需要消耗算力。</li></ul><h2 id="2-2-Post-Training-Static-Quantization-PTQ"><a href="#2-2-Post-Training-Static-Quantization-PTQ" class="headerlink" title="2.2 Post-Training Static Quantization (PTQ)"></a>2.2 Post-Training Static Quantization (PTQ)</h2><p><img src="https://pytorch.org/assets/images/quantization-practice/ptq-flowchart.svg" style="zoom:25%;"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Static quantization of a model consists of the following steps:</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     Fuse modules</span></span><br><span class="line"><span class="comment">#     Insert Quant/DeQuant Stubs</span></span><br><span class="line"><span class="comment">#     Prepare the fused module (insert observers before and after layers)</span></span><br><span class="line"><span class="comment">#     Calibrate the prepared module (pass it representative data)</span></span><br><span class="line"><span class="comment">#     Convert the calibrated module (replace with quantized version)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">backend = <span class="string">"fbgemm"</span>  <span class="comment"># running on a x86 CPU. Use "qnnpack" if running on ARM.</span></span><br><span class="line"></span><br><span class="line">m = nn.Sequential(</span><br><span class="line">     nn.Conv2d(<span class="number">2</span>,<span class="number">64</span>,<span class="number">3</span>),</span><br><span class="line">     nn.ReLU(),</span><br><span class="line">     nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>),</span><br><span class="line">     nn.ReLU()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">## EAGER MODE</span></span><br><span class="line"><span class="string">"""Fuse</span></span><br><span class="line"><span class="string">- Inplace fusion replaces the first module in the sequence with the fused module, and the rest with identity modules</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">torch.quantization.fuse_modules(m, [<span class="string">'0'</span>,<span class="string">'1'</span>], inplace=<span class="literal">True</span>) <span class="comment"># fuse first Conv-ReLU pair</span></span><br><span class="line">torch.quantization.fuse_modules(m, [<span class="string">'2'</span>,<span class="string">'3'</span>], inplace=<span class="literal">True</span>) <span class="comment"># fuse second Conv-ReLU pair</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""Insert stubs"""</span></span><br><span class="line">m = nn.Sequential(torch.quantization.QuantStub(), </span><br><span class="line">                  *m, </span><br><span class="line">                  torch.quantization.DeQuantStub())</span><br><span class="line"></span><br><span class="line"><span class="string">"""Prepare"""</span></span><br><span class="line">m.qconfig = torch.quantization.get_default_qconfig(backend)</span><br><span class="line">torch.quantization.prepare(m, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">"""Calibrate</span></span><br><span class="line"><span class="string">- This example uses random data for convenience. Use representative (validation) data instead.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    x = torch.rand(<span class="number">1</span>,<span class="number">2</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    m(x)</span><br><span class="line">    </span><br><span class="line"><span class="string">"""Convert"""</span></span><br><span class="line">torch.quantization.convert(m, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">"""Check"""</span></span><br><span class="line">print(m[[<span class="number">1</span>]].weight().element_size()) <span class="comment"># 1 byte instead of 4 bytes for FP32</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## FX GRAPH</span></span><br><span class="line"><span class="keyword">from</span> torch.quantization <span class="keyword">import</span> quantize_fx</span><br><span class="line">m.eval()</span><br><span class="line">qconfig_dict = &#123;<span class="string">""</span>: torch.quantization.get_default_qconfig(backend)&#125;</span><br><span class="line"><span class="comment"># Prepare</span></span><br><span class="line">model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_dict)</span><br><span class="line"><span class="comment"># Calibrate - Use representative (validation) data.</span></span><br><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    x = torch.rand(<span class="number">1</span>,<span class="number">2</span>,<span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    model_prepared(x)</span><br><span class="line"><span class="comment"># quantize</span></span><br><span class="line">model_quantized = quantize_fx.convert_fx(model_prepared)</span><br></pre></td></tr></table></figure><ul><li>静态量化比动态量化更快；</li><li>静态量化模型可能需要定期重新校准以保持对分布漂移的鲁棒性。</li></ul><h2 id="2-3-Quantization-aware-Training-QAT"><a href="#2-3-Quantization-aware-Training-QAT" class="headerlink" title="2.3 Quantization-aware Training (QAT)"></a>2.3 Quantization-aware Training (QAT)</h2><p><img src="https://pytorch.org/assets/images/quantization-practice/qat-flowchart.svg" style="zoom:25%;"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># QAT follows the same steps as PTQ, with the exception of the training loop before you actually convert the model to its quantized version</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">backend = <span class="string">"fbgemm"</span>  <span class="comment"># running on a x86 CPU. Use "qnnpack" if running on ARM.</span></span><br><span class="line"></span><br><span class="line">m = nn.Sequential(</span><br><span class="line">     nn.Conv2d(<span class="number">2</span>,<span class="number">64</span>,<span class="number">8</span>),</span><br><span class="line">     nn.ReLU(),</span><br><span class="line">     nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">8</span>),</span><br><span class="line">     nn.ReLU()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="string">"""Fuse"""</span></span><br><span class="line">torch.quantization.fuse_modules(m, [<span class="string">'0'</span>,<span class="string">'1'</span>], inplace=<span class="literal">True</span>) <span class="comment"># fuse first Conv-ReLU pair</span></span><br><span class="line">torch.quantization.fuse_modules(m, [<span class="string">'2'</span>,<span class="string">'3'</span>], inplace=<span class="literal">True</span>) <span class="comment"># fuse second Conv-ReLU pair</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""Insert stubs"""</span></span><br><span class="line">m = nn.Sequential(torch.quantization.QuantStub(), </span><br><span class="line">                  *m, </span><br><span class="line">                  torch.quantization.DeQuantStub())</span><br><span class="line"></span><br><span class="line"><span class="string">"""Prepare"""</span></span><br><span class="line">m.train()</span><br><span class="line">m.qconfig = torch.quantization.get_default_qconfig(backend)</span><br><span class="line">torch.quantization.prepare_qat(m, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">"""Training Loop"""</span></span><br><span class="line">n_epochs = <span class="number">10</span></span><br><span class="line">opt = torch.optim.SGD(m.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line">loss_fn = <span class="keyword">lambda</span> out, tgt: torch.pow(tgt-out, <span class="number">2</span>).mean()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">  x = torch.rand(<span class="number">10</span>,<span class="number">2</span>,<span class="number">24</span>,<span class="number">24</span>)</span><br><span class="line">  out = m(x)</span><br><span class="line">  loss = loss_fn(out, torch.rand_like(out))</span><br><span class="line">  opt.zero_grad()</span><br><span class="line">  loss.backward()</span><br><span class="line">  opt.step()</span><br><span class="line"></span><br><span class="line"><span class="string">"""Convert"""</span></span><br><span class="line">m.eval()</span><br><span class="line">torch.quantization.convert(m, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><ul><li>QAT 准确率比 PTQ 高；</li><li>量化参数可以随着模型一起训练；</li><li>QAT 的模型重训可能需要消耗不少时间。</li></ul><h1 id="3-灵敏度分析"><a href="#3-灵敏度分析" class="headerlink" title="3. 灵敏度分析"></a>3. 灵敏度分析</h1><p>并不是所有层在量化中起到的作用都是相等的，有些层对准确率的影响可能会比较大，要想找出最优的量化层与准确率的解是比较消耗时间的，所以人们提出使用 <em>one-at-a-time</em> 方法来分析那些层对准确率最敏感，然后再 32-bit 浮点数上重训：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ONE-AT-A-TIME SENSITIVITY ANALYSIS </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> quantized_layer, _ <span class="keyword">in</span> model.named_modules():</span><br><span class="line">  print(<span class="string">"Only quantizing layer: "</span>, quantized_layer)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># The module_name key allows module-specific qconfigs. </span></span><br><span class="line">  qconfig_dict = &#123;<span class="string">""</span>: <span class="literal">None</span>, </span><br><span class="line">  <span class="string">"module_name"</span>:[(quantized_layer, torch.quantization.get_default_qconfig(backend))]&#125;</span><br><span class="line"></span><br><span class="line">  model_prepared = quantize_fx.prepare_fx(model, qconfig_dict)</span><br><span class="line">  <span class="comment"># calibrate</span></span><br><span class="line">  model_quantized = quantize_fx.convert_fx(model_prepared)</span><br><span class="line">  <span class="comment"># evaluate(model)</span></span><br></pre></td></tr></table></figure><p>Pytorch 停工了分析工具：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># extract from https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.htmlimport torch.quantization._numeric_suite as nsdef SQNR(x, y):    # Higher is better    Ps = torch.norm(x)    Pn = torch.norm(x-y)    return 20*torch.log10(Ps/Pn)wt_compare_dict = ns.compare_weights(fp32_model.state_dict(), int8_model.state_dict())for key in wt_compare_dict:    print(key, compute_error(wt_compare_dict[key]['float'], wt_compare_dict[key]['quantized'].dequantize()))act_compare_dict = ns.compare_model_outputs(fp32_model, int8_model, input_data)for key in act_compare_dict:    print(key, compute_error(act_compare_dict[key]['float'][0], act_compare_dict[key]['quantized'][0].dequantize()))</span></span><br></pre></td></tr></table></figure><h1 id="4-模型量化工作流"><a href="#4-模型量化工作流" class="headerlink" title="4. 模型量化工作流"></a>4. 模型量化工作流</h1><p><img src="https://pytorch.org/assets/images/quantization-practice/quantization-flowchart2.png" alt></p><h1 id="5-写在最后"><a href="#5-写在最后" class="headerlink" title="5. 写在最后"></a>5. 写在最后</h1><ul><li>大模型（参数量大于 1 千万）对量化误差更鲁棒；</li><li>从 FP32 开始训练模型比 INT8 开始训练模型准确率更高；</li><li>如果模型中有大量的线性层或者递归层，可以首先考虑动态量化；</li><li>用 <code>MinMax</code> 对称 per-channel 量化权重，用 <code>MovingAverageMinMax</code> 非对称 per-tensor 量化激活函数。</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://pytorch.org/blog/quantization-in-practice/" target="_blank" rel="noopener">Practical Quantization in PyTorch</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> quantization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模型压缩——知识蒸馏</title>
      <link href="/2021/02/15/knowledge-distilling/"/>
      <url>/2021/02/15/knowledge-distilling/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220315151511.png" alt></p><blockquote><p>文章转载自：<a href="https://mp.weixin.qq.com/s/9S0hNMdinQMbAPlZyHclFg" target="_blank" rel="noopener">模型压缩 | 知识蒸馏经典解读</a>。</p></blockquote><p>知识蒸馏是一种模型压缩方法，是一种基于“教师-学生网络思想”的训练方法，由于其简单，有效，在工业界被广泛应用。这一技术的理论来自于2015年Hinton发表的一篇神作：<a href="https://arxiv.org/pdf/1503.02531.pdf" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a></p><a id="more"></a><p>Knowledge Distillation，简称KD，顾名思义，就是将已经训练好的模型包含的知识(”Knowledge”)，蒸馏(“Distill”)提取到另一个模型里面去。今天，我们就来简单读一下这篇论文，力求用简单的语言描述论文作者的主要思想。在本文中，我们将从背景和动机讲起，然后着重介绍“知识蒸馏”的方法，最后我会讨论“温度“这个名词:</p><blockquote><p><strong>「温度」</strong>: 我们都知道“蒸馏”需要在高温下进行，那么这个“蒸馏”的温度代表了什么，又是如何选取合适的温度？</p></blockquote><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>虽然在一般情况下，我们不会去区分训练和部署使用的模型，但是训练和部署之间存在着一定的不一致性:</p><ul><li>在训练过程中，我们需要使用复杂的模型，大量的计算资源，以便从非常大、高度冗余的数据集中提取出信息。在实验中，效果最好的模型往往规模很大，甚至由多个模型集成得到。而大模型不方便部署到服务中去，常见的瓶颈如下:<ul><li>推断速度慢</li><li>对部署资源要求高(内存，显存等)</li></ul></li><li>在部署时，我们对延迟以及计算资源都有着严格的限制。</li></ul><p>因此，模型压缩（在保证性能的前提下减少模型的参数量）成为了一个重要的问题。而<strong>「模型蒸馏」</strong>属于模型压缩的一种方法。</p><h2 id="“思想歧路”"><a href="#“思想歧路”" class="headerlink" title="“思想歧路”"></a>“思想歧路”</h2><p>人们在直觉上会觉得，要保留相近的知识量，必须保留相近规模的模型。也就是说，一个模型的参数量基本决定了其所能捕获到的数据内蕴含的“知识”的量。</p><p>这样的想法是基本正确的，但是需要注意的是:</p><ol><li>模型的参数量和其所能捕获的“知识“量之间并非稳定的线性关系(下图中的1)，而是接近边际收益逐渐减少的一种增长曲线(下图中的2和3)。</li><li>完全相同的模型架构和模型参数量，使用完全相同的训练数据，能捕获的“知识”量并不一定完全相同，另一个关键因素是训练的方法。合适的训练方法可以使得在模型参数总量比较小时，尽可能地获取到更多的“知识”(下图中的3与2曲线的对比)。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/640kd.png" style="zoom:50%;"></p><h1 id="知识蒸馏的理论依据"><a href="#知识蒸馏的理论依据" class="headerlink" title="知识蒸馏的理论依据"></a>知识蒸馏的理论依据</h1><h2 id="Teacher-Model-和-Student-Model"><a href="#Teacher-Model-和-Student-Model" class="headerlink" title="Teacher Model 和 Student Model"></a>Teacher Model 和 Student Model</h2><p>知识蒸馏使用的是Teacher—Student模型，其中teacher是“知识”的输出者，student是“知识”的接受者。知识蒸馏的过程分为2个阶段:</p><ol><li>原始模型训练: 训练”Teacher模型”, 简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对”Teacher模型”不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值。</li><li>精简模型训练: 训练”Student模型”, 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值。</li></ol><p>在本论文中，作者将问题限定在<strong>「分类问题」</strong>下，或者其他本质上属于分类问题的问题，该类问题的共同点是模型最后会有一个softmax层，其输出值对应了相应类别的概率值。</p><h2 id="知识蒸馏的关键点"><a href="#知识蒸馏的关键点" class="headerlink" title="知识蒸馏的关键点"></a>知识蒸馏的关键点</h2><p>如果回归机器学习最最基础的理论，我们可以很清楚地意识到一点(而这一点往往在我们深入研究机器学习之后被忽略): 机器学习<strong>「最根本的目的」</strong>在于训练出在某个问题上泛化能力强的模型。</p><ul><li>泛化能力强: 在某问题的所有数据上都能很好地反应输入和输出之间的关系，无论是训练数据，还是测试数据，还是任何属于该问题的未知数据。</li></ul><p>而现实中，由于我们不可能收集到某问题的所有数据来作为训练数据，并且新数据总是在源源不断的产生，因此我们只能退而求其次，训练目标变成在已有的训练数据集上建模输入和输出之间的关系。由于训练数据集是对真实数据分布情况的采样，训练数据集上的最优解往往会多少偏离真正的最优解(这里的讨论不考虑模型容量)。</p><p>而在知识蒸馏时，由于我们已经有了一个泛化能力较强的Net-T，我们在利用Net-T来蒸馏训练Net-S时，可以直接让Net-S去学习Net-T的泛化能力。</p><p>一个很直白且高效的迁移泛化能力的方法就是使用 softmax 层输出的类别的概率来作为 “soft target”。</p><ol><li>传统 training 过程 (hard targets): 对 ground truth 求极大似然。</li><li>KD 的 training 过程(soft targets): 用 large model 的 class probabilities 作为 soft targets。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/640kd1.png" alt></p><h3 id="为什么？"><a href="#为什么？" class="headerlink" title="为什么？"></a>为什么？</h3><p>softmax 层的输出，除了正例之外，负标签也带有大量的信息，比如某些负标签对应的概率远远大于其他负标签。而在传统的训练过程 (hard target) 中，所有负标签都被统一对待。也就是说，KD的训练方式使得每个样本给Net-S带来的信息量大于传统的训练方式。</p><p>举个例子来说明一下: 在手写体数字识别任务 MNIST 中，输出类别有10个。</p><p>假设某个输入的“2”更加形似”3”，softmax 的输出值中”3”对应的概率为0.1，而其他负标签对应的值都很小，而另一个”2”更加形似”7”，”7”对应的概率为0.1。这两个”2”对应的hard target的值是相同的，但是它们的soft target却是不同的，由此我们可见soft target蕴含着比hard target多的信息。并且soft target分布的熵相对高时，其soft target蕴含的知识就更丰富。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/640kd2.png" style="zoom:50%;"></p><p>这就解释了为什么通过蒸馏的方法训练出的 Net-S 相比使用完全相同的模型结构和训练数据只使用 hard target 的训练方法得到的模型，拥有更好的泛化能力。</p><h3 id="softmax-函数"><a href="#softmax-函数" class="headerlink" title="softmax 函数"></a>softmax 函数</h3><p>先回顾一下原始的 softmax 函数：</p><script type="math/tex; mode=display">\text{softmax}(z_i) = \frac{\exp(z_i)}{\sum_j \exp(z_j)}</script><p>但要是直接使用 softmax 层的输出值作为 soft target，这又会带来一个问题: 当 softmax 输出的概率分布熵相对较小时，负标签的值都很接近 0，对损失函数的贡献非常小，小到可以忽略不计。因此”温度”这个变量就派上了用场。</p><p>下面的公式时加了温度这个变量之后的 softmax 函数:</p><script type="math/tex; mode=display">\text{softmax}(z_i) = \frac{\exp(z_i/T)}{\sum_j\exp(z_j/T)}</script><ul><li>这里的 $T$ 就是<strong>「温度」</strong>。</li><li>原来的 softmax 函数是 $T = 1$ 的特例。$T$ 越高，softmax 的概率分布越趋于平滑，其分布的熵越大，负标签携带的信息会被相对地放大，模型训练将更加关注负标签。</li></ul><h1 id="知识蒸馏的具体方法"><a href="#知识蒸馏的具体方法" class="headerlink" title="知识蒸馏的具体方法"></a>知识蒸馏的具体方法</h1><h2 id="通用的知识蒸馏方法"><a href="#通用的知识蒸馏方法" class="headerlink" title="通用的知识蒸馏方法"></a>通用的知识蒸馏方法</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/640kd3.png" style="zoom:80%;"></p><ul><li><p>第一步是训练 Net-T；第二步是在高温 $T$ 下，蒸馏 Net-T 的知识到 Net-S。训练 Net-T 的过程很简单，下面详细讲讲第二步:高温蒸馏的过程。高温蒸馏过程的目标函数由 distill loss (对应soft target) 和 student loss (对应 hard target)加权得到，示意图如上。</p><script type="math/tex; mode=display">L = \alpha L_{\text{soft}} +\beta L_{\text{hard}}</script></li><li><p>Net-T 和 Net-S 同时输如训练集, 用 Net-T 输出概率来作为 soft target，Net-S 在相同温度 $T$ 下的 softmax 输出和 soft target 的 cross entropy 就是<strong>「Loss函数的第一部分」</strong>$L_{\text{soft}}$：</p><script type="math/tex; mode=display">L_{soft} = -\sum_j^N p_j^T \log(q_j^T)</script><p>其中 $p_i^T=\frac{\exp(v_i/T)}{\sum_k^N \exp(v_k/T)}$，$q_i^T=\frac{\exp(z_i/T)}{\sum_k^N \log(z_k/T)}$。</p></li><li><p>Net-S 在 $T=1$下的 softmax 输出和 ground truth 的 cross entropy 就是<strong>「Loss函数的第二部分」</strong> $L_{\text{hard}}$：</p><script type="math/tex; mode=display">L_{\text{hard}} = -\sum_j^N c_j \log(q_j^1)</script><p>其中，$c_j$ 表示在第 $j$ 类上的 ground truth 值，$c_j \in \{0,1\}$， 正标签取 1，负标签取 0。$q_j^1=\frac{\exp(z_i)}{\sum_j^N \exp(z_j)}$。</p></li><li><p>第二部分 $L_{\text{hard}}$ 的必要性其实很好理解: Net-T 也有一定的错误率，使用 ground truth 可以有效降低错误被传播给 Net-S 的可能。打个比方，老师虽然学识远远超过学生，但是他仍然有出错的可能，而这时候如果学生在老师的教授之外，可以同时参考到标准答案，就可以有效地降低被老师偶尔的错误“带偏”的可能性。</p></li></ul><h2 id="一种特殊情形-直接match-logits-不经过softmax"><a href="#一种特殊情形-直接match-logits-不经过softmax" class="headerlink" title="一种特殊情形: 直接match logits(不经过softmax)"></a>一种特殊情形: 直接match logits(不经过softmax)</h2><p>直接 match logits 指的是，直接使用 softmax 层的输入 logits（而不是输出）作为 soft targets，需要最小化的目标函数是 Net-T 和 Net-S 的 logits 之间的平方差。</p><p>由单个 case 贡献的 loss，推算出对应在 Net-S 每个 logit $z_i$上的梯度:</p><script type="math/tex; mode=display">\frac{\partial L_{\text{soft}}}{\partial z_i} = \frac{1}{T}(q_i-p_i) = \frac{1}{T}(\frac{e^{z_i/T}}{\sum_j e^{z_j/T}}-\frac{e^{v_i/T}}{\sum_j e^{v_j/T}})</script><p>当 $T \rightarrow \infty$ 时，$e^{x/T} \rightarrow 1 + x/T$ ，于是：</p><script type="math/tex; mode=display">\frac{\partial L_{\text{soft}}}{\partial z_i} = \frac{1}{T}(\frac{1+z_i/T}{N+z_j/T}-\frac{1+v_i/T}{N+ v_j/T})</script><p>假设 logits 是零均值的，即 $\sum_j z_j = \sum_j v_j=0$，则</p><script type="math/tex; mode=display">\frac{\partial L_{\text{soft}}}{\partial z_i} \approx \frac{1}{NT^2}(z_i-v_i)</script><p>也就是相当于最小化：</p><script type="math/tex; mode=display">L'_{\text{soft}} = \frac{1}{2}(z_i-v_i)^2</script><h1 id="关于”温度”的讨论"><a href="#关于”温度”的讨论" class="headerlink" title="关于”温度”的讨论"></a>关于”温度”的讨论</h1><p>【问题】 我们都知道“蒸馏”需要在高温下进行，那么这个“蒸馏”的温度代表了什么，又是如何选取合适的温度？</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/640kd4.png" alt></p><h4 id="温度的特点"><a href="#温度的特点" class="headerlink" title="温度的特点"></a>温度的特点</h4><p>在回答这个问题之前，先讨论一下<strong>「温度T的特点」</strong></p><ol><li>原始的softmax函数是 时的特例， 时，概率分布比原始更“陡峭”， 时，概率分布比原始更“平缓”。</li><li>温度越高，softmax上各个值的分布就越平均（思考极端情况: (i), 此时softmax的值是平均分布的；(ii) ，此时softmax的值就相当于,即最大的概率处的值趋近于1，而其他值趋近于0）</li><li>不管温度T怎么取值，Soft target都有忽略小的 携带的信息的倾向</li></ol><h4 id="温度代表了什么，如何选取合适的温度？"><a href="#温度代表了什么，如何选取合适的温度？" class="headerlink" title="温度代表了什么，如何选取合适的温度？"></a>温度代表了什么，如何选取合适的温度？</h4><p>温度的高低改变的是Net-S训练过程中对负标签的关注程度: 温度较低时，对负标签的关注，尤其是那些显著低于平均值的负标签的关注较少；而温度较高时，负标签相关的值会相对增大，Net-S会相对多地关注到负标签。</p><p>实际上，负标签中包含一定的信息，尤其是那些值显著<strong>「高于」</strong>平均值的负标签。但由于Net-T的训练过程决定了负标签部分比较noisy，并且负标签的值越低，其信息就越不可靠。因此温度的选取比较empirical，本质上就是在下面两件事之中取舍:</p><ol><li>从有部分信息量的负标签中学习 –&gt; 温度要高一些</li><li>防止受负标签中噪声的影响 –&gt; 温度要低一些</li></ol><p>总的来说，$T$ 的选择和 Net-S 的大小有关，Net-S 参数量比较小的时候，相对比较低的温度就可以了（因为参数量小的模型不能 capture all knowledge，所以可以适当忽略掉一些负标签的信息）</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> knowledge distillation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>预训练语言模型-前言</title>
      <link href="/2020/10/13/ptm-introduction/"/>
      <url>/2020/10/13/ptm-introduction/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/PLMfamily.jpg" alt></p><p>自从 2017 年 <em>Vaswani</em> 等人提出 <em>Transformer</em> 模型以后 <em>NLP</em> 开启了一个新的时代——预训练语言模型。而 2018 年的 <em>BERT</em> 横空出世则宣告着 <em>NLP</em> 的王者降临。那么，什么是预训练？什么是语言模型？它为什么有效？</p><a id="more"></a><p>在本系列文章中我们将会讨论跟预训练语言模型相关的技术。目前初步制定的计划如下：</p><p>一、前言</p><p>二、语言模型</p><p>三、NLP 中的迁移学习</p><p>四、预训练语言模型</p><p>五、预训练语言模型的应用</p><p>六、预训练语言模型的压缩</p><p>大体分成以上六个部分，本文为第一部分，将会对预训练语言模型的发展、技术路线做一个综述，使得我们能对这一领域有一个整体的认识。</p><h1 id="1-语言模型"><a href="#1-语言模型" class="headerlink" title="1. 语言模型"></a>1. 语言模型</h1><p>狭义上的语言模型就是对自然语言进行建模，自然语言指的是人类文明演化过程创造的用于人与人交流的语言，不包括编程语言等。自然语言通常包含语音和文字，语言模型指的是对文字进行建模。广义上来说，语言模型可以用来对任意的系列化结构进行建模。这里我们讨论的是狭义上的文字语言模型。</p><p>语言模型简单来讲就是让计算机判断一个词序是不是正常的句子，如果是则给出高概率，反之则给出低概率。本质上就是构建一个词序概率分布，一个序列的概率由序列中每个词出现的概率相乘得到，而每个词出现的概率由其前后出现的词来确定。用形式化语言描述为：</p><p>给定一个词表 $\mathcal{V}$，对于序列 $s = w_1w_2…w_n$，其中 $w_i \in \mathcal{V}, i \in {1, …, (n-1)}$ 以及 $n \ge 1$，$w_n$ 通常是一个特殊符号，用于标志序列的结束。语言模型可以定义为：</p><script type="math/tex; mode=display">\begin{equation}\nonumber\begin{aligned}p(s) &= p(w_1, w_2, ..., w_n)\\\\     &= p(w_1)p(w_2|w_1)...p(w_n|w_1,...,w_{n-1})\\\\     &= p(w_1)\prod_{i=2}^np(w_i|w_1,...,w_{n-1})\end{aligned}\end{equation}</script><p>语言模型在 <em>NLP</em> 领域有广泛的用途，比如机器翻译过程中，模型给出几个候选翻译结果，然后再根据语言模型选出最符合自然语言的句子，使得翻译结果更加流畅。</p><h2 id="1-1-统计语言模型"><a href="#1-1-统计语言模型" class="headerlink" title="1.1 统计语言模型"></a>1.1 统计语言模型</h2><p>在神经网络技术出现之前，人们通常是使用统计学方法从训练语料中去学习模型参数，因此此时的语言模型也称为统计语言模型（<em>Statistical Language Model</em>）。</p><p>标准的语言模型存在两个问题：</p><ul><li>参数空间过大。假设序列中的词全部来自 $\mathcal{V}$，那么对于一个长度为 $n$ 的序列来说，模型具有 $\mathcal{V}^n$ 个自有参数。我们可以看出，模型的自由参数随着序列长度增加成指数级增长。</li><li>数据稀疏性。表面上看，序列中的每个词都具有 $\mathcal{V}$ 种可能的取值，那么长度为 $n$ 的序列可以有 $\mathcal{V}$ 种组合方式，但是实际上我们的语料不可能出现这么多种组合。</li></ul><p>因此，直接对上面的语言模型进行求解几乎是不可能的，我们需要对问题进行一些合理的假设，从而简化模型。</p><h3 id="n-gram-语言模型"><a href="#n-gram-语言模型" class="headerlink" title="n-gram 语言模型"></a>n-gram 语言模型</h3><p>马尔科夫假设：</p><blockquote><p>句子中第 $i$ 个词出现的概率只依赖于前面 $i-1$ 个词。</p></blockquote><p>我们将这个假设再进一步弱化：</p><blockquote><p>句子中第 $i$ 个词出现的概率只依赖于前面 $n-1$ 个词，其中 $n \le i$。</p></blockquote><p>基于这个假设，我们就能够理解 <em>n-gram</em> 中的 <em>n</em> 实际上就是前面 <em>n-1</em> 个词的意思。（为啥说是 <em>n-1</em> 个词呢，因为通常认为第 <em>n</em> 个词表示第 <em>i</em> 个词本身）</p><p>比如：</p><blockquote><p>$n=1$ uni-gram:     $p(w_1, w_2, …, w_n) = p(w_i)$</p><p>$n=2$ bi-gram:     $p(w_1, w_2, …, w_n) = p(w_1)\prod_{i=2}^np(w_i|w_{i-1})$</p><p>$n=3$ tri-gram:     $p(w_1, w_2, …, w_n) = p(w_1)\prod_{i=2}^np(w_i|w_{i-2},w_{i-1})$</p><p>…</p><p>$n=n$ n-gram:     $p(w_1, w_2, …, w_n)=p(w_1)\prod_{i=2}^np(w_i|w_(i-1),…,w_{i-n+1})$</p></blockquote><p>在实际应用中，$n$ 的选取通常从计算复杂度和模型效果两个方面去考虑，假设 $|\mathcal{V}| = 2 \times 10^5$, 下表给出了常见的 $n$ 的取值对应的计算复杂度和模型效果：</p><div class="table-container"><table><thead><tr><th style="text-align:center">n-gram order</th><th style="text-align:center">模型参数量</th><th style="text-align:center">perplexity</th></tr></thead><tbody><tr><td style="text-align:center">$1$</td><td style="text-align:center">$2 \times 10^5$</td><td style="text-align:center">$962$</td></tr><tr><td style="text-align:center">$2$</td><td style="text-align:center">$(2 \times 10^5)^2=4 \times 10^{10}$</td><td style="text-align:center">$170$</td></tr><tr><td style="text-align:center">$3$</td><td style="text-align:center">$(2 \times 10^5)^3 = 8 \times 10^{15}$</td><td style="text-align:center">$109$</td></tr><tr><td style="text-align:center">$4$</td><td style="text-align:center">$(2 \times 10^5)^4=16 \times 10^{20}$</td><td style="text-align:center">$99$</td></tr></tbody></table></div><p>上表中 <em>perplexity</em> 表示语言模型的评估指标，值越小表明语言模型越好。</p><p>从表中我们可以看出，随着 $n$ 增大，模型参数量级是指数增加的，在实际应用中通常采用 $n=3$。</p><p>尽管 <em>n-gram</em> 模型将参数量大大降低，但是技术的发展，尤其是互联网技术的发展，我们可以轻松获得大量的语料。对于语言模型来说，一个基本的事实是语料越多模型效果越好，但是另一方面，模型的参数量也会随着词表的增大而剧增，这样就极大的限制了语言模型的发展。</p><h2 id="1-2-神经网络语言模型"><a href="#1-2-神经网络语言模型" class="headerlink" title="1.2 神经网络语言模型"></a>1.2 神经网络语言模型</h2><p>为了解决上述问题，人们开始考虑使用神经网络技术将语言模型映射到连续空间中，我们称之为“语义空间”。</p><p>最早提出使用神经网络对语言进行建模思想的是百度徐伟，在其 2000 年的论文《<a href="https://www.isca-speech.org/archive/archive_papers/icslp_2000/i00_1202.pdf" target="_blank" rel="noopener">Can Artificial Neural Networks Learn Language Models?</a>》提出一种构建 <em>2-gram</em> 语言模型（$p(w_i|w_{i-1})$）的方法。该方法的基本思路与后来的神经网络语言模型的建模方法已经差别不大了，但是由于他采用的是只有输入层和输出层，而没有隐藏层的神经网络，且只是二元语言模型，因此限制了其模型泛化能力和上下文语义捕捉能力。</p><p>到 2003 年 <a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="noopener"><em>Bengio</em></a> 等人提出了真正意义上的神经网络语言模型（<em>Feedforward Neural Network Language Model, FFNNLM</em>），该模型采用四层神经网络来构建语言模型——输入层、投影层、隐藏层和输出层。该模型不仅解决了统计语言模型的维度灾难问题和稀疏性问题，同时还诞生了一个非常重要的副产物——词向量（<em>word vector</em>）。词向量我们会在下文详细介绍。</p><p>直到 2010 年，<a href="https://www.isca-speech.org/archive/archive_papers/interspeech_2010/i10_1045.pdf" target="_blank" rel="noopener"><em>Mikolov</em></a> 等人提出基于 <em>RNN</em> 的语言模型。自此，神经网络语言模型逐渐成为语言模型的主流并得到快速发展。</p><p>2012 年，<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.248.4448&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener"><em>Sundermeyer</em></a> 等人提出使用 <em>LSTM</em> 构建语言模型，用来解决长程依赖问题。</p><p>随后的时间，各种神经网络语言模型如雨后春笋般涌现，用来解决各种各样的问题。直到 2018 年，以 <em>ELMO</em>、<em>GPT</em> 和 <em>BERT</em> 为代表的语言模型的出现，正式宣布预训练语言模型时代的到来。本小节先不讨论预训练语言模型的相关内容，留待下文讲解。这里我们先简单介绍一下 <em>FFNN</em>、<em>RNN</em> 、<em>LSTM</em> 三种里程碑式的语言模型的发展，为后续预训练技术在语言模型上的发展奠定基础。</p><h3 id="1-2-1-FFNN-语言模型"><a href="#1-2-1-FFNN-语言模型" class="headerlink" title="1.2.1 FFNN 语言模型"></a>1.2.1 FFNN 语言模型</h3><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20201019145634.png" alt></p><p><em>Bengio</em> 等人 2003 年提出的第一个 <em>FFNN</em> 语言模型结构如上图所示。图中最下方表示输入层，$w_{t-n+1}, …, w_{t-2}, w_{t-1}$ 表示前 $n-1$ 个词。上面一层表示投影层，$C(w)$ 表示将词投影到相应的词向量上，$C$ 表示一个大小为 $|\mathcal{V}| \times m$ 的词向量矩阵，矩阵中每一行对应词表中的一个词的词向量，$m$ 代表词向量的维度。实际上 $C(w)$ 就表示从 $C$ 矩阵中找到 $w$ 对应的向量。将词映射成词向量以后，将 $C(w_{t-n+1}),…,C(w_{t-2}), C(w_{t-1})$ 这 $n-1$ 个词向量首尾相接，得到一个 $(n-1)\times m$ 维的向量，记为 $x$。第三层为隐藏层，通过下式计算得到：</p><script type="math/tex; mode=display">o = \tanh(d + Hx)</script><p>其中，$H \in \mathbb{R}^{h \times (n-1)m}$。最后就是输出层：</p><script type="math/tex; mode=display">y = \mathrm{softmax}(b+Wx+U\cdot o)</script><p>其中 $U \in \mathbb{R}^{|\mathcal{V}|\times h}$，$W \in \mathbb{R}^{|\mathcal{V}| \times ({n-1})m}$。</p><p><em>FFNN</em> 语言模型存在以下几个问题：</p><ol><li>不能处理变长句子序列。由于输入层的神经元是固定的，因此模型必须输入固定长度的序列。</li><li>由于输入序列长度是固定的，因此，模型只能对固定长度的上下文进行建模，而不是对整段序列进行建模。</li><li>序列中的词没有包含位置信息，而实际上我们知道，对于相同的几个词，放在不同的位置整句话就会表达不同的意思。</li><li>尽管全接连神经网络需要学习的参数量远小于 <em>n-gram</em> 但是相比于其他结构的神经网络，其参数量还是过大。</li></ol><p>因此，<em>Bengio</em> 在论文最后提到，可以使用循环神经网络（<em>RNN</em>）来降低参数量。这就是后来 <em>Mikolov</em> 提出的 <em>RNN</em> 神经网络。</p><h3 id="1-2-2-RNN-语言模型"><a href="#1-2-2-RNN-语言模型" class="headerlink" title="1.2.2 RNN 语言模型"></a>1.2.2 RNN 语言模型</h3><p><img width="648" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/rnnlm.jpg"></p><p><em>Mikolov</em> 等人提出的 <em>RNN</em> 语言模型，将一个 <em>RNN</em> 神经网络展开，得到如上图所示的结构。每个时间步，句子中的当前词经过隐层编码预测下一个词。注意 $h_0$ 通常是零向量进行初始化。由于隐层 $h_t$  的传递性， <em>RNN</em> 语言模型实际上就相当于 <em>n-gram</em> 语言模型，而这个 <em>n</em> 是一个变量，即整个句子的长度。这样避免了马尔科夫假设，使得我们能够得到更加精准的语言模型。</p><p>我们将其中一个时间步展开可得到如下图所示的结构：</p><p><img width="512" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/rnnlm.png"></p><p>其中 $x_t$ 表示第 $t$ 个词，$y_t$ 表示第 $t$ 个词经过编码以后的输出，$h_t^{(i)}$ 表示第 $t$ 个隐藏层的第 $i$ 层，$p_t$ 表示通过第 $t$ 个词预测的第 $t+1$ 个词，$E$ 为词向量矩阵，$W_h$ 和 $W_o$ 分别表示隐层和输出层的权重矩阵。计算过程如下：</p><ol><li>将词映射成词向量：$h_t^{(0)} = Ex_t$</li><li>隐层编码：$h_t^{(1)} = \tanh(W_h[h_t^{(0)};h_{t-1}^{(1)}]^T)$</li><li>计算输出层：$y_t=W_oh_t^{(1)}$</li><li>计算输出概率：$p_t=\mathrm{softmax}(y_t)$</li></ol><p>注意：$[h_t^{(0)};h_{t-1}^{(1)}]^T$ 表示 $h_t^{(0)}, h_{t-1}^{(1)}$ 两个向量的首尾拼接。</p><p>相比于 <em>FFNN</em> 语言模型，<em>RNN</em> 语言模型能够处理变长序列，相当于对整个句子进行编码，即根据句子前面所有的词来预测当前词。这样我们能够获得更加准确的语言模型。另外由于隐层是共享权重的，因此语言模型的参数量被大大降低了。</p><p>虽然 <em>RNN</em> 语言模型有诸多优点，但是也存在严重缺陷。最棘手的问题是由于神经网络梯度消失或者爆炸导致模型在处理远程依赖问题上的无能为力。为了解决这个问题，<em>Sundermeyer</em> 等人提出了 <em>LSTM</em> 语言模型。</p><h3 id="1-2-3-LSTM-语言模型"><a href="#1-2-3-LSTM-语言模型" class="headerlink" title="1.2.3 LSTM 语言模型"></a>1.2.3 LSTM 语言模型</h3><p><img width="648" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/rnnlm_example.png"></p><p><em>LSTM</em> 是 <em>RNN</em> 的一种变体，包含了三个门结构：输入门，遗忘门和输出门。对于神经网络而言，造成梯度消失或者梯度爆炸的罪魁祸首是过深的连乘结构，而 <em>LSTM</em> 通过这三个门结构将原来的连乘结构，一部分连乘改成了加法形式，从而缓解了梯度消失或者爆炸问题。<em>LSTM</em> 语言模型就是利用这种特性对语言进行建模从而能够更好的处理长程依赖问题。</p><p>尽管 <em>RNN</em> 、<em>LSTM</em> 语言模型表现良好，但仍然存在一系列问题，比如训练慢、<em>OOV</em> 问题、<em>softmax</em> 需要考虑全部词表等等，后续的研究也在一步一步提出更加完善的语言模型。但无论如何，以这三种经典的神经网络语言模型为基础，神经网络语言模型正在蓬勃发展，这也为后来的预训练语言模型打下了坚实的基础。</p><h1 id="2-预训练"><a href="#2-预训练" class="headerlink" title="2. 预训练"></a>2. 预训练</h1><p>自然语言处理技术的发展大体经历了：基于规则处理方法、基l于统计概率的处理方法和基于神经网络的处理方法三个阶段。尤其是随着神经网络技术的发展，自然语言处理在各项任务上也取得了突飞猛进的发展。然而  <a href="https://arxiv.org/abs/1707.07328" target="_blank" rel="noopener">Jia and Liang, 2017</a> 和 <a href="https://arxiv.org/abs/1711.02173" target="_blank" rel="noopener">Belinkov and Bisk, 2018</a> 的研究指出，基于神经网络的 <em>NLP</em> 算法在泛化性上仍然非常脆弱，模型只能对在训练数据中“见过”的特征进行建模，一旦遇到“未见过”的数据，模型就会失效。</p><p>为了解决（或者说缓解）这个问题，通常的做法是使用更大的数据集去训练模型，但是这样又带来一个问题，训练神经网络模型通常是需要大量的标注数据的，更多的训练数据意味着更多的人工标注，使得训练模型的成本大大增加。</p><p>我们希望训练好的模型能够将学习到的知识应用到具有相似性的问题上去，即使新的问题在训练数据中从来没有出现过，就像人类一样具有“举一反三”，“照猫画虎”的能力，这样模型的泛化能力将会得到大大的增强。而这正是迁移学习的思想，预训练则可以认为就是一种迁移学习。</p><p>预训练首先在图像领域发光发热，这就不得不提 <strong><em>ImageNet</em></strong>。</p><h2 id="2-1-ImageNet"><a href="#2-1-ImageNet" class="headerlink" title="2.1 ImageNet"></a>2.1 ImageNet</h2><p>早期的目标识别任务面临一个过拟合的问题，因为当时训练模型的数据集都非常小。斯坦福大学的李飞飞受到 <em>George Miller</em> 的 <em>WordNet</em> 的启发，决定构建一个能够”覆盖世界上所有物体”（<em>map out the entire world of objects</em>）的数据集。</p><p><em>ImageNet</em> 项目正式启动于 2007 年，在短短三年的时间内构建了 300 多万张经过人工标注的图片，并将图片分成 5000 多个类别。2012 年，多伦多大学的 <em>Hinton</em> 研究小组发布了深度卷积神经网络（<em>CNN</em>）—— <em>AlexNet</em>，该网络在 <em>ImageNet</em> 上将错误率下降到 25%，这直接导致了深度学习的爆发。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/alexnet_ispravljeno.png" alt></p><p>研究人员很快发现，在 <em>ImageNet</em> 上训练好的最佳模型的权重参数可以被用来初始化新的模型，然后在新的数据集上进行训练，而且这种方法可以明显提升模型的能力。这一发现就为图像识别的打开了预训练方法的大门。</p><p>假设我们有一个任务 <em>A</em>，但是用于模型训练的数据十分匮乏并不能直接从头开始训练一个模型。但是我们有大量的其他类型的数据，这个时候我们可以先试用这些数据训练一个模型， 然后使用这个模型的权重来初始化我们的任务模型，然后在我们自己的任务数据上再训练模型，这样可以极大的加快模型收敛速度，同时提升模型效果。我们将在大数据集上训练模型的过程称之为“预训练（<em>Pre-Training</em>）”，将在我们自己的数据集上训练的过程称之为“微调（<em>Fine-Tuning</em>）”。</p><p>具体的做法如下：</p><ol><li>假设我们想要一个图像分类器，用于区分“大象”和“狮子”，但是“大象”和“狮子”的图片很少，但是我们有大量的“斑马”和“猴子”的图片，这个时候我们可以使用 <em>CNN</em> 先在“斑马”和“猴子”的图片上训练一个分类器。将训练好的权重参数保存下来。</li><li>然后采用和上面相同结构的网络模型，在比较浅的几层采用上面预训练模型相应层的权重作为初始化权重，在接近任务端的较深的几层仍然采用随机初始化。</li><li>之后我们就可以训练这个“大象”和“狮子”的分类器了。通常有三种训练方法：① 浅层加载的参数在训练过程不参与训练，称之为 “<em>Frozen</em>”；② 浅层加载的参数也参与训练，但是学习率会小于深层随机初始化的参数，使得浅层参数在一个较小的范围内变化；③ 浅层参数与深层参数一起训练，后两种称之为 “<em>Fine-Tuning</em>”。（实际上，一般人们说的 <em>Fine-Tuning</em> 指的是目标任务模型整个训练过程）</li></ol><p><img width="512" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/transfer-learning-768x431.jpg"></p><p>这么做的好处是，首先我们可以在较少的任务数据下训练一个可用的模型。其次，预训练模型具有可复用性。比如，这次我们需要的是“大象狮子”分类器，下次我们需要“熊猫老虎”分类器，仍然可以使用预训练的模型参数。再次，加载预训练模型参数的目标模型在训练的时候收敛速度更快，效果更好。最后，随着开源社区的发展，很多大公司比如微软、谷歌都愿意将自己的模型开源。这些机构通常有海量的数据和计算资源，使得它们能够在完成海量数据的训练任务，这样它们开源出来的预训练模型本身就是一个非常优秀的模型，对于下游的任务而言有非常大的帮助。因此，这种 <em>Pre-Training</em> + <em>Fine-Tuning</em> 的模式在图像领域很快就流行起来了。</p><p>一个自然而然会产生的问题是：预训练方法为什么有效？</p><p>这其实就是知识的成功迁移。以人脸识别为例，对于层级 <em>CNN</em> 模型来说，不同层的网络学习到了不同的特征：最底层学习到了线段特征；第二层学习到了人脸的五官轮廓；第三层学习到了人脸的轮廓……研究发现，越浅层的网络学习到的特征越基础，越具有通用性。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20201016104427.png" alt></p><p>既然预训练方法具有如此神奇的功效，为什么只在图像领域有效？为什么没有引入到自然语言处理领域呢？实际上这两个问题是不成立的，因为预训练方法很早就有人用在了自然语言处理领域，但是并没有取得像图像领域那么大的成功。下面我们就聊一聊 <em>NLP</em> 领域的预训练。</p><h2 id="2-2-Word-Embedding"><a href="#2-2-Word-Embedding" class="headerlink" title="2.2 Word Embedding"></a>2.2 Word Embedding</h2><p>语言是人类智慧的结晶，想要将计算机发展成一种具有通用智能的系统，对人类语言的理解是一个不可或缺的能力，由此诞生了自然语言处理（<em>NLP</em>）这门学科。对于自然语言来说，<strong>词</strong> 被认为是构成语言的有意义的最小单位，对于更高级的结构，比如词组、句子则是由词构成的，对于更低级的结构，比如单个字、字母通常是没有意义的。因此，如何将词转换成计算能够理解的表达（称之为“词向量”）就变成了一个非常重要的任务。</p><h3 id="2-2-1-ASCII-码表示法"><a href="#2-2-1-ASCII-码表示法" class="headerlink" title="2.2.1 ASCII 码表示法"></a>2.2.1 ASCII 码表示法</h3><p>一个最基础的想法是，计算集中的每个字（字母）都对应一个 <em>ASCII</em> 码，由于词是由字构成的，那么我们可以使用每个字对应的 <em>ASCII</em> 码组成的序列来表示词。比如：“desk“ 和 ”table“，可以分别表示成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">desk: 01100100 01100101 01110011 01101011</span><br><span class="line">table: 01110100 01100001 01100010 01101100 01100101</span><br></pre></td></tr></table></figure><p>但是这种表示法有一下几个缺陷：</p><ol><li>变长。由于词是由不同个数的字组成的，使用这种方式表示的词的长度也是变化的，不利于矩阵运算；</li><li>稀疏。以英文为例，英文字母有26个，英文单词通常由 1-20 个字母组成，为了说明问题我们这里取 10 作为平均数。这 26 个字母有 $26^{10}$ 种排列组合方式，但实际上的英文单词个数远小于这个数。也就是说，真正的英文单词在这种排列组合情况下式非常稀疏的。这种稀疏化的表示方法会给后续的工作带来一系列问题，比如存储、计算等。</li><li>无法表达词与词之间的关联性。比如上面的 “desk” 和 “table” 都有 “桌子” 的意思，那么他们应该具有相似的语义。但是这种 <em>ASCII</em> 码表示法完全无法将这种语义关联性表示出来。</li></ol><h3 id="2-2-2-One-Hot-表示法"><a href="#2-2-2-One-Hot-表示法" class="headerlink" title="2.2.2 One-Hot 表示法"></a>2.2.2 One-Hot 表示法</h3><p>给定一个固定的词表：$\mathcal{V}=\{w_1, w_2, …, w_{|\mathcal{V}|}\}$。我们使用一个维度为 $\mathcal{|V|}$ 的向量对每个词进行编码，这个向量中对应该词在词表中的索引的维度为 1， 其余为 0：</p><script type="math/tex; mode=display">w_i = \begin{cases}1 & \mathrm{if} ~ w = w_i\\\\0 & \mathrm{otherwise}\end{cases}</script><p>举个例子， 假设我们的词表为 [desk, table, apple, orange]，那么词表中的每个词可以用 <em>one-hot</em> 表示成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">desk: [1, 0, 0, 0]</span><br><span class="line">table: [0, 1, 0, 0]</span><br><span class="line">apple: [0, 0, 1, 0]</span><br><span class="line">orange: [0, 0, 0, 1]</span><br></pre></td></tr></table></figure><p>看起来 <em>one-hot</em> 的表示形式比 <em>ASCII</em> 码表示形式要好一些，至少每个词都有了固定的长度，而且不存在使用字母进行排列组合导致的词稀疏问题。在传统的机器学习算法上， 比如最大熵、SVM、CRF 等模型上，<em>one-hot</em> 表示法都是非常常见的。但是，这种表示法仍然存在问题：</p><ol><li>稀疏。从上面的定义我们可以看出，词的维度和词表的大小是一致的，那么如果一个词表过大会造成词向量（简单理解为词的向量化表示）的维度过高，而一个词中只有一个维度的值是 1， 其余全部是 0，这样也会造成高维稀疏化问题；</li><li>同 <em>ASCII</em> 码表示法一样，无法表达词与词之间的关联性；</li><li>如果遇到新词需要添加到词表中，词表中的每个词的维度都需要相应的增加，这样在实际使用过程中是非常不方便的， 甚至有可能会导致一些问题。</li></ol><h3 id="2-2-3-向量空间模型"><a href="#2-2-3-向量空间模型" class="headerlink" title="2.2.3 向量空间模型"></a>2.2.3 向量空间模型</h3><p><a href="https://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520" target="_blank" rel="noopener">Harris</a> 和 <a href="http://cs.brown.edu/courses/csci2952d/readings/lecture1-firth.pdf" target="_blank" rel="noopener">Firth</a> 分别在 1954 年和 1957 年提出了分布式假设（<em>distributional hypothesis</em>）：</p><blockquote><p>对于语义词表示学习而言，具有相似分布的的语言对象具有相似的含义。</p></blockquote><p>也就是说，如果两个词具有相似的分布，那么它们的语义也具有相似性。这个假设为后来的分布式词表示法奠定了基础。</p><div style="background: #FFCCCC"> 这里的分布式词表示法指的是 <i>distributed representation</i>，另外还有一个术语叫做 <i>distributional representation</i> 指的是在上述分布式假设下学习到的词义。</div><p><a href="https://dl.acm.org/doi/10.1145/361219.361220" target="_blank" rel="noopener">Salton</a> 等人于 1975 年提出向量空间模型（<em>Vector Space Model</em>），该模型最初是为了在信息检索中对文档进行建模，但是随着它的成功应用，逐渐扩展到其他领域。向量空间模型将词表示成一个连续向量，向量空间表示语义空间。</p><p>比如，[desk, table, apple, orange] 我们可以表示成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">desk: [0.1, 0.2, 0.5]</span><br><span class="line">table: [0.2, 0.4, 0.6]</span><br><span class="line">apple: [0.1, 0.5, 0.1]</span><br><span class="line">orange: [0.2, 0.9, 0.4]</span><br></pre></td></tr></table></figure><p>这样做的好处是：</p><ol><li>我们可以就算两个向量之间的距离，用于表示语义相似性；</li><li>向量维度可以大大降低；</li><li>由于是连续性分布，所以不存在稀疏性问题。</li></ol><p>早期的向量空间模型主要是基于词频统计（共现矩阵）的方法，比如布朗聚类（<em>Brown Cluster</em>），潜在语义分析（<em>Latent Semantic Analysis</em>）等方法。然而这些方法通常会存在参数量过大以及需要某种程度的降维等问题。因此仍然具有一定的局限性。</p><h3 id="2-2-4-词嵌入表示法"><a href="#2-2-4-词嵌入表示法" class="headerlink" title="2.2.4 词嵌入表示法"></a>2.2.4 词嵌入表示法</h3><p><em>Word Embedding</em> 通常翻译为词嵌入，实际上也属于 <em>distributed representation</em>，这里特指使用神经网络训练得到的词向量。</p><p>随着神经网络技术的发展，深度学习掀起了机器学习的革命风潮，词向量的研究也迎来了新的转机。谷歌于 2013 年发表的 <em>Word2Vec</em> 模型可以说是词嵌入的里程碑式模型。它能够从大量的语料中高效的学习到词向量。<em>Word2Vec</em> 几乎解决了以上提到的所有问题。</p><p>在介绍 <em>Word2Vec</em> 之前，我们先回顾一下词向量的发展。</p><ul><li><strong>FFNNLM</strong></li></ul><p>词向量通常是在训练语言模型的时候得到的副产物。前面我们在介绍 <em>FFNN</em> 语言模型的时候说过，词向量最初就是在训练在该模型中第一次通过神经网络训练得到的。</p><p>在 <em>FFNN</em> 语言模型中，有一个 $C$ 矩阵，该矩阵就是词向量。开始训练之前，将矩阵中的参数随机初始化，利用梯度下降对模型进行训练，模型训练结束的时候也表明，$C$ 矩阵得到了充分的训练，即词向量训练完成。</p><ul><li><strong>SENNA</strong></li></ul><p><a href="http://www.thespermwhale.com/jaseweston/papers/unified_nlp.pdf" target="_blank" rel="noopener"><em>Ronan Collobert</em> 和 <em>Jason Weston</em></a> 在 2008 年提出了一种新的词向量训练方法，并开源了他们的方法——<a href="http://ml.nec-labs.com/senna/" target="_blank" rel="noopener">SENNA</a>。实际上最初他们并不是想训练一份好的词向量，甚至不想训练语言模型，而是想要去完成 <em>NLP</em> 中的各种任务，比如词性标注、命名实体识别、语义角色标注等。</p><p><em>SENNA</em> 的训练思路是对一个窗口中的 $n$ 个连续的词进行打分，而不是预测下一个词的概率。这实际上是直接尝试近似求解 $p(w_{t-n+1}, …, w_{t-1}, w_t)$，打分越高说明越接近正常的话，打分越低说明越不像一句正常的话。有了这个假设，就可以定义目标函数：</p><script type="math/tex; mode=display">\sum_{x \in \mathcal{X}} \sum_{w \in \mathcal{D}} \max\{0, 1-f(x)+f(x^{(w)})\}</script><p>其中 $\mathcal{X}$ 为训练接中素有连续 $n$ 元短语，$\mathcal{D}$ 表示整个词表。 第一个求和相当于选取训练语料中的所有 $n$ 元短语作为正样本，第二个求和相当于随机选择词表中的词构建负样本，构建方法就是将短语 $x$ 最中间的词替换为 $w$，即$x^{(w)}$。这样构建负样本的好处是，通常情况下，一个正常的 $n$ 元短语被替换掉中间的词之后确实会变成负样本，这样保证负样本的可靠性，即使出现替换掉之后仍然是正样本的情况，也属于少数情况，不影响大局；另一方面，由于负样本仅仅是修改了正样本中的一个词，不会让正负样本距离太大影响分类效果。最后希望正样本的打分要比负样本的打分至少高 1 分。</p><p><em>SENNA</em> 模型的整体结构与 <em>FFNNLM</em> 类似，但是由于 <em>SENNA</em> 最后的输出是一个分数而不是下一个词的概率分布，因此输出层只有一个节点，这样大大降低了计算复杂度。</p><ul><li><strong>HLBL</strong></li></ul><p><em>HLBL</em> 是 “<em>Hierarchical Log-BiLinear</em>” 的简称，该模型是 <em>Andriy Mnih</em> 和 <em>Geoffrey Hinton</em> 于 2007 年和 2008 年连续两年致力于神经网络语言模型和词向量训练的研究成果。从最基本的受限玻尔兹曼机（<em>RBM</em>）逐步发展出来的模型。</p><p>2007 年的文章 <a href="https://icml.cc/imls/conferences/2007/proceedings/papers/425.pdf" target="_blank" rel="noopener">《Three new graphical models for statistical language modelling》</a> 提出 <em>Log-Bilinear</em> 模型：</p><script type="math/tex; mode=display">y_i = \sum_{i=1}^{n-1}C(w_j)^TH_iC(w_i)</script><p>通常我们将形如 $x^TWy$ 的模型称之为 “Bilinear”，上式中 $C(w)$ 表示词向量。</p><p>我们仔细看这个模型会惊喜的发现，这不就是注意力机制吗？$H_i$ 相当于是注意力权重，计算结果就是 $w_i$ 和 $w_j$ 的相似度。（关于注意力机制的介绍可以看<a href="https://rogerspy.gitee.io/2019/08/26/NLP%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%AE%80%E4%BB%8B%EF%BC%88%E4%B8%80%EF%BC%89/">这里</a>）</p><p>受限于当时计算机的内存和算力，最终模型只考虑了 3-5 个词的上下文，然后通过最后的 <em>softmax</em> 得到下一个词的概率分布。</p><p>由于这个模型最后做预测的时候还是用的 <em>softmax</em> 获得下一个词的概率分布，计算复杂度仍然很高。因此， <em>Andriy Mnih</em> 和 <em>Geoffrey Hinton</em> 在 2008 年又发表一篇论文 《<a href="https://www.cs.toronto.edu/~amnih/papers/hlbl_final.pdf" target="_blank" rel="noopener">A scalable hierarchical distributed language model</a>》引入层级结构做最后的预测，该层级结构将 <em>softmax</em> 的 $O(|\mathcal{V}|)$ 复杂度降为 $O(\log_2(|\mathcal{V}|))$，大大提升了预测效率。</p><p><a href="https://arxiv.org/abs/cs/0108006" target="_blank" rel="noopener"><em>Goodman</em></a> 在 2001 年的时候提出了一种加速预测下一个词的方法——基于分类的思想。简单来说，假设我们词表中有 10000 个词，在传统的方法是在这 10000 个词上做 <em>softmax</em> 获得每个词的概率分布，然后取出概率最大的词，这样我们需要计算 10000 次。如果我们将这 10000 个词进行分类，假设分成 100 个类别，每个类别 100 个词。这个时候我们的计算过程是，先用一个 <em>softmax</em> 计算下一个词是属于什么类别，然后再用一个 <em>softmax</em> 计算概率最大的类别中的词的概率分布，这样我们只需要两个 100 次的计算量，计算速度直接提升 50 倍。</p><p>基于这个思想，<a href="https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf" target="_blank" rel="noopener"><em>Frederic Morin &amp; Yoshua Bengio</em></a> 于 2005 年提出使用平衡二叉树来构建这种分类关系，能够将计算复杂度降到 $O(\log_2(|\mathcal{V}|))$。但是在他们的模型中分类使用的是 <em>WordNet</em> 中的 <em>IS-A</em> 关系，最后虽然达到了加速预测的效果，但是模型效果较差。</p><p><em>Andriy Mnih</em> 和 <em>Geoffrey Hinton</em> 希望从语料中学习并能自动构建一棵平衡二叉树。他们采用 <em>bootstrapping</em> 的方法，从随机树开始，根据分类结果不断调整迭代，最后得到一棵平衡二叉树。</p><p>值得一提的是，在他们的模型中，同一个词可能出现在多个不同的叶节点上，这实际上表示一词多义现象。歧义是自然语言处理中的一个非常重要的问题，也是早期限制预训练技术在自然语言处理领域发挥作用的重要阻碍。但是<em>Mnih</em> 和 <em>Hinton</em> 并没有重视模型中的这一细节。</p><ul><li><strong>MWP</strong></li></ul><p><em>Bengio</em> 2003 年的论文最后提到了多义词的问题，<em>Eric H. Huang</em> 等人在其 2012 年的论文<a href="https://www.aclweb.org/anthology/P12-1092/" target="_blank" rel="noopener">《Improving Word Representations via Global Context and Multiple Word Prototypes》</a> 中给出了一种解决方案。从论文题目我们可以看出，作者主要有两个贡献，首先是改进了 <em>SENNA</em> 模型，从局部信息扩展到全局信息，得到了更好的词向量，这一部分不过多介绍。另一个更重要的工作是创新的使用多个词向量来表示多义词。</p><p>他们提出的方法是将每个词的上下文各取 5 个词，对这 10 个词的词向量使用 <em>idf</em> 做加权平均，然后对得到的平均向量做 <em>k-means</em> 聚类，根据聚类的结果给每个词打上标签，不同类别中的同一个词当成不同词，然后重新训练词向量。思想很简单，但是最后的效果还不错。</p><ul><li><strong>Word2Vec</strong></li></ul><p>时间来到 2013 年，<em>Mikolov</em> 等人发表两篇开山之作—— <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener">Distributed Representations of Words and Phrases and their Compositionality</a> 和 <a href="https://arxiv.org/abs/1301.3781" target="_blank" rel="noopener">Efficient estimation of word representations in vector space</a> 宣告着 <em>Word2Vec</em> 的到来。相比于之前的工作，<em>Word2Vec</em> 极大的降低了计算复杂度，从而使我们能够在超大规模的语料上学习更高维的词向量。</p><p><em>Word2Vec</em> 提出了两种新的模型架构：<em>CBOW</em> 和 <em>Skip-Gram</em>。<em>CBOW</em> 的核心思想是从一个窗口中将中间的词扣掉，然后利用这个词的前后几个词来预测中间的词；<em>Skip-gram</em> 正好相反，利用中间的词来预测两边的词。</p><table><tr>    <td><img width="300" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/word2vec-cbow.png"></td>    <td><img width="300" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/word2vec-skip-gram.png"></td></tr></table>            <p>模型结构和 <em>FFNN</em> 语言模型的结构类似，不同点在于，<em>FFNN</em> 语言模型是利用上文预测下文，对于 <em>FFNN</em> 语言模型来说，词向量只是一个副产物，但对于 <em>Word2Vec</em> 来说，词向量才是主产物。</p><p>如之前讨论的那样，直接对预测输出使用 <em>softmax</em> 计算，计算复杂度非常高，因此，作者提出两种优化方案：层级 <em>softmax</em>（<em>Hierarchical softmax</em>） 和负采样（<em>Negative Sampling</em>）。关于 <em>word2vec</em> 的公开资料非常丰富，这里就不再赘述。</p><h2 id="2-3-词向量怎么用？"><a href="#2-3-词向量怎么用？" class="headerlink" title="2.3 词向量怎么用？"></a>2.3 词向量怎么用？</h2><p>我们前面花了很大篇幅介绍了词向量，那么词向量和预训练有什么关系呢？实际上，对于 <em>NLP</em> 来说（至少在 2018 年之前）词向量对应 <em>ImageNet</em> 预训练的底层权重。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/example-classification.png" alt></p><p>以分类任务为例，如果我们要从头开始训练一个分类模型，那么模型中的参数都是随机初始化的，模型的泛化能力很大程度上取决于数据量。如果我们能从海量的语料中学习一套词向量，在做分类任务的时候，使用预训练好的词向量对输入端的词向量矩阵做初始化，相当于我们在模型中注入了一定的先验知识。然后在后续的模型训练过程中，词向量矩阵不参与训练。这样有两个好处：1. 减少可训练参数，加快训练速度；2. 知识的迁移使得模型更具有泛化能力。</p><p>这个过程其实就是 <em>NLP</em> 中的预训练，与图像领域的预训练基本是一致的，只是 <em>word embedding</em> 只能初始化第一层，更高层就无能为力了，但通常 <em>NLP</em> 模型的深度也都比较浅。</p><p>既然采用的是基本相同的预训练方法，为什么在图像领域就取得巨大的成功，而在自然语言处理领域，不能说没有帮助，但是帮助非常有限呢？很显然，问题出在 <em>word embedding</em> 上。回想我们之前在介绍 <em>Huang</em> 等人提出的训练词向量的方法，一个非常重要的任务就是解决一词多义的问题。</p><p>在通常的 <em>NLP</em> 任务中，每个词对应的只有一个词向量，也就是只有一个语义。但一词多义在自然语言中是非常常见的现象，只用一个词向量是无法表达多个语义的。虽然 <em>Huang</em> 的方法是一个词训练多个词向量用来解决一词多义的问题，但是这种做法在 <em>NLP</em> 中并不常见，原因也很简单：1. 通常为了避免（缓解）<em>OOV</em> 问题，模型需要一个较大的词表，一般情况下，词向量的维度我们会选择 100-300 维。这样的话，单单是在词向量矩阵这部分就包含了几百上千万的参数，如果再考虑一个词对应多个词向量，那么这个数字还要大上几倍；2. 即使我们采用了一个词对应多个词向量的方法来解决一词多义的问题，那么每个词在具体的句子中要使用哪一个词向量呢？这也是一个问题。如果再在模型层面解决词向量选取的问题，那我们会发现，一个简单的分类模型的重点反而成了解决歧义问题，整个模型就显得头重脚轻。</p><h2 id="2-4-Sentence-Embedding"><a href="#2-4-Sentence-Embedding" class="headerlink" title="2.4 Sentence Embedding"></a>2.4 Sentence Embedding</h2><p>既然词可以变成向量，那么句子是不是也可以变成向量呢？如果我们把句子变成向量是不是就不需要考虑词的多义性问题了呢？</p><h3 id="2-4-1-Paragraph-vector"><a href="#2-4-1-Paragraph-vector" class="headerlink" title="2.4.1 Paragraph vector"></a>2.4.1 Paragraph vector</h3><p>2014 年，<em>Mikolov</em> 在提出 <em>word2vec</em> 不久之后就提出 <em>paragraph vector</em> 的设想，从论文题目《<a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" target="_blank" rel="noopener"><em>Distributed Representations of Sentences and Documents</em></a>》就不难看出，他是想将句子和篇章也表达成固定维度的分布式向量。文章中他借鉴了 <em>CBOW</em> 和 <em>Skip-gram</em>，提出 <em>PV-DM</em> 和 <em>PV-DBOW</em>。</p><table><tr>    <td align="center"><img width="300" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20201026153641.png"></td>    <td align="center"><img width="300" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20201026153707.png"></td></tr><tr>    <td align="center">PV-DM</td>    <td align="center">PV-DBOW</td></tr>    </table>            <ul><li><strong><em>PV-DM</em></strong></li></ul><p><em>PV-DM</em>（<em>Distributed Memory Model of Paragraph Vectors</em>），通过上文预测下一个词，但是在输入层的时候不仅输入上文的词，还需要输入一个文档的 id，然后模型预测下一个词。由于输入多了一个文档 id, 因此我们还需要另外维护一个文档表（<em>look-up table</em>），用于通过 id 查找到对应的向量。训练结束后，对于现有的文档，便可以直接通过查表的方式快速得到该文档的向量，而对于新的一篇文档则需要将重新分配一个 id 给他，添加到 <em>look-up table</em> 中。然后重新训一遍模型，此时其他参数是固定的，只需要更新 <em>look-up table</em> 即可，收敛后便可以得到新文档对应的向量了。</p><ul><li><strong><em>PV-DBOW</em></strong></li></ul><p><em>PV-DBOW</em>（<em>Distributed Bag of Words version of Paragraph Vector</em>）是通过文档来预测文档中的词。首先先随机选取一个文档片段，然后随机从片段中选取一个词，然后让模型去预测这个词。和上面一样，如果此时有一个新的文档，要想获得它的向量，我们需要重新跑一遍模型。</p><p>由于上面两种方法都需要重新训练来获得新的文档向量，因此这两种方法并没有得到广泛应用。</p><h3 id="2-4-2-Skip-thoughts"><a href="#2-4-2-Skip-thoughts" class="headerlink" title="2.4.2 Skip-thoughts"></a>2.4.2 Skip-thoughts</h3><p>2015 年，<em>Kiros</em> 等人借鉴 <em>Skip-gram</em> 的思想提出 <a href="https://arxiv.org/pdf/1506.06726.pdf" target="_blank" rel="noopener"><em>Skip-thoughts</em></a> 方法。<em>Skip-gram</em> 是根据一个词去预测它的上下文，这里的基本单位是词，而 <em>Skip-thought</em> 的基本单位是句子。具体来说就是，利用当前的句子预测前一句和后一句。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20201026170057.png" alt></p><p>首先利用 <em>RNN</em> 对当前句子进行建模，然后再用一个 <em>RNN</em> 来生成上一个句子和下一个句子。本质上这其实就是 <em>encoder-decoder</em> 结构的 <em>seq2seq</em> 模型，只不过 <em>Skip-thoughts</em> 有两个 <em>decoder</em>。</p><h3 id="2-4-3-Quick-thoughts"><a href="#2-4-3-Quick-thoughts" class="headerlink" title="2.4.3 Quick-thoughts"></a>2.4.3 Quick-thoughts</h3><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20201026171857.png" alt></p><p>2018 年，<em>Logeswaran</em> 等人觉得 <em>Skip-thoughts</em> 的 <em>decode</em> 效率太低，且无法在大规模的语料上很好的训练。所以，他们把预测上下句的生成任务变成了分类任务，提出了 <a href="https://arxiv.org/pdf/1803.02893.pdf" target="_blank" rel="noopener"><em>Quick-thoughts</em></a>。具体来说就是，选取一个窗口，把窗口内的句子标记为正例，窗口外的句子标记为负例，将这些句子输入模型，让模型判断这些句子是否是同一个窗口的句子。（几个月后的 <em>BERT</em> 也借鉴了这一思路）</p><h3 id="2-4-4-InferSent"><a href="#2-4-4-InferSent" class="headerlink" title="2.4.4 InferSent"></a>2.4.4 InferSent</h3><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20201026173831.png" alt></p><p>除了上述的无监督任务，研究人员还在监督学习任务上进行了尝试。比如 <em>Conneau</em> 等人提出 <a href><em>InferSent</em></a> 模型，基本思想是先在 <em>SNLI</em> （<em>Stanford Natural Language Inference</em>）数据集上训练一个特征提取器，然后利用这个特征提取器将句子转化成向量，再将句子向量应用于分类任务上，以此来判断句向量的质量。</p><h3 id="2-4-5-General-Purpose-Sentence-Representation"><a href="#2-4-5-General-Purpose-Sentence-Representation" class="headerlink" title="2.4.5 General Purpose Sentence Representation"></a>2.4.5 General Purpose Sentence Representation</h3><p>除了单任务的预训练，<em>Subramanian</em> 等人在 2018 年还提出使用多任务来预训练模型。他们在 2018 年发表的论文《<a href><em>Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning</em></a>》提出使用多任务联合学习来对模型进行预训练。论文中包含四种任务：<em>Natural Language Inference, Skip-thougts, Neural Machine Translation</em> 以及 <em>Constituency Parsing</em>，作者希望通过不同侧重点的任务，而不是特定任务来学习句子表征，达到 <em>general</em> 的目的。</p><p>具体的做法还是先在以上四个任务上做联合预训练，预训练好的模型保持不变，顶层添加一个全连接层作为分类器，然后训练一个新的分类器。在训练分类器的过程中，原来预训练的部分不参与训练，只训练顶层的全连接层。最后的实验结果也表明这种训练方式的有效性。</p><h3 id="2-4-6-Universal-Sentence-Encoder"><a href="#2-4-6-Universal-Sentence-Encoder" class="headerlink" title="2.4.6 Universal Sentence Encoder"></a>2.4.6 Universal Sentence Encoder</h3><p>同样在 2018 年，<em>Daniel Cer</em> 等人提出了和 <em>General Purpose Sentence Representation</em> 相同的方法，只是将其中的网络替换成了 <em>Transformer</em>。最后结果发现，利用 <em>Transformer</em> 作为特征提取器效果更好。</p><h2 id="2-5-小结"><a href="#2-5-小结" class="headerlink" title="2.5 小结"></a>2.5 小结</h2><p>直到此时我们会发现，人们对于应该如何在 <em>NLP</em> 上进行预训练还是没有一个清晰的认知。比如，采用什么网络结构，在什么任务上进行预训练，怎样使用预训练的模型等等问题都还是处在一个摸索的阶段。我们也应该看到，虽然没有形成统一的认识，但是一个大致的发展脉络已经隐约可见了，从最初的只使用词向量，到后来开始考虑上下文，再到直接使用预训练模型进行特征抽取；从最初使用简单的 <em>DNN</em>，到后来使用 <em>RNN</em> 或 <em>LSTM</em>，再到 <em>Transformer</em> 的尝试等等，都逐渐呈现拨开云雾见光明的趋势。虽然经历了在不同的任务上进行预训练，但是在语言模型上的尝试也在快速发展，接下来就是预训练语言模型的舞台了。</p><h1 id="3-预训练语言模型"><a href="#3-预训练语言模型" class="headerlink" title="3. 预训练语言模型"></a>3. 预训练语言模型</h1><p>在这部分，我们先简单介绍预训练语言模型的发展历史，随着预训练语言模型的发展，产生了不同的技术流派，我们也将对这些技术流派进行简单的梳理。</p><h2 id="3-1-预训练语言模型发展简史"><a href="#3-1-预训练语言模型发展简史" class="headerlink" title="3.1 预训练语言模型发展简史"></a>3.1 预训练语言模型发展简史</h2><p>首先我们将预训练语言模型的发展分成几个时期：</p><ul><li>2015 年至 2017 年：技术探索期</li><li>2017 年至 2018 年：技术成长期</li><li>2018 年：技术爆发期</li><li>2019 年至今： 百家争鸣期</li></ul><h3 id="3-1-1-技术探索期"><a href="#3-1-1-技术探索期" class="headerlink" title="3.1.1 技术探索期"></a>3.1.1 技术探索期</h3><p>2015 年，<em>Andrew M. Dai</em> 和 <em>Quoc V. Le</em> 首次尝试先使用语言模型在大规模无标注语料上进行预训练，然后以整个语言模型来初始化下游的分类模型的方法。</p><p>他们的提出两种预训练方法：</p><ol><li><p>使用 <em>LSTM</em> 训练一个标准的语言模型，即根据上文预测下一个词；</p></li><li><p>使用 <em>LSTM</em> 作为序列自编码器，将整个序列输入到模型中，将序列编码成一个固定维度的向量，然后根据这个向量去预测输入序列本身。</p></li></ol><p>然后使用训练好的 <em>LSTM</em> 权重去初始化一个 <em>LSTM</em> 分类模型，发现上述两种预训练方法不仅提升了模型的分类效果，而且在更多的相关数据集上进行预训练可以极大的提升泛化能力。</p><p>2017 年 4 月， <em>AllenAI</em> 研究小组提出了一个模型——<a href="https://arxiv.org/pdf/1705.00108.pdf" target="_blank" rel="noopener"><em>TagLM</em></a>，如下图所示：</p><p><img width="350" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20201022102114.png"></p><p>可以看到，模型采用两段式训练方法（虽然写了三个步骤）：① 从无标记的数据中学习词向量和语言模型，② 将句子作为输入传递给训练好的语言模型，然后将语言模型的输出与 <em>RNN</em> 第一层输出拼接在一起作为序列标记任务模型的输入传递给下游序列标记模型，然后训练下游模型。这个思路与后来的 <em>ELMO</em> 模型是基本一致的，而且由于 <em>ELMO</em> 也是 <em>AllenAI</em> 小组的研究成果，因此我们可以认为 <em>TagLM</em> 模型是 <em>ELMO</em> 的一次初探。</p><p>2017 年 8 月，<em>McCann</em> 等人从机器翻译角度出发，训练了一个两层 <em>Bi-LSTM</em> 的 <em>seq2sqe</em> 翻译模型。然后，将训练好的 <em>encoder</em> 部分拿出来作为预训练的权重用于分类任务。</p><p>2018 年 1 月，<em>Howard</em> 等人提出的 <a href="https://arxiv.org/pdf/1801.06146.pdf" target="_blank" rel="noopener"><em>ULMFiT</em></a> 开始对 <em>NLP</em> 中的预训练方法展开了另一次新的尝试：</p><p><img width="648" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20201022103646.png"></p><p>该模型采用的是三段式训练方法：① 在通用语料上训练词向量和语言模型；② 在专业领域语料上对词向量和语言模型进行 <em>fine-tuning</em>；③ 将训练好的语言模型作为句子特征提取器应用于下游分类任务。</p><p>可以看到，以上这些工作已经初现了现在的预训练方法：① 多段式训练，先在相关的数据上预训练一个模型，然后在具体任务数据上进行微调；② 不再只是单纯的使用预训练的词向量，而是开始考虑使用上下文。</p><p>这个时期的预训练更多的是尝试性的工作，比如应该如何训练语言模型，预训练的模型该怎么应用到下游任务等等。随着各种技术尝试的开展，从最开始的发散式尝试，逐渐开始有了一些技术聚焦点：从语言模型角度进行更多的尝试，采用双向 <em>LSTM</em> 网络结构，预训练方法在分类和序列标注任务上都有不俗的表现，这些都为后来的技术成长做好了铺垫。</p><h3 id="3-1-2-技术成长期"><a href="#3-1-2-技术成长期" class="headerlink" title="3.1.2 技术成长期"></a>3.1.2 技术成长期</h3><ul><li><strong>EMLo</strong></li></ul><p><em>ELMo</em> 全称 <em>“Embedding from Language Models”</em>，其核心点在于它的论文名《<a href="Deep contextualized word representations"><em>Deep contextualized word representations</em></a>》，即根据上下文（动态）生成词向量。比如：“我喜欢吃苹果”和“我喜欢苹果手机”，这两句话中的“苹果”分别表示两种含义，他们的词向量应该是不同的，那么 <em>ELMo</em> 可以根据这两句话表达的语义给“苹果”生成两个不同的词向量。</p><p><em>ELMo</em> 是一个两段式训练模型：先在语料上训练一个语言模型，然后利用训练好的语言模型去动态生成词向量，应用于下游任务。第一阶段训练语言模型使用的是双向 <em>LSTM</em> 网络，所谓双向 <em>LSTM</em> 就是一个前向 <em>LSTM</em> 和 一个后向 <em>LSTM</em>，前向 <em>LSTM</em> 通过给定的前 $k-1$ 个词预测第 $k$ 个词，后向 <em>LSTM</em> 就是通过给定的反向的 $k-1$ 个词预测第 $k$ 个词。</p><p>假设模型有 $L$ 层双向 <em>LSTM</em>，那么我们可以得到 $2L+1$ 个向量：前向和后向各一个向量，加上输入层的词向量。</p><p><img width="512" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/elmo-forward-backward-language-model-embedding.png"></p><p>语言模型预训练好以后，在下游任务中我们怎么用这 $2L+1$ 个向量呢？首先是将每层的前向和后向的向量拼接在一起，然后对每层的向量进行加权求和，每层的权重可以通过学习得到。求和之后再进行一定程度的缩放，将缩放后的向量与输入层的词向量再进行加权求和。这样我们就将 $2L+1$ 个向量整合成了一个向量，然后将这个向量作为下游任务的输入，训练下游任务模型。</p><p><img width="512" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/elmo-embedding.png"></p><p>在实验中，作者使用的是两层的双向 <em>LSTM</em>，第一层学习到语法信息，第二层学习语义信息。从上面我们可以看到，<em>ELMo</em> 相比于之前的工作，并没有本质上的创新，基本上是对前人工作的引申和扩展，但是他的效果又是如此的惊艳，在 2018 年初的时候横扫了 <em>NLP</em> 中 6 大任务的最好结果。</p><p>那么 <em>ELMo</em> 有什么缺点呢？</p><ol><li>2017 年谷歌提出了 <em>Transformer</em> 模型，很多研究表明 <em>Transformer</em> 的特征提取能力是要强于 <em>LSTM</em> 的，比如我们上面提到的 <em>Universal Sentence Encoder</em>;</li><li><em>ELMo</em> 采用的特征融合方法还是比较传统的拼接加权求和等方式，相比于后来的 <em>BERT</em> 的一体化融合方式，融合能力弱了一些。</li><li><em>ELMo</em> 是基于特征融合的方式来影响下游任务的，而从 <em>ImageNet</em> 的角度来看，也许 <em>fine-tuning</em> 的方式更适合下游任务（知识迁移）。</li></ol><ul><li><strong><em>GPT</em></strong></li></ul><table><tr>    <td align="center"><img width="300" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/640.png"></td>    <td align="center"><img width="512" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/OpenAI-GPT-transformer-decoder_web.jpg"></td>    </tr></table>  <p><em>GPT</em> 全称 <em>Generative Pre-Training</em>，模型的大致结构如上图左侧所示。<em>GPT</em> 与之前的预训练模型一样，首先是预训练一个语言模型，然后将语言模型应用到下游任务。它与 <em>ELMo</em> 的不同点在于：</p><ol><li>采用 <em>Transformer</em> 作为特征抽取器；</li><li>采用的是单向的语言模型；</li><li>将预训练和 <em>fine-tuning</em> 的结构进行统一，不再需要特征融合。</li></ol><p>其中 <em>Transformer</em> 部分是经过改造的 <em>decoder</em> 层，如上图右侧所示。由于原始的 <em>Transformer</em> 是 <em>encoder-decoder</em> 结构的机器翻译模型，在 <em>decoder</em> 部分需要与 <em>encoder</em> 语义融合所以多了一个 <em>multi-head attention</em> 层，而在语言建模时，不需要语义融合，因此可以将其去掉。（更多关于 <em>Transformer</em> 的内容可参见【<a href="https://rogerspy.gitee.io/2019/08/26/NLP%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%AE%80%E4%BB%8B%EF%BC%88%E4%B8%80%EF%BC%89/">1</a>】、【<a href="https://rogerspy.gitee.io/2019/08/27/NLP%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%AE%80%E4%BB%8B%EF%BC%88%E4%BA%8C%EF%BC%89/">2</a>】、【<a href="https://rogerspy.gitee.io/2019/09/01/analyse-transformer/">3</a>】、【<a href="https://rogerspy.gitee.io/2019/09/11/Transformer%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-pytorch/">4</a>】、【<a href="https://rogerspy.gitee.io/2019/09/16/Transformer%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-tensorflow/">5</a>】、【<a href="https://rogerspy.gitee.io/2019/09/18/transformer%E7%BC%96%E7%A0%81%E5%B1%82%E8%A1%A8%E7%A4%BA/">6</a>】）</p><p>预训练好语言模型以后，怎么用到下游任务呢？以前的预训练语言模型在做下游任务的时候，可以任意设计自己的网络结构，而预训练语言模型只作为一个特征抽取器而已，在训练下游任务模型的时候，预训练的语言模型参数固定不变，只更新下游任务模型的参数。但是 <em>GPT</em> 说，我不要做配角，我要做主角！所以在利用 <em>GPT</em> 做下游任务的时候，我们需要把下游任务的网络结构设计成 <em>GPT</em> 的样子，利用预训练好的 <em>GPT</em> 初始化下游模型参数，然后利用任务数据对整个模型进行 <em>fine-tuning</em>。这样做的好处是，一来不需要特征融合（设计特征融合的方式也加入了过多的人工干预）；二是和 <em>ULMFiT</em> 一样的思路，先在通用领域的是语料上预训练语言模型，在下游任务 <em>fine-tuning</em> 的时候相当于在训练下游任务的同时，也在利用领域语料 <em>fine-tuning</em> 语言模型，相比于 <em>ULMFiT</em>，<em>GPT</em> 更加简洁明了。</p><p>那么问题来了， <em>NLP</em> 的各种任务花样百出，怎么改造才能靠近 <em>GPT</em> 的网络结构呢？</p><p><img width="648" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20201028113642.png"></p><p>论文给出的改造方案也很简单：</p><ul><li>分类问题：直接在文本前后加上开始和结束符号；</li><li>判断句子关系问题：在两个句子之间添加一个分隔符即可；</li><li>文本相似性问题：将两个句子顺序颠倒做出两个输入，句子间仍然添加分隔符，之所以做成两个输入主要是告诉模型，句子的顺序不重要；</li><li>多选问题：制作多路输入，每一路是将文章和答案选项使用分隔符拼接在一起即可。</li></ul><p>从图中我们可以看出，不同 <em>NLP</em> 任务的改造并不困难，只需要修改输入部分即可。而输出部分也是一个简单的全连接层。</p><p><em>GPT</em> 最终的效果也是相当的惊艳，在实验涉及到的 12 项任务中，9 个达到了最佳效果！</p><p>但是，由于 <em>GPT</em> 采用的是单向语言模型，使得 <em>GPT</em> 存在语言建模过程中信息不健全的固有缺陷，而正是这一缺陷给了后来者 <em>BERT</em> 的可乘之机。</p><h3 id="3-1-3-技术爆发期"><a href="#3-1-3-技术爆发期" class="headerlink" title="3.1.3 技术爆发期"></a>3.1.3 技术爆发期</h3><p>时间来到 2018 年 10 月 11 日，这是一个本来平凡到不能再平凡的日子，一切都很平静。但是随着 <em>Jacob Devlin</em> 及其合作者在 <em>Arxiv</em> 上悄悄的放了一篇他们的最新论文 《<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener"><em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em></a>》，犹如一声惊雷彻底打破了宁静。首先是在 <em>Twitter</em> 上引发了巨大的浪潮，随后中文各大社区包括但不限于微信公众号、微博等几乎被刷屏。狂揽 11 项 <em>NLP</em> 任务的最佳效果，彻底宣告预训练语言模型的王者降临。</p><p><img width="512" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20201028162625.png"></p><p><em>BERT</em> 采用和 <em>GPT</em> 完全一致的两个阶段训练方式。与 <em>GPT</em> 最大的不同是采用了双向语言建模，为了防止双向语言建模时发生泄密，<em>BERT</em> 采用的是 <em>Mask Language Model</em> 方式训练语言模型。另外，<em>BERT</em> 还借鉴了 <em>Skip-thoughts</em> 的思想，通过预测下一个句子来获取句子信息（<em>Next Sentence Prediction， NSP</em>）。</p><ul><li><strong>输入层</strong></li></ul><p><em>BERT</em> 的输入包含三种 <em>Embedding</em>：<em>Token Embedding, Segment Embedding, Position Embedding</em>。其中 <em>Token Embedding</em> 和 <em>Position Embedding</em> 分别对应词向量和位置向量，这个不难理解。那个 <em>Segment Embedding</em> 是用于区分两种句子的向量，正如我们上面说的， <em>BERT</em> 不仅是从语言模型中得到信息，还有一个 <em>NSP</em> 任务。<em>BERT</em> 对 <em>NSP</em> 任务的处理方式和 <em>GPT</em> 类似，将两个句子用一个分隔符拼接在一起，如下图所示。</p><p><img width="512" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20201028162702.png"></p><ul><li><strong><em>Transformer Encoder</em></strong></li></ul><p>前面我们说，<em>BERT</em> 采用的是双向语言建模。我们知道 <em>transformer encoder</em> 是采用的自注意力机制，是一个并行结构，而不是像 <em>LSTM</em> 那样的串行结构。对于串行结构来说，我们将句子中的词按照正向顺序输入网络就得到正向模型，按照反向顺序输入模型就得到反向模型，将正向和反向的模型结合在一起就是双向模型。而对于并行结构来说，句子中的所有词是一起输入到模型中，并没有一个先后顺序，此时我们应该怎么理解 <em>BERT</em> 中的”双向“这个概念呢？</p><table>    <tr>        <td align="center"><img width="200" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20201029112541.png"></td>        <td align="center"><img width="128" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/1566120593103.png"></td>    </tr></table><p>如上左图，自注意力机制是将一句话中的每个词分别于其余的词计算点积（注意力权重），比如“我 爱 北京 天安门 。”，对于“北京”来说，当计算它与“我”、“爱”的注意力权重的时候即为前向，与“天安门”、“。”计算注意力权重的时候为后向。其实就是正常的自注意力的计算过程，被论文作者换了一个名字。</p><p>但是这里有一个问题，无监督的训练神经网络语言模型通常是使网络通过上文或者下文去预测当前的词，<em>GPT</em> 之所以用的是 <em>Transformer decoder</em> 很大程度考虑的也是通过 <em>Mask multi-head attention</em> 掩盖掉下文，避免下文信息泄露。而 <em>BERT</em> 直接采用 <em>encoder</em> 是对句子中的所有词都进行自注意力计算，这样就无可避免的会存在一个下文信息泄露。为了解决这个问题，<em>Devlin</em> 等人提出 <em>Masked Language Model</em>， 其核心思想就是随机将一些词替换成 “[Mask]” ，然后在训练过程中让模型利用上下文信息去预测被替换掉的词。其实它的本质和 <em>CBOW</em> 是一致的，只是说这里我们只预测 “[Mask]”。</p><p>但是这里会有一个问题：在训练的时候，训练数据中包含了大量的 “[Mask]”，但是在预测的时候，数据中是不包含 “[Mask]” 的，相当于认为地使在训练数据和真实数据产生了分布上的偏差，会使得模型在使用过程中出现问题。为了避免这个问题，<em>BERT</em> 的做法是：</p><ol><li><p>随机挑选 15% 的词；</p></li><li><p>将选中的词中的 80% 替换成 [Mask];</p></li><li><p>将选中的词中的 10% 随机替换成其他的词；</p></li><li><p>将选中的词中的 10% 不变。</p></li></ol><p>将 80% 的词替换成 [Mask] 是为了防止泄密，这个正如我们上面所说的。将 10% 的词随机替换成其他词，这样做的目的是使模型不知道哪些词被 [Mask] 了，迫使模型尽量学习每一个词的全局表征，使得 <em>BERT</em> 能更好的的获得上下文相关的词向量。将 10% 的词不做替换，是为了使模型得到一定程度的 <em>bias</em>，相当于是额外的奖励，将模型对于词的表征能够拉向词的真实表征。</p><ul><li><strong>输出层</strong></li></ul><p><em>BERT</em> 的下游任务主要有四种：句子关系分类、单句分类、阅读理解和序列标注。为了能适配这四种任务，<em>BERT</em> 设计了相应的输出层：</p><ol><li>句子关系分类：输出序列的第一个位置后面链接一个 <em>softmax</em> 层用于分类；</li><li>单句分类：输出序列的第一个位置后面链接一个 <em>softmax</em> 层用于分类；</li><li>阅读理解：输出序列中每个词的分类，并将起始和终止位置中间的词取出即可；</li><li>序列标注：输出序列中每个词的分类，将对应列类别取出即可。</li></ol><table>    <tr>        <td align="center"><img width="256" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20201028190537.png"></td>        <td align="center"><img width="256" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20201028190550.png"></td>    </tr>    <tr>        <td align="center"><img width="256" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20201028190607.png"></td>        <td align="center"><img width="256" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20201028190625.png"></td>    </tr></table><p>其实通过上面的介绍我们可以看出，<em>BERT</em> 本身的创新型并不强，算是一个前人工作的一个集大成者，类似于 <em>Transformer</em> 之于注意力。比如双向语言建模的特性是 <em>Transformer encoder</em> 自带的能力，<em>Masked Language Model</em> 实际上借鉴了 <em>CBOW</em> 的思想，而 <em>NSP</em> 则是借鉴 <em>Skip-thoughts</em>，在输出层的多任务适配更是借鉴了 <em>GPT</em> 的操作。</p><p>但是 <em>BERT</em> 的诞生也是具有划时代意义的：</p><ol><li>明确了 <em>NLP</em> 的一个发展方向，两段式训练，双向语言模型，自注意力机制等都在后来的工作中大放异彩；</li><li>给了 <em>NLP</em> 一个做知识迁移的优雅的解决方案，单单是这一点就足以使 <em>BERT</em> 成为与计算机视觉中 <em>ImageNet</em> 相媲美的里程碑式成就，甚至可能比 <em>ImageNet</em> 更有意义，因为 <em>BERT</em> 预训练用的是无监督学习，无需人工标注，而 <em>ImageNet</em> 仍然是标注数据；</li><li>将 <em>NLP</em> 的发展推向了一个新的高度。在此之后，预训练语言模型迎来了爆发式的大发展，同时预训练语言模型也进入了百家争鸣的时代。</li></ol><h3 id="3-1-4-百家争鸣期"><a href="#3-1-4-百家争鸣期" class="headerlink" title="3.1.4 百家争鸣期"></a>3.1.4 百家争鸣期</h3><p><em>BERT</em> 诞生以后，彻底掀起了预训练语言模型的研究热潮，有针对性 <em>BERT</em> 的各种改进版，有提出新思路的创新版。在短短两年的时间内，预训练语言模型已经发展成了一个大家族，为了厘清家族成员之间的关系，复旦大学的邱锡鹏老师小组对两年来预训练语言模型进行了一下梳理，并根据不同视角将预训练语言模型进行了分类：</p><ul><li>根据向量表示法将模型分成：上下文相关模型和非上下文相关模型；</li><li>根据模型结构将模型分成：使用 <em>LSTM</em> 模型；使用 <em>Transformer encoder</em> 模型；使用 <em>Transformer deocder</em> 模型；使用 <em>Transformer</em> 模型</li><li>根据预训练任务类型将模型分成：<em>Language Model</em>；<em>Masked Language Model</em>；<em>Permuted Language Model</em>；<em>Denoising Autoencoder</em>；<em>Contrastive Learning</em>；</li><li>根据模型的外围扩展：知识增强型；多语言型；特定语言；多模态；特定领域以及压缩模型等。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20201030094701.png" alt></p><h2 id="3-2-小结"><a href="#3-2-小结" class="headerlink" title="3.2 小结"></a>3.2 小结</h2><p>预训练语言模型在 <em>NLP</em> 各领域的强大能力令人兴奋，从 <em>BERT</em> 的提出到现在（2020.10.30）满打满算也不过两年的时间。预训练语言模型的发展也处于百花齐放的阶段，如同预训练技术在 <em>NLP</em> 的发展一样，经历了各种尝试，最后才发现了语言模型这条道路。预训练语言模型也一样，虽然现在每隔一段时间就会有一个新的模型出来刷榜，但是什么样的模型架构，以什么样的形式训练语言模型等等都还在处于探索阶段。在更高一层来看，目前的预训练语言模型并不能真正解决语言的认知能力问题。预训练语言模型的发展还远远没有达到成熟的阶段，但是基于这两年的发展，我们仍然能总结一些规律，也许能够使我们在往更强的模型发展上有迹可循。</p><p>首先，在保证数据质量的前提下，数据量越大，模型容量越大，训练越充分，预训练的模型效果越好；</p><p>其次，训练方式从传统的两段式扩展到四段式能达到更好的效果：</p><ol><li>在大规模通用数据集上预训练大模型；</li><li>在预训练好的通用领域模型上利用领域数据再训练一个领域预训练模型；</li><li>在任务数据上，去掉标签，进行一次任务预训练；</li><li>最后再在具体任务上进行微调</li></ol><p>再次，基于 <em>Transformer</em> 的架构往往能取得最佳效果，未来能否有新的架构取而代之我们拭目以待。</p><p>而目前预训练语言模型也存在着一些问题，比如有些新模型宣称能达到比 <em>BERT</em> 更好的效果，但实际上我们不能确定是因为新模型的模型结构在起作用还是由于它比 <em>BERT</em> 训练的更充分在起作用。也就是说，我们应该如何找到一个模型的能力上限是一个很重要的问题。另外，现在的预训练语言模型越来越大，训练数据越来越多，越来越考验计算硬件的能力，会将研究中心集中在一些资金充裕的地方，严重约束它的发展。</p><p>无论如何，预训练语言模型的成功使我们离 <em>AI</em> 更近了一步，目前存在的问题只有在一点一点的尝试、研究中去解决。</p><h1 id="4-为什么是语言模型？"><a href="#4-为什么是语言模型？" class="headerlink" title="4. 为什么是语言模型？"></a>4. 为什么是语言模型？</h1><p>我们回过头来看，<em>NLP</em> 有众多任务分支：阅读理解、机器翻译、语法分析、自然语言推理、语言模型等等，为什么最后是语言模型成了 <em>NLP</em> 的 <em>ImageNet</em>？</p><p>为了预测句子中下一个最可能的词，语言模型不仅需要学习到语法知识，还需要了解句子的语义。就像 <em>ELMo</em> 成功的关键就在于，他能从浅层获得句法特征，然后从深层获得语义特征一样。不仅如此，一个好的模型还要能从语料中学习到一些常识，比如 <em>“The service was poor, but the food was”</em>，为了预测下一个词，模型必须具备以下能力：</p><ol><li>知道下一个词是用来描述 <em>“food”</em> 的；</li><li>知道 <em>“but”</em> 表示语义转折，并且知道对应的词是 <em>“poor”</em> 的反义词；</li><li>知道 <em>“poor”</em> 的反义词都有哪些。</li></ol><p>语言模型能够获得与下游任务相关的信息，比如长程依赖、层级结构、感情信息等等。相比于 <em>Skip-thoughts</em> 和 <em>autoencoding</em> 等无监督的任务，语言模型更能获得语法信息（<a href="https://openreview.net/forum?id=BJeYYeaVJ7" target="_blank" rel="noopener"><em>Kelly et al. 2018</em></a>）。</p><p>而对于其他比如分类，机器翻译等任务来说，语言模型是无监督的。这样，语言模型的训练预料可以认为是无穷无尽的，而又无需人工标注。这一点对于 <em>NLP</em> 来说是至关重要的，目前世界上超过 1000 人使用的语言有 4500 多种，其中绝大多数的语言都是小语种，无论是直接获取语料资源还是进行人工标注，对于 <em>NLP</em> 任务来书都是巨大的挑战。有了无监督的语言模型，我们可以先在一些容易获取的，资源丰富的语言上先进性预训练，然后再在那些低资源（<em>low-resource</em>）语言上进行微调，对于小语种 <em>NLP</em> 的发展具有重要的意义。</p><p>从实际的发展来看，也印证了我们上面的说法，从 <em>ELMo</em> 到 <em>BERT</em> 正式开启了 <em>NLP</em> 的预训练时代，而这正是归功于预训练语言模型的发展。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://zhuanlan.zhihu.com/p/109954774" target="_blank" rel="noopener">02. 语言模型（language Model）发展历史</a> <em>crazysnailer</em></li><li><a href="https://www.isca-speech.org/archive/archive_papers/icslp_2000/i00_1202.pdf" target="_blank" rel="noopener">Can artificial neural network learn language models?</a> <em>W. Xu and A. Rudnicky</em></li><li><a href="jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a> <em>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Jauvin</em></li><li><a href="https://ruder.io/nlp-imagenet/" target="_blank" rel="noopener">NLP’s ImageNet moment has arrived</a> <em>Sebastian Ruder,  Andrey Kurenkov, Eric Wang, and Aditya Ganesh</em></li><li><a href="https://www.isca-speech.org/archive/archive_papers/interspeech_2010/i10_1045.pdf" target="_blank" rel="noopener">Recurrent neural network based language model</a> <em>T. Mikolov, M. Karafiat, L. Burget, ´J. Cernocky, and S. Khudanpur</em></li><li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.248.4448&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">LSTM neural networks for language modeling</a> <em>M. Sundermeyer, R. Schluter, and H. Ney</em></li><li><a href="https://docs.chainer.org/en/stable/examples/ptb.html" target="_blank" rel="noopener">RNN Language Models</a> <em>Chainer</em></li><li><a href="http://jd92.wang/assets/files/transfer_learning_tutorial_wjd.pdf" target="_blank" rel="noopener">迁移学习简明手册</a> <em>王晋东</em></li><li><a href="https://ruder.io/transfer-learning/index.html#whatistransferlearning" target="_blank" rel="noopener">Transfer Learning - Machine Learning’s Next Frontier</a> <em>Sebastian Ruder</em></li><li><a href="https://www.gehealthcare.com/article/what-is-imagenet-and-why-2012-was-so-important" target="_blank" rel="noopener">What is ImageNet and Why 2012 Was So Important</a> <em>GE Healthcare</em></li><li><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener">从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史</a> <em>张俊林</em></li><li><a href="http://josecamachocollados.com/book_embNLP_draft.pdf" target="_blank" rel="noopener">Embeddings in Natural Language Processing——Theory and Advances in Vector Representation of Meaning</a> <em>Mohammad Taher Pilehvar, Jose Camacho-Collados</em></li><li><a href="http://cs.brown.edu/courses/csci2952d/readings/lecture1-firth.pdf" target="_blank" rel="noopener">A synopsis of linguistic theory</a> <em>John R Firth</em></li><li><a href="https://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520" target="_blank" rel="noopener">Distributional structure</a> <em>Zellig S Harris</em></li><li><a href="https://www.aclweb.org/anthology/J92-4003" target="_blank" rel="noopener">Class-based n-gram models of natural language</a> <em>Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai</em></li><li><a href="https://dl.acm.org/doi/10.1145/361219.361220" target="_blank" rel="noopener">A vector space model for automatic indexing</a> <em>Gerard Salton, A. Wong, and C. S. Yang</em></li><li><a href="https://www.springer.com/gp/book/9789811555725" target="_blank" rel="noopener">Representation Learning for Natural Language Processing</a> <em>Liu, Zhiyuan, Lin, Yankai, Sun, Maosong</em></li><li><a href="http://www.thespermwhale.com/jaseweston/papers/unified_nlp.pdf" target="_blank" rel="noopener">A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning</a> <em>Ronan Collobert, Jason Weston</em></li><li><a href="https://dl.acm.org/doi/10.5555/1953048.2078186" target="_blank" rel="noopener">Natural Language Processing (Almost) from Scratch</a> <em>Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, Pavel Kuksa</em></li><li><a href="https://icml.cc/imls/conferences/2007/proceedings/papers/425.pdf" target="_blank" rel="noopener">Three new graphical models for statistical language modelling</a> <em>Andriy Mnih, Geoffrey Hinton</em></li><li><a href="https://www.cs.toronto.edu/~amnih/papers/hlbl_final.pdf" target="_blank" rel="noopener">A scalable hierarchical distributed language model</a> <em>Andriy Mnih, Geoffrey Hinton</em></li><li><a href="https://arxiv.org/abs/cs/0108006" target="_blank" rel="noopener">Classes for fast maximum entropy training</a> <em>Goodman, J.</em></li><li><a href="https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf" target="_blank" rel="noopener">Hierarchical probabilistic neural network language model</a> <em>Frederic Morin &amp; Yoshua Bengio</em></li><li><a href="https://www.aclweb.org/anthology/P12-1092/" target="_blank" rel="noopener">Improving Word Representations via Global Context and Multiple Word Prototypes</a> <em>Eric Huang, Richard Socher, Christopher Manning, Andrew Ng</em></li><li><a href="https://www.jianshu.com/p/81dddec296fa" target="_blank" rel="noopener">NLP的巨人肩膀（中）</a> <em>weizier</em></li><li><a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" target="_blank" rel="noopener">Distributed Representations of Sentences and Documents</a> <em>Quoc Le, Tomas Mikolov</em></li><li><a href="https://arxiv.org/pdf/1506.06726.pdf" target="_blank" rel="noopener">Skip-Thought Vectors</a> <em>Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, Sanja Fidler</em> </li><li><a href="https://arxiv.org/pdf/1803.02893.pdf" target="_blank" rel="noopener">AN EFFICIENT FRAMEWORK FOR LEARNING SENTENCE REPRESENTATIONS</a> <em>Lajanugen Logeswaran &amp; Honglak Lee</em></li><li><a href="https://arxiv.org/pdf/1705.02364.pdf" target="_blank" rel="noopener">Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</a> <em>Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, Antoine Bordes</em></li><li><a href="https://arxiv.org/abs/1804.00079" target="_blank" rel="noopener">Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning</a> <em>Sandeep Subramanian, Adam Trischler, Yoshua Bengio, Christopher J Pal</em></li><li><a href="https://arxiv.org/pdf/1803.11175.pdf" target="_blank" rel="noopener">Universal Sentence Encoder</a> <em>Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil</em></li><li><a href="https://arxiv.org/pdf/1511.01432.pdf" target="_blank" rel="noopener">Semi-supervised Sequence Learning</a> <em>Andrew M. Dai &amp; Quoc V. Le</em></li><li><a href="https://www.aclweb.org/anthology/K16-1006.pdf" target="_blank" rel="noopener">context2vec: Learning Generic Context Embedding with Bidirectional LSTM</a> <em>Oren Melamud, Jacob Goldberger, Ido Dagan</em></li><li><a href="https://arxiv.org/pdf/1705.00108.pdf" target="_blank" rel="noopener">Semi-supervised sequence tagging with bidirectional language models</a> <em>Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, Russell Power</em></li><li><a href="https://arxiv.org/pdf/1801.06146.pdf" target="_blank" rel="noopener">Universal Language Model Fine-tuning for Text Classification</a> <em>Jeremy Howard, Sebastian Ruder</em></li><li><a href="https://zhuanlan.zhihu.com/p/54743941" target="_blank" rel="noopener">放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较</a> <em>张俊林</em></li><li><a href="https://zhuanlan.zhihu.com/p/254821426" target="_blank" rel="noopener">乘风破浪的PTM：两年来预训练模型的技术进展</a> <em>张俊林</em></li><li><a href="https://arxiv.org/abs/1906.03591" target="_blank" rel="noopener">A Survey on Neural Network Language Models</a> <em>Kun Jing and Jungang Xu</em></li><li><a href="https://arxiv.org/abs/2003.08271" target="_blank" rel="noopener">Pre-trained Models for Natural Language Processing: A Survey</a> <em>Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai &amp; Xuanjing Huang</em></li><li><a href="http://licstar.net/archives/328" target="_blank" rel="noopener">Deep Learning in NLP （一）词向量和语言模型</a> <em>LICSTAR</em></li><li><a href="https://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf" target="_blank" rel="noopener">Neural Transfer Learning for Natural Language Processing</a> <em>Sebastian Ruder</em></li><li><a href="https://arxiv.org/abs/1707.07328" target="_blank" rel="noopener">Adversarial Examples for Evaluating Reading Comprehension Systems</a> <em>Jia, R. and Liang, P. (2017)</em></li><li><a href="https://arxiv.org/abs/1711.02173" target="_blank" rel="noopener">Synthetic and Natural Noise Both Break Neural Machine Translation</a> <em>Belinkov, Y. and Bisk, Y. (2018)</em></li><li><a href="Deep contextualized word representations"><em>Deep contextualized word representations</em></a> <em>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer</em></li><li><a href="https://openreview.net/forum?id=BJeYYeaVJ7" target="_blank" rel="noopener">Language Modeling Teaches You More than Translation Does: Lessons Learned Through Auxiliary Task Analysis </a> <em>Kelly W. Zhang, Samuel R. Bowman</em></li><li><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">Improving Language Understanding by Generative Pre-Training</a> <em>Alec Radford，Karthik Narasimhan，Tim Salimans，Ilya Sutskever</em></li><li><a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <em>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova</em></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247498922&amp;idx=1&amp;sn=d38ba70d63352f4355df21f41f07502e&amp;chksm=96ea232aa19daa3c27086a06ef3f8c9dfd7db011228b4bbf9622aee457d02728fea3cbab67b1&amp;scene=21" target="_blank" rel="noopener">后 BERT 时代的那些 NLP 预训练模型</a> <em>李理</em></li></ol>]]></content>
      
      
      <categories>
          
          <category> 语言模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Language Model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer家族之Deep Transformer</title>
      <link href="/2020/05/23/transformer%E5%AE%B6%E6%97%8F-deep/"/>
      <url>/2020/05/23/transformer%E5%AE%B6%E6%97%8F-deep/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5396ee05ly1g5pqn3ch6zj20u092znph.jpg" alt></p><p><em>Transformer</em> 的功能强大已经是学术界的共识，但是它难以训练也是有目共睹的。本身的巨大参数量已经给训练带来了挑战，如果我们想再加深深度可谓难上加难。这篇文章将会介绍几篇就如何加深 <em>Transformer</em> 的展开研究论文。从目前的研究来看 <em>Transformer</em> 之所以难训是由于梯度消失的问题，要对 <em>Transformer</em> 进行加深就必须要解决这个问题，今天我们介绍三种方法：</p><ul><li><em>Depth-Scaled Initialization</em></li><li><em>Lipschitz Constrained Parameter Initialization</em></li><li><em>Pre-Norm</em></li></ul><a id="more"></a><h1 id="1-Vanishing-Gradient-Analysis"><a href="#1-Vanishing-Gradient-Analysis" class="headerlink" title="1. Vanishing Gradient Analysis"></a>1. Vanishing Gradient Analysis</h1><p>从目前的研究来看，影响 <em>Transformer</em> 收敛的主要因素是残差连接和 <em>LayerNorm</em> 部分的梯度消失问题，比如 <a href="http://aclweb.org/anthology/P18-1008" target="_blank" rel="noopener">Chen et al. (2018)</a> ，<a href="https://doi.org/10.18653/v1/D19-1083" target="_blank" rel="noopener">Zhang et al. (2019)</a>，<a href="https://www.aclweb.org/anthology/P19-1176" target="_blank" rel="noopener">Wang et al. (2019)</a>，<a href="https://openreview.net/pdf?id=B1x8anVFPr" target="_blank" rel="noopener">Xiong et al. (2020)</a> 的研究都证实了这一假设。</p><p><em>Transformer</em> 残差连接和 <em>LayerNorm</em> 如下：</p><script type="math/tex; mode=display">y_l = x_l+\mathcal{F}(x_l;\theta) \\\\x_{l+1} = \mathrm{LayerNorm}(y_l)</script><p>令 $\mathcal{E}$ 表示损失，$x_L$ 表示顶层 <em>sub-layer</em> 的输出，根据链式法则有：</p><script type="math/tex; mode=display">\frac{\partial\mathcal{E}}{\partial x_l} = \frac{\partial \mathcal{E}}{\partial x_L} \frac{\partial x_L}{\partial x_l}</script><p>一层一层分解得：</p><script type="math/tex; mode=display">\frac{\partial x_L}{\partial x_l} = \frac{\partial x_L}{\partial x_{L-1}} \frac{\partial x_{L-1}}{\partial x_{L-2}} \cdots \frac{\partial x_{l+1}}{\partial x_l}</script><p>将 $x_{l+1}$ 表达式代入得：</p><script type="math/tex; mode=display">\frac{\partial x_{l+1}}{\partial x_l} = \frac{\partial x_{l+1}}{\partial y_l}\frac{\partial y_l}{\partial x_l}=\frac{\partial \mathrm{LayerNorm}(y_l)}{\partial y_l}\left(1 + \frac{\partial \mathcal{F}(x_l;\theta_l)}{\partial x_l}\right)</script><p>因此，我们可以得到：</p><script type="math/tex; mode=display">\frac{\partial \mathcal{E}}{\partial x_l} = \frac{\mathcal{E}}{\partial x_L} \times \prod_{k=l}^{L-1}\left[\frac{\partial \mathrm{LayerNorm(y_k)}}{\partial y_k}\left(1+\frac{\partial \mathcal{F}(x_k;\theta_k)}{\partial x_k}\right) \right]</script><p>从上面的分析可以看出，损失函数在先后传播的时候是连乘形式的，这样很容易出现梯度消失或者梯度爆炸问题。另外，<a href="https://openreview.net/pdf?id=B1x8anVFPr" target="_blank" rel="noopener">Xiong et al. (2020)</a> 进一步证明随着损失梯度的向后传播的深度，其大小是以 $(2/3)^{L-l}$ 指数形式下降的。</p><p>实验上，下面两张图分别是 <a href="https://doi.org/10.18653/v1/D19-1083" target="_blank" rel="noopener">Zhang et al. (2019)</a> 和 <a href="https://openreview.net/pdf?id=B1x8anVFPr" target="_blank" rel="noopener">Xiong et al. (2020)</a> 的独立实验结果。上图我们只看实线部分，下图只看橙色柱状图。我们会发现，实验结果所展示出来的原始的 <em>Transformer</em> 中损失梯度随层数的变化趋势和理论分析基本一致。因此，我们可以认为之前的理论分析是合理的。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/dsint.jpg" alt></p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200525170324.png" alt></p><p>现在，我们知道 <em>Transformer</em> 梯度消失之谜了，接下来我们就可以针对这个问题提出解决方案了。</p><h1 id="2-Depth-Scaled-Initialization-and-Merged-Attention"><a href="#2-Depth-Scaled-Initialization-and-Merged-Attention" class="headerlink" title="2. Depth-Scaled Initialization and Merged Attention"></a>2. Depth-Scaled Initialization and Merged Attention</h1><h2 id="2-1-Depth-Scaled-Initialization"><a href="#2-1-Depth-Scaled-Initialization" class="headerlink" title="2.1 Depth-Scaled Initialization"></a>2.1 Depth-Scaled Initialization</h2><p><a href="https://www.aclweb.org/anthology/D19-1083.pdf" target="_blank" rel="noopener">Zhang</a> 等人将 <em>Transformer</em> 的梯度消失问题归咎于残差连接输出的方差过大（方差过大会造成梯度消失可参考<a href="https://rogerspy.gitee.io/2019/09/01/analyse-transformer/">自注意力为什么scaled</a> 的相关讨论）。</p><p>传统的 <em>Transformer</em> 所有参数都是通过均匀分布随机采样初始化的：</p><script type="math/tex; mode=display">\mathbf{W} \in \mathbb{R}^{d_i \times d_o} \sim \mathcal{U}(-\gamma, \gamma) \\\\\gamma = \sqrt{\frac{6}{d_i + d_o}}</script><p>其中 $d_i$ 和 $d_o$ 分别表示输入和输出的维度。</p><p>作者定义了误差信号的变化比率 $\beta$ 来表示在传播过程中误差信号是增强还是减弱。$\beta = \beta_{RC}\cdot \beta_{LN}$，其中 $\beta_{RC}$ 和 $\beta_{LN}$ 分别表示残差连接和 <em>LayerNorm</em> 对误差信号的影响。为了保证训练过程的稳定，理论上我们应该尽量让 $\beta$ 保持在 $\beta \approx 1$。通过实验发现 <em>LayerNorm</em> 会削弱信号 $\beta_{LN} \lt 1$，残差连接会增强信号 $\beta_{RC} \gt 1$，并且而削弱的强度小于增强的强度，也就是说最终会导致 $\beta \gt 1$。</p><p>为了避免这种情况发生，作者提出一种新的初始化方法 —— <em>DS-Init</em>：</p><script type="math/tex; mode=display">\mathbf{W} \in \mathbb{R}^{d_i \times d_o} \sim \mathcal{U}(-\gamma \frac{\alpha}{\sqrt{l}}, \gamma \frac{\alpha}{\sqrt{l}}) \\</script><p>其中 $\alpha \sim [0, 1]$ 是一个超参数，$l$ 表示网络层深度。 </p><p>根据均匀分布的性质，使用 <em>DS-Init</em> 初始化后模型参数的方差会从 $\frac{\gamma^2}{3}$ 降到 $\frac{\gamma^2 \alpha^2}{3l}$，也就是说，$l$ 越大其输出的方差会越小。</p><p>上面的图中虚线部分则展示了，使用 <em>DS-Init</em> 初始化方法后每层的误差梯度。从图中可以看出，该初始化方法是有效的。利用 <em>DS-Init</em> 初始化方法来解决梯度消失问题的另一大优势是，无需修改模型结构，只需要修改初始化方法即可，简单有效又方便。</p><h2 id="2-2-Merged-Attention-Model"><a href="#2-2-Merged-Attention-Model" class="headerlink" title="2.2 Merged Attention Model"></a>2.2 Merged Attention Model</h2><p>随着模型深度的增加，计算量会变得很大，训练和推理时间都会大大增加。为了解决这个问题，作者提出 <em>Merged Attention Model</em>，该模型是 <em><a href="https://arxiv.org/pdf/1805.00631.pdf" target="_blank" rel="noopener">AAN（Average Attention Network）</a></em> 的一种简化：<strong>移除了出了线性变换之外所有的矩阵运算:</strong></p><script type="math/tex; mode=display">\mathrm{SAAN}(\mathbf{S}^{l-1}) = \left[ \mathbf{M}_a(\mathbf{S}^{l-1}\mathbf{W}_v)\right]\mathbf{W}_o</script><p>其中 $\mathbf{M}_a$ 表示 <em>AAN</em> 中的 <em>mask</em> 矩阵。然后通过如下方式将其与 <em>cross-attention</em> 相结合：</p><script type="math/tex; mode=display">\mathrm{MATT}(\mathbf{S}^{l-1}) = \mathrm{SAAN}(\mathbf{S}^{l-1}) + \mathrm{ATT}(\mathbf{S}^{l-1}, \mathbf{H}^L) \\\\\bar{\mathbf{S}}^l = \mathrm{LN}(\mathrm{RC}(\mathbf{S}^{l-1}, \mathrm{MATT}(\mathbf{S}^{l-1})))</script><p>其中 $\mathbf{W}_o$ 在 <em>SAAN</em> 和 <em>MATT</em> 中共享， $\mathbf{H}^L$ 为编码器的输出， $\mathrm{ATT}$ 是 <em>cross-attention</em>。具体的结构图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200527143508.png" alt></p><h1 id="3-Pre-Norm-for-Deep-Residual-Network"><a href="#3-Pre-Norm-for-Deep-Residual-Network" class="headerlink" title="3. Pre-Norm for Deep Residual Network"></a>3. Pre-Norm for Deep Residual Network</h1><p>除了对参数的方差进行归一化之外，<a href="https://arxiv.org/pdf/1906.01787.pdf" target="_blank" rel="noopener">Wang</a> 等人首次指出 <em>Transformer</em> 中的层正则化位置对于训练一个深层网络至关重要。通过重新定位层正则化的位置将其置于每个子层的输入之前，便能够有效解决深层网络当中容易出现的梯度爆炸或者梯度消失现象，这对训练深层网络的影响在之前并未被研究过。</p><h2 id="3-1-Pre-Norm"><a href="#3-1-Pre-Norm" class="headerlink" title="3.1 Pre-Norm"></a>3.1 Pre-Norm</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200527161113.png" alt></p><p><em>Transformer</em> 的残差连接和 <em>LayerNorm</em> 组合方式称为 <em>post-norm</em>。具体的计算流程如图（a）所示：<em>层输入-&gt;层计算-&gt;dropout-&gt;残差累加-&gt;层正则化。</em> 这种方式可能出现的问题如第 1 节讨论，连乘形式的损失梯度很容造成梯度消失或者爆炸。因此，深层 <em>Transformer</em> 通常不容易收敛。</p><p>针对这个问题，作者提出 <em>pre-norm</em> 的组合方式，计算流程如图（b）所示：<em>层输入-&gt;层正则化-&gt;层计算-&gt;dropout-&gt;残差累加。</em> 我们来分析下这种组合方式的梯度形况。</p><script type="math/tex; mode=display">x_{l+1} = x_l + \mathcal{F}(\mathrm{LN}(x_l); \theta_l)=x_l+\mathcal{F}(x_l;\theta_l)</script><p>我们仔细观察 <em>pre-norm</em> 会发现，它有一个重要的特性：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}x_L &= x_{L-1} + \mathcal{F}(x_{L-1};\theta_{L-1}) \\\\& = x_{L-2} + \mathcal{F}(x_{L-2};\theta_{L-2}) + \mathcal{F}(x_{L-1};\theta_{L-1})\\\\& \cdots \\\\&= x_l +\sum_{k=l}^{L-1} \mathcal{F}(x_k;\theta_k)\end{aligned}\end{equation}</script><p>这样 $x_L$ 相对 $x_l$ 的导数可以写作：</p><script type="math/tex; mode=display">\frac{\partial x_L}{\partial x_l} = 1+ \sum_{k=l}^{L-1}\frac{\partial \mathcal{F}(x_k;\theta_k)}{\partial x_l}</script><p>将该式带入误差的导数公式：</p><script type="math/tex; mode=display">\frac{\partial \mathcal{E}}{\partial x_l} = \frac{\partial \mathcal{E}}{\partial x_L} \times \left( 1+ \sum_{k=l}^{L-1}\frac{\partial \mathcal{F}(\mathrm{LN}(x_k);\theta_k)}{\partial x_l}\right)</script><p>对比一下 <em>post-norm</em> 的误差梯度：</p><script type="math/tex; mode=display">\frac{\partial \mathcal{E}}{\partial x_l} = \frac{\mathcal{E}}{\partial x_L} \times \prod_{k=l}^{L-1}\left[\frac{\partial \mathrm{LayerNorm(y_k)}}{\partial y_k}\left(1+\frac{\partial \mathcal{F}(x_k;\theta_k)}{\partial x_k}\right) \right]</script><p>我们会发现，等号右边第二项从连乘变成了连加。这样就解决了连乘可能带来的梯度消失或者爆炸问题。同时，通过 <em>pre-norm</em> 的方式网络在反向更新时，底层网络参数可以直接获得顶层梯度的信息，而不经过其他的变换，使得误差信息更容易传递到底层。</p><h2 id="3-2-Dynamic-Linear-Combination-of-Layers"><a href="#3-2-Dynamic-Linear-Combination-of-Layers" class="headerlink" title="3.2 Dynamic Linear Combination of Layers"></a>3.2 Dynamic Linear Combination of Layers</h2><p>对于深层网络来说，残差连接的方式可能准确度还不够，一个可能的原因是，只用了前一步的信息来预测当前的值。机器翻译的 “单步”特性导致模型可能会 “忘记”距离比较远的层。这就会导致底层的网络训练不充分，针对这个问题作者提出动态线性组合（<em>Dynamic Linear Combination of Layers, DLCL</em>）的方式在信息传递至下一层时对之前所有层的输出进行线性聚合。</p><p>令 $\{ y_0, …, y_l\}$ 表示 $0 \sim l$ 层的输出。定义 $l+1$ 层的输入：</p><script type="math/tex; mode=display">x_{l+1} = \mathcal{G}(y_0, ..., y_l)</script><p>其中 $\mathcal{G}(\cdot)$ 是是一个整合之前各层输出的线性函数，定义如下：</p><script type="math/tex; mode=display">\mathcal{G}(y_0,...,y_l) = \sum_{k=0}^l W_k^{(l+1)} \mathrm{LN}(y_k)</script><p>其中 $W_k^{(l+1)} \in \mathbb{R}$ 是一个可学习的标量，用来对每个输出层进行加权。</p><p><em>DLCL</em> 可以看成一种普适的方法，如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200527171827.png" alt></p><ul><li>（a）表示标准的残差网络：<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">He et al. (2016)</a>；</li><li>（b）表示均匀权重的稠密残差网络：<a href="https://arxiv.org/abs/1703.03906" target="_blank" rel="noopener">Britz et al. (2017)</a>；</li><li>（c）表示多层融合：<a href="https://www.aclweb.org/anthology/C18-1255/" target="_blank" rel="noopener">Wang et al. (2018)</a>；</li><li>（d）表示表示本文的方法。</li></ul><h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h1><ul><li><em>DS-Init</em> 实验结果</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200527172800.png" alt></p><ul><li><em>Pre-norm</em> 实验结果</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200527172827.png" alt></p><h1 id="5-Personal-Thought"><a href="#5-Personal-Thought" class="headerlink" title="5. Personal Thought"></a>5. Personal Thought</h1><p>本文介绍了两种加深 <em>Transformer</em> 的方法，一种是改变模型参数的初始化，一种是改变残差连接方式。无论哪一种目的都是解决深层 <em>Transformer</em> 的梯度消失/爆炸的问题。实际上还有几篇讨论加深 <em>Transformer</em> 的文章这里没有介绍，但是大致思路都差不多。</p><p><em>Kaiming</em> 大神在2016年发表了一篇论文讨论 <em>BN, activation</em> 和 <em>residual</em> 之间的关系：<a href="https://arxiv.org/pdf/1603.05027.pdf" target="_blank" rel="noopener">Identity Mappings in Deep Residual Networks</a>。结合关于加深 <em>Transformer</em> 的工作的各种方法来看，我们是不是可以大胆的猜测 <em>Residual</em>、<em>LN</em>、<em>Initialization</em>、<em>Gradient</em> 这四者之间，是否存在千丝万缕的联系？</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p><a href="https://www.aclweb.org/anthology/D19-1083.pdf" target="_blank" rel="noopener">Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention</a>. <em>Biao Zhang, Ivan Titov, Rico Sennrich. 2019. ACL</em> </p></li><li><p><a href="https://arxiv.org/pdf/1911.03179.pdf" target="_blank" rel="noopener">Lipschitz Constrained Parameter Initialization for Deep Transformers</a>. <em>Hongfei Xu, Qiuhui Liu, Josef van Genabith, Deyi Xiong, Jingyi Zhang. 2020. arXiv: 1911.03179</em> </p></li><li><p><a href="https://arxiv.org/pdf/1906.01787.pdf" target="_blank" rel="noopener">Learning Deep Transformer Models for Machine Translation</a>. <em>Qiang Wang, Bei Li , Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, Lidia S. Chao. 2019. arXiv: 1906.01787</em> </p></li><li><p><a href="https://openreview.net/pdf?id=B1x8anVFPr" target="_blank" rel="noopener">On Layer Normalization in the Transformer Architecture</a> <em>Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Huishuai Zhang, Yanyan Lan, Liwei Wang, Tie-Yan Liu. ICLR 2020 (reject)</em> </p></li><li><p><a href="https://zhuanlan.zhihu.com/p/103083593" target="_blank" rel="noopener">如何在NLP中有效利用Deep Transformer</a> AI科技点评, 知乎</p></li><li><p><a href="https://zhuanlan.zhihu.com/p/84614490" target="_blank" rel="noopener">香侬读 | Transformer中warm-up和LayerNorm的重要性探究</a> 香侬科技，知乎</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> Deep </tag>
            
            <tag> Initialization </tag>
            
            <tag> Norm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer家族之Guassian Transformer</title>
      <link href="/2020/05/13/transformer%E5%AE%B6%E6%97%8F-guasssian/"/>
      <url>/2020/05/13/transformer%E5%AE%B6%E6%97%8F-guasssian/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5396ee05ly1g5pqn3ch6zj20u092znph.jpg" alt></p><p>我们仔细回想一下 <em>Transformer</em> 在计算自注意力的过程， 我们会发现，序列中每个词在与其他词计算注意力权重的时候是无差别计算的。也就是说，这里隐藏着一个假设：词与词之间的距离对语义依赖是没有影响的（抛开位置编码的影响）。然而，根据我们的直觉，距离越近的词可能依赖关系会更强一些。那么事实是怎样的呢？<a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4614" target="_blank" rel="noopener">Guo 等人 2019 </a> 对这个问题进行了研究，并提出 <em>Gaussian Transformer</em> 模型。</p><a id="more"></a><h1 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1. Motivation"></a>1. Motivation</h1><p>在我们日常生活经验中，句子中的一个词通常会与其周围的词关系更为紧密。传统的自注意力计算中，并没有考虑距离的影响。虽然在 <em>Transformer</em> 中使用了位置编码，但实际上自注意力对此并不敏感，这一点我们在最后讨论，这里简单考虑没有添加位置编码的情况。举个例子：<em>I bought a new book yesterday with a new friend in New York.</em>     </p><p>句子中一共出现 3 次 “<em>new</em>”。对于 “<em>book</em>” 来说，只有第一个 “<em>new</em>” 是有意义的，其他两个对它没有任何影响，但是从下图 （a）我们可以看到，普通自注意力分配给了 3 个 “<em>new</em>” 以相同的注意力权重。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200514115327.png" alt></p><p>（b）是我c们考虑距离权重，距离越近权重越大，将这个先验的权重加入到自注意力计算过程，我们得到（c）的自注意力分布，从而更有效的对句子内部结构进行建模。</p><h1 id="2-Gaussian-Self-Attention"><a href="#2-Gaussian-Self-Attention" class="headerlink" title="2. Gaussian Self-Attention"></a>2. Gaussian Self-Attention</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200514102621.png" alt></p><p>假设 $x_i$ 表示句子 $x$ 中的第 $i$ 个词，普通的注意力计算如上图（a）所示：</p><script type="math/tex; mode=display">\tilde{x}_i = \sum_j \mathrm{softmax}(x_i \cdot x_j) \cdot x_j</script><p>为了考虑词与词之间的距离对词义依赖关系的影响，作者考虑加入先验的高斯分布。由于我们很难去真实地统计到底什么样的分布最符合实际情况，所以作者对比了多种不同的分布，最后发现高斯分布的效果最好，所以选择了高斯分布。</p><p>为了简单起见，定义标准正态分布：$\sigma^2=1/(2\pi)$，概率密度函数：$\phi(d) = e^{-\pi d^2}$，其中 $d$ 表示词与词之间的距离。将 $\phi(d_{i,j})$ 加入到上式当中：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}\tilde{x}_i &= \sum_j \frac{\phi(d_{i.j}) \cdot \mathrm{softmax}(x_i \cdot x_j)}{\sum_k \phi(d_{i, k})} \cdot x_j\\\\            &= \sum_j \frac{e^{-\pi d_{i, j}^2} \cdot e^{(x_i \cdot x_j)}}{\sum_k e^{-\pi d_{i, k}^2} \cdot e^{(x_i \cdot x_k)}} \cdot x_j \\\\            &= \sum_j \frac{e^{- \pi d^2_{i, j} + (x_i \cdot x_j)}}{\sum_k e^{-\pi d_{i, k}^2 + (x_i \cdot x_k)}} \cdot x_j \\\\            &= \sum_j \mathrm{softmax}(-\pi d_{i, j}^2 + (x_i \cdot x_j)) \cdot x_j\end{aligned}\end{equation}</script><p>上式第一步中分母 $\sum_k \phi(d_{i, k}^2)$ 是为了归一化。上式第一个公式是我们直接将高斯分布插入到注意力计算的方式，如上图（b）所示，然后我们发现该式可以通过后续的一系列约化，转化成最后一行公式的形式，如上图（c）所示。这样的约化的好处是将高斯项从因子项转化成偏置项，省去了乘法操作，只需要加法操作即可，这样可以省去很大的计算开销。</p><p>由于上面我们假设了高斯的方差为 $1/(2\pi)$，但实际情况不一定是这样的，所以引入一个 $w$ 因子用于弱化这个限制：</p><script type="math/tex; mode=display">\tilde{x}_i = \sum_j \mathrm{softmax}(-w \cdot \pi d_{i, j}^2 + (x_i \cdot x_j)) \cdot x_j</script><p>通过实验发现，我们再额外加入一个惩罚项 $b$ 用以减弱 $x_i$ 自身的影响效果会更好：</p><script type="math/tex; mode=display">\tilde{x_i} = \sum_j \mathrm{softmax}(- |w\cdot \pi d_{i, j}^2 + b| + (x_i \cdot x_j)) \cdot x_j</script><p>其中 $w&gt;0, b \le 0$ 是标量。加入了二者的高斯分布分别如下图所示： </p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200514102645.png" alt></p><h1 id="3-Gaussian-Transformer"><a href="#3-Gaussian-Transformer" class="headerlink" title="3. Gaussian Transformer"></a>3. Gaussian Transformer</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200514102203.png" alt></p><p><em>Gaussian Transformer</em> 模型面向的任务不是机器翻译，甚至不是序列生成任务，而是判别任务。具体来说是为了自然语言推断任务设计的模型。所谓自然语言推断（<em>Natural Language Inference, NLI</em>）又叫文本蕴含识别（<em>Recognizing Textual Entailment, RTE</em>），是研究文本之间的语义关系，含括蕴含（<em>entailment</em>）、矛盾（<em>contradiction</em>）和中性（<em>neutral</em>）。形式上，<em>NLI</em> 是一个文本分类的问题。</p><p>形式化描述为：输入一个句子对：$(\{p_i\}^{l_p}_{i=1}, \{h_j\}^{l_h}_{j=1})$ 分别表示前提（<em>premise</em>）和假设（<em>hypothesis</em>），其中 $p_i, h_j \in \mathbb{R}^V$，分别表示两个句子中第 $i$ 和第 $j$ 个词的 <em>one-hot</em> 向量，$l_p, l_h$ 分别表示两个句子的长度， $V$ 表示词表的大小。模型输出 $\{entailment, contradiction, neutral\}$ 代表的标签。</p><p>上图展示了 <em>Gaussian Transformer</em> 的整体结构。主要分为：<em>Embedding Block</em>、<em>Encoding Block</em>、<em>Interaction Block</em> 和 <em>Comparison Block</em> 四大部分，下面我们就详细介绍一下每一部分。</p><h2 id="3-1-Embedding-Block"><a href="#3-1-Embedding-Block" class="headerlink" title="3.1 Embedding Block"></a>3.1 Embedding Block</h2><p><em>Embedding Block</em> 的目的是将句子中的每个词转化成高维向量，主要包含三部分：字向量、词向量和位置向量。</p><ul><li><p><strong>字向量</strong></p><p>字向量使用随机初始化的 $n-grams$ 字向量进行映射，然后对每个 <em>token</em> 进行 <em>max-over-time pooling</em>：</p><script type="math/tex; mode=display">x_i^{(c)} = \max_t(\{x_{i, t}^{(c)}E_c\}_{t=1}^{l_{x_i}})</script></li><li><p><strong>词向量</strong></p><p>词向量就使用预训练的词向量矩阵进行映射：</p><script type="math/tex; mode=display">x_i^{(w)} = x_iE_w</script></li><li><p><strong>位置向量</strong></p><p>位置向量使用 <em>Transformer</em> 中的位置向量：</p><script type="math/tex; mode=display">x_{i, 2k}^{(p)} = \sin(i/10000^{2k/d_{model}}) \\\\x_{i, 2k+1}^{(p)} = \cos(i/10000^{2k/d_{model}})</script></li></ul><p>有了三个向量之后，将字向量和词向量拼接在一起，然后经过投影矩阵将拼接后的向量投影成 $d_{model}$ 维矩阵，然后与位置向量相加得到最终的 <em>Embedding Block</em> 输出。</p><script type="math/tex; mode=display">x_i^{(e)} = x_i^{(p)} + [x_i^{(w)}: x_i^{(c)}] \cdot W_e</script><p>位置向量是不需要训练的，字向量和词向量在映射成高维矩阵之后还会经过一个投影矩阵 $W_e$ 将其维度变换成模型需要的 $d_{model}$ 维。字向量和词向量在模型训练过程是固定的，即不需要训练，需要训练的只是投影矩阵。</p><blockquote><p>词向量不训练可以理解，但是字向量是随机初始化的，也不训练这样真的合适吗？</p></blockquote><h2 id="3-2-Encoding-Block"><a href="#3-2-Encoding-Block" class="headerlink" title="3.2 Encoding Block"></a>3.2 Encoding Block</h2><p>在 <em>Encoding Block</em> 中包含了 $M$ 个子模块用来抽取句子特征，每个子模块都有相同的结构，除了计算多注意力的时候引入了高斯分布以外，其他都与 <em>Transformer</em> 保持一致。</p><p>因此，每个子模块包含两部分：</p><ul><li>多头高斯自注意力层</li><li><em>FFN</em> 层</li></ul><p>层与层之间使用残差网络和 <em>LayerNorm</em> 连接。</p><p>这部分的核心是高斯自注意力，我们在上一章已经详细介绍过了，其他的和 <em>Transformer</em> 一致，因此不再赘述。</p><h2 id="3-3-Interaction-Block"><a href="#3-3-Interaction-Block" class="headerlink" title="3.3 Interaction Block"></a>3.3 Interaction Block</h2><p><em>Interaction Block</em> 的作用是将两个句子进行信息交互。这一部分与原始的 <em>Transformer</em> 的 <em>Decoder</em> 部分类似， 区别是我们去掉了 <em>Positional Mask</em> 和解码的部分。 通过堆叠 $N$ 个 <em>Interaction</em> 模块，我们可以捕获高阶交互的信息。</p><h2 id="3-4-Comparison-Block"><a href="#3-4-Comparison-Block" class="headerlink" title="3.4 Comparison Block"></a>3.4 Comparison Block</h2><p><em>Comparison Block</em> 的作用就是模型最后的输出预测了。这个模块包含两部分：</p><ul><li><p><strong>聚合层</strong></p><p>在聚合层，作者将 <em>Encoding Block</em> 以及 <em>Interaction Block</em> 的输出拼接到了一起，然后经过了两层带 <em>relu</em> 函数的全连接将维度从 $2d_{model}$ 变为 $d_{model }$。然后又经过了一个缩放的加法，后面就输入到了预测层：</p></li></ul><script type="math/tex; mode=display">v_i = \mathrm{Dense}(\mathrm{Relu}(\mathrm{Dense}([x_i:\tilde{x}_i]))) \\\\\bar{x} = \frac{1}{\sqrt{l_x}}\sum_{i=1}^{l_x}(v_i)</script><ul><li><p><strong>预测层</strong></p><p>我们使用经典的 <em>MLP</em> 分类器对输出进行分预测：</p><script type="math/tex; mode=display">y = \mathrm{softmax}(\mathrm{Dense}(\mathrm{Relu}(\mathrm{Dense}([\bar{p}: \bar{h}]))))</script></li></ul><h1 id="4-Experiment"><a href="#4-Experiment" class="headerlink" title="4. Experiment"></a>4. Experiment</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200514175344.png" alt></p><p>从实验结果可以看出，<em>Gaussian Transformer</em> 相比其他模型不仅参数量上非常少，而且效果也达到了最佳。另外在效率上，作者对比了 <em>ESIM</em> 模型，发现 <em>GT</em> 不仅准确率更高，而且训练速度提升了近 4 倍，推理效率提升了近8 倍。</p><h1 id="5-高斯假设的有效性问题"><a href="#5-高斯假设的有效性问题" class="headerlink" title="5. 高斯假设的有效性问题"></a>5. 高斯假设的有效性问题</h1><p>之前我们在最开始讨论的时候，假设在计算自注意力的时候不考虑位置编码。那么高斯假设和加上位置编码后的自注意力相比究竟是不是有效呢？这里作者进行了讨论。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200514180504.png" alt></p><p>作者对比了原始 <em>Transformer</em> 与 <em>GT</em> 在 <em>MultiNLI</em> 数据集上的表现，发现 <em>GT</em> 的效果更优。另外，作者也对比了不同的分布假设以及 <em>GT</em> 的各种变种，最终发现上面的方法是最优的。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p><a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4614" target="_blank" rel="noopener">Gaussian Transformer: A Lightweight Approach for Natural Language Inference</a>, <em>Maosheng Guo, Yu Zhang, Ting Liu. 2019. AAAI</em></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/75411024" target="_blank" rel="noopener">Gaussian Transformer论文解读</a>, 宋青原，知乎</p></li><li><p><a href="https://cloud.tencent.com/developer/article/1428620" target="_blank" rel="noopener">AAAI 2019 Gaussian Transformer: 一种自然语言推理的轻量方法</a></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> Gaussian </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer家族之Universal Transformer</title>
      <link href="/2020/05/11/transformer%E5%AE%B6%E6%97%8F-ut/"/>
      <url>/2020/05/11/transformer%E5%AE%B6%E6%97%8F-ut/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5396ee05ly1g5pqn3ch6zj20u092znph.jpg" alt></p><p>自从 2017 年谷歌提出 <em>Transformer</em> 模型以后，其在多个任务上的表现都超过了前辈 <em>RNN</em>, 但是在某些任务上表现却差强人意，比如复制字符串（输入 <em>abc</em>， 输出 <em>abcabc</em>）。随后谷歌对原始的 <em>Transformer</em> 进行了改进，提出了 <em>Universal Transformer</em> 模型使其具有更强的泛用性，同时该模型也是<a href="https://www.zhihu.com/question/20115374/answer/288346717" target="_blank" rel="noopener">图灵完备</a>的。</p><a id="more"></a><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p><em>Transformer</em> 解决了 <em>RNN</em> 的最大缺陷：无法并行处理输入序列以及最大长度依赖问题（梯度消失）。但是同时也放弃了 <em>RNN</em> 的两大优势：对迭代学习的归纳偏置（<em>inductive bias towards learning iterative</em>）和递归转换（<em>recursive transformations</em>），而这些优势在某些任务中起到了至关重要的作用。所以 <em>Transformer</em> 会在某些任务中被 <em>RNN</em> 轻易打败。</p><p>谷歌大脑的研究人员们针对这种情况，对 <em>Transformer</em> 进行了扩展，提出 <em>Universal Transfomer</em> 模型。该模型不仅保留了 <em>Transformer</em> 的并行能力和借助自注意力机制从距离较远的词中提取含义这两大优势，又引入时间并行的循环变换结构，相当于将 <em>RNN</em> 的两大优势也纳入其中。更重要的一点是：相比于 <em>RNN</em> 那种一个符号接着一个符号从左至右依次处理的序列处理方式，<em>Universal Transformer</em> 是一次同时处理所有的符号，而且 <em>Universal Transformer</em> 会<strong>根据自我注意力机制对每个符号的解释做数次并行的循环处理</strong>。</p><p>时间并行循环的大致计算过程如下：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/image1.gif" alt></p><p>在每个步骤中，每一个符号（比如句子中的一个词）的信息都可以借助自注意力机制与所有其他的符号进行沟通，就和原本的 <em>Transformer</em> 一样。不过，要对每个符号应用几次这种变换（也就是循环步骤的数目）可以预先手工设置为某个值（比如设置为定制，或者设置与输入长度相关），也可以由 <em>Universal Transformer</em> 自己在执行中动态地选择。为了能够达到后一种效果，研究人员为每个位置加入了一个自适应计算机制，它可以自定义在每个词上计算的次数。</p><p>举个例子：<em>I arrived at the bank after crossing the river</em></p><p>句子中 “<em>I</em>“, “<em>river</em>“ 等词意义比较明显，不存在什么歧义，所以模型可能只在这些词上计算 1 次（循环一次），但 “<em>bank</em>“ 就不一样了，这个词是一个歧c义词，需要通过上下文才能确定词义，因此，模型可能会多次计算该词的词义（循环多次）。这样的设定理论上讲，可以让 <em>UT</em> 具有更强的能力。</p><h1 id="2-模型结构"><a href="#2-模型结构" class="headerlink" title="2. 模型结构"></a>2. 模型结构</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200511165708.png" alt></p><p>对比 <em>Universal Transformer</em> 结构图和 <em>Transformer</em> 结构图可以发现，两者主要有三个区别：</p><ul><li>循环结构</li><li>位置编码多了一个 <em>Timestep embedding</em>;</li><li><em>FFN</em> 变成了 <em>Transition Function</em></li></ul><p>在循环结构上，如上面讨论的，对于每个词的循环次数可以有两种方法确定：① 作为超参数人工设定，如同 <em>Transformer</em> 那样设成 6；② 模型自动设定，要实现这个功能，模型需要加入一个新的机制 —— 自适应计算时间 （<em>Adaptive Computation Time</em>，即 <em>ACT</em>）</p><p>下面我们针对这四个变化详细介绍一下。</p><h2 id="2-1-Recurrent-机制"><a href="#2-1-Recurrent-机制" class="headerlink" title="2.1 Recurrent 机制"></a>2.1 Recurrent 机制</h2><h3 id="2-1-1-Encoder"><a href="#2-1-1-Encoder" class="headerlink" title="2.1.1 Encoder"></a>2.1.1 Encoder</h3><p>给定输入序列长度 $m$，词向量维度 $d$，初始序列嵌入矩阵 $H^0 \in \mathbb{R}^{m \times d}$。$H^t$ 表示经过 $t$ 次循环以后的序列嵌入矩阵。</p><script type="math/tex; mode=display">\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d}})V\\\\\mathrm{MultiHeadAttention}(H^t) = \mathrm{Concat}(head_1, ..., head_k)W^O\\\\head_i=\mathrm{Attention}(H^tW_i^Q, H^tW_i^K, H^tW_i^V)</script><p>其中 $W^Q \in \mathbb{R}^{d \times d/k}$，$W^K \in \mathbb{R}^{d \times d/k}$， $W^V \in \mathbb{R}^{d \times d/k}$。</p><p>在第 $t$ 步时， $H^t \in \mathbb{R}^{m \times d}$ 的计算如下：</p><script type="math/tex; mode=display">H^t = \mathrm{LayerNorm}(A^t + \mathrm{Transition}(A^t)) \\\\A^t = \mathrm{LayerNorm}((H^{t-1}+P^t) + \mathrm{MultiHeadAttention}(H^{t-1}+P^t))</script><p>其中 $\mathrm{Transition}(\cdot)$ 为 <em>Transition Function</em>；$P^t$ 为 <em>Timestep embedding</em> （或者 <em>coordinate embedding</em>），在后面详细介绍。</p><h3 id="2-1-2-Decoder"><a href="#2-1-2-Decoder" class="headerlink" title="2.1.2 Decoder"></a>2.1.2 Decoder</h3><p>解码器与编码器的循环结构基本相同，只是多了一个接受编码器最终状态的另一个多头注意力，其输入的 $Q$ 来自解码器， $K$ 和 $V$ 来自编码器。</p><ul><li><p><strong>训练</strong></p><p>训练的时候，对于一组输入输出序列样本解码器接受右移动一位的输出序列样本作为输入，相应解码器的自注意力机制也被修改成只能访问它左边的预测结果。每轮生成一个字符，通过 <em>softmax</em> 获得每个字符的输出概率：</p><script type="math/tex; mode=display">p(y_{pos}|y_{[1:pos-1]}, H^T)=\mathrm{softmax}(OH^T)</script><p>其中 $O \in \mathbb{R}^{d \times V}$。这部分和 <em>Transformer</em> 是一致的，不再赘述。</p></li><li><p><strong>推理</strong></p><p>在生成时编码器只运行一次而解码器反复运行。解码器接受的输入为已经生成的结果，每<strong>次</strong>(一次可以有多轮)的输出为下一个位置的符号概率分布。我们选择出现概率最高符号作为修订后的符号。</p></li></ul><h3 id="2-1-3-parallel-in-time-recurrent"><a href="#2-1-3-parallel-in-time-recurrent" class="headerlink" title="2.1.3 parallel-in-time recurrent"></a>2.1.3 parallel-in-time recurrent</h3><p>假设给定一个序列： $(a, b, c, d)$。<em>UT</em> 先将该序列经过 <em>embedding</em> 表示成 $(h^0_a, h^0_b, h^0_c, h^0_d)$ 初始化序列矩阵，然后经过 <em>MultiHeadAttention</em> 层和 <em>Transition</em> 层表示成 $(h^1_a, h^1_b, h^1_c, h^1_d)$。以此类推，经过 $t$ 次循环以后序列被表示成 $(h^t_a, h^t_b, h^t_c, h^t_d)$。</p><p>这个循环过程与 <em>RNN</em> 有着截然不同的计算方式。<em>RNN</em> 的循环计算过程是，先计算 $h^0_a$，然后依次计算$h^0_b, h^0_c, h^0_d$，然后进入下一个循环，直到 $t$ 步以后生成 $(h^t_a, h^t_b, h^t_c, h^t_d)$。也就是相当于对于 <em>RNN</em> 来讲，要循环计算 $t$ 次 $m$ 长度的序列，模型需要计算 $m \times t$ 次运算，而 <em>UT</em> 只需要计算 $t$ 次。</p><h2 id="2-2-Coordinate-Embedding"><a href="#2-2-Coordinate-Embedding" class="headerlink" title="2.2 Coordinate Embedding"></a>2.2 Coordinate Embedding</h2><p><em>Transformer</em> 中计算位置向量只需要考虑词的位置就好，这里又考虑了时间维度。</p><script type="math/tex; mode=display">P^t_{i, 2j} = \sin(i/10000^{2j/d}) + \sin(t/10000^{2j/d}) \\\\P^{t}_{i, 2j+1} = \cos(i/10000^{2j/d}) + \cos(t/10000^{2j/d})</script><p>其中 $P^t \in \mathbb{R}^{m \times d}$，维度与序列矩阵保持一致。 </p><h2 id="2-3-Transition-Function"><a href="#2-3-Transition-Function" class="headerlink" title="2.3 Transition Function"></a>2.3 Transition Function</h2><p>根据任务的不同，作者使用两种不同的 <em>transition function</em>：可分离卷积或全连接神经网络。</p><h2 id="2-4-Adaptive-Computation-Time-ACT"><a href="#2-4-Adaptive-Computation-Time-ACT" class="headerlink" title="2.4 Adaptive Computation Time (ACT)"></a>2.4 Adaptive Computation Time (ACT)</h2><p>所谓自适应计算时间，是 <a href="https://arxiv.org/pdf/1603.08983v4.pdf" target="_blank" rel="noopener">Graves 等人 2016 年</a> 提出的一种算法，该算法能自动学习 <em>RNN</em> 需要计算多少轮。用在 <em>UT</em> 中，使得模型能够对序列中不同的词有不同的循环次数，比如序列 $(a,b,c,d)$ 中 $a$ 只循环计算 1 次， $b$ 可能计算 2次，$c$ 会计算 5 次， $d$  计算 8 次。而每个词的循环计算次数由 <em>ACT</em> 决定。当某个位置“停止”后，它的隐状态直接拷贝到下一步，直到所有位置都停止循环。</p><p>简单来说 <em>ACT</em> 会计算每个位置上的词需要停止的概率 （$p \sim [0, 1]$），当 $p$ 大于某个阈值的时候该位置上的词及计算就会停止。为了避免死循环，还可以设置一个最大循环次数，当循环次数达到该值的时候，循环也会被强行停止。</p><h1 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3. Experiments"></a>3. Experiments</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200512151012.png" alt></p><p>作者利用 bAbI 数据集和 WMT14 En-De 数据集在问答，语言模型，机器翻译等任务上做了充分的实验，实验结果表明 <em>UT</em> 的表现能达到更好的效果。上图我们只展示机器翻译的结果，更详细的实验可参看原文。</p><h1 id="4-Personal-Thought"><a href="#4-Personal-Thought" class="headerlink" title="4. Personal Thought"></a>4. Personal Thought</h1><p>关于 <em>Universal Transformer</em> 的模型部分我们就介绍完了，总的来说 <em>UT</em> 具备了一些 <em>Transformer</em> 不具备的能力，解决了一些原有的缺陷。在问答、语言模型、翻译等任务上的表现都有所提升。</p><ul><li><em>Weight sharing</em>：归纳偏置是关于目标函数的假设，<em>CNN</em> 和 <em>RNN</em> 分别假设 <em>spatial translation invariance</em> 和 <em>time translation invariance</em>，体现为 <em>CNN</em> 卷积核在空间上的权重共享和 <em>RNN</em> 单元在时间上的权重共享，所以 <em>Universal Transformer</em> 也增加了这种假设，使 <em>recurrent</em> 机制中的权重共享，在增加了模型表达力的同时更加接近 <em>RNN</em> 的 <em>inductive bias</em>。</li><li><em>Conditional Computation Time</em>：通过加入 <em>ACT</em> 控制模型的计算次数，比固定 <em>depth</em> 的 <em>Universal Transformer</em> 取得了更好的结果。</li></ul><p>但是还是有一些问题文章中并没有说的很清楚，可能为接下来进一步的研究和优化留出了空间：</p><ul><li>空间位置和时间位置向量的直接相加略显粗糙;</li><li>为什么需要不同的 <em>Transition Function</em>，它们分别起到什么作用？</li><li>图灵完备对模型有什么用？</li></ul><h1 id="5-UT-with-Dynamic-Halting"><a href="#5-UT-with-Dynamic-Halting" class="headerlink" title="5. UT with Dynamic Halting"></a>5. UT with Dynamic Halting</h1><p>作者在附录中给出了 <em>Tensorflow</em> 实现的 <em>ACT</em> 代码，这里抄录一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># while-loop stops when this predicate is False</span></span><br><span class="line"><span class="comment"># i.e. all ((probability &lt; threshold) &amp; (counter &lt; max_steps)) are False</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">should_continue</span><span class="params">(u0, u1, halting_probability, y2, n_updates, u3)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_any(</span><br><span class="line">        tf.logical_and(</span><br><span class="line">            tf.less(halting_probability, threshold),</span><br><span class="line">            tf.less(n_updates, max_steps)</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># do while loop iterations until predicate above is False</span></span><br><span class="line">(_, _, _, remainder, n_updates, new_state) = tf.while_loop(</span><br><span class="line">    should_continue, ut_with_dynamic_halting, </span><br><span class="line">    (state, step, halting_probability, remainders, n_updates, previous_state)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># the computations in each step</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ut_with_dynamic_halting</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    state,</span></span></span><br><span class="line"><span class="function"><span class="params">    step,</span></span></span><br><span class="line"><span class="function"><span class="params">    halting_probability,</span></span></span><br><span class="line"><span class="function"><span class="params">    remainders,</span></span></span><br><span class="line"><span class="function"><span class="params">    n_updates,</span></span></span><br><span class="line"><span class="function"><span class="params">    previous_state</span></span></span><br><span class="line"><span class="function"><span class="params">)</span>:</span></span><br><span class="line">    <span class="comment"># Claculate the probablities based on the state</span></span><br><span class="line">    p = common_layers.dense(state, <span class="number">1</span>, activation=tf.nn.sigmoid, use_bias=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># Mask for inputs which have not halted yet</span></span><br><span class="line">    still = tf.cast(tf.less(halting_probability, <span class="number">1.0</span>), tf.float32)</span><br><span class="line">    <span class="comment"># Mask for inputs which halted at this step</span></span><br><span class="line">    new_halted = tf.cast(</span><br><span class="line">        tf.greater(</span><br><span class="line">            halting_probability + p * still_running, threshlod</span><br><span class="line">        ), tf.float32</span><br><span class="line">    ) * still_running</span><br><span class="line">    <span class="comment"># Mask of inputs which haven't halted, and didn't halt this step</span></span><br><span class="line">    still_running = tf.cast(</span><br><span class="line">        tf.less_equal(</span><br><span class="line">            halting_probablity + p * still_running,</span><br><span class="line">            threshold</span><br><span class="line">        ), tf.float32</span><br><span class="line">    ) * still_running</span><br><span class="line">    <span class="comment"># Add the halting prinbability for this step to the halting</span></span><br><span class="line">    <span class="comment"># pribabilities for those inputs which have not halted yet</span></span><br><span class="line">    halting_probability += p * still_running</span><br><span class="line">    <span class="comment"># Compute remainders for the inputs which halteed at this step</span></span><br><span class="line">    remaindes += new_halted * (<span class="number">1</span> - halting_probability)</span><br><span class="line">    <span class="comment"># Add the remainders to those inputs which halted at this step</span></span><br><span class="line">    halting_probability += new_halted * remainders</span><br><span class="line">    <span class="comment"># Increment n_updates for all inputs which are still running</span></span><br><span class="line">    n_updates += still_runnign + new_halted</span><br><span class="line">    <span class="comment"># Compute the weight to be applied to the new state and output</span></span><br><span class="line">    <span class="comment">#    0 when the input has already halted</span></span><br><span class="line">    <span class="comment">#    p when the input hasn't halted yet</span></span><br><span class="line">    <span class="comment">#    the remainders when it halted this step</span></span><br><span class="line">    update_weights = tf.expand_dims(</span><br><span class="line">        p * still_running + new_halted * remainders,</span><br><span class="line">        <span class="number">-1</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># Apply transformation to the state</span></span><br><span class="line">    transformed_state = transition_function(self_attention(state))</span><br><span class="line">    <span class="comment"># Interpolate transformed and prevous states for non-halted inputs</span></span><br><span class="line">    new_state = (</span><br><span class="line">        transformed_state * update_weights\</span><br><span class="line">        + previous_state * (<span class="number">1</span> - update_weights)</span><br><span class="line">    )</span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> (transformed_state, </span><br><span class="line">            step, </span><br><span class="line">            halting_probability, </span><br><span class="line">            remainders, </span><br><span class="line">            n_updates, </span><br><span class="line">            new_state)</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/pdf/1807.03819.pdf" target="_blank" rel="noopener">Universal Transformers</a>, <em>Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit Łukasz Kaiser, 2018,  ICLR 2019</em></li><li><a href="http://ai.googleblog.com/2018/08/moving-beyond-translation-with.html" target="_blank" rel="noopener">Moving Beyond Translation with the Universal Transformer</a>, <em>Google AI Blog</em></li><li><a href="https://zhuanlan.zhihu.com/p/51535565" target="_blank" rel="noopener">(简介)Universal Transformers</a>, wywzxxz, 知乎</li><li><a href="https://zhuanlan.zhihu.com/p/44655133" target="_blank" rel="noopener">【NLP】Universal Transformers详解</a>，李如，知乎</li><li><a href="https://arxiv.org/pdf/1603.08983v4.pdf" target="_blank" rel="noopener">Adaptive Computation Time for Recurrent Neural Networks</a>, <em>Alex Graves, 2016, arXiv: 1603.08983</em></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> parallel-recurrent </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer家族之Insertion-Deletion Transformer</title>
      <link href="/2020/04/29/transformer%E5%AE%B6%E6%97%8F-insertion-deletion/"/>
      <url>/2020/04/29/transformer%E5%AE%B6%E6%97%8F-insertion-deletion/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5396ee05ly1g5pqn3ch6zj20u092znph.jpg" alt></p><p><em>Levenshtein Transformer</em> 不仅具有序列生成的能力，还具有了序列修改的能力。然而我们会发现，整个模型实际上是很复杂的。从模型结构上讲，除了基础的 <em>Transformer</em> 结构，还额外增加了三个分类器：删除分类器、占位符分类器和插入分类器。从训练过程来讲，<em>LevT</em> 需要一个参考策略（<em>expert policy</em>），这个参考策略需要用到动态规划来最小化编辑距离。这样无论从训练还是才能够推理角度，我们都很难保证模型的效率。那么有没有一个既有 <em>LevT</em> 这样的强大的能力，又保持高效简洁的模型呢？<em>Insertion-Deletion Transformer</em> 就这样应运而生了（内心 os：你永远可以相信宋义进:joy:）。</p><a id="more"></a><h1 id="1-Abstract-Framework"><a href="#1-Abstract-Framework" class="headerlink" title="1. Abstract Framework"></a>1. Abstract Framework</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200429160205.png" alt></p><p><em>Insertion-Deletion Transformer</em> 实际上是 <em>KERMIT</em> 的扩展版，之前我们介绍过 <em>KERMIT</em> 是 <em>Insertion Transformer</em>  的泛化版，这里再次对模型进行进化。</p><p><em>Insertion-Deletion Transformer</em> 只包含两个步骤：插入和删除。其中插入操作和 <em>KERMIT</em> 是一样的，之后将插入生成的序列传递给删除模块，进行删除操作。</p><ul><li>$\vec{y}_t$ —— 表示 $t$ 时刻的目标序列；</li><li>$c \in C$ —— $C$ 表示词表；</li><li><p>$p(c, l|\hat{y}_t)$ —— 表示在 $l \in \{1, …, |\vec{y}_t|\}$ 每个位置上插入 $c$ 的概率分布；</p></li><li><p>$d \in [0, 1]$ —— 表示删除操作的概率，$d=0$ 表示不删除， $d=1$ 表示删除；</p></li><li>$p(d,l|\vec{y}_t)$ —— 表示 $l \in [0, |\vec{y}_t|]$ 每个位置上的元素的删除操作的概率分布。</li></ul><h2 id="1-1-Training"><a href="#1-1-Training" class="headerlink" title="1.1 Training"></a>1.1 Training</h2><ol><li>采样生成步骤 $i \sim \mathrm{Uniform([1, n])}$;</li><li>采样前 $i-1$ 次插入操作的排列组合 $z_{1:i-1} \sim p(z_{1:i-1})$；</li><li>将序列传入插入模型，按照 $p(c_i^z|x_{1:i-1}^{z, i-1})$ 概率分布进行插入操作（后续 $x_{1:i-1}^{z, i-1} $ 简写成 $\hat{x}_t$）；</li><li>将上面经过插入操作后的序列传入删除模型；</li><li>删除模型按照 $p(d_l| l, \hat{x}_t^\star)$ 的删除概率分布进行元素删除，然后将删除操作后的序列输出。</li></ol><p>实际上前面三步和 <em>KERMIT</em> 的过程是一致的。</p><h2 id="1-2-Learning"><a href="#1-2-Learning" class="headerlink" title="1.2 Learning"></a>1.2 Learning</h2><p>模型整体是将两个 <em>Transformer</em>  的解码器堆叠在一起，一个作为插入模型，另一个作为删除模型，同时训练各自的参数 $\theta_i$ 和 $\theta_d$。删除模型的信息依赖于插入模型的当前状态。我们的目标是通过并行解码最大化下式：</p><script type="math/tex; mode=display">\hat{c}_l = \mathop{\arg \max} \limits_{c} p(c|l, \hat{x}_t)</script><p>需要注意的是，在计算梯度的时候，两个模型是分开的，也就是说删除模型的梯度没有传递给插入模型。这个其实也好理解，举个例子：</p><p>目标序列是 $[A, B, C, D, E, F, G]$，解码的顺序是：</p><ol><li>$[]$；</li><li>$[D]$；</li><li>$[B, D, F]$；</li><li>$[A, B, C, D, E, F, G]$。</li></ol><p>假设现在 $t=3$，$\vec{y}_t=[B, D, F]$，这就是此时插入模型和删除模型的 <em>target</em>。由于并行解码用到的是平衡二叉树，那么每个时刻的 <em>target</em> 是固定的，我们就可以用这个 <em>target</em> 计算损失，然后计算删除模型和插入模型的各自梯度。</p><p>由于删除模型的信息依赖于插入模型，那么久有可能删除模型学习不到任何东西，即没有可删除的信息。有两方面原因会造成这个结果：</p><ol><li>插入模型太好了，删除模型没有什么好删除的，这样删除模型没有损失，也就没有梯度，也就不能更新权重，造成什么也没学到；</li><li>插入模型什么也没插入，那么删除模型也就没有什么可删除的，也会造成什么也学不到。</li></ol><p>为了解决这个问题，作者提出以 $p_{\mathrm{adv}}$ 的概率 <em>mask</em> 掉目标序列中的一部分元素，这样会使插入模型错误率上升，这样删除模型就能学到新的权重了。</p><h1 id="2-Experiments"><a href="#2-Experiments" class="headerlink" title="2. Experiments"></a>2. Experiments</h1><p>作者在这里没有做机器翻译的相关实验，而是做了另外两个实验：</p><ul><li><strong><em>Learning shifted alphabetic sequences</em></strong></li></ul><div class="table-container"><table><thead><tr><th style="text-align:left">Alphabetic Sequence Shifting</th><th>BLEU</th></tr></thead><tbody><tr><td style="text-align:left">Insertion Model (KERMIT)</td><td>70.15</td></tr><tr><td style="text-align:left">Insertion Deletion Model</td><td>91.49</td></tr></tbody></table></div><ul><li><strong><em>Learning Caesar’s Cipher</em></strong></li></ul><div class="table-container"><table><thead><tr><th>Caesar’s Cipher</th><th>BLEU</th></tr></thead><tbody><tr><td>Insertion Model (KERMIT)</td><td>35.55</td></tr><tr><td>Insertion Deletion Model</td><td>37.57</td></tr></tbody></table></div><blockquote><p>这篇文章很短，所以有些细节可能没有讲的很清楚，期待作者放出源码，到时候跟着源码再看一遍，应该会有新的收获。另外，由于作者这里没有放出机器翻译相关的实验，所以还不太清楚其在机翻上的表现，但是个人比较期待。</p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://arxiv.org/pdf/2001.05540.pdf" target="_blank" rel="noopener">Insertion-Deletion Transformer</a>, <em>Laura Ruis, Mitchell Stern, Julia Proskurnia &amp; William Chan. 2020. arXiv: 2001.05540</em></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> NMT </tag>
            
            <tag> insertion-deletion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer家族之Levenshtein Transformer</title>
      <link href="/2020/04/27/transformer%E5%AE%B6%E6%97%8F-levt/"/>
      <url>/2020/04/27/transformer%E5%AE%B6%E6%97%8F-levt/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5396ee05ly1g5pqn3ch6zj20u092znph.jpg" alt></p><p>之前我们介绍了几个 <em>Insertion-based</em> 的序列生成的方法，使我们跳出传统的从左到右的生成顺序的思维定式。既然有 <em>Insertion</em> 操作，那么一个很自然的想法就是，我们能不能再加入一个 <em>deletion</em> 操作呢？这样我们不仅能生成序列，还可以修改生成的序列，岂不美哉？<a href="https://arxiv.org/pdf/1905.11006.pdf" target="_blank" rel="noopener"><em>Gu et al. (2019)</em></a> 就针对这种想法提出了 <em>Levenshtein Transformer</em> 的模型。<em>Levenshtein Distance</em> 我们不陌生，也就是编辑距离，这里面涉及到三种操作：<em>insertion、deletion、replace</em>，严格意义上来讲 <em>replace</em> 实际上就是 <em>insertion</em> 和 <em>deletion</em> 的组合，所以 <em>LevT</em> 模型只用到了插入和删除操作。</p><a id="more"></a><h1 id="1-Abstract-Framework"><a href="#1-Abstract-Framework" class="headerlink" title="1. Abstract Framework"></a>1. Abstract Framework</h1><p><em>LevT</em> 不同于一般的生成模型，它不仅可以生成序列，还可以修改序列。这样的话他应该算是 <em>generation model</em> 和 <em>refinement model</em> 的组合体，比如，如果 解码端初始化是一个序列，那么该模型就是普通的生成模型；而如果解码端初始化是一段低质量的序列，那么该模型可以通过插入和删除操作将输入序列修改成一段高质量的序列，比如翻译后编辑任务（<em>translation post-editing</em>）。</p><p>作者将 <em>generation</em> 和 <em>refinement</em> 两个任务当成一个马尔科夫决策过程（<em>Markov Decision Process, MDP</em>），使用一个五元组来表示：$(\mathcal{Y, A, E, R}, y_0)$。</p><ul><li>$\mathcal{Y=V}^{N_{\max}}$ —— 表示一个序列集合，其中 $\mathcal{V}$ 表示词表；</li><li>$\pmb{a} \in \mathcal{A}$ —— 表示一个行为，$\mathcal{A}$ 表示动作集合；</li><li>$\pmb{r} \in \mathcal{R}$ —— 表示回馈，$\mathcal{R}$ 表示回馈函数；</li><li>$\mathcal{E}$ —— 表示一个主体（<em>agent</em>）;</li><li>$\pmb{y}_0$ —— 表示初始序列，要么为空，要么是由其他模型生成的序列。</li></ul><p>马尔科夫决策过程如下：一个主体 $\mathcal{E}$ 接受一个行为 $\pmb{a}$，得到一个序列 $\pmb{y}$， 然后反馈函数 $\mathcal{R}$ 度量这个序列与真实的序列的误差：$\mathcal{R(y)=-D(y, y^{\star})}$。由于操作基于插入和删除，自然地，我们可以使用 <em>Levenshtein Distance</em> 去度量。主体要采取什么动作则由策略 $\pi$ 决定：将目前的序列映射成行为概率分布，即 $\pi: \mathcal{Y \rightarrow P(A)}$。</p><p><em>LevT</em> 的任务就是给定一个序列 $\pmb{y}^k = ( y_1, y_2, …, y_n)$，生成 $\pmb{y}^{k+1} = \mathcal{E(\pmb{y}^k, \pmb{a}^{k+1})}$，其中 $y_1$ 和 $y_n$ 分别为 $&lt; s &gt;$ 和 $&lt; /s &gt;$。</p><blockquote><p>注意：在下面的描述中上角标 $k, k+1$ 省略，以及对于条件概率生成，比如机器翻译，源序列输入 $x$ 在下面的公式中也省略了。</p></blockquote><ul><li><p><strong>删除</strong></p><script type="math/tex; mode=display">\pi^{\mathrm{del}}(d|i, \pmb{y}) \sim \mathrm{Bernoulli}(0, 1)</script><p>删除策略是一个 $n$ 重伯努利分布：输入一个序列 $y$，对于序列中的每一个元素 $y_i \in \pmb{y}$ 要么保留，要么删除，即 $d = 1(删除)$ 或者 $d = 0 (保留)$。为了避免 $&lt; s &gt;$ 和 $&lt; /s &gt;$ 被删除，这里强制令 $\pi^{\mathrm{del}}(0|0, \pmb{y}) = \pi^{\mathrm{del}}(0|n, \pmb{y}) = 1$。</p></li><li><p><strong>插入</strong></p><p>相对删除操作，插入就比较复杂了，因为你进要预测插入的词，还要预测插入的位置，我们还希望在同一个位置尽可能多的插入更多的词，这样就相当于模型具有并行的能力。这里作者将插入位置的预测称之为占位符预测（<em>placeholder prediction</em>）；将词的预测称之为词预测（<em>token prediction</em>）。</p><p><strong>第一步：占位符预测</strong></p><script type="math/tex; mode=display">(y_i, y_{i+1}) = \pi^{\mathrm{plh}} (p|i, \pmb{y})</script><p>$(y_i, y_{i+1})$ 表示接下来的词插入到 $y_i, y_{i+1}$ 之间。注意这里可以预测多个插入位置。</p><p><strong>第二步：词预测</strong></p><script type="math/tex; mode=display">tokens = \pi^{\mathrm{tok}}(t|i, \pmb{y})</script><p>每个位置可以有多个词。</p><p>这两个步骤可以看成是 <em>Insertion Transformer</em> 和 <em>masked language model</em> 的混合体。</p></li><li><p><strong>删除和插入的综合</strong></p><p>最后，作者将序列的生成分成三个阶段：① 删除词；② 插入占位符；③ 用词替换占位符。其中每一步都是可以并行处理的。</p><p>给定序列 $\pmb{y}=(y_0, …, y_n)$，预测行为 $a = \{\underbrace{d_0, …, d_n}_{删除操作}; \underbrace{p_0,…,p_{n-1}}_{占位符预测}; \underbrace{t_0^1,…,t_0^{p_0},…,t_{n-1}^{p_{n-1}}}_{词预测}\}$：</p><script type="math/tex; mode=display">\pi(\pmb{a}|\pmb{y}) = \prod_{d_1 \in \pmb{d}}\pi^{\mathrm{del}}(d_i|i, \pmb{y})\cdot \prod_{p_i \in \pmb{p}}\pi^{\mathrm{plh}} (p_i|i, \pmb{y'}) \cdot \prod_{t_i \in \pmb{t}}\pi^{\mathrm{tok}}(t_i|i, \pmb{y''})</script><p>其中 $\pmb{y’}=\mathcal{E}(\pmb{y}, \pmb{d})$，$\pmb{y’’}=\mathcal{E}(\pmb{y’}, \pmb{p})$。</p></li></ul><h1 id="2-Levenshtein-Transformer"><a href="#2-Levenshtein-Transformer" class="headerlink" title="2. Levenshtein Transformer"></a>2. Levenshtein Transformer</h1><h2 id="2-1-Model"><a href="#2-1-Model" class="headerlink" title="2.1 Model"></a>2.1 Model</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200428155309.png" alt></p><p>模型的整体结构仍然采用 <em>Transformer</em>，第 $l$ 个注意力层的状态如下：</p><script type="math/tex; mode=display">\pmb{h}_0^{(l+1)}, ..., \pmb{h}_n^{(l+1)} = \begin{cases}E_{y_0}+P_0, ..., E_{y_n} + P_n, & l=0 \\\\\mathrm{TransformerBlock}_l(\pmb{h}_0^{(l)}, ..., \pmb{h}_n^{(l)})， & l \ne 0\end{cases}</script><p>其中 $E \in \mathbb{R}^{\mathcal{|V|}\times d_{model}}$ 表示词向量，$P \in \mathbb{R}^{N_\max \times N_\max}$ 表示位置向量。</p><p>从图中我们可以看到，解码器的输出 $(\pmb{h}_0,…,\pmb{h}_n)$ 被传入到三个分类器中：删除分类器、占位符分类器和词分类器。</p><ul><li><p><strong>删除分类器</strong></p><script type="math/tex; mode=display">\pi_\theta^{\mathrm{del}}(d|i, \pmb{y}) = \mathrm{softmax} (\pmb{h}_i \cdot A^T)</script><p>其中 $A \in \mathbb{R}^{2 \times d_{model}}$。对序列中除了 $&lt; s &gt;$ 和 $&lt; /s &gt;$ 之外的所有元素进行分类。</p></li><li><p><strong>占位符分类器</strong></p><script type="math/tex; mode=display">\pi_\theta^{\mathrm{plh}}(p|i, \pmb{y}) = \mathrm{softmax}(\mathrm{concate}(\pmb{h}_i,\pmb{h}_{i+1}) \cdot B^T)</script><p>其中 $B \in \mathbb{R}^{(K_\max+1)\times (2d_{model})}$，$(0 \sim K_\max)$ 表示在当前位置上插入占位符的个数，本文中占位符用 <code>[PLH]</code> 表示。比如 $(A, D) \rightarrow (A, [\mathrm{PLH}], [\mathrm{PLH}], D)$。</p></li><li><p><strong>词分类器</strong></p><script type="math/tex; mode=display">\pi_\theta^{\mathrm{tok}}(t|i, \pmb{y}) = \mathrm{softmax}(\pmb{h}_i \cdot C^T)</script><p>其中 $C \in \mathbb{R}^{\mathcal{|V|}\times d_{model}}$。</p></li></ul><h2 id="2-2-Dual-policy-Learning"><a href="#2-2-Dual-policy-Learning" class="headerlink" title="2.2  Dual-policy Learning"></a>2.2  Dual-policy Learning</h2><p>接下来一个关键的问题是，模型怎么学习？最简单的，我们可以用模仿学习。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200428185949.png" alt></p><p>假设我们现在有一个参考策略 $\pi^{\star}$，这个参考策略要么使用真值，要么可以加入少许噪声。我们的目标是最大化以下期望值：</p><script type="math/tex; mode=display">\underbrace{\mathop{\mathbb{E}_{\pmb{y}_{\mathrm{del}} \sim d_{\tilde{\pi}_{\mathrm{del}}}}} \limits_{\pmb{d}^\star \sim \pi^\star} \sum_{d_i^\star \in \pmb{d}^\star} \log \pi_\theta^{\mathrm{del}}(d^\star_i|i, \pmb{y}_{\mathrm{del}})}_{删除操作的目标}+\underbrace{\mathop{\mathbb{E}_{\pmb{y}_{\mathrm{ins}}\sim d_{\tilde{\pi}_{\mathrm{ins}}}}}\limits_{\pmb{p}^\star, \pmb{t}^\star \sim \pi^\star}\left[\sum_{p_i^\star \in \pmb{p}^\star} \log \pi_\theta^{\mathrm{plh}}(p_i^\star|i, \pmb{y}_{\mathrm{ins}})+\sum_{t_i^\star \in \pmb{t}^\star}\log \pi_\theta^{\mathrm{tok}}(t_i^\star|i, \pmb{y'}_{\mathrm{ins}})\right]}_{插入操作的目标}</script><p>其中 $\pmb{y’}_\mathrm{ins}$ 表示在 $\pmb{y}$ 上插入占位符以后的序列。$\tilde{\pi}_\mathrm{del}, \tilde{\pi}_\mathrm{ins}$ 是输入策略，我们不断从由它们导致的状态分布中抽样分布（序列），这些状态首先由参考策略运行，然后返回其行为，我们就是要去最大化这些行为概率。我们有两种方法定义输入策略：① 在真值上加噪； ② 使用对抗策略。如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200428173803.png" alt></p><ol><li><p>删除操作，我们可以定义：</p><script type="math/tex; mode=display">d_{\tilde{\pi}_\mathrm{del}} = \begin{cases}\pmb{y}^0, & u< \alpha \\\\\mathcal{E}(\mathcal{E}(\pmb{y'}, \pmb{p}^\star), \tilde{\pmb{t}}), & u \ge \alpha\end{cases}</script><p>其中 $\pmb{y}^0$ 是初始输入，$\alpha \in [0, 1]$ 表示我们交替从 $\pmb{y}^0$ 和 $\pmb{y’}$ 中选取样本，$u \sim U[0,1]$，$\pmb{y’}$ 是任意准备插入的序列，$\pmb{p}^\star$ 取自参考策略， $\tilde{\pmb{t}}$ 取自当前策略。用这种方法，我们既可以学习对初始序列 $\pmb{y}_0 $ 如何删除，也可以学习在整个过程中如何对序列删除。</p></li><li><p>插入操作，类似于删除操作：</p><script type="math/tex; mode=display">d_{\tilde{\pi}_\mathrm{ins}} = \begin{cases}\mathcal{E}(\pmb{y}^0, \pmb{d}^\star), & \pmb{d}^\star \sim \pi^\star, & u < \beta\\\\\mathcal{E}(\pmb{y}^\star, \tilde{\pmb{d}})， & \tilde{\pmb{d}} \sim \pi^{\mathrm{RND}}, & u\ge \beta\end{cases}</script><p>这里 $u \sim U[0,1]$，$\beta \in [0, 1]$，$\pi^{\mathrm{RND}}$ 是从真值序列中随机丢弃一个字符。</p></li></ol><p>现在还剩最后一个问题：<strong>如何构建参考策略？</strong> </p><ul><li><p><em>Oracle</em>：</p><script type="math/tex; mode=display">\pmb{a}^\star = \mathop{\arg \min} \limits_{\pmb{a}} \mathcal{D}(\pmb{y}^\star, \mathcal{E}(\pmb{y, a}))</script><p>其中 $\mathcal{D}$ 表示 <em>Levenshtein distance</em>。</p></li><li><p><em>Distillation</em>：我们首先训练一个 <em>AR</em> 模型，然后把真值序列 $\pmb{y}^\star$ 用 <em>distillation</em> 模型的 <em>beam search</em> 结果 $\pmb{y}^{AR}$ 进行替换。实验结果表明这个方法是对上一个方法的大幅改进，既可以边生成边修改，又可以并行。</p></li></ul><h2 id="2-3-Inference"><a href="#2-3-Inference" class="headerlink" title="2.3 Inference"></a>2.3 Inference</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200428190044.png" alt></p><p>在进行推理的时候采用 <strong>贪婪解码</strong>。解码终止条件有两个：</p><ul><li><em>Looping</em>：当两次连续的修改操作（插入和删除）返回相同的序列。可能有两个原因：① 没有可插入或者可删除的词了；② 插入和删除相互抵消了，解码过程陷入死循环。</li><li><em>Timeout</em>：通过设置最大迭代次数保证模型不会在一个较差的结果上耗费过多时间。</li></ul><p>另外为了防止不合理的空占位符的出现，作者这里和 <em>Insertion Transformer</em> 一样采取了惩罚措施：从占位符分类器的输出结果中剪掉 $\gamma \in [0,3]$ 项。</p><h1 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3. Experiments"></a>3. Experiments</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200428192142.png" alt></p><p>可以看到，<em>Levenshtein oracle</em> 的效果堪比 <em>Transformer</em>，而加了<em>Transformer distillation</em>之后表现几乎总好于 <em>Transformer</em>，并且还快得多。</p><p>另外，由于 <em>LevT</em> 还可以进行 <em>refinement</em>， 因此作者还评估了模型的修改效果，下表 <em>APE</em>（<em>automatic post-editing)</em>） 是实验结果：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200428192835.png" alt></p><p>左边的结果是 <em>BLEU</em> 值，右边的结果是 <em>TER</em> 翻译错误率。可以看到，<em>LevT</em> 几乎承包了所有任务上的最优结果，表明了这种修改的可行性。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/pdf/1905.11006.pdf" target="_blank" rel="noopener">Levenshtein Transformer</a>, <em>Jiatao Gu , Changhan Wang &amp; Jake Zhao (Junbo). 2019. arXiv: 1905.11006</em></li><li><a href="https://zhuanlan.zhihu.com/p/73417154" target="_blank" rel="noopener">香侬读 | 按什么套路生成？基于插入和删除的序列生成方法</a>, 香侬科技, 知乎</li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> NMT </tag>
            
            <tag> insertion-deletion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer家族之Transformer-InDIGO</title>
      <link href="/2020/04/24/transformer%E5%AE%B6%E6%97%8F-indigo/"/>
      <url>/2020/04/24/transformer%E5%AE%B6%E6%97%8F-indigo/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5396ee05ly1g5pqn3ch6zj20u092znph.jpg" alt></p><p>之前我们介绍的 <em>insertion-based</em> 生成模型实际上都是人为预先定义了生成顺序或者策略，那么我们能不能让模型自己决定要以怎样的顺序生成呢？这就是本文将要讨论的一种解决方案：<em><a href="https://arxiv.org/pdf/1902.01370.pdf" target="_blank" rel="noopener">Insertion-based Decoding with automatically Inferred Generation Order</a></em>，将序列的生成顺序当成一种隐变量，让模型在预测下一个词的时候自动推理这个词应该所处的位置。</p><a id="more"></a><h1 id="1-Abstract-Framework"><a href="#1-Abstract-Framework" class="headerlink" title="1. Abstract Framework"></a>1. Abstract Framework</h1><ul><li>$y=(y_1, …, y_T)$ —— 表示待生成的序列；</li><li>$x=(x_1, …, x_{T’})$ —— 表示输入序列；</li><li>$\pi=(z_2, …,z_T,z_{T+1}) \in \mathcal{P}_T$ —— 表示 $y$ 生成的顺序，其中 $\mathcal{P}_T$ 表示 $(1, …, T)$ 的排列组合；</li><li>$y_{\pi} = \{(y_2,z_2),…,(y_{T+1}, z_{T+1})\}$ ——每一步生成的词及其位置。</li></ul><p>与一般的标记不同，这里我们分别令 $(y_0, z_0) = (&lt; s &gt;, 0), (y_1, z_1) = (&lt; /s &gt;, T+1)$，表示序列的开始和结尾。</p><p>将生成顺序看作隐变量 $\pi$ ，那么对于一个输出句子 $y$ ，取所有顺序的概率之和，作为输出 $y$ 的概率：</p><script type="math/tex; mode=display">p_\theta(y|x) = \sum_{\pi \in \mathcal{P}_T} p_\theta (y_\pi|x)</script><p>其中每个生成顺序的概率为：</p><script type="math/tex; mode=display">p_\theta(y_\pi|x) = p_\theta(y_{T+2}|y_{0:T+1}, z_{0:T+1}, x_{1:T'}) \cdot \prod_{t=1}^Tp_\theta(y_{t+1}, z_{t+1}|y_{0:t}, z_{0:t}, x_{1:T'})</script><p>其中 $y_{T+2}=&lt; eod &gt;$ 表示结束解码的标志，用 $p(y_{T+2}|\cdot)$ 表示其概率分布。</p><p>另外 $z$ 表示生成的单词在句子中的绝对位置。但是这就会有一个问题：我们连最后生成的句序列长度都还不知道怎么能确定当前生成的词在最终序列的绝对位置呢？为了解决这个问题作者提出一个相对位置的概念。</p><h1 id="2-InDIGO"><a href="#2-InDIGO" class="headerlink" title="2. InDIGO"></a>2. InDIGO</h1><h2 id="2-1-Relative-Positions"><a href="#2-1-Relative-Positions" class="headerlink" title="2.1 Relative Positions"></a>2.1 Relative Positions</h2><p>为了解决绝对位置编码问题，作者提出相对位置 $r_{0:t}^t$ 来替代 $z_{0:t}^t$。$r_i^t \in \{-1, 0, 1\}^{t+1}$，且：</p><script type="math/tex; mode=display">r_{i,j}^t=\begin{cases}-1 & z_j^t > z_i^t & (\mathrm{left})\\\\0 & z_j^t 0 z_i^t & (\mathrm{middle}) \\\\1 & z_j^t < z_i^t & (\mathrm{right})\end{cases}</script><p>假设在 $t$ 时刻，对于第 $i$ 个词 $w_i$，用一个向量表示它的相对位置，每个维度取值只有 $(-1, 0, 1)$ 三种。$r_{i,j}^t$ 表示如果 $w_i$ 出现在 $w_j$ 的左边，则取 $-1$，如果出现在右边，则取 $1$，如果是同一个词，则取$0$。使用 $R^t=(r_0^t, r_1^t, …r_t^t)$ 表示序列中所有词的相对位置，每一列表示一个单词的位置向量，这个矩阵关于主对角线对称的元素其实是相反数。举个例子：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200426152417.png" alt></p><blockquote><ul><li>$t=1$ 时，初始化序列：$y=(&lt; s &gt;, &lt; /s &gt;)$，初始化矩阵 $R=\left[\begin{array}{cc}0 &amp; 1 \ -1 &amp; 0 \end{array} \right]$ 。矩阵 $R$ 第一列 $r_0^1=[0,-1]$ 表示 $&lt; s &gt;$ 的相对位置向量：$&lt; s &gt;$ 相对于 $&lt; s &gt;$ 自身，因为是同一个词，所以取值为 $0$；$&lt; s &gt;$ 相对于 $&lt; /s &gt;$ 是在左边，所以取值为 $-1$。同理 $R$ 的第二列表示 $&lt; /s &gt;$ 相对位置向量。</li><li>$t=2$ 时，$y = (&lt; s &gt;, &lt; /s &gt;, dream)$，$R = \left[\begin{array}{ccc} 0&amp; 1&amp; 1\ -1&amp; 0&amp; -1\ -1&amp; 1&amp; 0 \end{array}\right]$，$dream$ 相对于 $&lt; s &gt;$ 在其右边，所以相对位置向量第一维是 $1$；相对于 $&lt; /s &gt;$ 在其左边，所以相对位置向量第二维是 $-1$，相对于 $dream$ 自身，所以第三维是 $0$。</li><li>$t=3$ 时，$y = (&lt; s &gt;, &lt; /s &gt;, dream, I)$，$R = \left[\begin{array}{cccc}0&amp; 1&amp; 1&amp; 1\ -1&amp; 0&amp; -1&amp; -1\ -1&amp; 1&amp; 0&amp; -1\  -1&amp; 1&amp; 1&amp; 0\end{array}\right]$，$I$ 相对于 $&lt; s &gt;$ 在其右边，第一维是 $1$；相对于 $&lt; /s &gt;$ 在其左边，第二维是 $-1$；相对于 $dream$ 在其左边，第三维是 $-1$，最后相对于自身，取值$0$。</li><li>以此类推…</li></ul></blockquote><p>$R$ 这样一个相对位置矩阵的最大优势是，在下一刻计算相对位置的时候这个矩阵不需要重新计算，因为下一个词无论插在哪里都不会影响之前的词的相对顺序，所以在更新 $R$  的时候只需要给 $R$ 新增一行一列即可：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200426163728.png" alt></p><p>得到相对位置之后，我们就可以使用下式将相对位置映射成绝对位置了：</p><script type="math/tex; mode=display">z_i^t = \sum_{j=0}^t\max (0, r_{i, j}^t)</script><p>还以上面的例子加以说明：</p><blockquote><ul><li>$t=2$ 时，$y = (&lt; s &gt;, &lt; /s &gt;, dream)$，$r_{&lt; s &gt;, j}^2 = (0, -1, -1)$，$z_{&lt; s &gt;}^2=\sum_j \max(0, r_{&lt; s &gt;, j}^2)=0$，所以 $&lt; s &gt;$ 的绝对位置是 $0$；$r_{&lt; /s &gt;, j}^2 = (1, 0, 1)$，$z_{&lt; /s &gt;}^2=\sum_j \max(0, r_{&lt; /s &gt;, j}^2)=2$，所以 $&lt; /s &gt;$ 的绝对位置是 $2$；$r_{dream, j}^2 = (1, -1, 0)$，$z_{dream}^2=\sum_j \max(0, r_{dream, j}^2)=1$，所以$dream$ 的绝对位置是 $1$。</li><li>$t=3$ 时，$y = (&lt; s &gt;, &lt; /s &gt;, dream, I)$，$r_{&lt; s &gt;, j}^2 = (0, -1, -1, -1)$，$z_{&lt; s &gt;}^2=\sum_j \max(0, r_{&lt; s &gt;, j}^2)=0$，所以 $&lt; s &gt;$ 的绝对位置是 $0$；$r_{&lt; /s &gt;, j}^2 = (1, 0, 1, 1)$，$z_{&lt; /s &gt;}^2=\sum_j \max(0, r_{&lt; /s &gt;, j}^2)=3$，所以 $&lt; /s &gt;$ 的绝对位置是 $3$；$r_{dream, j}^2 = (1, -1, 0, 1)$，$z_{dream}^2=\sum_j \max(0, r_{dream, j}^2)=2$，所以 $dream$ 的绝对位置是 $2$；$r_{I, j}^2 = (1, -1, -1, 0)$，$z_{I}^2=\sum_j \max(0, r_{I, j}^2)=1$，所以 $I$ 的绝对位置是 $1$。</li><li>以此类推…</li></ul></blockquote><p>其实上面的式子的物理意义也很好理解：看相对位置向量中有多少个 $1$ 就表明该词在多少个词的右边，这样我们就可以知道该词所处的绝对位置了。</p><h2 id="2-2-Insertion-based-Decoding"><a href="#2-2-Insertion-based-Decoding" class="headerlink" title="2.2 Insertion-based Decoding"></a>2.2 Insertion-based Decoding</h2><p>给定一个长度为 $t$ 的序列 $y_{0:t}$  以及相对位置 $r_{0:t}$ ， 下一个词 $y_{t+1}$ 的相对位置的取值有 $3^{t+2}$ 种可能性，显然并不是任意一种可能性都能满足序列正确排序的要求的，我们需要找到最可能的 $r_{t+1}$。</p><p> <img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200426175835.png" alt></p><p>上图给出了一种算法框架：先从现有的序列中选取一个词 $y_k$， 其中 $0 \le k \le t$，然后向其左边或者右边插入 $y_{t+1}$，$r_{t+1}$ 由下式决定：</p><script type="math/tex; mode=display">r_{t+1, j} = \begin{cases}s & j=k \\\\r_{k, j} & j \ne k \end{cases}</script><p>其中如果 $y_{t+1}$ 插入 $y_k$ 的左边，则 $s=-1$；如果 $y_{t+1}$ 插入 $y_k$ 的右边，则 $s=1$，然后更新 $R$。</p><p>继续用上面的例子说明：</p><blockquote><ul><li>$t=1$ 时，假设我们选的 $y_k=&lt; s &gt;$，$y_{2}=dream$，其中 $k=0$，$j=k=0$ ，$dream$ 在 $&lt; s &gt;$ 的右边，所以 $r_{2, 0} = 1$；$j=1$，$r_{0, 1} = -1$，所以  $r_{2, 1}=r_{0, 1}=-1$，由于 $j=2$ 是$dream$ 本身，所以 $r_{2, 2}=0$。这样我们就得到了 $dream$ 的相对位置向量： $(1, -1, 0)$;</li><li>$t=2$ 时，假设我们选的 $y_k = dream$，$y_3 = I$，其中 $k = 2$，$j=0$， $r_{2, 0}=1$，所以 $r_{3, 0} = 1$；$j=1$，$r_{3, 1}=r_{2, 1}=-1$；$j=2$，$j=k=2$，$I$ 在 $dream$ 的左边，所以 $r_{3,2}=-1$；$j=3$ 是 $I$ 自身，所以 $r_{3,3}=0$。这样我们就得到了 $I$ 的相对位置向量：$(1, -1, -1, 0)$；</li><li>依次类推…</li></ul></blockquote><p>上面这个公式的物理意义也是很好理解的：首先我们要确定 $y_{t+1}$ 要插在哪个位置，这个位置有两种表示方式：第一，<em>slot</em>，也就是用两个词形成一个空隙， $y_{t+1}$ 就插在这个空隙里面；第二，<em>s</em>，也就是一个词的左右，我们找到一个词 $y_k$，$y_k$有左右两侧，左边就是 $-1$， 右边就是 $1$。之前我们介绍的方法都是使用 <em>slot</em>，显然，这里使用的是 <em>s</em>。序列中其他的词对 $y_{t+1}$ 相对位置和对 $y_{k}$ 是一样的（因为不考虑 $y_k$本身的话， 实际上 $y_{t+1}$ 就是插在了 $y_k$ 的位置上），所以直接使用 $r_{k, j}$ 即可。比如 $t$ 时刻的序列是 $(A, B, C, D)$，推理 $t+1$ 时刻的时候 $y_k=C$，$y_{t+1}=E$，对于 $(A, B, D)$ 来说，我们是把 $E$ 插在了 $C$ 的位置上 $(A, B, D)$ 相对 $C$ 和 $D$ 的相对位置是一样的。</p><p>从上面的描述来看，我们现在还有两个问题没有解决：</p><ol><li>$y_k$ 怎么选择？</li><li>$s$ 左右怎么确定？</li></ol><p>这两个问题我们就需要通过模型结构来解决了。</p><h1 id="3-Transformer-InDIGO"><a href="#3-Transformer-InDIGO" class="headerlink" title="3. Transformer-InDIGO"></a>3. Transformer-InDIGO</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200427100319.png" alt></p><p>整个模型框架包含三部分：</p><ul><li><em>(a)</em> —— 词和位置预测模块；</li><li><em>(b)</em> —— 相对位置矩阵更新；</li><li><em>(c)</em> —— 重排序后的序列输出。</li></ul><p>在传统的 <em>Transformer</em> 结构中有一个零件叫做 <em>positional encoding</em>，这是一个绝对位置的编码，但现在解码器的绝对位置是未知的，所以这部分是需要进行修改的。</p><p>令 $U=(u_0, …, u_t)$ 表示 $y_{0:t}$ 序列经过 <em>Embedding</em> 之后的隐层，$R^t$ 是其对应的相对位置矩阵。修改后的注意开计算方法如下：</p><script type="math/tex; mode=display">e_{i, j} = \frac{(u_i^TQ)\cdot(u_j^TK+A_{[r_{i, j}+1]})^T}{\sqrt{d_{model}}}</script><p>其中$Q, K \in \mathbb{R}^{d_{model}\times d_{model}}$，$A \in \mathbb{R}^{3 \times d_{model}}$ 是参数矩阵，$A_{[r_{i, j}+1]}$ 表示第 $r_{i, j}+1$ 列。</p><h2 id="3-1-Word-amp-Position-Prediction"><a href="#3-1-Word-amp-Position-Prediction" class="headerlink" title="3.1 Word &amp; Position Prediction"></a>3.1 Word &amp; Position Prediction</h2><p>  经过自注意力的计算之后我们得到一个矩阵 $H$，下一个词及其位置概率为：</p><script type="math/tex; mode=display">p(y_{t+1}, r_{t+1} | H) = p(y_{t+1}|H) \cdot p(r_{t+1}|y_{t+1}， H)</script><p>也就是先预测下一个单词是什么，再预测它的相对位置。当然也可以倒过来，只是实验效果不如这个。</p><ul><li><p><strong>预测 $y_{t+1}$</strong></p><script type="math/tex; mode=display">p_{\mathrm{word}} (y|H) = \mathrm{softmax}((h_t^TF)\cdot W^T)</script><p>其中 $W \in \mathbb{R}^{d_V \times d_{model}}$ 表示词向量矩阵，$d_V$ 表示词表大小，$F \in \mathbb{R}^{d_{model} \times d_{model}}$ 是用来将 $h_t$ 线性变换的权重矩阵。</p></li><li><p><strong>预测 $k$</strong></p><p>这里我们就要回答上一节我们遗留的两个问题 $y_k$ 和 $s$ 我们怎么确定？</p><script type="math/tex; mode=display">p_{\mathrm{pointer}}(k|y_{t+1}, H) = \mathrm{softmax}\left( (h_t^TE+W_{[y_{t+1}]})\cdot \left[\begin{array}{ccc} H^TC \\ H^TD\end{array}\right]^T\right)</script><p>其中 $C,D,E \in \mathbb{R}^{d_{model}\times d_{model}}$，$W_{[y_{t+1}]}$ 是 $y_{t+1}$ 的词向量。$C, D$ 是分别用来获取左、右的矩阵。注意我们最后得到的 $k_{t+1} \in [0, 2t+1]$，因为每个词都有左右两个位置。另外，为了避免 $y_{t+1}$ 出现在 $&lt; s &gt;$ 的左边和 $&lt; /s &gt;$ 的右边，作者手动设置 $p_{\mathrm{pointer}}(0|\cdot)=p_{\mathrm{pointer}}(2+t|\cdot) = 0$。</p><blockquote><p>一个词在 $y_k$ 的右边实际上等价于在 $y_{k+1}$ 的左边。那其实这两个预测结果都是对的。虽然最后的 $r$ 向量都是一样的。</p></blockquote></li></ul><p>相对位置的更新和输出序列的重排序我们前面已经介绍了，这里就不再赘言。</p><h2 id="3-2-Learning"><a href="#3-2-Learning" class="headerlink" title="3.2 Learning"></a>3.2 Learning</h2><p>因为一个句子的可能排列顺序太多了，不可能一一枚举，所以这里最大化 <em>ELBO（evidence lower-bound ）</em> 来代替最开始的概率之和。</p><p>对于输入 $x$ 和输出 $y$，首先定义一个生成顺序 $\pi$ 的近似后验 $q(\pi|x, y)$。然后 <em>ELBO</em> 可以表示为：</p><script type="math/tex; mode=display">\begin{equation}\nonumber\begin{split}\mathcal{L}_{\mathrm{ELBO}} &= \mathbb{E}_{\pi \sim q} \log p_\theta(y_\pi|x) + \mathcal{H}(q)\\\\&= \mathbb{E}_{r_{2:T+1}\sim q}\left(\sum_{t=1}^{T+1}\underbrace{\log p_\theta(y_{t+1}|y_{0:t}, r_{0:t}, x_{1:T'})}_{生成下一个字符} + \sum_{t=1}^{T}\underbrace{\log p_\theta(r_{t+1}|y_{0:t+1}, r_{0:t}, x_{1:T'})}_{生成下一个字符的相对位置} \right) + \mathcal{H}(q)  \end{split}\end{equation}</script><p>其中 $\pi = r_{2:T+1}$， 表示从 $q(\pi|x, y)$ 中采样的相对位置， $\mathcal{H}(q)$ 表示熵项，如果 $q$ 是固定的，那么这一项可以忽略。</p><p>为了计算位置损失，作者这里定义：</p><script type="math/tex; mode=display">p_\theta(r_{t+1}) = p_{\mathrm{pointer}}(k^l|\cdot)+p_{\mathrm{pointer}}(k^r|\cdot)</script><p>$k^l$ 和 $k^r$ 就是我们前面讲的  $y_k$ 的右边等价于在 $y_{k+1}$ 的左边。</p><p>然后就可以根据近似后验来进行采样，优化这个函数了，那么这个近似后验怎么定义呢？作者这里考两种方法：</p><ul><li><p>常见的确定顺序，如下表：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200427114103.png" alt></p><p>这种情况下，模型其实就变成了和普通的序列生成模型差不多了，只用最大化一个生成顺序的概率就行了，区别就是多了相对位置编码。</p></li><li><p><em>Searched Adaptive Order (SAO)</em>，其实就是 <em>beam search</em>。传统的序列生成模型其实也有 <em>beam search</em>，不过那是在每个时刻解码概率最大那些子序列。 而这里的 <em>beam search</em> 空间更大，搜索的是整个排列的空间。 也就是在每个时刻，遍历所有的下一个单词和它的相对位置，找出最大的 $B$ 个子序列。 最后的目标函数变为了：</p><script type="math/tex; mode=display">\mathcal{L}_{\mathrm{SAO}} = \frac{1}{B}\sum_{\pi \in \mathcal{B}} \log p_\theta(y_\pi|x)</script><p>这里作者假设：</p><script type="math/tex; mode=display">q(\pi|x, y) = \begin{cases}1/B, & \pi \in \mathcal{B}\\\\0, & \mathrm{otherwise}\end{cases}</script></li></ul><p>上面的 <em>beam search</em> 可以帮助我们找到贪心下最优的生成顺序，但是，同时也减少了生成全局最优的可能性，为此，我们可以在 <em>beam search</em> 的同时加入 <em>dropout</em>。</p><p>在实际的实验中，用上面的方法导致模型生成很多标点和功能符号，这是因为位置预测模块的学习比字符预测模块的学习快得多。为此，我们可以用一种固定顺序如 <em>L2R</em> 去预训练整个模型，然后再用<em>SAO</em> 去微调。</p><blockquote><p>现在我们可以由模型生成任意顺序了，但是，这种方法有一个问题：不能并行。因为该方法还是需要一个一个去生成字符和位置。</p></blockquote><h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h1><p>实验主要做了机器翻译、词序还原、代码生成和图像标签生成，这里就简单看一下机器翻译结果，其他的详见论文。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200427115841.png" alt></p><p>这里，中间的若干行都是不同的自回归生成策略。可以看到，在三个数据集和各种指标下，<em>SAO</em> 都比自回归好。下面给出一个实际的例子：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200427120201.png" alt></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p><em><a href="https://arxiv.org/pdf/1902.01370.pdf" target="_blank" rel="noopener">Insertion-based Decoding with automatically Inferred Generation Order</a>, Jiatao Gu , Qi Liu , Kyunghyun Cho. 2019. arXiv: 1902.01370</em></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/73417154" target="_blank" rel="noopener">香侬读 | 按什么套路生成？基于插入和删除的序列生成方法</a>, 香侬科技, 知乎</p></li><li><a href="https://zhuanlan.zhihu.com/p/101185011?utm_source=wechat_session" target="_blank" rel="noopener">论文赏析[TACL19]生成模型还在用自左向右的顺序？这篇论文教你如何自动推测最佳生成顺序</a>, <em>godweiyang</em>, 知乎</li><li><em><a href="https://github.com/kweonwooj/papers/issues/122" target="_blank" rel="noopener">Insertion-based Decoding with Automatically Inferred Generation Order #122</a>, kweonwooj, Github Pages</em></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> NMT </tag>
            
            <tag> insertion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer家族之Non-Monotonic Transformer</title>
      <link href="/2020/04/21/transformer%E5%AE%B6%E6%97%8F-non-monotonic/"/>
      <url>/2020/04/21/transformer%E5%AE%B6%E6%97%8F-non-monotonic/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5396ee05ly1g5pqn3ch6zj20u092znph.jpg" alt></p><p>之前我们介绍的两种 <em>insertion-based</em> 文本生成方法预先规定了每次生成最中间的词，这样一来我们虽然利用树实现了并行，但是却丢失了其中的生成模式，我们不知道模型在生成的时候经历了什么。那么我们能不能让模型自动生成一棵树呢？比如，现在生成了一个根节点，然后再生成左右子节点，然后再生成子节点的子节点，以此类推，但不同的是，这棵树不一定平衡，甚至可能退化成一条链，但我们获得了模型的生成模式，如下图所示：</p><a id="more"></a><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200421163414.png" alt></p><ul><li>可以从任意位置开始生成；</li><li>图中绿框中的数字表示生成顺序；</li><li>图中蓝筐中 的数字表示重构顺序；</li><li>传统的从左向右被做为二叉树的一种特殊情况。</li></ul><h1 id="1-Abstract-Framework"><a href="#1-Abstract-Framework" class="headerlink" title="1. Abstract Framework"></a>1. Abstract Framework</h1><ul><li>$Y=(w_1, …, w_N)$ ——表示生成的序列，其中 $w_i \in V$，是一个词表;</li><li>$\tilde{V} = V \bigcup \{&lt; end &gt;\}$ —— 表示所有可能生成的字符串；</li><li>$S = \tilde{V}^*$ —— 表示序列的状态空间；</li><li>$s \in S$ —— 表示状态空间中的一个序列，该序列中每个元素都来源于 $\tilde{V}$。比如上图 $s_1=(are), s_2=(are, how),…,s_4=(are, how, ?, &lt; end &gt;)$；</li><li>$a$ —— 表示将 $\tilde{V}$ 中的一个元素添加到 $S$ 后面的操作；</li><li>所有的叶子结点都是 $&lt; end &gt;$ 时表示序列生成过程的结束，此时 $T=2N+1$，其中 $N$ 表示序列中非 $&lt; end &gt;$ 的元素， $T$ 表示整个序列的长度， $s_T$ 表示最终状态。</li><li>$\tau(t)$ —— 表示按层序遍历树上的第 $t$ 的节点，则 $(a_{\tau(1)},…,a_{\tau(T)})$；</li><li>以上图为例，最终的序列是将图中蓝筐中的数字映射成绿框的顺序，然后删掉所有的 $&lt; end &gt;$。</li><li>$\pi(a|s)$ —— 表示根据给定一个序列状态生成一个 $a$ 操作的策略。我们知道树的遍历有很多种方式，因此对于同一个序列，我们可以有不同的树，我们就有多种生成模式。</li></ul><h1 id="2-Learning-for-Non-Monotonic-Generation"><a href="#2-Learning-for-Non-Monotonic-Generation" class="headerlink" title="2. Learning for Non-Monotonic Generation"></a>2. Learning for Non-Monotonic Generation</h1><p>我们考虑两种情况下的文本生成：</p><ul><li>非条件生成：类似语言模型，没有额外信息输入；</li><li>条件生成：类似机器翻译，根据输入序列生成新的序列。</li></ul><p>我们先考虑非条件生成的情况。这种情况比较复杂，因为我们只知道最终的序列是什么样的，但是生成这样的序列的那棵树长什么样子我们并不知道。因为我们是通过遍历树的叶子结点得到最终的序列，而树的遍历有很多种方法，要用不同的遍历方法得到同一个序列，此时树的结构就不尽相同。树的结构不同意味着每次生成都会有多种操作的可能性，这样我们就不能使用传统的监督学习方法了。为了解决这个问题我哦们可以使用 <em>learning-to-search</em> 和 <em>imitation learning</em>。</p><blockquote><p><strong>Key  Idea</strong></p><p>假定我们只有一个序列 $Y$。现在的想法是，在第一步我们首先生成任意一个单词 $w \in Y$ 作为整棵树的根节点，然后类似快速排序的思想，在 $w$ 左边和右边递归地生成，由于我们希望树的中序遍历可以得到原始序列 $Y$，所以 $w$ 左边的字符必须在 $w$ 的左子树，同理对右子树。然后用 <em>direct loss minimization</em> 及相关的技术学习一个参考策略 $\pi^{\ast}$ 当成当前策略 $\pi $ 的首选策略。</p></blockquote><h2 id="2-1-Unconditional-Generation"><a href="#2-1-Unconditional-Generation" class="headerlink" title="2.1 Unconditional Generation"></a>2.1 Unconditional Generation</h2><p>所谓 <em>Learning-to-search</em>，就是模仿一个参考策略 $\pi^{\ast}$ 来学习当前策略 $\pi$。我们定义一个输入策略（<em>roll-in</em>）$\pi^{in}$ 和一个输出策略（<em>roll-out</em>）$\pi^{out}$。下面我们不断从 $\pi^{in}$ 中抽样状态 $s$，然后在 $\pi^{out}$ 下对每个行为 $a$ 计算一个运行代价，之后学到的 $\pi $ 被训练去最小化这个运行代价。</p><p>用数学语言来说就是:</p><script type="math/tex; mode=display">\mathbb{E}_{Y \sim D}\mathbb{E}_{t \sim U([1, 2|Y|+1])}\mathbb{E}_{s_t\sim d_{\pi^{in}}^t}\left[C(\pi;\pi^{out}, s_t) \right]</script><ul><li>$U(T)$ 表示 $[1,…,T]$ 的均匀分布；</li><li>$d_\pi^{in}$ 表示在 $\pi$ 策略下进行 $t$ 步得到的状态分布；</li><li>$C(\pi;\pi^{out}, s)$ 表示运行代价</li></ul><p>通过选择不同的 $\pi;\pi^{out}, s$ 我们可以得到不同的 <em>learning-to-search</em> 算法，我们希望找到一个策略能够在获得 $s_t$ 上表现得和 $\pi^*$ 一样好，甚至更好。</p><ul><li><p><strong>$\pi^{in}$ 的选择</strong></p><p>$\pi^{in}$ 决定了我们要学习的策略 $\pi$ 训练的状态分布。我们可以选择 $\pi$ 与 $\pi^{out}$ 的混合作为 $\pi^{in}$，也可以选择仅使用 $\pi^{out}$ 。后者更简单，我们本着从简的原则，使用后者。</p></li><li><p><strong>$\pi^{out}$ 的选择</strong></p><p>$\pi^{out}$ 就是我们要模仿的策略。由于 $\pi^{out}$ 是参考策略，所以我们可以通过 $\pi ^{out}$ 完全根据序列构建出树，于是我们可以把 $\pi^{out} $ 视为树的生成过程，在每一步都对应一个状态 $s_t$ 和序列 $Y_t$ 。在每个 $s_t$ ，$Y_t$ 包含了合法行为，比如下图，在第一步，我们可以选择生成 $(a, b, c, d)$ 中的一个，比如我们选择了 $b$，之后，左节点的选择就只有 $a$ 了，右节点的选择就有 $(c,d)$，这些选择就是“合法”的。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200422192321.png" alt></p><p>给定一个连续子序列 $Y_t=(w_1’, …, w_{N^{‘}}’)$，$\pi^{out}$ 可以定义为：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200422193519.png" alt></p><p>其中 $\sum_{a \in Y}p_a=1$。我们可以看到 $\pi^{out}$ 策略主要由 $p_a$ 决定，这里作者预定义了三种不同的策略：均匀策略（或者任意顺序策略，<em>uniform oracle</em>）、指导策略（<em>coaching oracle</em>）和退火指导策略（<em>annealed coaching oracle</em>）。</p><ul><li><p><strong>均匀策略</strong></p><p>令 $p_a = 1/n$，记为 $\pi^{\ast}_{\mathrm{uniform}}$。</p></li><li><p><strong>指导策略</strong></p><p>任意顺序的策略会导致一个问题：难以使得 $\pi $ 去模仿。为此，我们可以考虑加入当前学习的策略 $\pi  $：</p><script type="math/tex; mode=display">\pi^{\ast}_{\mathrm{coaching}}(a|s) \propto \pi^*_{\mathrm{uniform}}(a|s)\pi(a|s)</script><p>这样一来，既可以避免不合法的行为，也可以按照当前策略 $\pi $ 继续学习。</p></li><li><p><strong>退火指导策略</strong></p><p>指导策略也有一个问题：它不会引导模型进行多样化的学习。因此，我们可以再适当加入$\pi^{\ast}_{\mathrm{uniform}}$：</p><script type="math/tex; mode=display">\pi^*_{\mathrm{annealed}}(a|s) = \beta \pi^{\ast}_{\mathrm{uniform}}(a|s) | (1-\beta) \pi^{\ast}_{\mathrm{coaching}}(a|s)</script><p>这里 $\beta$ 随着训练从1线性递减到0。</p></li></ul></li><li><p><strong>$C$ 的选择</strong></p><p>$C$ 度量的是经过输入策略的选择得到的状态与输出策略之间的误差，最常见的方法是使用平方误差。然而，有研究表明使用 <em>RNN</em> 时平方误差表现不佳，所以我们可以转而使用 <em>KL</em> 散度:</p><script type="math/tex; mode=display">C(\pi, \pi^{out}, s) = D_{KL}(\pi^{out}(\cdot|s)||\pi(\cdot|s)) = \sum_{a \in \tilde{V}}\pi^{out}(a|s)\log \pi(a|s)+C</script></li></ul><h2 id="2-2-Conditional-Generation"><a href="#2-2-Conditional-Generation" class="headerlink" title="2.2 Conditional Generation"></a>2.2 Conditional Generation</h2><p>上面说的是给定单个句子 $Y$ ，如果我们要学习 $X \rightarrow Y$ 怎么办呢？</p><p>我们将条件输入 $X$ 编码成一组 $d_{enc}$ 维的向量，记为 $f^{enc}(X)$，然后经过神经网络，比如 <em>LSTM</em> 或者 <em>Transformer</em>，得到隐状态向量 $H \in \mathbb{R}^{|X| \times d_{enc}}$，然后再送入该模型中即可。</p><h1 id="3-神经网络结构"><a href="#3-神经网络结构" class="headerlink" title="3. 神经网络结构"></a>3. 神经网络结构</h1><p>作者选择使用神经网络实现上面的二叉树生成策略，因为神经网络能有效的对不同尺寸的输入进行编码以及预测输出。这里作者选择两种神经网络：<em>LSTM</em> 和 <em>Transformer</em>，一个是老一代的 <em>NLP</em> 武林盟主，一个是新生代江湖俊杰。</p><h2 id="3-1-LSTM-Policy"><a href="#3-1-LSTM-Policy" class="headerlink" title="3.1 LSTM Policy"></a>3.1 LSTM Policy</h2><p>将二叉树的层序遍历节点 $(a_1, …, a_t)$ 作为序列输入到 <em>LSTM</em> ，然后 <em>LSTM</em> 将序列编码成向量 $h_t$，然后计算 $a_i$ 的概率分布：</p><script type="math/tex; mode=display">\pi(a|s_t) \propto \exp(u_a^Th_t+b_a)</script><h2 id="3-2-Transformer-Policy"><a href="#3-2-Transformer-Policy" class="headerlink" title="3.2 Transformer Policy"></a>3.2 Transformer Policy</h2><p>同样是将 $(a_1, …, a_t)$ 作为输入，然后使用多头注意力计算 $h_t$，再计算 $a_i$ 的概率分布。</p><h2 id="3-3-Auxiliary-lt-end-gt-Prediction"><a href="#3-3-Auxiliary-lt-end-gt-Prediction" class="headerlink" title="3.3 Auxiliary $&lt; end &gt;$ Prediction"></a>3.3 Auxiliary $&lt; end &gt;$ Prediction</h2><p>作者将 $&lt; end &gt;$ 的预测和 $a_i$ 的预测分开进行，先利用伯努利分布判断是否是 $&lt; end &gt;$，设定一个阈值 $\tau$ ，当概率大于 $\tau$ 时，则认为 $a_t = &lt; end &gt;$，否则根据 $\pi$ 计算 $a_t$。</p><h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h1><p>作者最后在多个任务上进行了实验：</p><ol><li>语言模型</li><li>句子补齐</li><li>词序重排</li><li>机器翻译</li></ol><p>这里展示几个生成样例。</p><ul><li>语言模型</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200423101930.png" alt></p><ul><li>机器翻译</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200423110508.png" alt></p><p>从图中我们可以清晰地看到生成模式。更多详细的实验可以去看原文。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/pdf/1902.02192.pdf" target="_blank" rel="noopener">Non-Monotonic Sequential Text Generation</a>, <em>Sean Welleck, Kiante Brantley, Hal Daume III, Kyunghyun Cho. 2019. arXiv: 1902.02192</em></li><li><a href="https://github.com/kweonwooj/papers/issues/121" target="_blank" rel="noopener">Non-Monotonic Sequential Text Generation #121</a>, <em>kweonwooj. Github Pages. 2019</em></li><li><a href="https://zhuanlan.zhihu.com/p/73417154" target="_blank" rel="noopener">香侬读 | 按什么套路生成？基于插入和删除的序列生成方法</a>, 香侬科技，知乎</li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> NMT </tag>
            
            <tag> LSTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer家族之KERMIT</title>
      <link href="/2020/04/16/transformer%E5%AE%B6%E6%97%8F-kermit/"/>
      <url>/2020/04/16/transformer%E5%AE%B6%E6%97%8F-kermit/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5396ee05ly1g5pqn3ch6zj20u092znph.jpg" alt></p><p>我们注意到 <em>Insertion Transformer</em> 提出一种很有意思的文本生成框架：<em>Insertion-based</em> 。但是它仍然使用的是<em>Encoder-Decoder</em> 框架，这种框架有一个缺陷，就是 $(x, y)$ 无法对 联合概率 $p(x, y)$ 进行建模。对此 <em>William Chan</em> 等人于 2019 年提出一种新的架构：<em>KERMIT</em>，该模型抛弃了传统的 <em>Encoder-Decoder</em>  架构，使得我们能对 $p(x, y)$ 联合概率进行建模。训练阶段可以通过句子对 $(x, y)$ 获得联合概率 $p(x, y)$，也可以通过非句子对分别获得边缘概率 $p(x)$ 或者 $p(y)$。推理阶段我们可以获得条件概率 $p(x|y)$ 和 $p(y|x)$。</p><a id="more"></a><h1 id="1-Abstract-Framework"><a href="#1-Abstract-Framework" class="headerlink" title="1. Abstract Framework"></a>1. Abstract Framework</h1><p><em>KERMIT</em> 算是 <em>Insertion Transformer</em> 的泛化版，损失函数用的是平衡二叉树，解码同样可以采用贪心解码和并行解码两种方式。</p><ul><li>序列：$x=(x_1,…,x_n)$;</li><li>生成 $x$ 的顺序：$z \in \mathrm{permut}(\{1,…,n\})$，其中 $\{1,…,n\}$ 对应的是 $x$ 中每个词的绝对位置索引。$z$ 属于位置索引的排列组合，即 $x$ 中元素的生成顺序；</li><li>对应序列：$((c_1^z, l_1^z), …, (c_n^z, l_n^z))$，其中 $c_i^z \in C$ 是词表中的词，$1 \le l_i^z \le i$ 是目前序列的插入相对位置。</li><li>$(x_1^{z, i}, …, x_i^{z, i})$，表示在 $\{z_1, …, z_i\}$ 顺序下 $x$ 的子序列。</li></ul><p><em>KERMIT</em> 对输出的词和词的位置进行建模：</p><script type="math/tex; mode=display">p(c, l|\hat{x}) = \mathrm{KERMIT}(\hat{x})</script><p>举个例子：</p><p>序列 $x = (A, B, C)$ 的生成顺序是 $() \rightarrow (C) \rightarrow (A, C) \rightarrow (A, B, C)$，那么 $z = (3, 2, 1)$，对应序列为 $(c_1^z, l_1^z)=(C, 1), ~(c_2^z, l_2^z)=(A, 1), ~(c_3^z, l_3^z)=(B, 2)$。注意 $z$ 和  $l$ 都表示元素的索引，不同的是 $z$ 表示的是完整序列位置的索引，$l$ 表示的是序列生成过程中，当前序列的位置索引。此时 $(x_1^{z,2}, x_2^{z,2})=(A, C)$。</p><h1 id="2-KERMIT"><a href="#2-KERMIT" class="headerlink" title="2. KERMIT"></a>2. KERMIT</h1><p>有了以上定义，我们就可以得到：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200416175815.png" alt></p><p>最后一步使用了 <em>Markov</em> 假设：插入的顺序不重要，只是一个结果。对于 $p(z)$ 我们使用均匀分布，其他部分使用平衡二叉树（详见<a href="https://rogerspy.gitee.io/2020/04/09/transformer家族-insert/">Transformer家族之Insertion Transformer</a>）。</p><h2 id="2-1-Learning"><a href="#2-1-Learning" class="headerlink" title="2.1 Learning"></a>2.1 Learning</h2><p>上式要直接求解比较麻烦，但是可以用 <em>Jensen</em> 不等式得到 $p(x)$ 的下限：</p><script type="math/tex; mode=display">\log p(x) = \log \sum_{z \in S_n} p(z) p(x|z) \ge \sum_{z \in S_n} p(z) \log p(x|z) =: L(x)</script><p>带入刚刚得到的 $p(x|z)$ 表达式：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200416185623.png" alt></p><p>下面将 $L(x)$ 分成是三部分：$(z_1,…,z_{i-1})$ 表示之前的插入，$z_i$ 表示下一步插入，$(z_{i+1}, …,z_n)$ 表示以后的插入：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200416190330.png" alt></p><p>最后一步是由于 $\sum_{z_{i+1:n}}p(z_{i+1:n}|z_{1:i})=1$。</p><p>通过下面几个简单的采样过程就可以计算 $L(x)$：</p><ol><li>采样生成步骤 $i \sim \mathrm{Uniform([1, n])}$;</li><li>采样前 $i-1$ 次插入操作的排列组合 $z_{1:i-1} \sim p(z_{1:i-1})$;</li><li>计算 $\log p\left((c_i^z, l_i^z)|x_{1:i-1}^{z,i-1}\right)$，$p(z_i|z_{1:i-1})$，二者相乘然后乘以 $n$。</li></ol><p>前两步不多说，说下最后一步。</p><blockquote><p>$i=1$，则 $\log p\left((c_1^z, l_1^z)|x_{1:0}^{z,0}\right)=\log p\left((C, 1)|(&lt; BOS &gt;)\right)$，$p(z_1|z_{1:0})=p(3|&lt; BOS &gt;)$;</p><p>$i=2$，则 $\log p\left((c_2^z, l_2^z)|x_{1:1}^{z,1}\right)=\log p\left((A, 1)|(&lt; BOS &gt;, C)\right)$，$p(z_2|z_{1:1})=p(1|(&lt; BOS &gt;, 3))$;</p><p>$i=3$，则 $\log p\left((c_3^z, l_3^z)|x_{1:2}^{z,2}\right)=\log p\left( (B, 2)|(&lt; BOS &gt;, A, C)\right)$，$p(z_3|z_{1:2})=p(2|(&lt; BOS &gt;, 3, 1))$;</p></blockquote><h2 id="2-2-Inference"><a href="#2-2-Inference" class="headerlink" title="2.2 Inference"></a>2.2 Inference</h2><ul><li><em>Greedy decoding</em></li></ul><script type="math/tex; mode=display">(\hat{c}_t, \hat{l}_t) = \arg \max_{c, l} p(c, l|\hat{x})</script><ul><li><em>Parallel decoding</em></li></ul><script type="math/tex; mode=display">\hat{c}_{l} = \arg \max_{c} p(c | l,\hat{x}_t)</script><h2 id="2-3-Pairs-of-Sequences"><a href="#2-3-Pairs-of-Sequences" class="headerlink" title="2.3 Pairs of Sequences"></a>2.3 Pairs of Sequences</h2><p>目前为止我们讨论的都是单个序列。我们可以通过将两个序列拼接在一起实现对 $(x, y)$ 的直接建模，即：</p><script type="math/tex; mode=display">(x, y) = (x_1, ...,x_n, y_1, ..., y_m)</script><p>比如，$x=(A,B,C,&lt; EOS &gt;),~y=(A’,B’,C’,D’,E’,&lt; EOS &gt;)$，拼接后成为 $(x, y)=(A,B,C,&lt; EOS &gt;, A’,B’,C’,D’,E’,&lt; EOS &gt;)$。相对于 <em>Encoder-Decoder</em> 结构，这样的好处是，$x, y$可以互为源序列和目标序列。</p><p><strong>对于多模态数据，这种结构可能会成为未来的趋势。</strong></p><p>通过这种结构我们可以很轻易的对 $p(x, y)$ 联合概率进行建模，同时还能获得边缘概率 $p(x),p(y)$ 以及条件概率 $p(x|y), p(y|x)$。我们还可以进行针对性的训练：</p><ul><li>如果给出完整的 $x$ 或 $y$ （没有拼接成一个序列），则可以训练条件概率；</li><li>如果 $x$ 或 $y$ 有一个为空，则训练边缘概率。</li></ul><h2 id="2-4-Model"><a href="#2-4-Model" class="headerlink" title="2.4 Model"></a>2.4 Model</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200416200010.png" alt></p><p><em>KERMIT</em> 的大致结构如图所示，采用 <em>Transformer</em> 的<em>Decoder</em> 部分，并且去掉了掩码注意力层。由于没有 <em>Encoder</em> ，所以注意力层是完全的自注意力层，不需要和 <em>Encoder</em> 进行 <em>cross-attention</em>。</p><p>另外，<em>KERMIT</em> 的损失函数用的是平衡二叉树，最后计算 $p(c, l)$ 的时候用的是 <em>Insertion Transformer</em> 中的因式分解方法。</p><p>所以我们可以认为 <em>KERMIT</em> 是 <em>Insertion Transformer</em> 的泛化版，很多后者不具备的能力都可以在它身上找到。尤其是在翻译领域，这种对称式的多模态训练很可能会成为未来的趋势。</p><p>比如：输入源序列 $(x, y)$ 分别是 $(en，zh)$，输出目标序列是 $(zh,en)$。这样我们相当于在一个模型上实现了两种语言的互相翻译。</p><h1 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3. Experiments"></a>3. Experiments</h1><ul><li><em>Unidirectional</em> 表示和传统方法一样，输入单一序列，输出单一序列；</li><li><em>Bidirectional</em> 表示输入单一序列，输出单一序列，但是在同一个模型中训练两种语言；</li><li><em>Join</em> 表示两种序列拼接在一起输入模型，另外使用 $p(x)，p(y)$ 进行改善，输出同样也是单一序列和两种序列。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200416195840.png" alt></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><em><a href="https://arxiv.org/pdf/1906.01604.pdf" target="_blank" rel="noopener">KERMIT: Generative Insertion-Based Modeling for Sequences</a>, William Chan , Nikita Kitaev, Kelvin Guu , Mitchell Stern , Jakob Uszkoreit, 2019, arXiv: 1906.01604</em></li><li><a href="https://zhuanlan.zhihu.com/p/73417154" target="_blank" rel="noopener">香侬读 | 按什么套路生成？基于插入和删除的序列生成方法</a>，香侬科技，知乎</li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> NMT </tag>
            
            <tag> insertion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer家族之Insertion Transformer</title>
      <link href="/2020/04/09/transformer%E5%AE%B6%E6%97%8F-insert/"/>
      <url>/2020/04/09/transformer%E5%AE%B6%E6%97%8F-insert/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5396ee05ly1g5pqn3ch6zj20u092znph.jpg" alt></p><p>传统的文本生成，比如机器翻译无论是自回归或者半自回归的推理方式，都有一个特点：通常是自左向右依次生成文本序列。本文将介绍一篇文章，打破思维定式，突破自左向右的顺序生成。<em>Insertion Transformer</em>采用随机插入式序列生成：</p><ul><li>以任意顺序生成；</li><li>支持自回归或者半自回归生成（同时在不同位置插入）。</li></ul><p><em>Insertion Transformer</em>不仅在效果上远超非自回归模型，而且能以$log(n)$的推理速度，效果上达到原始<em>Transformer</em>的水平。</p><a id="more"></a><h1 id="1-Abstract-Framework"><a href="#1-Abstract-Framework" class="headerlink" title="1. Abstract Framework"></a>1. Abstract Framework</h1><ul><li>$x$ —— 源序列</li><li>$y$ —— 目标序列</li><li>$\hat{y}_t$ —— $t$ 时刻的输出序列，由于我们只有插入操作，因此 $\hat{y}_t$ 必须是最终输出序列的子序列，比如如果最终输出序列是$[A, B, C, D, E, F]$，那么$\hat{y}_t=[B, C]$ 是合法的，而$[C, B]$则不可以</li><li>$\hat{y}$ —— 最终输出序列</li><li>$C$ —— 词表</li><li>$c$ —— $c \in C$，上下文</li><li>$l$ —— $l \in [0, |\hat{y}_t|]$，需要插入词的位置</li></ul><p><em>Insertion Transformer</em> 需要对输出的词和词的位置都要进行建模：</p><script type="math/tex; mode=display">p(c, l|x, \hat{y}_t) = \mathrm{InsertionTransformer}(x, \hat{y}_t)</script><p>举个例子：</p><p>假设$\hat{y}_t = [B, D]$，$p(c, l| x, \hat{y}_t) = (c=C, l=1)$，也就是说把 $C$ 插入到 $\hat{y}_t$的 索引值为1的地方（从0开始计算索引），这就意味着$\hat{y}_{t+1} = [B, C, D]$。</p><h1 id="2-Insertion-Transformer"><a href="#2-Insertion-Transformer" class="headerlink" title="2. Insertion Transformer"></a>2. Insertion Transformer</h1><ul><li><strong>Full Decoder Self-Attention</strong></li></ul><p>将 <em>Transformer</em> 中的 <em>causal self-attention</em> 替换为 <em>full self-attention</em>。</p><ul><li><strong>Slot Representation via Concatenated Outputs</strong></li></ul><p>标准的 <em>Transformer</em> 是给输入 $n$ 个词，然后模型输出 $n$ 个向量，然后取最后一个词作为输出序列的下一个词。而为了实现插入操作，作者对 <em>decoder</em> 做了修改。因为插入需要的是在词与词之间插入，也就是说，<em>decoder</em> 需要对<em>slot</em> 进行编码，$n$ 个词就有 $n+1$ 个 <em>slot</em>，也就是说给定 $n$ 个词，<em>decoder</em> 输出必须是 $n+1$ 个向量：$n$ 个词之间有 $n-1$ 个<em>slot</em>，而序列开始和结束的地方还有两个 <em>slot</em>，一共 $n+1$ 个<em>slot</em>。</p><p>比如：$[A, C, E]$  一共有3个词，$A, C$和$C, E$之间两个<em>slot</em>，模型下一个预测的词也有可能需要插入到$A$之前，或者 $B$ 之后，所以 $A$ 前和 $B$ 后还有两个<em>slot</em>，3个词4个 <em>slot</em>。</p><p>为了实现这一目标，我们在 <em>decoder</em> 的输入序列上加入特殊的开始和结束符，比如<code>&lt;BOS&gt;</code>或者<code>&lt;EOS&gt;</code>。这样我们就构造了一个 $n+2$ 的输入序列，最终 <em>decoder</em> 也将输出 $n+2$ 个向量。然后我们再将相邻的两个向量拼接在一起，这样就形成了 $n+1$ 个向量了。每个向量代表一个 <em>slot</em>。因此，每个 <em>slot</em> 相当于综合了该位置前后两个词的语义。</p><p>举个例子：</p><ol><li>给定 $[A, C, E]$；</li><li>加入 <code>&lt;BOS&gt;</code> 和 <code>&lt;EOS&gt;</code>，成为 $[&lt; BOS &gt;, A, C, E, &lt; EOS &gt;]$；</li><li>输入给<em>decoder</em>；</li><li><em>decoder</em> 输出 $[[1,2,3], [2,3,4], [3,4,5], [4,5,6], [5,6,7]]$；</li><li>将相邻的两个向量拼接起来：$[[1,2,3,2,3,4],[2,3,4,3,4,5],[3,4,5,4,5,6],[4,5,6,5,6,7]]$;</li></ol><h2 id="2-1-Model-Variants"><a href="#2-1-Model-Variants" class="headerlink" title="2.1 Model Variants"></a>2.1 Model Variants</h2><p>得到了 <em>slot</em> 向量，怎样得到最终的 $p(c, l)$，作者对此也进行了一番探索。</p><p>在继续介绍之前，我们先定义一些变量：</p><ul><li>$H$ —— $H \in \mathbb{R}^{(T+1) \times d}$，表示上面我们得到的 <em>slot</em> 特征矩阵， 其中 $d$ 是向量维度，$T$ 是输入序列长度；</li><li>$W$ —— $W \in \mathbb{R}^{d \times |C|}$，是标准的 <em>softmax</em> 投影矩阵，其中 $|C|$ 表示词表大小。</li></ul><p>好了，我们继续下面的介绍。</p><ul><li><strong>Content-Location Distribution</strong></li></ul><p>为了得到 $p(c, l|x, \hat{y}_t)$，作者提出两种方法：第一种是直接对 $(\mathrm{content}, \mathrm{slot})$进行建模；另一种方式是因式分解。</p><ol><li>直接建模</li></ol><script type="math/tex; mode=display">p(c, l) = \mathrm{softmax}(\mathrm{flatten}(HW))</script><p>注意这里得到的是一个$(T+1) \times |C|$ 的矩阵，其物理意义为：在$(T+1)$ 个<em>slot</em> 的每个位置上最可能出现的词。</p><ol><li>因式分解</li></ol><p>我们可以利用条件因式分解得到：</p><script type="math/tex; mode=display">p(c, l) = p(c|l) \cdot p(l) \\\\p(c|l) = \mathrm{softmax}(h_l \cdot W) \\\\p(l) = \mathrm{softmax}(H \cdot q)</script><p>其中 $h_l \in \mathbb{R}^{d}$ 表示 $H$ 的第 $l$ 行，$q \in \mathbb{R}^{d}$ 表示一个可学习的 <em>query</em> 向量。</p><p>这种方法的好处是相对直接建模的方式内存消耗要小得多。</p><ul><li><strong>Contextualized Vocabulary Bias</strong></li></ul><p>为了增加 <em>slot</em> 之间的信息共享，我哦们可以在 $H$ 上增加一个最大池化的操作，得到一个上下文向量 $g \in \mathbb{R}^d$，然后用一个可学习的投影矩阵  $V \in \mathbb{R}^{d \times |C|}$ 将 $g$ 投影到词表空间: $b=g \cdot V \in \mathbb{R}^{|C|}$，将 $b$ 作为偏置量添加到每个位置上。整个过程如下：</p><script type="math/tex; mode=display">g = \mathrm{maxpool}(H) \\\\b = g \cdot V \\\\B = \mathrm{repmat}(b, [T+1, 1]) \\\\p(c, l) = \mathrm{softmax}(HW+B)</script><ul><li><strong>Mixtrue-of-Softmaxes Output Layer (Optional)</strong></li></ul><p>讲道理，这部分我没有太想明白为什么需要 <em>MoS</em>。如果是从语言模型本身的复杂性来考虑，<em>MoS</em> 确实有可能会起到效果， 但是如果从同时编码 $(c, l)$ 的角度考虑的话，我就不太明白了。从文章的实验结果来看，这部分修改并没有使模型达到最佳效果（当然如果只考虑以上三种变种的话，加上 <em>MoS</em> 确实有提升效果，但是加上后续的一些 <em>tricks</em> 综合整个模型试验，它并没有达到最佳效果）。因此，这里对 <em>MoS</em> 做简单介绍吧，更详细的内容请去读<a href="https://arxiv.org/pdf/1711.03953.pdf" target="_blank" rel="noopener">原文</a>。</p><p>假设我们有一个上下文矩阵 $H \in \mathbb{R}^{N \times d}$，一个词向量矩阵 $W \in \mathbb{R}^{|C| \times d}$，其中 $|C|$ 是词表大小。给定上下文，我们希望从矩阵 $A$ 中去预测下一个词 $p(x|c)$，其中 $A \in \mathbb{R}^{N \times |C|}$。我们的模型是希望通过从真实的世界中采样 $N$ 个样本，训练出尽可能接近 $A$ 分布的 $A’$ 矩阵。</p><script type="math/tex; mode=display">H = \left[\begin{array}{c}\mathbf{h}_{c1}^T\\\\\vdots \\\\\mathbf{h}_{cN}^T\end{array}\right];W = \left[\begin{array}{c}\mathbf{w}_{x1}^T\\\\\vdots \\\\\mathbf{w}_{xM}^T\end{array}\right];A = \left[\begin{array}{ccc}\log p(x_1|c_1), & \cdots, & \log p(x_M|c_1)\\\\\vdots , & \ddots, & \vdots\\\\\log p(x_1|c_N), & \cdots, & \log p(x_M|c_N),\end{array}\right]</script><p>我们希望 $A’ = HW^T$，尽量接近 $A$ 的分布。</p><p>矩阵乘法有一个性质：两矩阵乘积的秩（<em>rank</em>）等于两个矩阵秩较小的那个：</p><script type="math/tex; mode=display">\mathrm{rank}(A \cdot B) = \min(\mathrm{rank}(A), \mathrm{rank}(B))</script><p>通常我们的训练样本 $N$ 会远远大于词向量的维度，也就是说 $A’$ 的秩的上限由词向量维度 $h$ 决定的。而真实世界的 $A$ 的秩很可能接近 $|C|$，通常 $|C| \gg d$。这就造成一个问题，我们希望通过 $A’$ 来逼近 $A$ 的分布，但是实际上$A’$ 的秩是远低于 $A$ 的，也就是说我们的模型训练出来的矩阵表达能力是远低于真实世界的。</p><p><strong>简单一句话就是：由于词向量维度的限制，我们训练出来的模型表达能力不足以完整的描述真实的世界。</strong></p><p>要解决这个问题的最直接的方法就是让词向量的维度等于词表的大小，即 $d = |C|$。但这会引入另外一个问题，那就是<strong>参数量爆炸！</strong>。</p><p>因此，作者提出 <em>Mixture of Softmax</em>:</p><script type="math/tex; mode=display">[\mathbf{h}_0, ..., \mathbf{h}_K] = \tanh(\mathbf{h}P^T)</script><script type="math/tex; mode=display">[\pi_0, ..., \pi_K] = \mathrm{softmax}(\mathbf{h}M^T)</script><script type="math/tex; mode=display">P_{\theta}(x|\mathbf{h}) = \sum_{k=1}^{K}\pi_{c, k} \frac{\exp(\mathbf{h}_{c, k}^T \mathbf{w}_x)}{\sum_{x' \exp(\mathbf{h}_{c, k}^T \mathbf{w}_{x'})}}</script><p>其中 $\sum_{k=1}^K\pi_{c, k}=1$。</p><p>这里引入了两个权重：</p><ul><li>$P \in \mathbb{R}^{Kd\times d}$，它将 $\mathbf{h}$ 映射成 $K$ 个不同的 $\mathbf{h}$，每个 $\mathbf{h_k} \in \mathbb{R}^d$；</li><li>$M \in \mathbb{R}^{d \times K}$，决定这 $K$ 个模型如何混合。</li></ul><p>这个模型的基本原理就相当于使用 $K$ 个 <em>softmax</em> 将它们混合起来，既避免了参数量爆炸，又保持了模型的表达能力。这 $K$ 个不同的模型组合求加权平均效果往往比单一的模型要好， $K$ 个模型联合训练也能避免一些单模型的缺陷。</p><h1 id="3-Training-and-Loss-Functions"><a href="#3-Training-and-Loss-Functions" class="headerlink" title="3. Training and Loss Functions"></a>3. Training and Loss Functions</h1><p><em>Insertion Transformer</em>的机制支持它以任何顺序来生成目标语句，因此在训练时，可以通过设计<em>loss function</em>来将先验的顺序信息施加给模型，从而使得模型在预测时也按照先验的顺序进行预测。</p><h2 id="3-1-Left-to-Right"><a href="#3-1-Left-to-Right" class="headerlink" title="3.1 Left-to-Right"></a>3.1 Left-to-Right</h2><p>文中将原始自回归模型从左到右的生成方式作为一个特例进行对比。固定每次插入的单词位置都在最右侧，就退化成了原始的序列生成方式。</p><script type="math/tex; mode=display">\mathrm{loss}(x, \hat{y}) = - \log p(y_{k+1}, k | x, \hat{y})</script><h2 id="3-2-Balanced-Binary-Tree"><a href="#3-2-Balanced-Binary-Tree" class="headerlink" title="3.2 Balanced Binary Tree"></a>3.2 Balanced Binary Tree</h2><p>显然从左到右的解码方式不能做到并行。因此，作者提出使用平衡二叉树的方式最大化并行解码能力。基本思想是：每次生成目标序列最中间的词。</p><p>比如，目标序列是 $[A, B, C, D, E, F, G]$，解码的顺序是：</p><ol><li>$[]$；</li><li>$[D]$；</li><li>$[B, D, F]$；</li><li>$[A, B, C, D, E, F, G]$。</li></ol><p>为了达到这个目的，作者提出 <em>soft binary tree loss</em>：</p><ol><li><p>随机挑选一个 $k$ 值，$k \sim \mathrm{uniform}(0, |y|)$；</p></li><li><p>打乱 $y$ 中每个词的索引顺序，选择其中前 $k$ 个索引对应的词，这样就得到了一个长度为 $k$ 的子序列；</p></li><li><p>这个长度为 $k$ 的子序列包含 $k+1$ 个<em>slot</em>，每个 <em>slot</em> 对应的位置为 $l = 0, 1, …, k$。令 $(y_{i_l}, y_{i_l+1}, …, y_{j_l})$为 $ y$ 中剩余的对应第 $l$ 个 <em>slot</em> 位置上的词。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200410151034.png" alt></p><blockquote><p>以上图为例：解释$(y_{i_l}, y_{i_l+1}, …, y_{j_l})$是什么？$[A, C, D, I, M]$ 是随机挑选出来的子序列，</p><ul><li>$l=1$ 对应的 $(y_{i_1}, y_{i_1+1}, …, y_{j_1})=[B]$；</li><li>$l=3$ 对应的 $(y_{i_3}, y_{i_3+1}, …, y_{j_3})=[E, F, G, H]$；</li><li>$l=4$ 对应的 $(y_{i_4}, y_{i_4+1}, …, y_{j_4})=[J, K, L]$；</li><li>$l=5$ 对应的 $(y_{i_5}, y_{i_5+1}, …, y_{j_5})=[N, O]$；</li></ul><p>对于$l=0, 2$ 的情况在后面讨论。</p><p>注意 $i_l, j_l$ 分别是该词在原始 $y$ 中的索引。</p></blockquote><p>得到了 $(y_{i_l}, y_{i_l+1}, …, y_{j_l})$ 列表以后，我们去计算每个 <em>slot</em> 与其对应的$(y_{i_l}, y_{i_l+1}, …, y_{j_l})$ 中每个词的距离：</p><script type="math/tex; mode=display">d_l(i) = \left|\frac{i_l+j_l}{2}-i\right|</script><blockquote><ul><li><p>$l=1$ : $d_1 = \left|\frac{1+1}{2}-i|_{i=1}\right| = [0]$；</p></li><li><p>$l=3$ : $d_3 = \left|\frac{4+7}{2}-i|_{i=4,5,6,7}\right| = [1.5, 0.5, 0.5, 1.5]$；</p></li><li>$l=4$ : $d_4 = \left|\frac{9+11}{2}-i|_{i=9,10,11}\right| = [1,0,1]$；</li><li>$l=5$ : $d_5 = \left|\frac{13+14}{2}-i|_{i=13,14}\right| = [0.5, 0.5]$；</li></ul></blockquote><p>然后我们根据上面计算出来的距离，给每个距离一个权重：</p><script type="math/tex; mode=display">w_l(i) = \frac{\exp(-d_l(i)/\tau)}{\sum_{i^{'}}^{j_l}\exp(-d_l(i^{'})/\tau)}</script></li><li><p>有了上面的权重，我们就可以定义每个位置上的 <em>slot loss</em> 了：</p><script type="math/tex; mode=display">\mathrm{SlotLoss}(x, \hat{y}, l)=\sum_{i=i_l}^{j_l} -\log p(y_i, l|x, \hat{y}) \cdot w_l(i)</script></li><li><p>最后总的 <em>loss</em> 为：</p><script type="math/tex; mode=display">\mathrm{loss} = \frac{1}{k+1}\sum_{l=0}^k \mathrm{SlotLoss}(x, \hat{y}, l)</script></li></ol><p><em>SlotLoss</em> 中的 $\tau$ 是一个温度超参数，用来控制 $w_l$ 的平滑程度的：当 $\tau \rightarrow 0$ 时，将会给最中间的位置以非常高的权重，两侧的位置权重几乎为 0；当 $\tau \rightarrow \infty$ 时，每个位置的权重基本相等。</p><h2 id="3-3-Uniform"><a href="#3-3-Uniform" class="headerlink" title="3.3 Uniform"></a>3.3 Uniform</h2><p>作者也做了不给模型施加约束，让模型自己探索生成方式的尝试，即鼓励模型uniform地生成每个slot中的各个单词。实现的方式很简单，将$\tau \rightarrow \infty$ 即可。</p><h2 id="3-4-Termination-Condition"><a href="#3-4-Termination-Condition" class="headerlink" title="3.4 Termination Condition"></a>3.4 Termination Condition</h2><p>选取解码时的停止条件也是一个很关键的问题。<em>Insertion Transformer</em> 提出了两种停止条件:</p><ul><li><p><em>Slot Finalization</em> 在训练时引入了 <em>end-of-slot token</em>，来和 <em>label</em> 为空的 <em>slot</em> 计算损失函数。在推理时，当且仅当<strong>全部</strong> <em>slot</em> 的预测都为 <em>end-of-slot</em> 时，停止继续解码。</p><blockquote><p>上面我们在介绍 <em>soft binary tree loss</em> 的时候遗留了一个问题：$l=0, 2$ 时怎么办？我们看到当 $l=0, 2$ 时我们不需要插入任何词，这时候我们定义一个 <em>end-of-slot token</em>，这个时候 $l=0, 2$ 对应的 $(y_{i_1}, y_{i_1+1}, …, y_{j_1})=[\mathrm{EndOfSlot}]$ ，然后我们用 $\mathrm{EndOfSlot}$ 计算损失。推理的时候，当所有的 <em>slot</em> 对应的 $y_{i_l}$ 都是  $\mathrm{EndOfSlot}$ 时停止解码。</p></blockquote></li><li><p><em>Sequence Finalization</em> 则还是用的传统的 <em>end-of-sequence token</em>，在全部单词都生成之后，将每个 <em>slot</em> 都和 <em>end-of-sequence token</em> 计算损失函数。在推理时，当<strong>任意</strong>一个 <em>slot</em> 的预测结果是 <em>end-of-sequence token</em> 时，停止解码。</p><blockquote><p>回到上面那个 $l=0, 2$ 的问题，在上面使用 <em>Slot Finalization</em> 时，我们令 $(y_{i_1}, y_{i_1+1}, …, y_{j_1})=[\mathrm{EndOfSlot}]$ 然后计算相关损失。当使用 <em>Sequence Finalization</em> 时，遇到 $l=0, 2$ 这种情况的时候，我们认为这里需要插入空字符串，跳过这两个位置的损失计算。直到所有的 <em>slot</em> 都不再产生损失（也就是所有 <em>slot</em> 都要插入空字符串）的时候，我们让每个 <em>slot</em> 都和 <em>end-of-sequence token</em> 计算损失。推理的时候，任意一个 <em>slot</em> 的预测是 <em>end-of-sequence token</em> 则停止解码。</p></blockquote></li></ul><h1 id="4-Inference"><a href="#4-Inference" class="headerlink" title="4. Inference"></a>4. Inference</h1><h2 id="4-1-Greedy-Decoding"><a href="#4-1-Greedy-Decoding" class="headerlink" title="4.1 Greedy Decoding"></a>4.1 Greedy Decoding</h2><p><em>Greedy decoding</em> 支持 <em>Slot Finalization</em> 和 <em>Sequence Finalization</em> 两种终止条件的训练模式。推理的时候选择概率最高的词和对应的位置：</p><script type="math/tex; mode=display">(\hat{c}_t, \hat{l}_t) = \arg \max p(c, l|x, \hat{y})</script><p>然后再 $\hat{l}_t$ 位置上插入 $\hat{c}_t$。</p><h2 id="4-2-Parallel-Decoding"><a href="#4-2-Parallel-Decoding" class="headerlink" title="4.2 Parallel Decoding"></a>4.2 Parallel Decoding</h2><p>采用 <em>Slot Finalization</em> 方式支持并行训练和解码。具体来说，就是对于每个 <em>slot</em> 计算最高概率的词：</p><script type="math/tex; mode=display">\hat{c}_{l, t} = \arg \max p(c | l,x,\hat{y}_t)</script><p>这样相当于每次推理我们能填满所有 <em>slot</em>，理论上，只需要 $\log_2(n)+1$ 步就可以生成一个长度为 $n$ 的序列。</p><h1 id="5-Tricks"><a href="#5-Tricks" class="headerlink" title="5. Tricks"></a>5. Tricks</h1><p>作者对模型做了非常充分的实验，也做了很多有意思的讨论。这里总结一些比较有用的小 <em>tricks</em>：</p><ul><li>由于两种终止条件在训练的时候引入了过多的 $&lt; EOS &gt;$, 会导致模型在推理时很容易生成 $&lt; EOS &gt;$造成早停的问题。因此文章引入了 $&lt; EOS &gt;$  惩罚来在推理时对   $&lt; EOS &gt;$ 人为地增加预测难度：仅当   $&lt; EOS &gt;$ 的概率大于第二可能单词的概率一个阈值 $\beta$ 的时候才会真正生成 $&lt; EOS &gt;$。从实验中看来，这是一个很重要的 <em>trick</em>。</li><li>使用<em>base Transformer</em> 作为模型的 <em>teacher model</em>，来做知识蒸馏也会提升模型的能力。</li></ul><h1 id="6-Experiments"><a href="#6-Experiments" class="headerlink" title="6. Experiments"></a>6. Experiments</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200410172118.png" alt></p><p>这个实验是分析 2.1 节讨论的模型的不同变种的效果。从结果上看，不加 $&lt; EOS &gt;$ 惩罚的时候，<code>Contextual + Mixture</code> 能达到最佳效果，但是加上惩罚之后这种效果提升就消失了。说明模型的核心结构在适当的调整下已经足够强大有效，无需做太大的调整。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200410172252.png" alt></p><p>这个实验室采用并行解码的方式进行推理。我们发现之前介绍的平衡二叉树损失是非常有效的，另外$&lt; EOS &gt;$ 惩罚带来的提升已经不太明显了。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200410172333.png" alt></p><p>最后，与其他模型相比较，<em>Insertion Transformer</em> 不仅在效率上提升巨大，而且在效果上也达到了与自回归模型相同的水准。这是一个非常令人兴奋的结果。</p><p>最后，下面这张图展示了 <em>Insertion Transformer</em> 的一个实际推理的例子。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200410172424.png" alt></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/pdf/1902.03249.pdf" target="_blank" rel="noopener">Insertion Transformer: Flexible Sequence Generation via Insertion Operations</a>, <em>Mitchell Stern, William Chan,  Jamie Kiros, Jakob Uszkoreit,  2019, arXiv: 1902.03249</em></li><li><a href="https://arxiv.org/pdf/1711.03953.pdf" target="_blank" rel="noopener">Breaking The Softmax Bottleneck: a High-Rank RNN Language Model</a>, <em>Zhilin Yang , Zihang Dai, Ruslan Salakhutdinov, William W. Cohen, 2018, arXiv: 1711.03953</em></li><li><a href="https://github.com/kweonwooj/papers/issues/123" target="_blank" rel="noopener">Insertion Transformer: Flexible Sequence Generation via Insertion Operations #123</a>, <em>kweonwooj, Github Pages</em></li><li><a href="https://zhuanlan.zhihu.com/p/89209220" target="_blank" rel="noopener">Non-Autoregressive NMT: Insertion Transformer</a>, <em>Leo Guo</em>, 知乎</li><li><a href="https://zhuanlan.zhihu.com/p/73417154" target="_blank" rel="noopener">香侬读 | 按什么套路生成？基于插入和删除的序列生成方法</a>, 香侬科技, 知乎</li><li><a href="https://smerity.com/articles/2017/mixture_of_softmaxes.html" target="_blank" rel="noopener">Understanding the Mixture of Softmaxes (MoS)</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> NMT </tag>
            
            <tag> insertion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer家族之Sparse Transformer</title>
      <link href="/2020/03/30/transformer%E5%AE%B6%E6%97%8F-sparse/"/>
      <url>/2020/03/30/transformer%E5%AE%B6%E6%97%8F-sparse/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5396ee05ly1g5pqn3ch6zj20u092znph.jpg" alt></p><p>目前来看，自注意力机制有一统NLP的趋势，其凭借能够捕捉序列中任意两个元素的关联信息，且易于并行等优势，在与传统的NLP武林盟主<em>RNN</em>的较量中，几乎是全方位碾压。但是它也并不是没有弱点，之前我们介绍过在机器翻译过程中，它的推理过程是<em>auto-regression</em>的，严重制约了它的推理效率。因此，很多研究人员对它做了一定程度上的改善。今天我们继续来对它进行其他方面的优化，也就是变形金刚家族的另一成员 —— <em>Sparse Transformer</em>。</p><a id="more"></a><p>在介绍 <em>Sparse Transformer</em> 之前我们要先思考一个问题：我们为什么要对它进行稀疏化改进？稀疏注意力能解决现有的什么问题？</p><h1 id="1-Why-you-need-Sparsity"><a href="#1-Why-you-need-Sparsity" class="headerlink" title="1. Why you need Sparsity?"></a>1. Why you need Sparsity?</h1><h2 id="1-1-计算复杂度"><a href="#1-1-计算复杂度" class="headerlink" title="1.1 计算复杂度"></a>1.1 计算复杂度</h2><p>从理论上来讲，<em>Self Attention</em>的计算时间和显存占用量都是$O(n^2)$级别的（$n$是序列长度），这就意味着如果序列长度变成原来的2倍，显存占用量就是原来的4倍，计算时间也是原来的4倍。现在，AI 研究中的一项挑战是对长序列的精细相关性建模，比如图像、声音等。如果我们在每一层都构建一个$n \times n$的注意力矩阵的话会消耗大量的内存。例如：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200330162729.png" alt></p><p>而目前用于深度学习的标准GPU显存是12-32G。因此全自注意力（<em>full self-attention</em>）严重制约了模型的编码长度。</p><h2 id="1-2-注意力集中问题"><a href="#1-2-注意力集中问题" class="headerlink" title="1.2 注意力集中问题"></a>1.2 注意力集中问题</h2><p>理解自然语言需要注意最相关的信息。例如，在阅读过程中，人们倾向于把注意力集中在最相关的部分来寻找他们心中问题的答案。然而，如果不相关的片段对阅读理解产生负面影响，就会出现检索问题。这种分心会阻碍理解过程，而理解过程需要有效的注意力。比如：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200330164544.png" alt></p><p>与<em>tim</em>相关度最高的是<em>heart</em>及周围的几个词，而传统的<em>transformer</em>也给了其他不相关的词很高的权重，这样造成了注意力的分散。<em>Sparse Transformer</em>可以将注意力集中在几个最重要的元素上，避免或者缓解这一问题。</p><h1 id="2-Sparse-Transformer"><a href="#2-Sparse-Transformer" class="headerlink" title="2. Sparse Transformer"></a>2. Sparse Transformer</h1><p>这里我们呢主要介绍四种<em>Sparse Transformer</em> :</p><ul><li><a href="https://arxiv.org/pdf/1904.10509.pdf" target="_blank" rel="noopener">Sparse Transformers</a></li><li><a href="https://arxiv.org/abs/1905.07799" target="_blank" rel="noopener">Adaptive Span Transformers</a></li><li><a href="https://arxiv.org/pdf/1909.00015.pdf" target="_blank" rel="noopener">Adaptively Sparse Transformers</a></li><li><a href="https://openreview.net/pdf?id=Hye87grYDH" target="_blank" rel="noopener">Explicit Sparse Transformer</a></li></ul><h2 id="2-1-注意力模式"><a href="#2-1-注意力模式" class="headerlink" title="2.1 注意力模式"></a>2.1 注意力模式</h2><p>既然要将注意力稀疏化，那么如何稀疏就是个需要思考的问题。为了更好的处理这个问题，<em>Child</em>等人在图像上探究了<em>Transformer</em> 的注意力模式发现其中许多模式表现出了可解释和结构化的稀疏模式。以下每幅图像都显示了哪个输入像素（白色高亮标出）由一个给定的注意力头处理，以预测图像中的下一个值。当输入部分集中在小的子集上并显示出高度规律性时，该层就易于稀疏化。以下是 CIFAR-10 图像上 128 层模型的样本：</p><table><tr><td><center class="half"><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/gif2_rowcol_lay19-head0.gif" style="zoom:250%;"></center></td><td><center><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/gif2_rowcol_lay20-head1.gif" style="zoom:250%;"></center></td></tr></table><p>左图是<em>Layer 19</em>的注意力模式（白色高亮），右图是<em>Layer 20</em>的注意力模式。可以看到<em>Layer 19</em>集中了当前行的注意力，<em>Layer  20</em>集中了当前列的注意力。</p><table><tr><td><center class="half"><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/gif3_memo_lay6-head1.gif" style="zoom:250%;"></center></td><td><center><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/gif3_datadep_lay36-head0.gif" style="zoom:250%;"></center></td></tr></table><p>左图是<em>Layer 6</em>的注意力模式，右图是<em>Layer 36</em>的注意力模式。可以看到<em>Layer 6</em>无论输入是什么，注意力的集中点都具有相似的模式，<em>Layer 36</em>的注意力高度依赖具体的数据。</p><p>另外，<em>Sukhbaatar</em>等人也对比了两个<em>Transformer</em>注意力头的注意力模式：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200331164300.png" alt></p><p>可以看到，<em>Head A</em>的注意力主要在最近的20个<em>token</em>，前面的80个<em>token</em>注意力权重很低，<em>Head B</em>的注意力主要集中在最近的20个<em>token</em>，但前80个<em>token</em>的注意力是均匀分布的。</p><p>从上面两个实验可以看出，注意力通常是稀疏的，而且在不同的层有不同的模式。虽然许多层显示出稀疏的结构，但有些层清晰地显示出了动态注意力，这种注意力延伸到整个图像。这个结论和我们在<a href="[https://rogerspy.gitee.io/2019/09/18/transformer%E7%BC%96%E7%A0%81%E5%B1%82%E8%A1%A8%E7%A4%BA/](https://rogerspy.gitee.io/2019/09/18/transformer编码层表示/">《Transformer的每一个编码层都学到了什么？》</a>)中讨论的结果基本一致。</p><p>由于注意力机制的稀疏模式，研究人员提出了不同的稀疏化方法，下面我们介绍其中几种。</p><h2 id="2-2-Sparse-Transformers"><a href="#2-2-Sparse-Transformers" class="headerlink" title="2.2 Sparse Transformers"></a>2.2 Sparse Transformers</h2><p>2019年OpenAI研究人员研发出一种<em>Sparse Transformers</em>，该模型在预测长序列方面创造了新纪录——无论预测的是文本、图像还是声音，可以从长度可能是之前30倍的序列中提取模式。</p><p>对于图像这种具有周期性结构的数据来说，作者提出<em>Strided Sparse Transformer</em>。从上面的<em>Layer 19</em>和<em>Layer 20</em>可以看出注意力分为关注当前行和当前列。作者可以根据这两种注意力模式设计两个稀疏注意力矩阵。</p><h3 id="2-2-1-Full-Self-Attention"><a href="#2-2-1-Full-Self-Attention" class="headerlink" title="2.2.1 Full Self Attention"></a>2.2.1 Full Self Attention</h3><p><img src="https://kexue.fm/usr/uploads/2019/07/775103900.png" alt></p><p>在上图中，左边显示了注意力矩阵，右变显示了关联性。这表明每个元素都跟序列内所有元素有关联。注意力稀疏化一个基本的思路就是减少关联性的计算，也就是认为每个元素只跟序列内的一部分元素相关，这就是稀疏注意力的基本原理。</p><h3 id="2-2-2-Atrous-Self-Attention"><a href="#2-2-2-Atrous-Self-Attention" class="headerlink" title="2.2.2 Atrous Self Attention"></a>2.2.2 Atrous Self Attention</h3><p>首先考虑列性注意力。对于一张图片来说，我们如果把图片展开成一个一维序列，对于之前注意力只关注当前列实际上就意味着，在这个展开的长序列中，注意力的关注点是间隔的，不连续的。这样引入一个新概念——<em>Atrous Self Attention</em>：</p><p><img src="https://kexue.fm/usr/uploads/2019/07/4107095412.png" alt></p><p><em>Atrous Self Attention</em> 强行要求每个元素只跟它相对距离为$k,2k,3k,…$的元素关联，其中$k&gt;1$是预先设定的超参数。由于现在计算注意力是“跳着”来了，所以实际上每个元素只跟大约$n/k$个元素算相关性，这样一来理想情况下运行效率和显存占用都变成了$O(n^2/k)$，也就是说能直接降低到原来的$1/k$。</p><h3 id="2-2-3-Local-Self-Attention"><a href="#2-2-3-Local-Self-Attention" class="headerlink" title="2.2.3 Local Self Attention"></a>2.2.3 Local Self Attention</h3><p>再考虑行性注意力。当注意力只关注在一行的内容时相当于每个元素只与前后$k$个元素以及自身有关联，如下图：</p><p><img src="https://kexue.fm/usr/uploads/2019/07/713126535.png" alt></p><p>其实<em>Local Self Attention</em>就跟普通卷积很像了，都是保留了一个$2k+1$大小的窗口，然后在窗口内进行一些运算，不同的是普通卷积是把窗口展平然后接一个全连接层得到输出，而现在是窗口内通过注意力来加权平均得到输出。对于<em>Local Self Attention</em>来说，每个元素只跟2k+12k+1个元素算相关性，这样一来理想情况下运行效率和显存占用都变成了$O((2k+1)n)∼O(kn)$了，也就是说随着$n$而线性增长，这是一个很理想的性质——当然也直接牺牲了长程关联性。</p><h3 id="2-2-4-Stride-Sparse-Self-Attention"><a href="#2-2-4-Stride-Sparse-Self-Attention" class="headerlink" title="2.2.4 Stride Sparse Self Attention"></a>2.2.4 Stride Sparse Self Attention</h3><p>到此，就可以很自然地引入OpenAI的<em>Sparse Self Attention</em>了。OpenAI将<em>Atrous Self Attention</em>和<em>Local Self Attention</em>合并为一个，形成适用于图像的<em>Strided Sparse Transformer</em>:</p><p><img src="https://kexue.fm/usr/uploads/2019/07/1199615308.png" alt></p><p>这样一来Attention就具有<strong>局部紧密相关和远程稀疏相关</strong>的特性，这对很多任务来说可能是一个不错的先验，因为真正需要密集的长程关联的任务事实上是很少的。</p><h3 id="2-2-5-Fix-Sparse-Self-Attention"><a href="#2-2-5-Fix-Sparse-Self-Attention" class="headerlink" title="2.2.5 Fix Sparse Self Attention"></a>2.2.5 Fix Sparse Self Attention</h3><p>对于文本这种非周期的数据，上面的<em>Stride Sparse Transformer</em>并不能很好的获取数据特征，作者认为是因为对于文本来说，元素的空间坐标和它所处的位置并没有必然的联系，它可能与未来的元素关联性更大，因此，作者提出另一种稀疏注意力模式——<em>Fix Sparse Transformer</em>。</p><p><em>Fix Sparse Transformer</em>同样是由两个注意力机制合并组成的，一种如下图，相当于将完整序列划分成多个子序列，在每个子序列内部做<em>full self attention</em>。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/34545654654.png" alt></p><p>另一种如下图，相当于只计算序列上固定几个位置的元素计算注意力权重。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/serwerw4335.png" alt></p><p>两种注意力相结合同样保证了<strong>局部紧密相关和远程稀疏相关</strong>特性。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/sdgfghfg.png" alt></p><h2 id="2-3-Adaptive-Span-Transformers"><a href="#2-3-Adaptive-Span-Transformers" class="headerlink" title="2.3 Adaptive Span Transformers"></a>2.3 Adaptive Span Transformers</h2><p>上面的稀疏化方法是研究人员利用先验知识，人工设计的一种稀疏化方法。这些方法可以很好的处理明显具有稀疏化特征的注意力机制，比如<em>Layer 19/20</em>，但是对于具有全局注意力和依赖数据特征的注意力机制，利用上述的稀疏化方法会影响最后的效果。因此，我们就想能不能设计一种自适应的注意力稀疏化机制，让模型自己决定要怎样稀疏化，这样可以避免人工设计的缺陷。</p><p>针对这个问题，Facebook的研究人员提出一种新的方法，利用一个$M_z$函数自动过滤一定长度的子序列，不参与注意力计算。$M_z$函数定义如下：</p><script type="math/tex; mode=display">m_z(x)=\min[\max[\frac{1}{R}(R+z-x), 0], 1]</script><p>这个函数的大致形状如下：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200401105157.png" alt></p><p>其中$R$是超参数，用来控制斜率。$z$是一个需要训练的参数，$x$是相对距离。得到这样一个函数以后，计算注意力的方法如下：</p><script type="math/tex; mode=display">a = \frac{m_z(t-r)\exp(s_{tr})}{\sum_{q=t-s}^{t-1}m_z(t-q)\exp(s_{tq})}</script><p>在损失函数中，给z添加一个<em>L1</em>惩罚项：</p><script type="math/tex; mode=display">L = -\log P(w_1,...w_T)+\frac{\lambda}{M} \sum_iz_i</script><p>另外，我们也可以用动态方式来学习$z$，即$z$是基于当前输入的一个输出，称之为动态宽度。</p><script type="math/tex; mode=display">z_t = S\sigma(\mathbf{v}^T\mathbf{x}_t+b)</script><p>从上面的函数图可以看出来，</p><ol><li>当$z$大于两元素的相对距离时，最后的注意力相当于<em>Full self attention</em>;</li><li>当$z$小于两元素的相对距离时，注意力会更集中在近距离元素上，相当于<em>Local self attention</em>；</li><li>当$z$很小时，远距离的元素上不会有任何注意力</li></ol><p>可以看出这样同样是既保留了局部的依赖，又处理了远程的稀疏性，而这样一个过程是模型自行决定，有效避免了人为设计的缺陷。</p><h2 id="2-4-Adaptively-Sparse-Transformers"><a href="#2-4-Adaptively-Sparse-Transformers" class="headerlink" title="2.4 Adaptively Sparse Transformers"></a>2.4 Adaptively Sparse Transformers</h2><p>回想前面的稀疏化方法，我们发现之前的两种稀疏化方法都存在一个问题就是，注意力是连续性的。比如<em>Adaptive Span Transformer</em>，会忽略掉远距离的元素；虽然<em>Sparse Transformer</em>中包含了<em>Atrous Attention</em>，但是这种不连续性是人为设计的，具有固定的模式，不能很好的适应不同的数据。因此，本文提出一种新的方法，既能处理不连续的注意力，又能使这种不连续的注意力做到自适应不同的数据。</p><p>纵观我们从介绍注意力机制开始，到<em>Transformer</em>，再到后来的各种变种，有一个东西是自始至终都和注意力形影不离，那就是<strong>Softmax</strong>。<em>Softmax</em>是将一个向量进行归一化，将向量中每一个元素赋予概率的意义，而这个概率本身就是连续的。因此，如果要处理不连续性的注意力机制，我们是否可以将<em>softmax</em>进行稀疏化呢？</p><p>本文就引入一个新的<em>Softmax</em>函数，实现了注意力的不连续稀疏化——$\alpha$-$\rm{entmax}$：</p><script type="math/tex; mode=display">\alpha - \mathrm{entmax}(\mathbf{z}) = \arg \max_{\mathbf{p} \in \Delta^d} \mathbf{p}^T\mathbf{z}+\mathbf{H}_\alpha^T(\mathbf{p})</script><p>其中$\Delta^d=\{\mathbf{p} \in \mathbb{R}^d:\sum_i\mathbf{p}_i=1\}$，对于$\alpha \ge 1$，$\mathrm{H}_\alpha^T$是<em>Tsallis</em>广延熵族：</p><script type="math/tex; mode=display">\mathbf{H}_\alpha^T(\mathbf{p})=\begin{cases}\frac{1}{\alpha(\alpha-1)}\sum_j(p_j-p_j^\alpha), \alpha \ne 1, \\\\\\\\-\sum_jp_j\log p_j, \alpha =1.\end{cases}</script><p>可以看到，这样一个函数是非连续性的，面临一个凸优化的问题。实际上我们可以通过下面的公式对其进行优化：</p><script type="math/tex; mode=display">\alpha-\mathrm{entmax}(\mathbf{z}) = \left[ (\alpha-1)\mathbf{z}-\tau\mathbf{1}\right]_+^{1/\alpha-1}</script><p>其中$\mathbf{1}$表示元素全为1的向量，$\tau$是一个拉格朗日乘子为了保证$\sum_ip_i=1$，$[\cdot]_+$表示$\mathrm{ReLU}$的正数部分。</p><p>看公式实在头疼，看不出为啥这样一个公式能将注意力进行稀疏化，那我们就来看图：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200401145025.png" alt></p><p>左边是二维图像，右边的两幅图分别是<em>softmax</em>和$\alpha=2$的$\alpha$-$\mathrm{entmax}$。可以看出当 $t$ 过小的时候，输出就会变成0；$t$ 过大的时候，输出就会变成1，这样也就相当于将注意力稀疏化了。</p><p>剩下的 工作就是为了确定 $\tau$，以及为了自适应不同的注意力头（<em>transformer</em>是多注意头的）的 $\alpha$ 值，作者将 $\alpha$ 作为网络的参数，利用后向传播进行优化等一系列细节，这里就不做详细介绍了。</p><p>本文涉及到的数学原理和公式的推导在引文和文章附录中都有详细推导，这里就不搬上来了，有兴趣可以自己看。</p><h2 id="2-5-Explicit-Sparse-Transformer"><a href="#2-5-Explicit-Sparse-Transformer" class="headerlink" title="2.5 Explicit Sparse Transformer"></a>2.5 Explicit Sparse Transformer</h2><p><em>Explicit Sparse Transformer</em>虽然实现了不连续的自适应稀疏化自注意力，但是其实整个过程蛮复杂的，尤其是其中涉及到的数学，看了让人头秃（我边推公式边看着头发往下掉，内心毫无波动…）。有没有一种既简单易实现，又能做到不连续自适应的稀疏化自注意力呢？当然有咯，接下来就来介绍这样一个工作。</p><p><em>Explicit Sparse Transformer</em>的想法非常简单：它认为在计算注意力的时候，只有注意力最高的$k$个词对信息的获取有作用，其他低注意力的属于噪声，非但不会帮助模型获取有效信息，还会干扰模型做出正确决策。因此，在计算自注意力的时候，每个词只取注意力权重最高的$k$个词，其他的全部设置成$-\infty$。计算过程如下：</p><ol><li>首先计算注意力矩阵$P$；</li><li>找出 $P$ 中每行的  $k$ 个最大元素，记录其位置，并得到一个阈值向量，$t=[t_1, t_2, …, t_{lQ}]$，$t_i$ 表示第 $i$ 行中$k$ 个元素中注意力最低的那个值；</li><li>得到一个$Masking$矩阵：</li></ol><script type="math/tex; mode=display">M(P, k)_{ij} = \begin{cases}P_{ij},  \qquad P_{ij} \ge t_i \\\\\\\\\mathrm{-} \infty, \qquad P_{ij} \lt t_i\end{cases}</script><ol><li>归一化</li></ol><script type="math/tex; mode=display">A = \mathrm{softmax} (M(P, k))</script><ol><li>输出表示</li></ol><script type="math/tex; mode=display">C = AV</script><p>整个流程如下：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200401174029.png" alt></p><p>根据作者的实验表明，序列长度与<em>vanilla transformer</em>一致时，$k=8$能得到最佳结果。</p><p>关于取$\mathrm{top}-k$以后的后向传播问题，作者在论文的附录中给出了解释，有兴趣的可以看原文哟。</p><p>最后说几句吧，这个文章是投稿给了<em>ICLR 2020</em>，但是被拒稿了，拒稿的理由主要是效果没有达到<em>SOTA</em>，额，我觉得嘛，黑猫白猫，能抓老鼠就是好猫。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol><li><a href="https://kexue.fm/archives/6853" target="_blank" rel="noopener">为节约而生：从标准Attention到稀疏Attention</a>，苏剑林， 科学空间</li><li><a href="https://arxiv.org/pdf/1904.10509.pdf" target="_blank" rel="noopener">Generative Modeling with Sparse Transformers</a>, <em>Rewon Child, Scott Gray,  Alec Radford, Ilya Sutskever, 2019, Arxiv:1904.10509</em></li><li><a href="https://openai.com/blog/sparse-transformer/" target="_blank" rel="noopener">Generative Modeling with Sparse Transformers</a>, <em>OpenAI’s blog</em></li><li><a href="https://openreview.net/pdf?id=Hye87grYDH" target="_blank" rel="noopener">EXPLICIT SPARSE TRANSFORMER: CONCENTRATED ATTENTION THROUGH EXPLICIT SELECTION</a>, <em>Guangxiang Zhao,</em>  <em>Junyang Lin</em>, <em>Zhiyuan Zhang</em>,<em>Xuancheng Ren</em>, <em>Xu Sun, 2019, Arxiv:1912.11637</em></li><li><a href="https://arxiv.org/abs/1905.07799" target="_blank" rel="noopener">Adaptive Attention Span in Transformers</a>, <em>Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, Armand Joulin, 2019</em>，<em>Arxiv:1905.07799</em></li><li><a href="https://zhuanlan.zhihu.com/p/88702600" target="_blank" rel="noopener">Transformer之自适应宽度注意力</a>, 张雨石， 知乎</li><li><a href="https://arxiv.org/pdf/1909.00015.pdf" target="_blank" rel="noopener">Adaptively Sparse Transformers</a>, <em>Goncalo M. Correia， Vlad Niculae，Andre F.T. Martins，2019，Arxiv:1909.00015</em></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> NMT </tag>
            
            <tag> sparse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer家族之Blockwise Transformer</title>
      <link href="/2020/03/26/transformer%E5%AE%B6%E6%97%8F-block/"/>
      <url>/2020/03/26/transformer%E5%AE%B6%E6%97%8F-block/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5396ee05ly1g5pqn3ch6zj20u092znph.jpg" alt></p><p>本文将继续介绍关于<em>transformer</em>在 <em>non-auto regression</em>方面的研究，今天要介绍的是<em>Google Brain</em>的工作<a href="https://papers.nips.cc/paper/8212-blockwise-parallel-decoding-for-deep-autoregressive-models.pdf" target="_blank" rel="noopener">Blockwise Parallel Decoding for Deep Autoregressive Models</a>。</p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>对于<em>seq2seq</em>任务来说，<em>decoding</em>过程是影响推理速度的关键。效率和效果似乎是一个难以和平共处的老对手，纵观之前我们介绍的方法，大致分成三类：<em>auto regression</em>、<em>semi-auto regression</em>、<em>non-auto regression</em>。<em>Auto regression</em>是效果最好的，但是推理速度是最慢的；<em>non-regression</em>是推理速度最快的（可以比前者快甚至几十倍），但是效果就差了；而<em>semi-auto regression</em>介于两者之间，效果略差但速度较快。之前我们介绍的<em>Share transformer</em>在效率和效果上都有所提升，但是这种提升是来源于对模型结构的优化，以更小的计算量实现的，虽然我很喜欢，但是有没有一种更新的，在<em>decoding</em>上的创新方法，既能提升效果，又能提高效率呢？下面要介绍的这篇文章就完成了这样一个设想。</p><h1 id="2-Blockwise-Parallel-Decoding"><a href="#2-Blockwise-Parallel-Decoding" class="headerlink" title="2.Blockwise Parallel Decoding"></a>2.Blockwise Parallel Decoding</h1><p><img src="https://img.vim-cn.com/76/437388cec32870c4ad92e3128071830643baac.png" alt></p><p>上面展示的是<em>BPD</em>的基本方法。</p><p>假设我们有一组模型$\{p_1, p_2, …, p_k\}$，其中$p_1$是 原始模型，$p_2, …, p_k$是辅助模型。从图中我们可以看到，推理过程分成三步：</p><ul><li><strong>Predict</strong>: 使用 $p_1$ 预测一个长度为$k$的序列；</li></ul><script type="math/tex; mode=display">\hat{y}_{j+i}=\arg max_{y_{j+i}} p_i(j_{j+i}| \hat{y}_{\le j}, x), ~~ i=1,2, ..., k</script><ul><li><strong>Verify</strong>: 使用辅助模型找到$i$的最大值$\hat{k}$，使得</li></ul><script type="math/tex; mode=display">\hat{y}_{j+i}=\arg max_{y_{j+i}} p_1(j_{j+i}| \hat{y}_{\le j+i-1}, x), ~~ 1 \le i \le k</script><ul><li><strong>Accept</strong>: 扩展预测序列，从$\hat{y}_{\le j}$变成$\hat{y}_{\le j+\hat{k}}$，同时设置$j \leftarrow j +\hat{k}$。</li></ul><blockquote><p>在读论文的时候感觉作者在对<em>predict</em>和<em>verify</em>过程进行详细描述的时候给搞反了，不知道是真的写反了还是我理解错了，已经给作者发邮件了，截止到现在作者还没有回我。这里暂时按照我的理解做介绍，如果是我理解错了，再做修改。</p></blockquote><p>假设我们现在已经有一个种子序列：</p><script type="math/tex; mode=display">\mathrm{I\quad saw\quad a\quad dog\quad ride}</script><ul><li><p>预测阶段：将种子序列作为$p_1$的输入，使$p_1$并行输出一个候选序列： $\rm{in \quad the\quad bus}$</p></li><li><p>验证阶段：</p><p>① 将 $\mathrm{I \quad saw \quad a \quad dog \quad ride}$作为$p_1$的输入去预测 $\rm{in}$</p><p>② 将 $\mathrm{I \quad saw \quad a \quad dog \quad ride \quad in}$作为$p_2$的输入去预测 $\rm{the}$</p><p>③ 将 $\mathrm{I \quad saw \quad a \quad dog \quad ride}$作为$p_3$的输入去预测 $\rm{bus}$</p></li></ul><p><img src="https://img.vim-cn.com/f2/a7da5078c2becf83362e61ed60abc8bf20b98a.png" alt></p><p>$p_1$模型的输入来源无需多说，就是原始的种子序列；$p_2$的输入是种子序列+在预测阶段$p_1$的输出的第一个词；$p_3$的输入是种子序列+预测阶段$p_1$的输出的前两个词，以此类推。在验证阶段每个模型都是以<em>Greedy decoding</em>模式进行推理的，即只预测下一个词。因为每个模型都是独立运行的，所以$p_1, p_2, p_3$是并行运算的。</p><ul><li><p>验收阶段：</p><p>在前面的验证阶段我们的到了一个候选集：</p></li></ul><script type="math/tex; mode=display">\rm{I\quad saw\quad a\quad dog\quad ride\quad in} \\\\\rm{I\quad saw\quad a\quad dog\quad ride\quad in\quad the} \\\\\rm{I\quad saw\quad a\quad dog\quad ride\quad in\quad the\quad car}</script><p>与预测阶段$p_1$的输出对比发现$p_1$的输出最后一个词是$\rm{bus}$，而$p_3$输出的词是$\rm{car}$，相当于$p_1$和$p_3$发生了冲突，那么在验收阶段我们只取验证阶段与预测阶段输出相同的结果的最长序列，即我们取$\rm{I\quad saw\quad a\quad dog\quad ride\quad in \quad the}$作为这一轮预测的输出，同时也是下一轮的输入。</p><p>相比于传统的<em>auto-regression</em>每一轮预测只能生成一个词，这种<em>Blockwise decoding</em>每一轮可以产生多个词（上面的例子中，同时生成$\rm{in\quad the}$两个词，当然需要注意的是每一轮生成词序的长度并不是固定的），同时还能保证这些词是以<em>auto-regression</em>的方式产生的，因此在提升推理效率的同时，效果一定不会比<em>auto-regression</em>差。</p><p>这里再唠叨几句，为什么这种方法是合理的呢？因为我们先用$p_1$生成一个短序列，然后用$p_1, p_2, …,p_k$去验证的时候，相当于是将一个时序的推理过程并行化了。首先，我们先假设$p_1$生成的序列是合理的，在验证过程中用第一个模型去验证如果是在自回归模式下$p_1$是否会生成 $\rm{in}$，用第二个模型验证自回归模式下$p_1$ 是否会在$\rm{in}$后面生成$\rm{the}$，以此类推，知道我们发现验证阶段的输出和验证阶段的输出不一致的时候，我们就认为并行化生成的序列在这个位置及以后的输出是不合理的，所以我们可以放心的使用之前的生成的序列。</p><p>前面我们是以已经存在一个种子序列作为初始条件介绍的，那么在模型推理的一开始还没有种子序列的时候其实也差不多。<em>Auto-regression</em>的时候第一步推理也是根据<em>encoding</em>生成第一个词，然后逐步生成我们想要的序列。在<em>blockwise decoding</em>的时候也是一样的，初始的时候$p_1$不是生成一个词，而是生成一个序列。这样我们就可以重复上面的三个步骤了。</p><p>文章中给出了一个实际的推理过程的例子：</p><p><img src="https://img.vim-cn.com/60/413eb68aeb9dd9c4f24c597d73c1d1a4eaec0d.png" alt></p><p>另外，还有一个需要注意的细节，在文章当中并没有提及，我不太清楚作者是怎么处理的，这里我说下我个人的想法。还以上面的句子为例：初始序列是: $\mathrm{I \quad saw \quad a \quad dog \quad ride}$</p><p>假设预测阶段$p_1$生成的序列是$\rm{in \quad the \quad bus}$，但是在验证阶段$p_1$生成的是$\rm{on}$，也就是说，验证阶段的第一个模型输出就与预测阶段生成的第一个词不一致，这种情况该怎么办呢？当然需要指出的是，这种情况的概率应该是很低的，因为都是$p_1$模型的输出所以在最靠近输入序列的输出应该有相同的分布，但是在写代码的时候不得不考虑这种情况。我个人的想法是应该是以验证阶段$p_1$的输出为准，毕竟这个时候的模型推理模式是<em>auto-regression</em>式的。虽然文章没有确切提出这一处理方式，但是在下面的介绍中，我们可以看到作者实际上确实是采用了这种方法的。</p><h1 id="3-Combined-Scoring-and-Proposal-Model"><a href="#3-Combined-Scoring-and-Proposal-Model" class="headerlink" title="3. Combined Scoring and Proposal Model"></a>3. Combined Scoring and Proposal Model</h1><p>从上面的推理步骤可以看出，每一步推理都需要至少两次模型调用：预测阶段调用$p_1$和验证阶段调用$p_1, p_2, …, p_k$。理想情况下，我们希望生成长度为$m$的序列，只调用$m/k$次模型，但是上面的步骤需要$2m/k$次模型调用。那么有没有什么办法尽可能接近$m/k$次呢？接下来我们就介绍对上面步骤的改进，使得调用次数从$2m/k$次下降到$m/k+1$次。</p><p><img src="https://img.vim-cn.com/d0/b5e3d64b7e1e913745c7882eefe7361619caae.png" alt></p><p>和之前一样预测阶段先由$p_1$生成一个短序列，但是在验证阶段就发生了变化：之前是每个模型生成一个词，现在我们令每个模型和预测阶段的$p_1$一样，都生成一个短序列，这个实际上就相当于是验证我那个阶段和预测阶段合二为一了。我们仍然通过对比生成序列的第一个词来进行验收，而下一个推理周期的预测序列采用刚刚验收的序列生成的短序列。</p><p>以上图为例：</p><p>第一步：使$p_1$并行输出一个候选序列： $\rm{in \quad the\quad bus}$</p><p>第二步：</p><p>① 将 $\mathrm{I \quad saw \quad a \quad dog \quad ride \quad in }$作为$p_1$的输入去预测 $\rm{the \quad car \quad last}$</p><p>② 将 $\mathrm{I \quad saw \quad a \quad dog \quad ride \quad in \quad the }$作为$p_2$的输入去预测 $\rm{car \quad this \quad week}$</p><p>③ 将 $\mathrm{I \quad saw \quad a \quad dog \quad ride \quad in \quad the \quad bus}$作为$p_3$的输入去预测 $\rm{last \quad week \quad when}$</p><p>第三步：我们提取出 $\mathrm{I \quad saw \quad a \quad dog \quad ride \quad in \quad the }$作为我们这一轮的推理的输出，同时将$\rm{car \quad this \quad week}$作为下一轮推理的预测序列。这样我们就相当于每一轮推理只需要调用一次模型（时间）就完成了验证和预测两步。</p><p>我们可以看到，这种推理方法的核心点在于$p_1$必须具有并行生成序列的能力，不能是<em>RNN</em>一次生成一个词的模型，所以只能是<em>CNN</em>或者<em>Transformer</em>这种具有并行能力的模型。</p><h1 id="4-Approximate-Inference"><a href="#4-Approximate-Inference" class="headerlink" title="4. Approximate Inference"></a>4. Approximate Inference</h1><p>到目前为止，我们就介绍了<em>BPD</em>的基本方法，实际上我们还可以放松验证标准实现额外的加速推理。</p><h2 id="4-1-Top-k-Selection"><a href="#4-1-Top-k-Selection" class="headerlink" title="4.1 Top-$k$ Selection"></a>4.1 Top-$k$ Selection</h2><p> 之前在验证阶段，我们要求辅助模型的输出必须和原始模型的输出高度一致才可以，但是实际上我们可以认为，只要原始模型的输出在辅助模型的输出的<em>top-k</em>个输出中就可以，即</p><script type="math/tex; mode=display">\hat{y}_{j+i} \in \mathrm{top}k ( p_1(y_{j+i}|\hat{y}_{\le j+i-1}, x))</script><p>比如前面$\mathrm{I \quad saw \quad a \quad dog \quad ride \quad in \quad the \quad car  \quad this \quad week}$ 其中 $\rm{car}$ 与 $\rm{bus}$ 不一致，所以我们的输出序列只取到$\mathrm{I \quad saw \quad a \quad dog \quad ride \quad in \quad the}$。那么现在如果 $\rm{car}$ 存在于输出概率的<em>top-k</em>候选里面，我们也认为它是正确的，那么我们这一轮的推理输出结果应该是$\mathrm{I \quad saw \quad a \quad dog \quad ride \quad in \quad the \quad car}$。</p><h2 id="4-2-Distance-Based-Selection"><a href="#4-2-Distance-Based-Selection" class="headerlink" title="4.2 Distance-Based Selection"></a>4.2 Distance-Based Selection</h2><p>如果两个词在语义空间中有相近的含义，那么我们也可以用近义词代替，即：</p><script type="math/tex; mode=display">d(\hat{y}_{j+i}, \arg max_{y_{j+i}} p_1(y_{j+i}|\hat{y}_{j+i-1}, x)) \le \epsilon</script><p>比如 $\rm{car}$ 和 $\rm{bus}$ 的意思相近，</p><p>因此我们任然可以得到 $\mathrm{I \quad saw \quad a \quad dog \quad ride \quad in \quad the \quad car}$。</p><h2 id="4-3-Minimum-Block-Size"><a href="#4-3-Minimum-Block-Size" class="headerlink" title="4.3 Minimum Block Size"></a>4.3 Minimum Block Size</h2><p>在推理的时候有可能出现炎症阶段第一个词就不正确的情况（上面我们讨论过），这样的话我们就只能一次生成一个词，那么久退回到自回归的推理模式了，为了保证推理的加速，我们可以设置一个最小值$l$，$1\le l \le k$，无论对错，每次最小生成 $l$ 个词。当$l=1$时，模型退化到自回归；当 $l=k$ 时模型变成纯并行模型。</p><h1 id="5-Traning"><a href="#5-Traning" class="headerlink" title="5. Traning"></a>5. Traning</h1><p>从上面描述我们可以看到，这种推理方式是非常消耗内存的，因为需要同时存在多个模型，这样我们就不能计算全部模型的loss了，取而代之，我们可以随机选择其中一个模型的loss作为整体的loss。</p><h1 id="6-Experiments"><a href="#6-Experiments" class="headerlink" title="6. Experiments"></a>6. Experiments</h1><p><img src="https://img.vim-cn.com/99/d5f7574dcd44e5f528f917130b2e50cdd20740.png" alt></p><h1 id="7-Reference"><a href="#7-Reference" class="headerlink" title="7. Reference"></a>7. Reference</h1><p><em>Mitchell Stern, Noam Shazeer, Jakob Uszkoreit</em>, NeurIPS 2018, <a href="https://papers.nips.cc/paper/8212-blockwise-parallel-decoding-for-deep-autoregressive-models.pdf" target="_blank" rel="noopener">Blockwise Parallel Decoding for Deep Autoregressive Models.</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> NMT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer家族之Share Attention Networks</title>
      <link href="/2019/09/30/transformer%E5%AE%B6%E6%97%8F-share/"/>
      <url>/2019/09/30/transformer%E5%AE%B6%E6%97%8F-share/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5396ee05ly1g5pqn3ch6zj20u092znph.jpg" alt></p><p>接下来我们介绍一下<a href="https://arxiv.org/abs/1906.11024" target="_blank" rel="noopener">Sharing Attention Weights for Fast Transformer</a>这篇文章。实际上这篇文章之前还有两个关于加速<em>Transformer</em>推理的文章，一个类似<em>Latent Transformer</em>的引入离散隐变量的类<em>VAE</em>的方法，另一个是引入语法结构达到<em>non-auto regression</em>的方法。个人感觉没什么意思，就直接跳过了。本文要介绍的这篇文章比较有意思，引入共享权重的概念。从近期关于<em>Bert</em>小型化的研究（比如DistilBERT，ALBERT，TinyBERT等）来看，实际上<em>Transformer</em>中存在着大量的冗余信息，共享权重的方法应该算是剔除冗余信息的一种有效的手段，因此这篇文章还是比较有意思的。</p><a id="more"></a><h1 id="1-Attention-Weights"><a href="#1-Attention-Weights" class="headerlink" title="1. Attention Weights"></a>1. Attention Weights</h1><p>我们在介绍<em>Transformer</em>的时候说过，<em>Multi-Head Attention</em>其实和<em>Multi-dimension Attention</em>是一回事。在多维注意力机制中，我们希望每一维注意力都能学到不同的含义，但是实际上<a href="https://openreview.net/pdf?id=BJC_jUqxe" target="_blank" rel="noopener">Lin et al. 2017</a>研究发现，多维注意力机制经常会出现多个维度学到的东西是相同的的情况，即不同维度的注意力权重分布是相似的。</p><p>本文做了一个类似的研究，发现不同层之间的注意力权重也有可能具有相似的分布，说明不同层之间的注意力权重也在学习相同的信息。作者分别计算了不同层之间的注意力权重的JS散度，用以说明不同注意力权重有多大的差异性，下图是作者的计算结果：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/457db2eeb701029cdb46fb1d801e9af3448fbc.png" alt></p><p>注意作者的意图是加速<em>Transformer</em>的推理速度，因此着重研究的是<em>decoder</em>部分，因此上图表示的<em>decoder</em>的JS散度矩阵。我们知道每个<em>decoder block</em>中包含两个注意力矩阵，一个是<em>masked atteniton</em>，一个是<em>encoder-decoder attention</em>，上图左图表示<em>masked attnetion</em>，右图表示<em>encoder-decoder attention</em>。图中颜色越深表示差异性越小，即两个注意力权重矩阵越相似。从图中可以看到<em>self-attention</em>部分的相似非常大，而<em>encoder-decoder attention</em>相似性虽然不如<em>self-attention</em>，但是1,2,3,4之间和5,6层之间的相似性也比较大。</p><p>由于不同层的注意力权重相似性较大，因此可以在不同层中共享同一个注意力权重，减少参数从而达到加速推理的目的。</p><blockquote><p>这里作者只讨论了<em>decoder</em>的情况，还记得我们之前介绍过一篇论文<a href="https://www.aclweb.org/anthology/W18-5431" target="_blank" rel="noopener">An Analysis of Encoder Representations in Transformer-Based Machine Translation</a>，讲的是在<em>encoder</em>的每一层注意力都学到了什么信息。这里我希望对比一下用本文的方法和上面的研究方法对比一下，看看得到结论是否能相互印证。因此我和本文的作者进行了交流，很遗憾的是作者并没有像<em>decoder</em>那样仔细研究<em>encoder</em>的情况，但是作者认为理论上<em>encoder</em>的情况应该是和<em>decoder</em>的<em>self-attention</em>的情况是一致的，因为<em>encoder</em>中的<em>attention</em>和<em>decoder</em>的<em>self-attention</em>层都是对单一的序列进行编码，不同的是前者是对源序列，后者是对目标序列，因此两者应该有相似的表现。虽然作者没有计算<em>encoder</em>注意力权重的JS散度，但是在实验过程中，尝试过对<em>encoder</em>的注意力进行共享，发现第1, 2层计算，3-6层使用第1层的权重，此种情况下对性能也未发现明显的下降趋势，因此作者认为<em>encoder</em>端不同层的注意力权重同样存在着较多的相似情况。</p></blockquote><h1 id="2-模型结构"><a href="#2-模型结构" class="headerlink" title="2. 模型结构"></a>2. 模型结构</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/bd0042560fedcfb699684b22b00f4d2f2d96b0.png" alt></p><p>模型结构如上图。其实<em>SAN</em>的基本想法很简单，就是计算一次注意力权重，多次重复使用。具体的数学描述如下：</p><ul><li><strong>Self-Attention</strong></li></ul><p>定义第$m$层的注意力自注意力权重矩阵为：</p><script type="math/tex; mode=display">S^m = s(Q^m, K^m)</script><p>有了第$m$层的注意力权重，把它共享给第$m+1$层：</p><script type="math/tex; mode=display">S^{m+i} = s(Q^m, K^m), i \in [1, \pi-1]</script><p>其中$\pi$表示有多少层共享第$m$层的注意力权重，比如对于一个6层的<em>decoder</em>来说，我们可以让前两层共享一个权重矩阵$\pi_1=2$，让后面的4层共享两一个权重矩阵$\pi_2=4$，具体的共享策略在后面介绍。</p><ul><li><strong>Encoder-Decoder Attention</strong></li></ul><p>对于<em>encoder-decoder</em>注意力层来说，采取同样的操作。但是为了进一步加速推理，这里作者采用了一个小技巧，即$K, V$使用<em>encoder</em>的输出：</p><script type="math/tex; mode=display">A^{m+i} = A^m = S^m \cdot V, i \in [1, \pi-1]</script><p>其中$A^m$是第$m$层的 注意力输出，$V$是<em>encoder</em>的输出。</p><p>另外为了减少内存消耗，共享的注意力权重只需要拷贝给相应的层即可，不需要配置到在每一层的内存中去。</p><h1 id="3-Learning-to-Share"><a href="#3-Learning-to-Share" class="headerlink" title="3. Learning to Share"></a>3. Learning to Share</h1><p>那么，现在的问题是，我们怎么知道那些层需要共享注意力权重呢？一个最简单的方法就是遍历测试，然后在<em>development set</em>上进行微调。显然这不是最优的方法，因为我们要在不同的程度上控制注意力权重的共享程度。</p><p>这里作者提出采用动态策略——利用JS散度计算层与层之间的相似度：</p><script type="math/tex; mode=display">\mathrm{sim}(m,n) = \frac{\sum_{i=m}^n \sum_{j=m}^n(1-\delta(i,j))\mu(i, j)}{(n-m+1)\cdot (n-m)}</script><p>其中$\mu(i, j)$表示第$i$层和第$j$层的JS相似度，$\delta(i,j)$表示<em>Kronecker delta function</em>。上式表示$m,n$层之间的相似性，当$\mathrm{sim}(m,n) &gt; \theta$时那么$m,n $层就共享注意力权重。</p><p>首先从第一层开始计算满足$\theta$阈值的最大的$\pi_n$，如此往复，直到所有的注意力层都计算完了。这个时候我们会得到一个注意力权重的共享策略$\{\pi_1, …, \pi_N\}$，$\pi_i$实际上前面已经解释了表示什么，但是为了更直观的解释，这里我们还是举个例子吧：</p><p>假设<em>deocder</em>有6层注意力层，共享策略为$\{\pi_1=2,\pi_2=4\}$，表示第$1,2$层共享注意力权重，第$3,4,5,6$共享注意力权重。</p><p>一旦共享策略确定了下来之后，我们要重新训练模型，对注意力权重进行微调，直到模型收敛。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/56794343674c0e10d5472a191f55512a193361.png" alt></p><h1 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4. 实验结果"></a>4. 实验结果</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/c7ed7dba2280159161a92a1c0a7de50419dda7.png" alt></p><h1 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5. 参考资料"></a>5. 参考资料</h1><p><a href="https://arxiv.org/pdf/1906.11024.pdf" target="_blank" rel="noopener">Sharing Attention Weights for Fast Transformer</a> Tong Xiao et al., 2019</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> NMT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer家族之Semi-Autoregressive Transformer</title>
      <link href="/2019/09/30/transformer%E5%AE%B6%E6%97%8F-sa/"/>
      <url>/2019/09/30/transformer%E5%AE%B6%E6%97%8F-sa/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5396ee05ly1g5pqn3ch6zj20u092znph.jpg" alt></p><p>从题目就可以看出来，本文将要介绍一种半自动回归的机器翻译解码技术。之前我们介绍了各种非自动回归的解码技术，其中有一个<em>Latent Transformer</em>是先用<em>auto-regression</em>生成一个短序列，然后用这个短序列并行生成目标序列。当时我们说这其实算是一种半自动回归的方法。今天我们要介绍另一种半自动回归的方法——<em>SAT</em>。</p><a id="more"></a><p>和<em>Latent Transformer</em>不同，<em>Semi-Auto regressive Transformer （SAT）</em>仍然是<em>auto-regression</em>的运行机制，但是不像<em>auto-regression</em>那样一次生成一个元素，<em>SAT</em>是一次生成多个元素。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/8dda2a1d508a3e3108c09a543a4a4c918c2e76.png" alt></p><h1 id="1-模型结构"><a href="#1-模型结构" class="headerlink" title="1. 模型结构"></a>1. 模型结构</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/f6757519d2f1b55a549102a43e27fbc55872ea.png" alt></p><p>模型结构如图。我们可以看到模型基本上和<em>Transformer</em>保持一致，只在图中红虚线框中有所改变。</p><h2 id="1-1-Group-Level-Chain-Rule"><a href="#1-1-Group-Level-Chain-Rule" class="headerlink" title="1.1 Group-Level Chain Rule"></a>1.1 Group-Level Chain Rule</h2><p>给定一个目标序列，通常的建模方式是词级别的链式法则：</p><script type="math/tex; mode=display">p(y_1, y_2, ..., y_n|X) = \Pi_{t=1}^n p(y_t|y_1, ..., y_{t-1}, x)</script><p>每个词都依赖之前的词。在<em>SAT</em>中，对序列进行分组</p><script type="math/tex; mode=display">G_1, G_2, ..., G_{[(n-1)/K]+1} = y_1, ..., y_K, y_{K+1}, ..., y_{2K}, ...,y_{[(n-1/K)\times K+1]}, ..., y_n</script><p>其中$K$表示每组的大小，$K$越大 表示并行能力越强，除了最后一组其他每组中必须包含$K$个词。这样的话，上面的链式法则则变成：</p><script type="math/tex; mode=display">p(y_1, ..., y_n|X) = \Pi_{t=1}^{[(n-1)/K]+1} p (G_t|G_1, ..., G_{t-1}, x)</script><p>作者将之前的<em>auto-regression</em>称之为<em>short-distance prediction</em>，而将<em>SAT</em>称之为<em>long-distance prediction</em>。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/30cac6ad33a85cd226d29a38b3f3a70fe8a773.png" alt></p><h2 id="1-2-Relaxed-Causal-Mask"><a href="#1-2-Relaxed-Causal-Mask" class="headerlink" title="1.2 Relaxed Causal Mask"></a>1.2 Relaxed Causal Mask</h2><p>在<em>Transformer</em>中由于是连续的词矩阵，因此在做<em>mask</em>的时候直接使用下三角矩阵，但是在<em>SAT</em>中由于答对词进行了分组，再使用下三角矩阵就不合适了，作者这里提出一个<strong>粗粒度下三角矩阵</strong>（<em>coarse-grained lower triangular matrix</em>）的 <em>mask</em>矩阵。如图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/0c0a6933a204fd767ede1e4794779d4f51378f.png" alt></p><p>图中左边表示标准的下三角矩阵，右边表示粗粒度下三角矩阵。</p><p>数学形式：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/49d1f6c42a21bfff8b118dbefe0d5cd004a2ea.png" alt></p><h1 id="2-实验结果"><a href="#2-实验结果" class="headerlink" title="2. 实验结果"></a>2. 实验结果</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/42129a015c5e78016cab0e78fc549d6d48afce.png" alt></p><h1 id="3-参考资料"><a href="#3-参考资料" class="headerlink" title="3. 参考资料"></a>3. 参考资料</h1><p><a href="https://arxiv.org/pdf/1808.08583.pdf" target="_blank" rel="noopener">Semi-Autoregressive Neural Machine Translation</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> NMT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer家族之NA Trasnsformer</title>
      <link href="/2019/09/26/transformer%E5%AE%B6%E6%97%8F-nat/"/>
      <url>/2019/09/26/transformer%E5%AE%B6%E6%97%8F-nat/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5396ee05ly1g5pqn3ch6zj20u092znph.jpg" alt></p><p>本文继续介绍关于<em>transformer</em>在<em>non-auto regression</em>方面的研究，今天要介绍的是<em>Gu et al. 2018</em> 发表在<em>ICLR 2018</em>上的文章<a href="http://arxiv.org/abs/1711.02281" target="_blank" rel="noopener">Non-autoregressive neural machine translation</a> 。</p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>之前我们介绍了由于<em>transformer</em>的<em>auto-regression</em>机制，导致模型在做推理的时候会变得非常慢。针对这个问题很多研究者都做了探索，之前介绍的几篇论文都没有真正做到<em>non-auto regression</em>，而今天我们要介绍的这篇文章则从根本上做到了<em>non-auto regression</em>。</p><h2 id="1-1-Auto-Regressive-Decoding"><a href="#1-1-Auto-Regressive-Decoding" class="headerlink" title="1.1 Auto-Regressive Decoding"></a>1.1 Auto-Regressive Decoding</h2><p>给定一个源序列$X=\{x_1, x_2, …, x_{T’}\}$，翻译模型通过链式条件概率的方式预测输出序列$Y = \{ y_1, y_2, …, y_T\}$:</p><script type="math/tex; mode=display">p_{AR}(Y|X;\theta) = \Pi_{t=1}^{T+1} p(y_t|y_{0:t-1}, x_{1:T'};\theta)</script><p>其中$y_0$表示句子开始符，比如$BOS$；$y_{T+1}$表示句子结束符，比如$EOS$ 。</p><h2 id="1-2-Maximum-Likelihood-training"><a href="#1-2-Maximum-Likelihood-training" class="headerlink" title="1.2  Maximum Likelihood training"></a>1.2  Maximum Likelihood training</h2><p>训练的时候直接使用<em>Maximum Likelihood training</em>方法对模型进行训练：</p><script type="math/tex; mode=display">L_{ML} = \log P_{AR}\{Y|X;\theta\} = \sum_{t=1}^{T+1} \log p(y_t|y_{0:t-1},x_{1:T'};\theta)</script><h2 id="1-3-Non-Auto-regressive-Decoding"><a href="#1-3-Non-Auto-regressive-Decoding" class="headerlink" title="1.3 Non-Auto regressive Decoding"></a>1.3 Non-Auto regressive Decoding</h2><p>从上面的$p_{AR}(Y|X;\theta)$中可以看出，实际上要预测$Y=\{y_t\}$需要两个条件：</p><ul><li>知道$Y$的长度$T$，虽然对于<em>auto-regression</em>来说，解码过程并不知道$T$显示的值，但是由于编码的开始和结束可以通过句子开始符（$&lt; BOS&gt;$）和句子结束符（$&lt; EOS&gt;$）来控制，编码过程是一个词一个词的生成 ，知道遇到结束符，则编码过程结束。因此，模型可以隐式的知道$T$的值。但是对于并行生成句子序列，我们必须提前知道句子的长度，才能一次性生成一个长度为$T$的序列，所以对于<em>Non-auto reregression</em>来说$T$必须是显式的；</li><li>第二点当然就是$Y$序列本身了，当知道了需要预测的序列长度，就可以根据输入预测输出了。</li></ul><p>因此，我们可以把预测$Y=\{y_t\}$任务分为两部分，第一部分预测$T$的大小，第二部分生成长度为$T$的序列：</p><script type="math/tex; mode=display">p_{NA}(Y|X;\theta) = p_L(T|x_{1:T'};\theta)\cdot \Pi_{t=1}^{T} p(y_t|x_{1:T'};\theta)</script><p>这样我们就可以分别独立的训练这两部分，而在推理的时候 能并行计算。</p><h2 id="1-4-多模态问题"><a href="#1-4-多模态问题" class="headerlink" title="1.4 多模态问题"></a>1.4 多模态问题</h2><p>这种简单的方法虽然看起来合理，但是实际上有一个很大的问题：多模态问题 。具体来说就是同一句话有多种翻译方式，比如<em>Thank you</em>可以翻译成<em>谢谢</em>、<em>感谢</em>等。由于$p(y_t)$只与$X$有关，所以无法获得训练数据中<em>谢谢</em>、<em>感谢</em> 等不同翻译方式的分布。</p><p>当<em>A B</em>既可以翻译成<em>1 2 3</em>，又可以翻译成<em>4 5 6</em>时，实际上相当于</p><script type="math/tex; mode=display">\{A, B\} => \{1, 2, 3, 4, 5, 6\}</script><p>的一个映射，而最佳的映射组合是$\{1,2,3\}$和$\{4,5,6\}$，但是由于<em>non-auto regression</em>每个词都是独立的，所以无法获取到词与词之间的依赖关系，每种序列组合称为<em>mode</em>。</p><p>另外使用<em>Maxium Likelihood</em>进行训练的时候，模型倾向于使用在训练集中出现概率最大的<em>mode</em>覆盖掉其他小概率<em>mode</em>，这实际上是有一定问题的。</p><p>要解决多模态问题，一般有三种方法：</p><ul><li>增强模型处理多模态的能力；</li><li>在训练集中减少<em>mode</em>的数量；</li><li>改变学习目标</li></ul><p>实际上最有效的还是第一种方法，这篇论文提出了一种训练技术用来解决多模态问题。</p><h1 id="2-Non-Autoregressive-Transformer"><a href="#2-Non-Autoregressive-Transformer" class="headerlink" title="2. Non-Autoregressive Transformer"></a>2. Non-Autoregressive Transformer</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/e3c7fba55c89d79ee2c7fdee8860a7bd7d48f0.png" alt></p><p>模型结构图如上。模型包含四个部分：<em>Encoder</em>、<em>Fertility predictor</em>、<em>Decoder</em>和<em>Translation Predictor</em>。其中黑色实线箭头表示可微分操作，浅色虚线表示不可微分操作，每个<em>sublayer</em>也都包含<em>LayerNorm</em>和残差连接。</p><p><em>Encoder</em>部分作者并没有做什么变化，与<em>Transformer</em>保持一致，因此这里我们不做介绍，主要介绍其他几部分，以及训练技巧。</p><h2 id="2-1-Decoder-Stack"><a href="#2-1-Decoder-Stack" class="headerlink" title="2.1 Decoder Stack"></a>2.1 Decoder Stack</h2><p>从图中可以看到，<em>Decoder Stack</em>包含了四部分：</p><ul><li>Multi-Head Self-Attention</li><li>Multi-Head Positional Attention</li><li>Multi-Head Inter-Attention</li><li>MLP</li></ul><p>其中<em>MLP</em>就是<em>transformer</em>中的<em>position wise feed forward</em>层。<em>Multi-Head Inter-Attention</em>就是<em>transformer</em>中<em>decoder block</em>的第二个<em>multi-head attention</em>层。这两个相比原来的<em>transformer</em>没有什么变化，这里就跳过不讲。下面主要介绍其余两个模块以及<em>Decoder Stack</em>的输入。</p><h3 id="2-1-1-Decoder-Inputs"><a href="#2-1-1-Decoder-Inputs" class="headerlink" title="2.1.1 Decoder Inputs"></a>2.1.1 Decoder Inputs</h3><p>在进行解码之前，<em>Non-Autoregressive Transformer</em>（<em>NAT</em>）需要知道要生成的序列的长度。另一方面如果只输入位置编码的话最后的效果会很差，因此解码器的输入也是非常重要的。为此，作者设计了两种解码器的输入：</p><ul><li><strong>Copy source inputs uniformly</strong>：根据下面的规则从源序列中拷贝一个序列出来作为解码器的输入</li></ul><script type="math/tex; mode=display">Round(T't/T)</script><p>假设源序列长度是$T’$，目标序列长度是$T$，那么解码器的输入序列中第$t$个位置的元素为源序列中第$Round(T’t/T)$个元素。举个例子：</p><blockquote><p>源序列：[Thank, you, !]</p><p>目标序列：[谢谢, ！]</p></blockquote><p>源序列的长度是3，目标序列的长度是2。那么解码器的输入第0个位置的元素应该对应源序列中第 $3 \times 0/2=0$，个元素，即<em>Thank</em>；解码器的输入序列的第1个元素，应该对应源序列中第$3 \times 1/2=1.5$，四舍五入得2，即<em>！</em>。那么解码器的输入序列应该为$\{Thank , !\}$。</p><ul><li><strong>Copy source inputs using fertilities</strong>：同样是从源序列中拷贝一个序列出来作为解码器的输入，但是拷贝规则有了变化。如同结构图中显示的，编码器会在编码结束后除了输出注意力状态，还会输出一个长度与源序列长度相同的序列，序列中每个元素都是一个整数。将源序列中的每个位置上的元素拷贝相应的整数倍之后作为解码器的输入，而解码器的输入长度$T$则是编码器输出的整数序列之和。举个例子：</li></ul><blockquote><p>源序列：[Thank, you, !]</p><p>encoder output: [1, 0, 1] </p></blockquote><p>那么解码器的输入序列为： $\{Thank \times 1, you \times 0, ! \times 1 \}$，即$\{Thank, !\}$为解码器的输入。</p><h2 id="2-1-2-Non-causal-self-attention"><a href="#2-1-2-Non-causal-self-attention" class="headerlink" title="2.1.2 Non-causal self-attention"></a>2.1.2 Non-causal self-attention</h2><p>由于这里是并行生成目标序列，因此不需要对注意力权重矩阵进行mask。但是在实际的测试过程中作者发现把元素自身所在的位置mask掉后会取得更好的结果，即（深色部分表示被mask掉的）：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/beac1906681d79de2a36ba7e872129973babd4.png" alt></p><h3 id="2-1-3-Positional-Attention"><a href="#2-1-3-Positional-Attention" class="headerlink" title="2.1.3 Positional Attention"></a>2.1.3 Positional Attention</h3><p>作者还在每个<em>decoder layer</em>中加入了<em>positional attention</em>，使得模型获得序列位置向量的能力更强了。而所谓的<em>positional attention</em>顾名思义就是<em>positional encoding + attention</em>，分别对应<em>transformer</em>中的<em>positional encoding</em>和<em>multi-head attention</em>：</p><script type="math/tex; mode=display">p(j, k) = \sin(j/10000^{k/d}), k为偶数</script><script type="math/tex; mode=display">p(j,k) = \cos(j/10000^{k/d}), k为奇数</script><script type="math/tex; mode=display">Attention(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_{model}}})V</script><h2 id="2-2-Modeling-Fertility-解决多模态问题"><a href="#2-2-Modeling-Fertility-解决多模态问题" class="headerlink" title="2.2 Modeling Fertility 解决多模态问题"></a>2.2 Modeling Fertility 解决多模态问题</h2><p>本文提出使用隐变量（<em>Latent Variable</em>）方法来解决这一问题。具体的想法如下图</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/1adb5756acb8ee4ffb980d2b895080ae0ff905.png" alt></p><p>左边代表可能的各种可能的组合元素，所有元素可以两两组合，也可以三三组合等等可以以任意方式组合，因为对于<em>NA</em>来说词与词之间存在一个独立假设。我们可以从大的组合集中进行采样得到小的组合集（如图中蓝色圈圈画出来的小样本集合），这样模型就相当于在一个小的样本空间中进行建模，这样就可以强化模型的多模态处理能力。而我们把采样得到的样本空间称之为隐变量$z$，引入$z$后多模态的翻译分布则变成了：</p><script type="math/tex; mode=display">p_{NA}(Y|X;\theta) = \sum_z\left[ p_z(z|x_{1:T'};\theta)\cdot p_L(T|x_{1:T'};\theta;z)\cdot \Pi_{t=1}^{T}p(y_t|x_{1:T'};\theta;z)\right]</script><p>$z$需要满足以下条件：</p><ul><li>需要比较简单的从端到端的训练中获得；</li><li>$z$要尽可能的考虑到不同说出之间的关系，使得其余位置的输出尽可能条件独立性；</li><li>$z$要很容易从平行语料中推理得到，又不能信息量过于丰富让$p$显得无关紧要。</li></ul><p>所以，模型的关键是对$z$建模。本文中$z$就是<em>decoder</em>的输入——利用<em>Fertility</em>的思想从<em>encoder</em>输入中拷贝元素。所谓<em>fertility</em>指的是，源序列中每个元素会被翻译多少次。这种思想源于早期的统计机器翻译中，每个词被翻译的次数不同，则输出序列会不，比如<em>Thank</em>被翻译一次可能是<em>谢谢</em>，而如果翻译两次的话可能会输出<em>多谢</em>。每个词的翻译次数实际上是个隐藏的含义，并不具有显式意义，因此它是一种<em>Latent Variable</em>，可以用来表示一种<em>translation mode</em>。</p><p>因此根据<em>fertitlity</em>我们可以把方程写成：</p><script type="math/tex; mode=display">p_{NA}(Y|X;\theta) = \sum_{f_1, ..., f_T'\in F}\left( \Pi_{t'=1}^{T'}p_F(f_{t'}|x_{1:T'};\theta) \cdot \Pi_{t=1}^{T}p(y_t|x_1\{f_1\}, ...x_{T'}\{f_{T'};\theta\})\right)</script><p>其中$F=\{f_1, …, f_{T’} | \sum_{t’=1}^{T’}f_{t’}=T, f_{t’}\in \mathbb{Z^*}\}$。</p><p><em>Fertility</em>序列中每个词重复的次数通过一个<em>softmax</em>层预测。</p><h1 id="3-训练"><a href="#3-训练" class="headerlink" title="3. 训练"></a>3. 训练</h1><p>由于模型中引入了离散的隐变量，是的整个模型不能直接使用后向传播进行训练。作者引入了一个外部的对齐函数——<em>Fast-align</em>。<em>Fast-align</em>能够将输入输出进行词对齐，即目标序列中的每个词对应源序列中的相应的词。这样我们就可以得到一个外部的<em>Fertility</em>序列。我们可以用这个外部的<em>fertility</em>序列当成<em>fertility</em>的监督项，这样整个模型的损失项来源于两部分：<em>decoding</em>和<em>fertility</em>：</p><script type="math/tex; mode=display">L_{ML}= \log p_{NA}(Y|X;\theta) = \log \sum_{f_{1:T'}\in F} p_F(f_{1:T'}|x_{1:T'};\theta)\cdot p(y_{1:T}|x_{1:T'},f_{1:T'};\theta)</script><script type="math/tex; mode=display">L_{ML} \ge \mathbb{E}_{f_{1:T'}\sim q}(\underbrace{\sum_{t=1}^T\log(y_t|x_1\{f_1\}, ...,x_{T'}\{f_{T'}\};\theta)}_{\mathrm{Translation Loss}}+\underbrace{\sum_{t'=1}^{T'}\log p_F(f_{t'}|x_{1:T'};\theta)}_{\mathrm{Fertility~Loss}})+H(q)</script><p>其中$q$表示外部对齐函数估计的<em>Fertility</em>序列分布。</p><p>这样的话整个模型就相当于由两部分监督学习组成，就可以直接使用后向传播进行训练了。</p><h2 id="3-1-知识蒸馏"><a href="#3-1-知识蒸馏" class="headerlink" title="3.1 知识蒸馏"></a>3.1 知识蒸馏</h2><p>之前我们提到解决多模态问题有三种方法：增强模型处理多模态的能力，减少<em>mode</em>数，改变学习目标。其中增强模型处理多模态的能力是核心，前面我们已经介绍过了。下面我们介绍一下后两种方法，注意这几种方法结合使用，而不是分别使用，也就是说是在同一个模型中一起使用者三种方法，<em>fertility</em>是核心。</p><p>知识蒸馏其实蒸馏的就是<em>mode</em>，通过减少<em>mode</em>数，从而提升模型的学习效果。这里使用的方法是先用一个标准的<em>Transformer</em>学习一个模型，将这个标准<em>Transformer</em>的推理结果作为<em>NAT</em>的目标序列。</p><h2 id="3-2-Fune-Tuning"><a href="#3-2-Fune-Tuning" class="headerlink" title="3.2 Fune Tuning"></a>3.2 Fune Tuning</h2><p>前面我们提到使用<em>Maximum Likelihood</em>本身更倾向于学习概率更大的<em>mode</em>，因此作者在这里引入一个微调项，用来做目标纠正——<em>reverse K-L divergence</em>:</p><script type="math/tex; mode=display">L_{RKL}(f_{1:T'};\theta) = \sum_{t=1}^{T}\sum_{y_t} [\log p_{AR}(y_t|\hat{y}_{1:t},x_{1:T'})\cdot p_{NA}(y_t|x_{1:T'},f_{1:T'};\theta)]</script><p>其中$\hat{y}_{1:T}=G(x_{1:T’}, f_{1:T’};\theta)$，所以最终的损失函数为：</p><script type="math/tex; mode=display">L_{FT} = \lambda(\underbrace{\mathbb{E}_{f_{1:T'}\sim p_F}(L_{RKL}(f_{1:T'})-L_{RKL}(\overline{f_{1:T'}}))}_{L_{RL}}+\underbrace{\mathbb{E}_{f_{1:T'}}(L_{RKL}(f_{1:T'}))}_{L_{BP}})+(1-\lambda)L_{KD}</script><h1 id="4-推理"><a href="#4-推理" class="headerlink" title="4. 推理"></a>4. 推理</h1><p>推理阶段主要的任务是获取<em>fertility</em>序列，作者提出三种方法：</p><ul><li><strong>Argmax decoding</strong></li></ul><p>由于在模型训练阶段，<em>fertility</em>序列随着模型的训练一起呗训练了 ，因此我们可以直接使用$\mathrm{arg~max}$来得到<em>fertility</em>序列：</p><script type="math/tex; mode=display">\hat{Y}_{argmax} = G(x_{1:T'},\hat{f}_{1:T'};\theta),其中\hat{f}_{t'}=\mathrm{arg}\max p_F(f_{t'}|x_{1:T'};\theta)</script><ul><li><strong>Average decoding</strong></li></ul><p>也可以通过求对应的<em>softmax</em>的期望来得到<em>fertility</em>序列：</p><script type="math/tex; mode=display">\hat{Y}_{average} = G(x_{1:T'}, \hat{f}_{1:T'};\theta), 其中\hat{f_{t'}}=Round(\sum_{f_{t'}}^{L}p_F(f_{t'}|x_{1:T'};\theta)f_{t'})</script><ul><li><strong>Noisy parallel decoding (NPD)</strong></li></ul><script type="math/tex; mode=display">\hat{Y}_{NPD} = G(x_{1:T'},\mathrm{arg}\max_{f_{t'}\sim p_F} p_{AR}(G(x_{1:T'}, f_{1:T'};\theta)|X;\theta);\theta)</script><p>最后使用一个之前在做知识蒸馏的时候训练好的<em>Transformer</em>对输出的句子进行<em>re-ranking</em>，得到一个最佳的翻译结果：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/6a37db1447abd4218ebf12c1f19c8d3f0009f3.png" alt></p><h1 id="5-实验结果"><a href="#5-实验结果" class="headerlink" title="5. 实验结果"></a>5. 实验结果</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/eb257dc95fc7fc9bea69525e523c9e8cfe5bd8.png" alt></p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/175160ea64dacce506c4413783ff5ac0c4428b.png" alt></p><h1 id="6-参考资料"><a href="#6-参考资料" class="headerlink" title="6. 参考资料"></a>6. 参考资料</h1><ol><li><p><a href="http://arxiv.org/abs/1711.02281" target="_blank" rel="noopener">Non-autoregressive neural machine translation</a> <em>Gu et al. 2018</em></p></li><li><p><a href="https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/79547509" target="_blank" rel="noopener">直播实录 | 非自回归神经机器翻译 + ICLR 2018 论文解读</a></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> NMT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer家族之Latent Transformer</title>
      <link href="/2019/09/25/transformer%E5%AE%B6%E6%97%8F-latent/"/>
      <url>/2019/09/25/transformer%E5%AE%B6%E6%97%8F-latent/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5396ee05ly1g5pqn3ch6zj20u092znph.jpg" alt></p><p>之前提到<em>Auto-regression</em>的decoding方法使得<em>transformer</em>在推理上的表现很慢，所以很多研究者在这方面做了很多研究，本文就介绍一个使用<em>Non-Auto Regression</em>的方法——<strong>Discrete Latent Variable</strong>。该方法与<em>Auto-regression</em>方法相比，效果上要稍差 一些，但是取得了比其他<em>Non-auto regression</em>方法都好的结果，而效率上也有很大的提升。</p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><h2 id="1-1-Auto-Regression"><a href="#1-1-Auto-Regression" class="headerlink" title="1.1 Auto-Regression"></a>1.1 Auto-Regression</h2><p><em>RNN</em>在机器翻译领域有着非常重要的应用，但是它本身由于不能进行并行计算，限制了它的效率，所以后来有些研究者希望能用<em>CNN</em>替代<em>RNN</em>。而<em>Transformer</em>的横空出世，使得机器翻译在训练效果和效率上都上了一个台阶，但是仍然存在一个问题。</p><p><em>Transformer</em>在生成一个序列的时候，通常需要根据之前的序列来预测下一个词，即当预测$y_n$时，需要利用$y_1, y_2, …, y_{n-1}$作为模型的输入。所以<em>transformer</em>在生成序列的时候是一个词一个词的生成，每生成一个词就需要进行一次推理，因此造成效率很低。这也就是所谓的<em>Auto-regression</em>问题。而<em>transformer</em>的<em>auto-regression</em>问题比<em>RNN</em>和<em>CNN</em>更加严重，因为<em>RNN</em>是根据前一个状态预测下一个状态，<em>CNN</em>是根据前<em>K</em>（kernel大小）个状态预测下一个状态，而<em>transformer</em>则是利用之前的所有状态预测下一个状态。虽然<em>transformer</em>在训练的时候可以很高效的训练，这是因为训练时的输出序列都已知，所以不需要<em>auto-regression</em>；但在进行decoding的时候输出是未知的，必须进行<em>auto-regression</em>，所以效率反而更低。</p><h2 id="1-2-Latent-Transformer"><a href="#1-2-Latent-Transformer" class="headerlink" title="1.2 Latent Transformer"></a>1.2 Latent Transformer</h2><p>为了克服<em>Auto-regression</em>问题，<a href="https://arxiv.org/pdf/1803.03382.pdf" target="_blank" rel="noopener">Kaiser et al. 2018</a>提出使用离散隐变量方法加速decoding推理。这种方法算不上真正解决了<em>Auto-regression</em>问题，但是算是对问题进行了优化吧，或者应该叫做<em>Semi-auto regression</em>。</p><p>这种方法简而言之就是，先用<em>auto-regression</em>生成一个固定长度的短的序列$l= \{l_1, l_2, …, l_m\}$，其中$m&lt;n$，然后再用$l$并行生成$y = \{y_1, y_2, …, y_n\}$。为了实现这种方法，我们需要变分自编码器。由于句子序列是离散的序列，在使用离散隐变量的时候会遇到不可求导的问题，因此如何解决这个问题就需要一些离散化的技术了。</p><h1 id="2-离散化技术"><a href="#2-离散化技术" class="headerlink" title="2. 离散化技术"></a>2. 离散化技术</h1><p>我们主要介绍四种离散化技术：</p><ul><li>Gumbel-softmax (<a href="http://arxiv.org/abs/1611.01144" target="_blank" rel="noopener">Jang et al., 2016;</a> <a href="https://arxiv.org/abs/1611.00712" target="_blank" rel="noopener">Maddison et al., 2016</a>)</li><li>Improved Semantic Hashing (<a href="https://arxiv.org/abs/1801.09797" target="_blank" rel="noopener">Kaiser &amp; Bengio, 2018</a>)</li><li>VQ-VAE (<a href="http://arxiv.org/abs/1711.00937" target="_blank" rel="noopener">van den Oord et al., 2017</a>)</li><li>Decomposed Vector Quantization</li></ul><p>给定目标序列$y=\{y_1, y_2, …, y_n\}$，将$y$输入到一个编码器（自编码器中的编码器，并非机器翻译模型中的编码器，下文的解码器同理，如非特殊说明<em>encoder</em>和<em>decoder</em>指的都是自编码器中的编码器和解码器）中产生一个隐变量表示$enc(y) \in \mathbb{R}^D$，其中$D$是隐变量空间的维度。令$K$为隐变量空间的大小，$[K]$表示集合$\{1, 2, …, K\}$。将连续隐变量$enc(y)$传入到一个<em>discretization bottleneck</em>中产生离散隐变量$z_d(y) \in [K]$，然后输入$z_q(y)$到解码器$dec$中。对于整数$i, m$我们使用$\tau_m(i)$代表用$m$ bits表示的二进制$i$，即用$\tau_m^{-1}$将$i$从二进制转换成 十进制。</p><p>下面我们主要介绍<em>discretization bottleneck</em>涉及到的离散化技术。</p><blockquote><p>实际上离散化技术是一个在VAE、GAN、RL中都有很重要应用的技术，本文只简单介绍它在文本生成方向的应用，而涉及到技术细节以及数学原理等更加详细的内容，以后会专门讨论，这里只说怎么用不说为什么。</p></blockquote><h2 id="2-1-Gumbel-Softmax"><a href="#2-1-Gumbel-Softmax" class="headerlink" title="2.1 Gumbel-Softmax"></a>2.1 Gumbel-Softmax</h2><p>将连续隐变量$enc(y)$变成离散隐变量的方法如下：</p><script type="math/tex; mode=display">l = W enc(y) , W \in \mathbb{R}^{K\times D}</script><script type="math/tex; mode=display">z_d(y) = \mathrm{arg} \max_{i\in[K]}~ l_i</script><ul><li>评估和推理时</li></ul><script type="math/tex; mode=display">z_q(y) = e_j</script><p>其中$e \in \mathbb{R}^{K \times D}$，类似词向量的查询矩阵；$j=z_d(y)$。这一步相当于编码器生成一个短句子序列，然后这个短句子序列作为解码器的输入，通过查询词向量矩阵将句子中的词变成向量。</p><ul><li>训练时</li></ul><p>使用<em>Gumbel-softmax</em>采样生成$g_1, g_2, …, g_K$个独立同分布的<em>Gumbel</em>分布样本：</p><script type="math/tex; mode=display">g_i \sim -\log(-\log(u))</script><p>其中$u \sim U(0,1)$表示均匀分布。然后用下式计算<em>softmax</em>得到$w \in \mathbb{R}^K$:</p><script type="math/tex; mode=display">w_i = \frac{\exp((l_i+g_i)/\tau)}{\sum_i\exp((l_i+g_i)/\tau)}</script><p>得到$w$以后我们就可以简单地用：</p><script type="math/tex; mode=display">z_q(y) = we</script><p>来获得$z_q(y)$。</p><p>注意<em>Gumbel-softmax</em>是可导的，也就是说我们可以直接通过后向传播对模型进行训练。</p><h2 id="2-2-Improved-Semantic-Hashing"><a href="#2-2-Improved-Semantic-Hashing" class="headerlink" title="2.2 Improved Semantic Hashing"></a>2.2 Improved Semantic Hashing</h2><p><em>Improved Semantic Hashing</em>主要来源于<a href="https://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf" target="_blank" rel="noopener">Salakhutdinov &amp; Hinton, 2009</a>提出的<em>Semantic Hahsing</em>算法。</p><script type="math/tex; mode=display">\sigma'(x) = \max(0, \min(1, 1.2\sigma(x)-0.1))</script><p>这个公式称为<em>饱和sigmoid</em>函数（<a href="https://arxiv.org/pdf/1511.08228.pdf" target="_blank" rel="noopener">Kaiser &amp; Sutskever, 2016;</a> <a href="https://papers.nips.cc/paper/6295-can-active-memory-replace-attention.pdf" target="_blank" rel="noopener">Kaiser &amp; Bengio, 2016</a>），</p><ul><li>训练时</li></ul><p>在$z_e(y) = enc(y)$中加入高斯噪声$\eta \sim \mathcal{N}(0,1)^D$，然后传入给饱和sigmoid函数</p><script type="math/tex; mode=display">f_e(y) = \sigma'(z_e(y) + \eta)</script><p>使用下式将$f_e(y)$进行离散化：</p><p><img src="https://img.vim-cn.com/98/5790fa5e2da74037e22462e5dfb60e07035bd8.png" alt></p><p>解码器的输入用两个嵌入矩阵计算$e^1, e^2 \in \mathbb{R}^{K \times D}$：</p><script type="math/tex; mode=display">z_q(y) = e^1_{h_{e(y)}}+e^2_{1-h_{e(y)}}</script><p>其中$h_{e}$是从$f_e$或者$g_e$中随机选择的。</p><ul><li>推理时</li></ul><p>令$f_e=g_e$</p><h2 id="2-3-Vector-Quantization"><a href="#2-3-Vector-Quantization" class="headerlink" title="2.3 Vector Quantization"></a>2.3 Vector Quantization</h2><p><em>Vector Quantized - Variational Autoencoder (VQ-VAE)</em>是<a href="http://arxiv.org/abs/1711.00937" target="_blank" rel="noopener">van denOord et al., 2017</a>提出的一种离散化方法。<em>VQ-VAE</em>的基本方法是使用最近邻查找矩阵$e \in \mathbb{R}^{K\times D}$将$enc(y)$进行数值量化。具体方法如下：</p><script type="math/tex; mode=display">z_q = e_k, k=\mathrm{arg} \min_{j\in [K]} \|enc(y) -e_j \|_2</script><p>对应的离散化隐变量$z_d(y)$是$e$矩阵中与$enc(y)$距离$k$索引最近的值。损失函数定义如下：</p><script type="math/tex; mode=display">L = l_r +\beta\|enc{y}-sg(z_q(y)) \|_2</script><p>其中$sg(\cdot)$定义如下：</p><p><img src="https://img.vim-cn.com/cd/12fa252851f7de219a8e5f3334759406d5a27c.png" alt></p><p>$l_r$即为给定$z_q(y)$后模型的损失（比如交叉熵损失等）。</p><p>使用下面两个步骤获得<em>exponential moving average (EMA)</em>：</p><ol><li>每个$j \in [K]$都用$e_j$；</li><li>统计编码器隐状态中使用$e_j$作为最近邻量化的个数$c_j$。</li></ol><p>$c_j$的更新方法如下：</p><script type="math/tex; mode=display">c_j \leftarrow \lambda c_j+(1-\lambda)\sum_l 1[z_q(y_l)=e_j]</script><p>然后对$e_j$进行更新：</p><script type="math/tex; mode=display">e_j \leftarrow \lambda e_j +(1+\lambda)\sum_l \frac{1[z_q(y_l)=e_j]enc(y_l)}{c_j}</script><p>其中$1[\cdot]$是一个指示函数，$\lambda$是延迟参数，实验中设置为$0.999$。</p><h2 id="2-4-Decomposed-Vector-Quantization"><a href="#2-4-Decomposed-Vector-Quantization" class="headerlink" title="2.4 Decomposed Vector Quantization"></a>2.4 Decomposed Vector Quantization</h2><p>当离散隐变量空间很大的时候<em>VQ-VAE</em>会有一个问题——<em>index collapse</em>：由于“富人越富，穷人越穷”效应，只有少数的嵌入向量能得到训练。</p><p>具体来说就是如果一个嵌入向量$e_j$距离很多编码器的输出$enc(y_1), enc(y_2), …, enc(y_i)$都很近，那么它就能通过上面$c_j$和$e_j$的更新更加靠近，到最后只有少数几个嵌入向量被用到。因此，本文提出了一个<em>VQ-VAE</em>的变种——<em>DVQ</em>使$K$值很大的时候也能做到充分利用嵌入向量。</p><h3 id="2-4-1-Sliced-Vector-Quantization"><a href="#2-4-1-Sliced-Vector-Quantization" class="headerlink" title="2.4.1 Sliced Vector Quantization"></a>2.4.1 Sliced Vector Quantization</h3><p><em>Sliced vector quantization</em>顾名思义，就是将$enc(y)$切成$n_d$个小的切片：</p><script type="math/tex; mode=display">enc^1(y)\odot enc^2(y)...\odot enc^{n_d}(y)</script><p>其中每一个$enc(y)$的维度为$D/N_d$，$\odot$表示拼接。</p><h3 id="2-4-2-Projected-Vector-Quantization"><a href="#2-4-2-Projected-Vector-Quantization" class="headerlink" title="2.4.2 Projected Vector Quantization"></a>2.4.2 Projected Vector Quantization</h3><p>另一个方法是，使用固定的随机初始化投影集合：</p><script type="math/tex; mode=display">\{ \pi^i \in \mathbb{R}^{D\times D/n_d} | i \in [n_d]\}</script><p>将$enc(y)$投影到$R^{D/n_d}$的向量空间中去。</p><h1 id="3-Latent-Transformer"><a href="#3-Latent-Transformer" class="headerlink" title="3. Latent Transformer"></a>3. Latent Transformer</h1><p>介绍了这么多离散化的技术，下面就需要将这些离散化的技术应用到模型中去。给定输入输出序列对：$(x, y) = (x_1, x_2, …, x_k, y_1, y_2, …, y_n)$，<em>Latent Transformer</em>包含下面三个部分：</p><ul><li>$ae(y, x)$函数用来对$y$进行编码成$l=l_1, l_2, …, l_m$；</li><li>使用<em>Transformer</em> （即$lp(x)$）对$l$进行预测</li><li>$ad(l, x)$函数并行化产生$y$</li></ul><p>损失函数分成两部分：</p><ol><li>$l_r = compare(ad(ae(y,x), x), y)$;</li><li>$l = compare(ae(y, x), lp(x))$</li></ol><script type="math/tex; mode=display">L = l_r + l</script><h3 id="3-1-ae-y-x-函数"><a href="#3-1-ae-y-x-函数" class="headerlink" title="3.1 $ae(y,x)$函数"></a>3.1 $ae(y,x)$函数</h3><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/a8426346afc84ad83df607d7bfecdbd1be2f6b.png" alt></p><p>结构如图。其中<em>bottleneck</em>即为上面介绍的各种离散隐变量的方法。</p><h3 id="3-2-ad-y-x-函数"><a href="#3-2-ad-y-x-函数" class="headerlink" title="3.2 $ad(y, x)$函数"></a>3.2 $ad(y, x)$函数</h3><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/a21145d923a21d55c75575909194dd3d355f1f.png" alt></p><p>结构如图。</p><h1 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4. 实验结果"></a>4. 实验结果</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/8dcc671f32187d72fd9f063da5bbeeb5eee9ee.png" alt></p><p>图中作为<em>baseline</em>的<em>NAT</em>是<a href="http://arxiv.org/abs/1711.02281" target="_blank" rel="noopener">Gu et al. 2017</a>另一种<em>Non-auto regression</em>的方法。</p><h1 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5. 参考资料"></a>5. 参考资料</h1><ol><li><p><a href="https://arxiv.org/pdf/1803.03382.pdf" target="_blank" rel="noopener">Fast Decoding in Sequence Models Using Discrete Latent Variables</a> Kaiser et al., 2018</p></li><li><p><a href="http://arxiv.org/abs/1611.01144" target="_blank" rel="noopener">Categorical reparameterization with gumbel-softmax</a> Jang et al. 2016</p></li><li><p><a href="http://arxiv.org/abs/1611.00712" target="_blank" rel="noopener">The concrete distribution: A continuous relaxation of discrete random variables</a> Maddison et al., 2016</p></li><li><p><a href="https://arxiv.org/abs/1610.08613" target="_blank" rel="noopener">Can active memory replace attention?</a> Kaiser, Łukasz and Bengio, Samy. 2016</p></li><li><a href="http://arxiv.org/abs/1801.09797" target="_blank" rel="noopener">Discrete autoencoders for sequence models</a> Kaiser, Łukasz and Bengio, Samy. 2018</li><li><a href="https://arxiv.org/abs/1511.08228" target="_blank" rel="noopener">Neural GPUs learn algorithms</a> Kaiser, Łukasz and Sutskever, Ilya. 2016</li><li><a href="http://arxiv.org/abs/1711.02281" target="_blank" rel="noopener">Non-autoregressive neural machine translation</a> Gu et al., 2017</li><li><a href="http://arxiv.org/abs/1711.00937" target="_blank" rel="noopener">Neural discrete representation learning</a> van den Oord et al., 2017</li><li><a href="https://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf" target="_blank" rel="noopener"> Semantic hashing</a> Salakhutdinov, Ruslan and Hinton, Geoffrey E. 2009</li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> NMT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer家族之Average Attention Network</title>
      <link href="/2019/09/24/transformer%E5%AE%B6%E6%97%8F-average/"/>
      <url>/2019/09/24/transformer%E5%AE%B6%E6%97%8F-average/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5396ee05ly1g5pqn3ch6zj20u092znph.jpg" alt></p><p><em>Transformer</em>虽然在训练上比<em>RNN</em>和<em>CNN</em>快，但是在做推理（<em>decoding</em>）的时候由于采用的是<em>Auto-regression</em>不能做到并行计算，所以速度很慢（甚至可能比纯<em>RNN</em>还要慢），所以针对这种情况很多研究者提出了<em>decoding</em>时也能采用并行计算的改进方案，下面要介绍的这个<em>transformer</em>大家族的以为成员就是其中之一：<strong>Average Attention Network</strong>。</p><a id="more"></a><h1 id="1-Average-Attention-Network"><a href="#1-Average-Attention-Network" class="headerlink" title="1. Average Attention Network"></a>1. Average Attention Network</h1><p>这篇论文作者认为造成<em>transformer</em>在decoding的过程慢的原因有两个：</p><ul><li>Auto-regression</li><li>self-attention</li></ul><p>是的，作者认为在<em>decoder</em>中，对目标句子编码时计算目标句子内部依赖关系的时候使用自注意力机制会造成解码速慢，因此作者提出使用<em>AAN</em>进行代替。</p><h2 id="1-1-模型结构"><a href="#1-1-模型结构" class="headerlink" title="1.1 模型结构"></a>1.1 模型结构</h2><p><img src="https://img.vim-cn.com/04/c4f0be76f3145e451270ec8d3e0fa91f7c5a4c.png" alt></p><p>从模型结构图中可以看到，基本结构和<em>Transformer</em>基本相同，唯一不同的点在于<em>decoder layer</em>中<em>Masked Multi-head attention</em>变成了<em>Average Attention</em>。那么下面我们就来看看这个<em>Average Attention</em>是何方神圣？</p><h2 id="1-2-Average-Attention"><a href="#1-2-Average-Attention" class="headerlink" title="1.2 Average Attention"></a>1.2 Average Attention</h2><p><img src="https://img.vim-cn.com/fd/334ec556af95178141cbbc11f4f0b63124c3f7.png" alt></p><p>结构如图所示，给定输入$\mathbf{y = \{y_1, y_2, …, y_m\}}$</p><ul><li><em>AAN</em>首先计算累加平均：</li></ul><script type="math/tex; mode=display">\overline{\mathbf{y}_j} = \frac{1}{j} \sum_{k=1}^j \mathbf{y}_k</script><p>假设模型auto-regression产生了$j=3$个词，“我，喜欢， 打”，对其中每个词进行累加平均得（average(我), average(我+喜欢), average(我+喜欢+打)）。</p><ul><li>然后经过<em>FFN</em>层进行线性变换，其中<em>FFN</em>层即<em>Point wise feeed forward</em>：</li></ul><script type="math/tex; mode=display">\mathbf{g}_j = \mathrm{FFN}(\overline{\mathbf{y}_j})</script><p>这两部虽然很简单，但是却是<em>AAN</em>中最核心的部分:</p><ol><li>每个位置上的向量都是通过之前的词计算得来，所以词与词之间并非独立的，而是存在依赖关系的；</li><li>无论输入向量有多长，之前的词都被融合进同一个向量中，也就是说词与词之间的距离是不变的，这就保证了<em>AAN</em>可以获得长距离依赖关系。</li></ol><blockquote><p>注意：在作者提供的源码中，FFN层是可以跳过的，即计算出平均值以后不经过FFN层，直接进行接下来的计算。</p></blockquote><ul><li>拼接原始的输入和累加平衡后的输出</li></ul><script type="math/tex; mode=display">c = \mathrm{Concat}(\mathbf{y}_j, \mathbf{g}_j)</script><ul><li>由于后面加入了<em>LSTM</em>的遗忘门机制， 因此这里先计算各个信息流所占的比例：</li></ul><script type="math/tex; mode=display">\mathbf{i}_j, \mathbf{f}_j = \sigma(Wc)</script><p>其中，$\mathbf{i}_j$表示原始输入在接下来的信息流中所占的比例，$\mathbf{f}_j$表示在接下来的信息流中<em>Average</em>所占的比重。</p><ul><li>遗忘门</li></ul><script type="math/tex; mode=display">\widetilde{\mathbf{h}}_j = \mathbf{i}_j \odot \mathbf{y}_j + \mathbf{f}_j \odot \mathbf{g}_j</script><ul><li>残差连接</li></ul><script type="math/tex; mode=display">\mathbf{h}_j = \mathrm{LayerNorm}(\mathbf{y}_j + \widetilde{\mathbf{h}}_j)</script><p>至此，整个<em>AAN</em>就计算完成了，这也是<em>original AAN</em>。</p><h2 id="1-2-Masked-ANN"><a href="#1-2-Masked-ANN" class="headerlink" title="1.2 Masked ANN"></a>1.2 Masked ANN</h2><p>之前我们介绍<em>origin AAN</em>时说到求累加平均的时候举了个例子，假设我们想要预测“我非常喜欢打篮球”， 在auto-regression时：</p><p>输入：$(我, 非常, 喜欢, 打)$</p><p>计算累加平均的时候要分别计算：average(我)，average(非常)，average（喜欢），average(打)。也就是说我们要多次计算平均值，这样就不能实现并行化计算，所以我们希望能通过矩阵一次性求出这三个平均向量：</p><script type="math/tex; mode=display">\left\{\begin{array}{cccc}1 & 0 & 0 & 0\\\\1/2 & 1/2 & 0 & 0 \\\\1/3 & 1/3 & 1/3 & 0 \\\\1/4 & 1/4 & 1/4 & 1/4\end{array}\right\} \times \left( \begin{array}{c}y_1 \\\\y_2 \\\\y_3 \\\\y_4\end{array}\right) =\left( \begin{array}{c}y_1 \\\\\frac{y_1+y_2}{2} \\\\\frac{y_1+y_2+y_3}{3} \\\\\frac{y_1+y_2+y_3+y_4}{4}\end{array}\right)</script><h2 id="1-3-Decoding-Acceleration"><a href="#1-3-Decoding-Acceleration" class="headerlink" title="1.3 Decoding Acceleration"></a>1.3 Decoding Acceleration</h2><p>不同于<em>transformer</em>中的自注意力机制，<em>AAN</em>可以以非常快的速度进行推理：</p><script type="math/tex; mode=display">\widetilde{\mathbf{g}}_j = \widetilde{\mathbf{g}}_{j-1} + \mathbf{y}_j \\\\\mathbf{g}_j = \mathrm{FFN}(\frac{\widetilde{\mathbf{g}}_j}{j})</script><p>即，在进行<em>auto-regression</em>的时候每次都只需要用之前的结果与本次的输入相加，然后求平均即可。注意$\widetilde{\mathbf{g}}_0=0$。我们只需要根据前一次的状态就可以确定当前的状态，而不需要像自注意力机制那样依赖之前所有的状态。</p><p>至此关于<em>AAN</em>的部分我们就介绍完了，模型其他部分的结构和<em>transformer</em>保持一致，在作者的实验中，虽然<em>BLEU</em>值较<em>transformer</em>略微降低，但是推理效率上提升很大， 尤其是对长句。</p><h1 id="2-实验结果"><a href="#2-实验结果" class="headerlink" title="2. 实验结果"></a>2. 实验结果</h1><p><img src="https://img.vim-cn.com/2e/bec247df017d256ee656ca350cf04e57aeb2e2.png" alt></p><h1 id="3-核心代码"><a href="#3-核心代码" class="headerlink" title="3. 核心代码"></a>3. 核心代码</h1><h2 id="3-1-pytorch"><a href="#3-1-pytorch" class="headerlink" title="3.1 pytorch"></a>3.1 pytorch</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AAN</span><span class="params">(nn.Modules)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(...)</span>:</span></span><br><span class="line">        super(AAN, self).__init__()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(...)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h2 id="3-2-tensorflow"><a href="#3-2-tensorflow" class="headerlink" title="3.2 tensorflow"></a>3.2 tensorflow</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AAN</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(...)</span>:</span></span><br><span class="line">        super(AAN, self).__init__()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(...)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h1 id="4-参考资料"><a href="#4-参考资料" class="headerlink" title="4. 参考资料"></a>4. 参考资料</h1><p><a href="https://arxiv.org/pdf/1805.00631.pdf" target="_blank" rel="noopener">Accelerating Neural Transformer via an Average Attention Network</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> NMT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer家族之Weighted Transformer</title>
      <link href="/2019/09/19/transformer%E5%AE%B6%E6%97%8F-weighted/"/>
      <url>/2019/09/19/transformer%E5%AE%B6%E6%97%8F-weighted/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5396ee05ly1g5pqn3ch6zj20u092znph.jpg" alt></p><p>之前我们介绍了擎天柱的工作原理以及内部构造。对擎天柱已经有了深入的了解，那么本文就来介绍一下汽车人家族中的其他成员——Transformer的各种变种。</p><a id="more"></a><h1 id="1-Weighted-Transformer"><a href="#1-Weighted-Transformer" class="headerlink" title="1. Weighted Transformer"></a>1. Weighted Transformer</h1><!--more--><h1 id="1-Weighted-Transformer-1"><a href="#1-Weighted-Transformer-1" class="headerlink" title="1. Weighted Transformer"></a>1. Weighted Transformer</h1><p>为了更快的训练和更好的发挥<em>Transformer</em>的信息表示能力，<a href="https://arxiv.org/pdf/1711.02132.pdf" target="_blank" rel="noopener">Ahmed et al. 2017</a>提出了这种新的结构。</p><h2 id="1-1-模型结构"><a href="#1-1-模型结构" class="headerlink" title="1.1 模型结构"></a>1.1 模型结构</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5ff2618d07150a95ff4f3dc544418284e574c2.png" alt></p><p>模型在整体结构上和<em>Transformer</em>差不多，不同点有两个：</p><ul><li>使用<em>Multi-branch</em>代替<em>Multi-Head</em>；</li><li>在<em>FFN</em>上不是直接线性转换，而是<em>Multi-branch</em>线性转换后加权求和。</li></ul><p>公式如下：</p><script type="math/tex; mode=display">head_i = Attention(QW_i^Q, KW_I^K, VW_I^V)</script><script type="math/tex; mode=display">\overline{head_i} = head_i W^{O_i} \times \kappa_i</script><script type="math/tex; mode=display">BranchedAttention(Q, K, V) = \sum_{i=1}^M \alpha_i \mathrm{FFN}_i(\overline{head_i})</script><h2 id="1-2-Multi-branch-Attention"><a href="#1-2-Multi-branch-Attention" class="headerlink" title="1.2 Multi-branch Attention"></a>1.2 Multi-branch Attention</h2><p>在<em>Weighted Transformer</em>中对<em>Attention</em>的计算和标准的<em>Transformer</em>计算过程是一致的，所以这里不做介绍。接下来对计算完的<em>scaled dot-product attention</em>的处理上，模型就在原始<em>Transformer</em>上做了修改。作为对比，我们把原始的<em>Transformer</em>在这一步的处理也列出来：</p><script type="math/tex; mode=display">\overline{head_i} = head_iW^{Q_i}</script><p><em>Transformer</em>是直接将<em>heads</em>进行线性变换，而<em>Weighted transformer</em>在对每个<em>head</em>进行线性变换后还乘上一个$\kappa$参数，这个参数是可训练的，而且必须满足条件：$\sum_i \kappa_i =1$。这个参数作者称之为<em>concatenation weight</em>。</p><p>我们知道<em>Multi-head</em>中的每一个<em>head</em>的作用是学习句子的不同信息，<em>Transformer</em>认为每个<em>head</em>学到的信息对任务来说是平权的，因此直接将多个<em>head</em>直接等权拼接，然后线性变换。而<em>Weighted transformer</em>认为每个<em>head</em>对任务的作用是不同的，因此为每个<em>head</em>分配一个权重，用于表明这个<em>head</em>对任务的重要性，而权重的大小令模型自动从任务中学习。这种假设显然应该比<em>Transformer</em>的平权假设要更加合理。</p><h2 id="1-3-Weighted-point-wise-feed-forward-network"><a href="#1-3-Weighted-point-wise-feed-forward-network" class="headerlink" title="1.3 Weighted point wise feed forward network"></a>1.3 Weighted point wise feed forward network</h2><p><del>这一部分我认为作者要么是对Transformer的理解有误，要么是论文的表述不准确，在对比Transformer和Weighted Transformer的时候有点小冲突，比如作者说Transformer对应的FFN公式是$BranchedAttention(Q, K, V)=\mathrm{FFN}(\sum_i^M \overline{head_i})$，先不纠结<em>BranchedAttention</em>的函数名问题，作者认为每个<em>head</em>是通过求和， 然后再经过FFN。但是<em>Transformer</em>原始论文写的很清楚<em>head</em>是通过<em>Concat</em>拼接在一起的，并非求和。造成作者在这里使用$\sum_i^M\overline{head_i}$，我个人猜测有两个可能的原因：</del></p><p><del>1. 作者使用$\sum$的意图其实是<em>Concat</em></del> </p><p><del>2.作者可能把Transformer结构图中Add当成了对head求和</del></p><p><del>无论什么原因，下面的介绍我都会替换成<em>Concat</em>。另外，作者介绍<em>Weighted transformer</em>的FFN的时候使用的也是$\sum$，但是从作者在其他的地方的表述来看，这里的求和应该指的也是<em>Concat</em>。比如作者将$\kappa$命名为<em>concatenation weight</em>，另外作者认为<em>weighted transformer</em>的参数只比<em>transformer</em>多了$\alpha$和$\kappa $，所以总的参数量应该是相同的，但是如果在<em>weighted transformer</em>中这一步使用了求和的话，假设$h=8, d_k=d_v=64$， 那么FFN的输出维度应该是（batch_size, seq_len, 64），而<em>Transformer</em>的输出维度是（batch_size, seq_len, 512），这样参数量是不同的， 除非在<em>weighted transformer</em>中作者令$d_k=d_v=512$，但是如果是这样的话，每个<em>head</em>的参数又不同了，所以无论如何<em>weighted trnasformer</em>和<em>transformer</em>的参数都是不同的。因此，我认为这里应该是<em>Concat</em>。</del></p><blockquote><p>刚开始的时候由于思考的不周全，以为是作者在论文中的表述不准确，所以自己瞎讨论半天，后来发现作者的表述没有任何问题，而是自己的问题，所以上面的内容只保留删除线，不把内容删除，用来提醒自己曾经犯过的错误。</p><p>这里解释一下为什么作者表述是正确的，而我的理解是错误的呢？首先说作者在描述<em>transformer</em>的时候用的公式$BranchedAttention(Q, K, V)=\mathrm{FFN}(\sum_i^M \overline{head_i})$，我之前认为原始论文中这里应该是<em>Concat</em>而不应该是$\sum$，但是我忽略了一点，就是在<em>transformer</em>原始论文中，是先进行<em>Concat</em>，这个时候输出<em>tensor.shape == (batch_size, seq_len, d_model)</em>，再进行线性变换的时候$W^{O_i}$的形状应该是<em>（d_model, d_model）</em>，所以FFN的输出是<em>(batch_size, seq_len, d_model)</em>。但是本文中是先进行的线性变换，我原先想的是线性变换的<em>tensor.shape == (batch_size, seq_len, d_v)</em>，而$W^{Q_i}.shape == (d_v, d_v)$，这样得到的输出形状是<em>(batch_size, seq_len, d_v)</em>，然后平权求和，如果是这样的话就会出现我上面的错误，缺少<em>Concat</em>和输出维数对应不上的问题。但实际上这里的$W^{Q_i}.shape == (d_v, d_{model})$，这样会输出$M$个形状为<em>(batch_size, seq_len, d_model)</em>的<em>tensor</em>（这就是$\overline{head_i}=head_iW^{O_i}$这一步做的事情），然后通过沿着head方向求和就可以得到一个形状为<em>(batch_size, seq_len, d_model)</em>的<em>tensor</em>（这就是$\mathrm{FFN}(\sum \overline{head_i})$这一步做的事情），实际上本文作者的操作和<em>transformer</em>的原始论文的操作是等效的。我的思考主要问题出现在了线性变换这一步的输出上。下面我们继续跟随作者的脚步，看下他在FFN上做了什么文章。</p></blockquote><p><em>Transformer</em>在计算<em>FFN</em>的过程如下：</p><script type="math/tex; mode=display">MultiHeadAttention(Q, K, V) = \mathrm{FFN}(\sum_{i=1}^M \overline{head_i})</script><p>可以看到两者的区别仍然是对不同<em>head</em>信息的加权方式不同，<em>transformer</em>仍然认为是平权的，但是<em>weighted transformer</em>认为是各有不同的权重，和$\kappa$一样，$\alpha$是从任务中学习的，且满足$\sum_i\alpha_i=1$。作者给$\alpha$取了一个名字叫做<em>addition weight</em>。</p><h1 id="2-模型细节"><a href="#2-模型细节" class="headerlink" title="2. 模型细节"></a>2. 模型细节</h1><p>除了以上两点修改以外，其他方面没有做任何修改。但是在训练的时候$\alpha$和$\kappa$的学习率由下式确定：</p><script type="math/tex; mode=display">lr = (d_{model}/N)^{-0.5}\cdot \min(steps^{-0.5}, steps \cdot 400^{-1.5})</script><p>也就是说将<em>warmup_steps</em>改成400。</p><h1 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3. 代码实现"></a>3. 代码实现</h1><h2 id="3-1-pytorch核心代码"><a href="#3-1-pytorch核心代码" class="headerlink" title="3.1 pytorch核心代码"></a>3.1 pytorch核心代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiBranchAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, depth, d_model, d_ff, n_branches, dropout)</span>:</span></span><br><span class="line">        super(MultiBranchAttention, self).__init__()</span><br><span class="line">        self.depth = depth</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.d_ff = d_ff</span><br><span class="line">        self.n_branches = n_branches</span><br><span class="line">        <span class="comment"># in practice, d_model == d_k * n_branches</span></span><br><span class="line">        <span class="keyword">assert</span> d_model == d_k * n_branches</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Q, K, V Linear</span></span><br><span class="line">        self.w_q = Linear([d_model, d_model])</span><br><span class="line">        self.w_k = Linear([d_model, d_model])</span><br><span class="line">        self.w_v = Linear([d_model, d_model])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># scaled dot-product attention</span></span><br><span class="line">        self.attentions = nn.ModuleList([</span><br><span class="line">            <span class="comment"># custom define</span></span><br><span class="line">            ScaledDotProductAttention(depth, dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_branches)</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># additional parameters for BranchedAttention</span></span><br><span class="line">        <span class="comment"># custom define</span></span><br><span class="line">        self.w_o = nn.ModuleList([Linear(depth, d_model) <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_branches)])</span><br><span class="line">        self.w_kp = torch.rand(n_branches)</span><br><span class="line">        self.w_kp = nn.Parameter(self.w_kp/self.w_kp.sum())</span><br><span class="line">        self.w_a = torch.rand(n_branches)</span><br><span class="line">        self.w_a = nn.Parameter(self.w_a/self.w_a.sum())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Position wise feed forward network</span></span><br><span class="line">        self.ffn = nn.ModuleList([</span><br><span class="line">            <span class="comment"># custom define</span></span><br><span class="line">            PositionwiseFeedForwardNetwork(d_model, d_ff//n_branches, dropout) </span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_branches)])</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="comment"># layer normalization</span></span><br><span class="line">        <span class="comment"># custom define</span></span><br><span class="line">        self.layer_norm = LayerNormalization(d_model)</span><br><span class="line"></span><br><span class="line">        init.xavier_normal(self.w_o)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, q, k, v, attn_mask)</span>:</span></span><br><span class="line">        <span class="comment"># q: (batch_size, len_q, d_model)</span></span><br><span class="line">        <span class="comment"># k: (batch_size, len_k, d_model)</span></span><br><span class="line">        <span class="comment"># v: (batch_size, len_v, d_model) note (len_k == len_v)</span></span><br><span class="line">        residual = q</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Linear</span></span><br><span class="line">        Q = self.w_q(q)  <span class="comment"># (batch_size, len_q, d_model)</span></span><br><span class="line">        K = self.w_k(k)  <span class="comment"># (batch_size, len_q, d_model)</span></span><br><span class="line">        V = self.w_v(v)  <span class="comment"># (batch_size, len_q, d_model)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># split</span></span><br><span class="line">        Qs = Q.split(self.depth, dim=<span class="number">-1</span>)  <span class="comment"># (b_size, len_q, depth) x n_branches</span></span><br><span class="line">        Ks = K.split(self.depth, dim=<span class="number">-1</span>)  <span class="comment"># (b_size, len_k, depth) x n_branches</span></span><br><span class="line">        Vs = V.split(self.depth, dim=<span class="number">-1</span>)  <span class="comment"># (b_size, len_v, depth) x n_branches</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># scaled dot product attention</span></span><br><span class="line">        <span class="comment"># scaled_attn: (batch_size, len_q, d_v) x n_branch</span></span><br><span class="line">        scaled_attn = [</span><br><span class="line">            attn(Qs[i], Ks[i], Vs[i], mask) <span class="keyword">for</span> i, attn <span class="keyword">in</span> enumerate(self.attentions)</span><br><span class="line">        ]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># multi-branch attention</span></span><br><span class="line">        <span class="comment"># outputs: (b_size, len_q, d_model) x n_branches </span></span><br><span class="line">        outputs = [self.w_o[i](scaled_attn[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_branches)]</span><br><span class="line">        outputs = [kappa * output <span class="keyword">for</span> kappa, output <span class="keyword">in</span> zip(self.w_kp, outputs)]</span><br><span class="line">        <span class="comment"># FFN</span></span><br><span class="line">        outputs = [ffn(output) <span class="keyword">for</span> ffn, output <span class="keyword">in</span> zip(self.ffn, outputs)]</span><br><span class="line">        outputs = [alpha * output <span class="keyword">for</span> alpha, output <span class="keyword">in</span> zip(self.w_a, outputs)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output: (b_size, len_q, d_model)</span></span><br><span class="line">        output = self.dropout(torch.stack(outputs).sum(dim=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> self.layer_norm(residual + output)</span><br></pre></td></tr></table></figure><h2 id="3-2-tensorflow核心代码"><a href="#3-2-tensorflow核心代码" class="headerlink" title="3.2 tensorflow核心代码"></a>3.2 tensorflow核心代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiBranchAttention</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement Multi-branch attention layer.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, depth, d_model, d_ff, n_branches, dropout)</span>:</span></span><br><span class="line">        super(MultiBranchAttention, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.depth = depth</span><br><span class="line">        self.d_model= d_model</span><br><span class="line">        self.d_ff = d_ff</span><br><span class="line">        self.n_branches = n_branches</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># K, Q, V, linear</span></span><br><span class="line">        self.wq = tf.keras.layers.Dense(d_model)</span><br><span class="line">        self.wk = tf.keras.layers.Dense(d_model)</span><br><span class="line">        self.wv = tf.keras.layers.Dense(d_model)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># scaled dot product attention</span></span><br><span class="line">        self.attentions = [</span><br><span class="line">            <span class="comment"># custom define</span></span><br><span class="line">            scaled_dot_product_attention(depth, dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_branches)</span><br><span class="line">        ]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># additional parameters for BranchedAttention</span></span><br><span class="line">        self.w_o = [tf.keras.layers.Dense(d_model) <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_branches)]</span><br><span class="line">        self.w_kp = np.random.random((n_branches,))</span><br><span class="line">        self.w_kp = tf.Variable(self.w_kp/self.w_kp.sum(), trainable)</span><br><span class="line">        self.w_a = np.random.random((n_branches,))</span><br><span class="line">        self.w_a = tf.Variable(self.w_a/self.w_a.sum(), trainable)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Position wise feed forward network</span></span><br><span class="line">        self.ffn = [</span><br><span class="line">            <span class="comment"># custom define</span></span><br><span class="line">            PositionwiseFeedForwardNetwork(d_model, d_ff//n_branches, dropout) </span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_branches)]</span><br><span class="line">        self.dropout = tf.keras.layers.Dropout(dropout)</span><br><span class="line">        <span class="comment"># layer normalization</span></span><br><span class="line">        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=<span class="number">1e-6</span>)</span><br><span class="line">        </span><br><span class="line">        self.dense = tf.keras.layers.Dense(d_model)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, q, k, v, mask)</span>:</span></span><br><span class="line">        residual = q</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># First linear transition step</span></span><br><span class="line">        Q = self.wq(q)  <span class="comment"># (batch_size, seq_len, d_model)</span></span><br><span class="line">        K = self.wk(k)  <span class="comment"># (batch_size, seq_len, d_model)</span></span><br><span class="line">        V = self.wv(v)  <span class="comment"># (batch_size, seq_len, d_model</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Split K, Q, V into multi-branch</span></span><br><span class="line">        Qs = tf.split(Q, n_branches, axes=<span class="number">-1</span>)  <span class="comment"># (batch_size, len_q, depth) x n_branches</span></span><br><span class="line">        Ks = tf.split(K, n_branches, axes=<span class="number">-1</span>)  <span class="comment"># (batch_size, len_k, depth) x n_branches</span></span><br><span class="line">        Vs = tf.split(V, n_branches, axes=<span class="number">-1</span>)  <span class="comment"># (batch_size, len_v, depth) x n_branches</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Scaled Dot-Product Attention step</span></span><br><span class="line">        <span class="comment"># head_i = Atteniton(QW_Q, KW_K, VW_V)</span></span><br><span class="line">        scaled_attention = [</span><br><span class="line">            attn(Qs[i], Ks[i], Vs[i], mask) <span class="keyword">for</span> i, attn <span class="keyword">in</span> enumerate(self.attentions)</span><br><span class="line">        ]</span><br><span class="line">        <span class="comment"># scaled_attention.shape == (batch_size, len_q, depth)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># multi-branch attention</span></span><br><span class="line">        <span class="comment"># outputs: (b_size, len_q, d_model) x n_branches </span></span><br><span class="line">        outputs = [self.w_o[i](scaled_attention[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_branches)]</span><br><span class="line">        outputs = [kappa * output <span class="keyword">for</span> kappa, output <span class="keyword">in</span> zip(self.w_kp, outputs)]</span><br><span class="line">        <span class="comment"># FFN</span></span><br><span class="line">        outputs = [ffn(output) <span class="keyword">for</span> ffn, output <span class="keyword">in</span> zip(self.ffn, outputs)]</span><br><span class="line">        outputs = [alpha * output <span class="keyword">for</span> alpha, output <span class="keyword">in</span> zip(self.w_a, outputs)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output: (b_size, len_q, d_model)</span></span><br><span class="line">        output = self.dropout(tf.stack(outputs).sum(dim=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> self.layer_norm(residual + output)</span><br></pre></td></tr></table></figure><h1 id="4-参考资料"><a href="#4-参考资料" class="headerlink" title="4. 参考资料"></a>4. 参考资料</h1><ol><li><a href="https://arxiv.org/pdf/1711.02132.pdf" target="_blank" rel="noopener">Weighted Transformer Network for Machine Translation</a>, Ahmed et al.,  arxiv 2017 </li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> NMT </tag>
            
            <tag> weighted-head </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer的每一个编码层都学到了什么？</title>
      <link href="/2019/09/18/transformer%E7%BC%96%E7%A0%81%E5%B1%82%E8%A1%A8%E7%A4%BA/"/>
      <url>/2019/09/18/transformer%E7%BC%96%E7%A0%81%E5%B1%82%E8%A1%A8%E7%A4%BA/</url>
      
        <content type="html"><![CDATA[<p><em>Transformer</em>现在已经被广泛应用于NLP领域的各项任务中，并且都取得了非常好的效果。其核心层使用了自注意力机制，关于为什么使用自注意力机制，作者提出了三点原因：</p><a id="more"></a><ul><li>计算复杂度：<em>Transformer</em>的计算复杂度比<em>RNN</em>和<em>CNN</em>都要低</li><li>并行计算：<em>Transformer</em>可以进行并行计算，这也是作者提出<em>Transformer</em>模型的初衷</li><li>远距离长程依赖的路径距离：<em>Transformer</em>有更短的路径距离，因此更容易学习到远程的依赖关系。</li></ul><p>前两个原因我们不做过多的介绍，只要仔细思考就可以理解，而且这两个是属于确定性问题，是可以通过理论分析得出的结论。但是第三点却是<em>Transformer</em>有效性的决定因素，而且无法进行理论分析（现在深度学习中的模型可解释性仍然是个研究热点），只有通过实验进行分析。本文就通过解读<a href="https://www.aclweb.org/anthology/W18-5431" target="_blank" rel="noopener">An Analysis of Encoder Representations in Transformer-Based Machine Translation</a>这篇论文来看下<em>Transformer</em>作者提出的第三点原因是否成立，并且深入理解<em>Transformer</em>每一层注意力都学到了什么。</p><p>本文通过不同方法分析了<em>encoder</em>层的注意力权重：</p><ul><li>可视化注意力权重</li><li>注意力权重的树结构生成</li><li>将<em>encoder</em>作为不同预测任务的输入</li><li>将其中一个<em>encoder</em>的知识迁移到另一个里面</li></ul><p>在研究<em>Transformer</em>中的注意力之前先训练一个<em>Transformer</em>模型，表1列出了训练模型的数据，表2列出了每组数据的<em>bleu</em>值：</p><p>表1：训练样本统计</p><div class="table-container"><table><thead><tr><th></th><th># Training sentences</th></tr></thead><tbody><tr><td>English → Czech</td><td>51,391,404</td></tr><tr><td>English → German</td><td>25,746,259</td></tr><tr><td>English → Estonian</td><td>1,064,658</td></tr><tr><td>English → Finnish</td><td>2,986,131</td></tr><tr><td>English → Russian</td><td>9,140,469</td></tr><tr><td>English → Turkish</td><td>205,579</td></tr><tr><td>English → Chinese</td><td>23,861,542</td></tr></tbody></table></div><p>表2：BLEU值</p><div class="table-container"><table><thead><tr><th></th><th>newstest 2017</th><th>newstest 2018</th></tr></thead><tbody><tr><td>English → Czech</td><td>18.11</td><td>17.36</td></tr><tr><td>English → German</td><td>23.37</td><td>34.46</td></tr><tr><td>English → Estonian</td><td>-</td><td>13.35</td></tr><tr><td>English → Finnish</td><td>15.06</td><td>10.32</td></tr><tr><td>English → Russian</td><td>21.30</td><td>18.96</td></tr><tr><td>English → Turkish</td><td>6.93</td><td>6.22</td></tr><tr><td>English → Chinese</td><td>23.10</td><td>23.75</td></tr></tbody></table></div><h1 id="1-注意力权重可视化"><a href="#1-注意力权重可视化" class="headerlink" title="1. 注意力权重可视化"></a>1. 注意力权重可视化</h1><p>注意力权重可视化应该是最直接的一种研究方法。<em>Transformer</em>的<em>encoder</em>里面包含了6层注意力，每层注意力有8个head，要把这些权重全部可视化出来比较困难，所以作者选择了一些具有高视觉可解释性的注意力权重。</p><p>通过可视化他们发现这些注意力权重有四种模式：</p><ul><li>在初始注意力层中，每个词的注意力会在自身上；</li><li>注意力层数增加后，注意力会集中在前几个词</li><li>或者注意力集中在后几个词</li><li>最后注意力会集中在句子的末尾</li></ul><p><img src="https://img.vim-cn.com/a2/1daa1130c9fa37195edb3385b35563a33905fa.png" alt></p><p>假设输入句子是“there is also an economic motive.”，<em>Transformer</em>中典型的注意力权重分布。可以看到layer 0中注意力都在词本身，后面基层的注意力会在词的前后几个词上，再到最后一层所有词的注意力全部放在句子末尾。这就预示着<em>Transformer</em>的高层注意力试图发现词与词之间的长程依赖关系，而低层注意力试图在局部发现依赖关系，这一发现很好的印证了<em>Transformer</em>作者的预想。</p><h1 id="2-树结构生成"><a href="#2-树结构生成" class="headerlink" title="2. 树结构生成"></a>2. 树结构生成</h1><p><em>Transformer</em>的权重矩阵可以看成是一个加权图：每个次都是一个节点，词与词之间的注意力是边。尽管模型没有任何生成树形结构的训练，但是我们可以使用这种图结构来抽取出一棵树来看看它是否能反应词之间的依赖关系。</p><p>作者在<em>CoNLL 2017 Share Task</em>的<em>English PUD treebank</em>上做实验。</p><p><img src="https://img.vim-cn.com/9b/0edca00f20e5f0e9e855fb9c06481dd1042b76.png" alt></p><p>上图展示了<em>UAS F1-Score</em>，实验中作为对比作者使用<em>random baseline</em>的得分是10.1，从上图我们可以看到虽然注意力层没有训练用于生成树结构，但是每一层的得分都好于随机，这说明模型可以学习到一些语法关系。</p><p>基本上得分最高的层都集中在前三层，说明前三层主要用于学习句子的语法结构的。</p><p>但是作者也通过在注意力权重可视化中发现的规律作为基准，计算<em>UAS F1-Score</em>得到最好的分数为35.08，而我们可以从上表中看到，表中最好的结果并没有比基准高多少，并且之前我们的结论是高层注意力用于学习远距离的依赖关系，但是这里我们看到主要是低层注意力学习到了有效的语法结构，这说明<em>Transformer</em>很难有效地处理更加复杂和长程的依赖。</p><p>总的来说，对于语料更丰富的数据来说，模型更能学习到语法结构（对比<em>English-Turkish</em>和其他），但是当语料达到一定程度的时候，模型并不能学到更多的语法知识了。</p><h1 id="3-探索序列标注任务"><a href="#3-探索序列标注任务" class="headerlink" title="3. 探索序列标注任务"></a>3. 探索序列标注任务</h1><p>作者通过四个序列标注任务对<em>encoder</em>进行研究：</p><ul><li>Part-of-Speech tagging （ the Universal Dependencies English Web Treebank v2.0数据集）</li><li>Chunking（CoNLL2000 Chunking shared task数据集）</li><li>Named Entity Recognition（ CoNLL2003 NER shared task数据集）</li><li>Semantic tagging（Parallel Meaning Bank (PMB) for Semantic tagging数据集）</li></ul><p>表3：各数据集统计结果</p><div class="table-container"><table><thead><tr><th></th><th>#labels</th><th>#training sentences</th><th>#testing sentences</th><th>average sent.length</th></tr></thead><tbody><tr><td>POS</td><td>17</td><td>12543</td><td>1000</td><td>21.2</td></tr><tr><td>Chunk</td><td>22</td><td>8042</td><td>2012</td><td>23.5</td></tr><tr><td>NER</td><td>9</td><td>14987</td><td>3684</td><td>12.7</td></tr><tr><td>SEM</td><td>80</td><td>62739</td><td>4351</td><td>6.4</td></tr></tbody></table></div><p><img src="https://img.vim-cn.com/da/b0a2810894b50fd0501e6dd70c0edbd38ebb53.png" alt></p><p>上图展示了测试结果。</p><ul><li>对于POS和CHUNK任务来说，最高的结果基本出现在前三层，说明前三层用于学习语法结构，这一结论与之前的结果相吻合。</li><li>NER和SEM这种注重语义知识的任务， 在模型的高层注意力中表现的更好，说明高层注意力主要用于学习语义知识。</li><li>上表数据右侧展示的是句子长度匹配的错误率，从表中我们可以看出，前三层的句子长度错误率普遍低于高层的注意力层，这说明低层注意力不仅要学习语法结构，还会对句子长度进行编码， 但到了高层注意力，句子长度信息就会丢失。唯一例外的是SEM任务，但是我们看下训练集的数据统计列标可以发现，SEM数据集的句子普遍较短，对模型来说更容易预测。而模型低层注意力本身就是在学习短程依赖方面比较有优势，高层注意力处理远距离依赖的乏力可能和句子长度信息的丢失有一定关系。</li><li>对比BLEU值我们可以发现，BLEU值越高，在序列标注任务中的表现也越好，这说明<em>encoder</em>对句子语法的编码越好，翻译质量会越高。</li></ul><h1 id="4-Transfer-Learning"><a href="#4-Transfer-Learning" class="headerlink" title="4. Transfer Learning"></a>4. Transfer Learning</h1><p>为了进一步研究编码进注意力层的知识是否对少语料的情景有所帮助，作者做了两个实验：</p><ul><li>使用English-German训练好的编码层，初始化English-Turkish编码层，并且允许编码层权重微调（TL1）</li><li>使用English-German训练好的编码层，初始化English-Turkish编码层，并且保持权重不变（TL2）</li></ul><p><img src="https://img.vim-cn.com/8b/e008811c89ae0e7792e9d59f0bfb70ffe1b0e8.png" alt></p><p>上图展示了实验的结果。从结果中我们可以看到，效果很明显。</p><h1 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h1><p>本文主要研究了<em>Transformer</em>每层注意力都学到了什么，通过实验发现：</p><ul><li>低层注意力能有效地获得句子的短程依赖关系</li><li>高层注意力尝试学习句子的远距离依赖关系，但是效果并不明显</li><li>低层注意力能够对句子长度进行编码，高层注意力可能丢失句子长度编码信息，这也可能是导致远距离依赖关系处理困难的原因</li><li>低层注意力主要学习句子语法结构</li><li>高层注意力主要学习句子语义信息</li><li>语料越多，模型越能学到足够多的语法信息，但是存在一个瓶颈</li><li>语法信息越丰富，翻译的效果越好</li><li>用大量语料学习到的知识可迁移到少量预料任务中去</li></ul><h1 id="6-参考资料"><a href="#6-参考资料" class="headerlink" title="6. 参考资料"></a>6. 参考资料</h1><p><a href="https://www.aclweb.org/anthology/W18-5431" target="_blank" rel="noopener">An Analysis of Encoder Representations in Transformer-Based Machine Translation</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer代码实现-Tensoflow版</title>
      <link href="/2019/09/16/Transformer%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-tensorflow/"/>
      <url>/2019/09/16/Transformer%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-tensorflow/</url>
      
        <content type="html"><![CDATA[<p>前面介绍了Transformer的<code>pytorch</code>版的代码实现，下面我们再介绍一下<code>tensorflow</code>版的代码实现。</p><a id="more"></a><p>本文主要参考的是<code>tensorflow</code><a href="https://www.tensorflow.org/beta/tutorials/text/transformer" target="_blank" rel="noopener">官方教程</a>，使用的是<code>tensoflow 2.0</code>，因此首先还是要先搭建代码环境，可以参考这里：<a href="https://tf.wiki/zh/basic/installation.html" target="_blank" rel="noopener">简单粗暴 TensorFlow 2.0</a>。</p><h1 id="1-前期准备"><a href="#1-前期准备" class="headerlink" title="1. 前期准备"></a>1. 前期准备</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import, division, print_function, unicode_literals</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    %tensorflow_version <span class="number">2.</span>x</span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">import</span> tensorflow_datasets <span class="keyword">as</span> tfds</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><h1 id="2-Scaled-Dot-Product-Attention"><a href="#2-Scaled-Dot-Product-Attention" class="headerlink" title="2. Scaled Dot-Product Attention"></a>2. Scaled Dot-Product Attention</h1><p><img src="https://img.vim-cn.com/ed/97e04d7d6067cb360e8fef1d29cf41978d353e.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scaled_dot_product_attention</span><span class="params">(q, k, v, mask)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Calculate the attention weights.</span></span><br><span class="line"><span class="string">    q, k, v must have matching leading dimension.</span></span><br><span class="line"><span class="string">    k, v must have matching penultimate dimension, i.e.:seq_len_k = seq_len_v.</span></span><br><span class="line"><span class="string">    The mask has different shapes depending on its type (padding or look ahead)</span></span><br><span class="line"><span class="string">    but it must be broadcastable for addition.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    :params q: query shape == (..., seq_len_q, depth)</span></span><br><span class="line"><span class="string">    :params k: key shape == (..., seq_len_k, depth)</span></span><br><span class="line"><span class="string">    :params v: value shape == (..., seq_len_v, depth)</span></span><br><span class="line"><span class="string">    :params mask: Float tensor with shape bradcastable to </span></span><br><span class="line"><span class="string">                  (None, seq_len_q, seq_len_k), Default is None.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># MatMul step in above Fig</span></span><br><span class="line">    matmul_qk = tf.matmul(q, k, transpose_b=<span class="literal">True</span>)  <span class="comment"># (..., seq_len_q, seq_len_k)</span></span><br><span class="line">   </span><br><span class="line">    <span class="comment"># Scale step in above Fig</span></span><br><span class="line">    <span class="comment"># This is done because for large values of depth, the dot product grows </span></span><br><span class="line">    <span class="comment"># large in magnitude pushing the softmax function where it has small </span></span><br><span class="line">    <span class="comment"># gradients resulting in a very hard softmax.</span></span><br><span class="line">    dk = tf.cast(tf.shape(k)[<span class="number">-1</span>], tf.float32)</span><br><span class="line">    scaled_attention = matmul_qk / tf.math.sqrt(dk)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Mask step in above Fig</span></span><br><span class="line">    <span class="comment"># This is done because the mask is summed with the scaled matrix </span></span><br><span class="line">    <span class="comment"># multiplication of Q and K and is applied immediately before a softmax. </span></span><br><span class="line">    <span class="comment"># The goal is to zero out these cells, and large negative inputs to </span></span><br><span class="line">    <span class="comment"># softmax are near zero in the output.</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scaled_attention += (mask * <span class="number">-1e9</span>) </span><br><span class="line">        </span><br><span class="line">    <span class="comment"># SoftMax step in above Fig</span></span><br><span class="line">    <span class="comment"># softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1</span></span><br><span class="line">    attention_weights = tf.nn.softmax(scaled_attention, axis=<span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The last MatMul step in above Fig</span></span><br><span class="line">    out = tf.matmul(attention_weights, v)  <span class="comment"># (..., seq_len_q, depth_v)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> out, attention_weights</span><br></pre></td></tr></table></figure><h1 id="3-Multi-Head-Attention"><a href="#3-Multi-Head-Attention" class="headerlink" title="3. Multi-Head Attention"></a>3. Multi-Head Attention</h1><p><img src="https://img.vim-cn.com/b1/e4bc841abc55d366813340f92f6696c5d59e95.png" alt></p><p><em>Multi-Head Attention</em>有四部分组成：</p><ul><li>线性转换层和multi-head (Q, K, V)</li><li><em>Multi-head Scaled dot-product attention</em></li><li><em>Concatenation of heads</em></li><li>最后的线性转换层</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement Multi=head attention layer.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_mode, num_heads)</span>:</span></span><br><span class="line">        super(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.d_model= d_model</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># after `Concat`, concatenated heads dimension must equal to d_model</span></span><br><span class="line">        <span class="keyword">assert</span> d_model % num_heads == <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        self.depth = d_model // num_heads</span><br><span class="line">        </span><br><span class="line">        self.wq = tf.keras.layers.Dense(d_model)</span><br><span class="line">        self.wk = tf.keras.layers.Dense(d_model)</span><br><span class="line">        self.wv = tf.keras.layers.Dense(d_model)</span><br><span class="line">        </span><br><span class="line">        self.dense = tf.keras.layers.Dense(d_model)</span><br><span class="line">   </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">split_heads</span><span class="params">(self, x, batch_size)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Split the last dimension (word vector dimension) into (num_heads, depth).</span></span><br><span class="line"><span class="string">        Transpose the result such that the shape is </span></span><br><span class="line"><span class="string">        (batch_size, num_heads, seq_len, depth)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        x = tf.reshape(x, (batch_size, <span class="number">-1</span>, self.num_heads, self.depth))</span><br><span class="line">        <span class="keyword">return</span> tf.transpose(x, perm=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, q, k, v, mask)</span>:</span></span><br><span class="line">        batch_size = tf.shape(q)[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># First linear transition step in above Fig</span></span><br><span class="line">        q = self.wq(q)  <span class="comment"># (batch_size, seq_len, d_model)</span></span><br><span class="line">        k = self.wk(k)  <span class="comment"># (batch_size, seq_len, d_model)</span></span><br><span class="line">        v = self.wv(v)  <span class="comment"># (batch_size, seq_len, d_model</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Split K, Q, V into multi-heads</span></span><br><span class="line">        q = self.split_heads(q, batch_size)  <span class="comment"># (batch_size, num_heads, seq_len_q, depth)</span></span><br><span class="line">        k = self.split_heads(k, batch_size)  <span class="comment"># (batch_size, num_heads, seq_len_k, depth)</span></span><br><span class="line">        v = self.split_heads(v, batch_size)  <span class="comment"># (batch_size, num_heads, seq_len_v, depth)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Scaled Dot-Product Attention step in above Fig</span></span><br><span class="line">        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)</span><br><span class="line">        <span class="comment"># scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)</span></span><br><span class="line">        <span class="comment"># attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Concat step in above Fig</span></span><br><span class="line">        scaled_attention = tf.transpose(scaled_attention, perm=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">        <span class="comment"># scale_attention.shape == (batch_size, seq_len_q, num_heads, depth)</span></span><br><span class="line">        concat_attention = tf.reshape(scaled_attention, (batch_size, <span class="number">-1</span>, self.d_model))</span><br><span class="line">        <span class="comment"># concate_attention.shaoe == (batch_size, seq_len_q, d_model)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Final linear transition step in above Fig</span></span><br><span class="line">        out = self.dense(concat_attention)  <span class="comment"># (batch_size, seq_len_q, d_model)</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> out, attention_weights</span><br></pre></td></tr></table></figure><h1 id="4-Point-wise-feed-forward-network"><a href="#4-Point-wise-feed-forward-network" class="headerlink" title="4. Point wise feed forward network"></a>4. Point wise feed forward network</h1><p><em>Point wise feed forward network</em>由两个全连接层组成，激活函数使用<em>Relu</em>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">point_wise_feed_forward_network</span><span class="params">(d_model, d_ff)</span>:</span></span><br><span class="line">    ffn = tf.keras.Sequential([</span><br><span class="line">        tf.keras.layers.Dense(d_ff, activation=<span class="string">'relu'</span>),  <span class="comment"># (batch_size, seq_len, d_ff)</span></span><br><span class="line">        tf.keras.layers.Dense(d_model)  <span class="comment"># (batch_size, seq_len, d_model)</span></span><br><span class="line">    ])</span><br><span class="line">    <span class="keyword">return</span> ffn</span><br></pre></td></tr></table></figure><h1 id="5-Positional-encoding"><a href="#5-Positional-encoding" class="headerlink" title="5. Positional encoding"></a>5. Positional encoding</h1><script type="math/tex; mode=display">PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})</script><script type="math/tex; mode=display">PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_angles</span><span class="params">(pos, i, d_model)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Get the absolute position angle from each word.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    :params pos: position index</span></span><br><span class="line"><span class="string">    :params i: word embedding dimension index at each position</span></span><br><span class="line"><span class="string">    :params d_model: model dimension</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    angle_rates = <span class="number">1</span> / np.power(<span class="number">10000</span>, (<span class="number">2</span>*(i//<span class="number">2</span>)) / np.float32(d_model))</span><br><span class="line">    <span class="keyword">return</span> pos * angle_rates</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positional_encoding</span><span class="params">(position, d_model)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute positional encoding.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    :params position: length of sentence</span></span><br><span class="line"><span class="string">    :params d_model: model dimension</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    angle_rads = get_angles(np.arange(position)[:, np.newaxis],</span><br><span class="line">                            np.arange(d_model)[np.newaxis, :],</span><br><span class="line">                            d_model)  <span class="comment"># (position, d_model)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># apply sin to even indices in the array; 2i</span></span><br><span class="line">    angle_rads[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(angle_rads[:, <span class="number">0</span>::<span class="number">2</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># apply cos to odd indices in the array; 2i+1</span></span><br><span class="line">    angle_rads[:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(angle_rads[:, <span class="number">1</span>::<span class="number">2</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># positional encoding</span></span><br><span class="line">    positional_encoding = angle_rads[np.newaxis, ...]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tf.cast(positional_encoding, dtype=tf.float32)</span><br></pre></td></tr></table></figure><h1 id="6-Masking"><a href="#6-Masking" class="headerlink" title="6. Masking"></a>6. Masking</h1><p>这里有两种Mask，一种用来mask掉输入序列中的padding，一种用来mask掉解码过程中“未来词”。</p><ul><li>Mask每个batch中所有序列的padding token，使得模型不会把padding token当成输入：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_padding_mask</span><span class="params">(seq)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Mask all the pad tokens in the batch of sequence. </span></span><br><span class="line"><span class="string">    It ensures that the model does not treat padding as the input. </span></span><br><span class="line"><span class="string">    The mask indicates where pad value 0 is present: </span></span><br><span class="line"><span class="string">    it outputs a 1 at those locations, and a 0 otherwise.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    seq = tf.cast(tf.math.equal(seq, <span class="number">0</span>), tf.float32)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># add extra dimensions to add the padding to the attention logits.</span></span><br><span class="line">    <span class="keyword">return</span> seq[:, tf.newaxis, tf.newaxis, :]  <span class="comment"># (batch_size, 1, 1, seq_len)</span></span><br></pre></td></tr></table></figure><ul><li>Mask掉解码过程中的“未来词”：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_look_ahead_mask</span><span class="params">(size)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    The look-ahead mask is used to mask the future tokens in a sequence. </span></span><br><span class="line"><span class="string">    In other words, the mask indicates which entries should not be used.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    mask = <span class="number">1</span> - tf.linalg.band_part(tf.ones((size, size)), <span class="number">-1</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> mask  <span class="comment"># (seq_len, seq_len)</span></span><br></pre></td></tr></table></figure><h1 id="7-Encoder-and-Decoder"><a href="#7-Encoder-and-Decoder" class="headerlink" title="7. Encoder and Decoder"></a>7. Encoder and Decoder</h1><p><img src="https://img.vim-cn.com/3a/78ea12dca1ce0f99f9a9705466afc16c58c3cf.png" alt></p><p>Transformer和标准的<em>seq2seq with attention</em>模型一样，采用<em>encoder-decoder</em>结构，<em>encoder / decoder</em>都包含了6个结构相同的<em>encoder layer</em>和<em>decoder layer</em>。</p><h2 id="7-1-Encoder"><a href="#7-1-Encoder" class="headerlink" title="7.1 Encoder"></a>7.1 Encoder</h2><p><em>Encoder layer</em>由两个sub-layer组成：</p><ul><li>Multi-head attention</li><li>Point wise feed forward network</li></ul><p>每个sub-layer后面都接一个layer normalization，使用残差连接防止梯度消失。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements Encoder Layer.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, num_heads, d_ff, rate=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.multihead_attention = MultiHeadAttention(d_model, num_heads)</span><br><span class="line">        self.ffn = point_wise_feed_forward_network(d_model, d_ff)</span><br><span class="line">        </span><br><span class="line">        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=<span class="number">1e-6</span>)</span><br><span class="line">        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=<span class="number">1e-6</span>)</span><br><span class="line">        </span><br><span class="line">        self.dropout1 = tf.keras.layers.Dropout(rate)</span><br><span class="line">        self.fropout2 = tf.keras.layers.Dropout(rate)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x, training, padding_mask)</span>:</span></span><br><span class="line">        <span class="comment"># Multi-head attention sub-layer</span></span><br><span class="line">        attention_out, _ = self.multihead_attention(x, x, x, padding_mask)  </span><br><span class="line">        <span class="comment"># attention_out.shape == (batch_size, input_seq_len, d_model)</span></span><br><span class="line">        attention_out = self.dropout1(attention_out, training=training)</span><br><span class="line">        attn_norm_out = self.layernorm1(x + attention_out)  </span><br><span class="line">        <span class="comment"># attn_norm_out.shape == (batch_size, input_seq_len, d_model)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># point wise feed forward network sub-layer</span></span><br><span class="line">        ffn_out = self.ffn(attn_norm_out)  <span class="comment"># (batch_size, input_seq_len, d_model)</span></span><br><span class="line">        ffn_out = self.dropout2(ffn_out, training)</span><br><span class="line">        ffn_norm_out = self.layernorm2(attn_norm_out + ffn_out)  </span><br><span class="line">        <span class="comment"># ffn_norm_out.shape == (batch_size, input_seq_len, d_model)</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> ffn_norm_out</span><br></pre></td></tr></table></figure><p><em>Encoder</em>由三部分组成：</p><ul><li>输入Embedding</li><li>Positional Encoding</li><li>N个<em>encoder layer</em></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_layers, d_model, num_heads, </span></span></span><br><span class="line"><span class="function"><span class="params">                 d_ff, input_vocab_size, rate=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Embedding layer</span></span><br><span class="line">        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)</span><br><span class="line">        <span class="comment"># Positional encoding layer</span></span><br><span class="line">        self.pos_encoding = positional_encoding(input_vocab_size, d_model)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># encoder layers</span></span><br><span class="line">        self.encoder_layers = [EncoderLayer(d_model, num_heads, d_ff, rate)</span><br><span class="line">                               <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers)]</span><br><span class="line">        self.dropout = tf.keras.layers.Dropout(rate)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x, training, padding_mask)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        The input is put through an embedding which is summed with the positional encoding.</span></span><br><span class="line"><span class="string">        The output of this summation is the input to the encoder layers. </span></span><br><span class="line"><span class="string">        The output of the encoder is the input to the decoder.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        seq_len = tf.shape(x)[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># adding embedding and positional encoding</span></span><br><span class="line">        x = self.embedding(x)  <span class="comment"># (batch_size, input_seq_len, d_model)</span></span><br><span class="line">        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  <span class="comment"># ????</span></span><br><span class="line">        x += self.pos_encoding[:, :seq_len, :]</span><br><span class="line">        x = self.dropout(x, training=training)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># encoder layer</span></span><br><span class="line">        <span class="keyword">for</span> encoder <span class="keyword">in</span> self.encoder_layers:</span><br><span class="line">            x =encoder(x, training, padding_mask)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> x  <span class="comment"># (batch_size, input_seq_len, d_model)</span></span><br></pre></td></tr></table></figure><h2 id="7-2-Decoder"><a href="#7-2-Decoder" class="headerlink" title="7.2 Decoder"></a>7.2 Decoder</h2><p><em>Decoder layer</em>由三个sub-layer组成：</p><ul><li>Masked multi-head attention (with look ahead mask and padding mask)</li><li>Multi-head attention (with padding mask)。其中Q（query）来自于前一层（或者输入层）的输出， K（key）和V（value）来源于<em>Encoder</em>的输出。</li><li>Point wise feed forward networks</li></ul><p>与<em>encoder layer</em>类似，每个sub-layer后面会接一个layer normalization，同样使用残差连接。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, num_heads, d_ff, rate=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.multihead_attention1 = MultiHeadAttention(d_model, num_heads)</span><br><span class="line">        self.multihead_attention2 = MultiHeadAttention(d_model, num_heads)</span><br><span class="line">        </span><br><span class="line">        self.ffn = point_wise_feed_forward_network(d_model, d_ff)</span><br><span class="line">        </span><br><span class="line">        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=<span class="number">1e-6</span>)</span><br><span class="line">        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=<span class="number">1e-6</span>)</span><br><span class="line">        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=<span class="number">1e-6</span>)</span><br><span class="line"></span><br><span class="line">        self.dropout1 = tf.keras.layers.Dropout(rate)</span><br><span class="line">        self.dropout2 = tf.keras.layers.Dropout(rate)</span><br><span class="line">        self.dropout3 = tf.keras.layers.Dropout(rate)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x, encoder_out, training, look_ahead_mask, padding_mask)</span>:</span></span><br><span class="line">        <span class="comment"># enc_output.shape == (batch_size, input_seq_len, d_model)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Masked multi-head attention (with look ahead mask and padding mask)</span></span><br><span class="line">        attention_out1, attn_weights1 = self.multihead_attention1(x, x, x, padding_mask)</span><br><span class="line">        <span class="comment"># attention_out.shape == (batch_size, target_seq_len, d_model)</span></span><br><span class="line">        attention_out1 = self.dropout1(attention_out1, training=training)</span><br><span class="line">        attn_norm_out1 = self.layernorm1(attention_out1 + x)</span><br><span class="line">        <span class="comment"># attn_nor_out.shape == (batch_size, target_seq_len, d_model)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Multi-head attention (with padding mask)</span></span><br><span class="line">        attention_out2, attn_weights2 = self.multihead_attention2(attention_out1, </span><br><span class="line">                                                                  encoder_out,</span><br><span class="line">                                                                  encoder_out,</span><br><span class="line">                                                                  padding_mask)</span><br><span class="line">        <span class="comment"># attention_out2.shape == (batch_size, target_seq_len, d_model)</span></span><br><span class="line">        attention_out2 = self.dropout2(attention_out2, training=training)</span><br><span class="line">        attn_norm_out2 = self.layernorm2(attention_out2 + attn_norm_out1)</span><br><span class="line">        <span class="comment"># attn_nor_out2.shape == # (batch_size, target_seq_len, d_model)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Point wise feed forward networks</span></span><br><span class="line">        ffn_out = self.ffn(attn_norm_out2)  <span class="comment"># (Point wise feed forward networks)</span></span><br><span class="line">        ffn_out = self.dropout3(ffn_out, training=training)</span><br><span class="line">        ffn_norm_out = self.layernorm3(ffn_out + attn_norm_out2)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> ffn_norm_out, attn_weights1, attn_weights2</span><br></pre></td></tr></table></figure><p><em>Decoder</em>由三部分组成：</p><ul><li>Output Embedding</li><li>Positional Encoding</li><li>N个<em>decoder layer</em></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_layers, d_model, num_heads, d_ff, target_vocab_size, rate=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line"></span><br><span class="line">        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)</span><br><span class="line">        self.pos_encoding = positional_encoding(target_vocab_size, d_model)</span><br><span class="line"></span><br><span class="line">        self.decoder_layers = [DecoderLayer(d_model, num_heads, dff, rate) </span><br><span class="line">                           <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers)]</span><br><span class="line">        self.dropout = tf.keras.layers.Dropout(rate)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x, encoder_out, training, look_ahead_mask, padding_mask)</span>:</span></span><br><span class="line">        seq_len = tf.shape(x)[<span class="number">1</span>]</span><br><span class="line">        attention_weights = &#123;&#125;</span><br><span class="line">        </span><br><span class="line">        x = self.embedding(x)  <span class="comment"># (batch_size, target_seq_len, d_model)</span></span><br><span class="line">        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))</span><br><span class="line">        x += self.pos_encoding[:, :seq_len, :]</span><br><span class="line">        </span><br><span class="line">        x = self.dropout(x, training=training)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_layers):</span><br><span class="line">            x, block1, block2 = self.decoder_layers[i](x, encoder_out, training, </span><br><span class="line">                                        look_ahead_mask, padding_mask)</span><br><span class="line">            attention_weights[<span class="string">'decoder_layer&#123;&#125;_block1'</span>.format(i+<span class="number">1</span>)] = block1</span><br><span class="line">            attention_weights[<span class="string">'decoder_layer&#123;&#125;_block2'</span>.format(i+<span class="number">1</span>)] = block2</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># x.shape == (batch_size, target_seq_len, d_model)</span></span><br><span class="line">        <span class="keyword">return</span> x, attention_weights</span><br></pre></td></tr></table></figure><h1 id="8-Create-the-Transformer"><a href="#8-Create-the-Transformer" class="headerlink" title="8. Create the Transformer"></a>8. Create the Transformer</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_layers, d_model, num_heads, d_ff, </span></span></span><br><span class="line"><span class="function"><span class="params">                 input_vocab_size, target_vocab_size, rate=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(Transformer, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.encoder = Encoder(num_layers, d_model, num_heads, d_ff, </span><br><span class="line">                               input_vocab_size, rate)</span><br><span class="line">        self.decoder = Decoder(num_layers, d_model, num_heads, d_ff,</span><br><span class="line">                               target_vocab_size, rate)</span><br><span class="line">        </span><br><span class="line">        self.final_layer = tf.keras.layers.Dense(target_vocab_size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, targets, training, encode_padding_mask,</span></span></span><br><span class="line"><span class="function"><span class="params">             look_ahead_mask, decode_padding_mask)</span>:</span></span><br><span class="line">        encoder_output = self.encoder(inputs, training, encode_padding_mask)</span><br><span class="line">        <span class="comment"># encoder_output.shape = (batch_size, inp_seq_len, d_model)</span></span><br><span class="line">        </span><br><span class="line">        decoder_output, attention_weights = self.decoder(</span><br><span class="line">            targets, encoder_output, training, look_ahead_mask, decode_padding_mask</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># decoder_output.shape = (batch_size, tar_seq_len, d_model)</span></span><br><span class="line">        </span><br><span class="line">        final_output = self.final_layer(decoder_output)</span><br><span class="line">        <span class="comment"># final_output.shape = (batch_size, tar_seq_len, target_vocab_size)</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> final_output, attention_weights</span><br></pre></td></tr></table></figure><h1 id="9-实验"><a href="#9-实验" class="headerlink" title="9. 实验"></a>9. 实验</h1><p>我们的实验还是将Transformer用于机器翻译——葡萄牙语翻译成英语。模型训练以后，我们输入葡萄牙语，模型返回英语。</p><h2 id="9-1-优化器"><a href="#9-1-优化器" class="headerlink" title="9.1 优化器"></a>9.1 优化器</h2><p>论文中使用的优化器是<em>Adam</em>， 使用下式自定义学习率：</p><script type="math/tex; mode=display">l_{rate} = d_{model}^{-0.5} \cdot \min(step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5})</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomSchedule</span><span class="params">(tf.keras.optimizers.schedules.LearningRateSchedule)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, warmup_steps=<span class="number">4000</span>)</span>:</span></span><br><span class="line">        super(CustomSchedule, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.d_model = tf.cast(self.d_model, tf.float32)</span><br><span class="line">        </span><br><span class="line">        self.warmup_steps = warmup_steps</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, step)</span>:</span></span><br><span class="line">        arg1 = tf.math.rsqrt(step)</span><br><span class="line">        arg2 = step * (self.warup_steps ** <span class="number">-1.5</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = CustomSchedule(d_model)</span><br><span class="line">optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=<span class="number">0.9</span>, beta_2=<span class="number">0.98</span>, epsilon=<span class="number">1e-9</span>)c</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">temp_learning_rate_schedule = CustomSchedule(d_model)</span><br><span class="line"></span><br><span class="line">plt.plot(temp_learning_rate_schedule(tf.range(<span class="number">40000</span>, dtype=tf.float32)))</span><br><span class="line">plt.ylabel(<span class="string">"Learning Rate"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Train Step"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://www.tensorflow.org/beta/tutorials/text/transformer_files/output_f33ZCgvHpPdG_1.png" alt="png"></p><h2 id="9-2-Loss-and-Metrics"><a href="#9-2-Loss-and-Metrics" class="headerlink" title="9.2 Loss and Metrics"></a>9.2 Loss and Metrics</h2><p>由于target sentence被padding了，因此计算损失的时候使用padding mask也是至关重要的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss_object = tf.keras.losses.SparseCategoricalCrossentropy(</span><br><span class="line">    from_logits=<span class="literal">True</span>, reduction=<span class="string">"none"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_function</span><span class="params">(real, pred)</span>:</span></span><br><span class="line">    mask = tf.math.logical_not(tf.math.equal(real, <span class="number">0</span>))</span><br><span class="line">    loss_ = loss_object(real, pred)</span><br><span class="line">    </span><br><span class="line">    mask = tf.cast(mask, dtype=loss_.dtype)</span><br><span class="line">    loss_ *= mask</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(loss_)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_loss = tf.keras.metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=<span class="string">'train_accuracy'</span>)</span><br></pre></td></tr></table></figure><h2 id="9-3-模型超参数设置"><a href="#9-3-模型超参数设置" class="headerlink" title="9.3 模型超参数设置"></a>9.3 模型超参数设置</h2><p>为了保证模型较小，训练速度相对够快，实验过程中的超参数不会和论文保持一致， <em>num_layers</em>，<em>d_model</em>，<em>d_ff</em>都会有所减小：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">num_layers = <span class="number">4</span></span><br><span class="line">d_model = <span class="number">128</span></span><br><span class="line">dff = <span class="number">512</span></span><br><span class="line">num_heads = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">input_vocab_size = tokenizer_pt.vocab_size + <span class="number">2</span></span><br><span class="line">target_vocab_size = tokenizer_en.vocab_size + <span class="number">2</span></span><br><span class="line">dropout_rate = <span class="number">0.1</span></span><br></pre></td></tr></table></figure><h2 id="9-4-数据pipeline"><a href="#9-4-数据pipeline" class="headerlink" title="9.4 数据pipeline"></a>9.4 数据pipeline</h2><ul><li>数据集</li></ul><p>数据集使用<a href="https://www.tensorflow.org/datasets" target="_blank" rel="noopener">TFDS</a>从<a href="https://www.ted.com/participate/translate" target="_blank" rel="noopener">TED Talks Open Translation Project</a>中加载 <a href="https://github.com/neulab/word-embeddings-for-nmt" target="_blank" rel="noopener">Portugese-English translation dataset</a>。这个数据集包含大概5万训练数据，1100验证数据和2000测试数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">examples, metadata = tfds.load(<span class="string">'ted_hrlr_translate/pt_to_en'</span>, with_info=<span class="literal">True</span>,</span><br><span class="line">                               as_supervised=<span class="literal">True</span>)</span><br><span class="line">train_examples, val_examples = examples[<span class="string">'train'</span>], examples[<span class="string">'validation'</span>]</span><br></pre></td></tr></table></figure><ul><li>Tokenizer</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(</span><br><span class="line">    (en.numpy() <span class="keyword">for</span> pt, en <span class="keyword">in</span> train_examples), target_vocab_size=<span class="number">2</span>**<span class="number">13</span>)</span><br><span class="line"></span><br><span class="line">tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(</span><br><span class="line">    (pt.numpy() <span class="keyword">for</span> pt, en <span class="keyword">in</span> train_examples), target_vocab_size=<span class="number">2</span>**<span class="number">13</span>)</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sample_string = <span class="string">'Transformer is awesome.'</span></span><br><span class="line"></span><br><span class="line">tokenized_string = tokenizer_en.encode(sample_string)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Tokenized string is &#123;&#125;'</span>.format(tokenized_string))</span><br><span class="line"></span><br><span class="line">original_string = tokenizer_en.decode(tokenized_string)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'The original string: &#123;&#125;'</span>.format(original_string))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> original_string == sample_string</span><br></pre></td></tr></table></figure><blockquote><p>Tokenized string is [7915, 1248, 7946, 7194, 13, 2799, 7877]<br>The original string: Transformer is awesome.</p></blockquote><p>Tokenizer会将不在词表中的词拆分成子字符串：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> ts <span class="keyword">in</span> tokenized_string:</span><br><span class="line">    print(<span class="string">'&#123;&#125;---&gt;&#123;&#125;'</span>.format(ts, tokenizer_en.decode([ts])))</span><br></pre></td></tr></table></figure><blockquote><p>7915 ——&gt; T<br>1248 ——&gt; ran<br>7946 ——&gt; s<br>7194 ——&gt; former<br>13 ——&gt; is<br>2799 ——&gt; awesome<br>7877 ——&gt; .</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BUFFER_SIZE = <span class="number">20000</span></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br></pre></td></tr></table></figure><ul><li>向输入和输出中添加开始和结束符</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(lang1, lang2)</span>:</span></span><br><span class="line">    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(</span><br><span class="line">             lang1.numpy()) + [tokenizer_pt.vocab_size+<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(</span><br><span class="line">             lang2.numpy()) + [tokenizer_en.vocab_size+<span class="number">1</span>]</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> lang1, lang2</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf_encode</span><span class="params">(pt, en)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.py_function(encode, [pt, en], [tf.float64, tf.float64])</span><br></pre></td></tr></table></figure><ul><li>为了使模型不至于太大，且实验相对较快，我们过滤掉太长的句子</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MAX_LEN = <span class="number">40</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter_max_len</span><span class="params">(x, y, max_len=MAX_LEN)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.logical_and(tf.size(x) &lt;= max_len, tf.size(y) &lt;= max_len)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = train_examples.map(tf_encode)</span><br><span class="line">train_dataset = train_dataset.filter(filter_max_length)</span><br><span class="line"><span class="comment"># cache the dataset to memory to get a speedup while reading from it.</span></span><br><span class="line">train_dataset = train_dataset.cache()</span><br><span class="line">train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(</span><br><span class="line">    BATCH_SIZE, padded_shapes=([<span class="number">-1</span>], [<span class="number">-1</span>]))</span><br><span class="line">train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val_dataset = val_examples.map(tf_encode)</span><br><span class="line">val_dataset = val_dataset.filter(filter_max_length).padded_batch(</span><br><span class="line">    BATCH_SIZE, padded_shapes=([<span class="number">-1</span>], [<span class="number">-1</span>]))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pt_batch, en_batch = next(iter(val_dataset))</span><br><span class="line">pt_batch, en_batch</span><br></pre></td></tr></table></figure><blockquote><p>(<tf.tensor: id="207697," shape="(64," 40), dtype="int64," numpy="array([[8214," 1259, 5, ..., 0, 0], [8214, 299, 13, 59, 8, 95, 3, 5157, 1, 4479, 7990, 0]])>,<br> <tf.tensor: id="207698," shape="(64," 40), dtype="int64," numpy="array([[8087," 18, 12, ..., 0, 0], [8087, 634, 30, 16, 13, 20, 17, 4981, 5453, 0]])>)</tf.tensor:></tf.tensor:></p></blockquote><h2 id="9-5-Training-and-checkpointing"><a href="#9-5-Training-and-checkpointing" class="headerlink" title="9.5 Training and checkpointing"></a>9.5 Training and checkpointing</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">transformer = Transformer(num_layers, d_model, num_heads, dff,</span><br><span class="line">                          input_vocab_size, target_vocab_size, dropout_rate)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_masks</span><span class="params">(inp, tar)</span>:</span></span><br><span class="line">    <span class="comment"># Encoder padding mask</span></span><br><span class="line">    encode_padding_mask = create_padding_mask(inp)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># Used in the 2nd attention block in the decoder.</span></span><br><span class="line">    <span class="comment"># This padding mask is used to mask the encoder outputs.</span></span><br><span class="line">    decode_padding_mask = create_padding_mask(inp)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># Used in the 1st attention block in the decoder.</span></span><br><span class="line">    <span class="comment"># It is used to pad and mask future tokens in the input received by </span></span><br><span class="line">    <span class="comment"># the decoder.</span></span><br><span class="line">    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[<span class="number">1</span>])</span><br><span class="line">    decode_target_padding_mask = create_padding_mask(tar)</span><br><span class="line">    combined_mask = tf.maximum(decode_target_padding_mask, look_ahead_mask)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> encode_padding_mask, combined_mask, decode_padding_mask</span><br></pre></td></tr></table></figure><p>管理checkpoint，每N轮保存一次</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">checkpoint_path = <span class="string">"./checkpoints/train"</span></span><br><span class="line"></span><br><span class="line">ckpt = tf.train.Checkpoint(transformer=transformer,</span><br><span class="line">                           optimizer=optimizer)</span><br><span class="line"></span><br><span class="line">ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># if a checkpoint exists, restore the latest checkpoint.</span></span><br><span class="line"><span class="keyword">if</span> ckpt_manager.latest_checkpoint:</span><br><span class="line">    ckpt.restore(ckpt_manager.latest_checkpoint)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'Latest checkpoint restored!!'</span>)</span><br></pre></td></tr></table></figure><p>target被分成两份：<code>tar_inp</code>和<code>tar_real</code>。其中<code>tar_inp</code>用于传入给<code>decoder</code>，<code>tar_real</code>是和输入一样的，只是向右移动一个位置，例如：</p><p><code>sentence = &quot;SOS A lion in the jungle is sleeping EOS&quot;</code></p><p><code>tar_inp = &quot;SOS A lion in the jungle is sleeping&quot;</code></p><p><code>tar_real = &quot;A lion in the jungle is sleeping EOS&quot;</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The @tf.function trace-compiles train_step into a TF graph for faster</span></span><br><span class="line"><span class="comment"># execution. The function specializes to the precise shape of the argument</span></span><br><span class="line"><span class="comment"># tensors. To avoid re-tracing due to the variable sequence lengths or variable</span></span><br><span class="line"><span class="comment"># batch sizes (the last batch is smaller), use input_signature to specify</span></span><br><span class="line"><span class="comment"># more generic shapes.</span></span><br><span class="line"></span><br><span class="line">train_step_signature = [</span><br><span class="line">    tf.TensorSpec(shape=(<span class="literal">None</span>, <span class="literal">None</span>), dtype=tf.int64),</span><br><span class="line">    tf.TensorSpec(shape=(<span class="literal">None</span>, <span class="literal">None</span>), dtype=tf.int64),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function(input_signature=train_step_signature)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(inp, tar)</span>:</span></span><br><span class="line">    tar_inp = tar[:, :<span class="number">-1</span>]</span><br><span class="line">    tar_real = tar[:, <span class="number">1</span>:]</span><br><span class="line">  </span><br><span class="line">    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions, _ = transformer(inp, tar_inp, </span><br><span class="line">                                     <span class="literal">True</span>, </span><br><span class="line">                                     enc_padding_mask, </span><br><span class="line">                                     combined_mask, </span><br><span class="line">                                     dec_padding_mask)</span><br><span class="line">        loss = loss_function(tar_real, predictions)</span><br><span class="line"></span><br><span class="line">    gradients = tape.gradient(loss, transformer.trainable_variables)    </span><br><span class="line">    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))</span><br><span class="line">  </span><br><span class="line">    train_loss(loss)</span><br><span class="line">    train_accuracy(tar_real, predictions)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">EPOCHS = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCHS):</span><br><span class="line">    start = time.time()</span><br><span class="line">  </span><br><span class="line">    train_loss.reset_states()</span><br><span class="line">    train_accuracy.reset_states()</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># inp -&gt; portuguese, tar -&gt; english</span></span><br><span class="line">    <span class="keyword">for</span> (batch, (inp, tar)) <span class="keyword">in</span> enumerate(train_dataset):</span><br><span class="line">        train_step(inp, tar)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> batch % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">          <span class="keyword">print</span> (<span class="string">'Epoch &#123;&#125; Batch &#123;&#125; Loss &#123;:.4f&#125; Accuracy &#123;:.4f&#125;'</span>.format(</span><br><span class="line">                  epoch + <span class="number">1</span>, batch, train_loss.result(), train_accuracy.result()))</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        ckpt_save_path = ckpt_manager.save()</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'Saving checkpoint for epoch &#123;&#125; at &#123;&#125;'</span>.format(epoch+<span class="number">1</span>,</span><br><span class="line">                                                         ckpt_save_path))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'Epoch &#123;&#125; Loss &#123;:.4f&#125; Accuracy &#123;:.4f&#125;'</span>.format(epoch + <span class="number">1</span>, </span><br><span class="line">                                                train_loss.result(), </span><br><span class="line">                                                train_accuracy.result()))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'Time taken for 1 epoch: &#123;&#125; secs\n'</span>.format(time.time() - start))</span><br></pre></td></tr></table></figure><blockquote><p>W0814 01:06:36.753235 140098807473920 deprecation.py:323] From /tmpfs/src/tf_docs_env/lib/python3.5/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:455: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.<br>Instructions for updating:<br>Apply a constraint manually following the optimizer update step.</p><p>Epoch 1 Batch 0 Loss 4.7365 Accuracy 0.0000<br>Epoch 1 Batch 50 Loss 4.3028 Accuracy 0.0033<br>Epoch 1 Batch 100 Loss 4.1992 Accuracy 0.0140<br>Epoch 1 Batch 150 Loss 4.1569 Accuracy 0.0182<br>Epoch 1 Batch 200 Loss 4.0963 Accuracy 0.0204<br>Epoch 1 Batch 250 Loss 4.0199 Accuracy 0.0217<br>Epoch 1 Batch 300 Loss 3.9262 Accuracy 0.0242<br>Epoch 1 Batch 350 Loss 3.8337 Accuracy 0.0278<br>Epoch 1 Batch 400 Loss 3.7477 Accuracy 0.0305<br>Epoch 1 Batch 450 Loss 3.6682 Accuracy 0.0332<br>Epoch 1 Batch 500 Loss 3.6032 Accuracy 0.0367<br>Epoch 1 Batch 550 Loss 3.5408 Accuracy 0.0405<br>Epoch 1 Batch 600 Loss 3.4777 Accuracy 0.0443<br>Epoch 1 Batch 650 Loss 3.4197 Accuracy 0.0479<br>Epoch 1 Batch 700 Loss 3.3672 Accuracy 0.0514<br>Epoch 1 Loss 3.3650 Accuracy 0.0515<br>Time taken for 1 epoch: 576.2345867156982 secs</p><p>Epoch 2 Batch 0 Loss 2.4194 Accuracy 0.1030<br>Epoch 2 Batch 50 Loss 2.5576 Accuracy 0.1030<br>Epoch 2 Batch 100 Loss 2.5341 Accuracy 0.1051<br>Epoch 2 Batch 150 Loss 2.5218 Accuracy 0.1076<br>Epoch 2 Batch 200 Loss 2.4960 Accuracy 0.1095<br>Epoch 2 Batch 250 Loss 2.4707 Accuracy 0.1115<br>Epoch 2 Batch 300 Loss 2.4528 Accuracy 0.1133<br>Epoch 2 Batch 350 Loss 2.4393 Accuracy 0.1150<br>Epoch 2 Batch 400 Loss 2.4268 Accuracy 0.1165<br>Epoch 2 Batch 450 Loss 2.4125 Accuracy 0.1182<br>Epoch 2 Batch 500 Loss 2.4002 Accuracy 0.1196<br>Epoch 2 Batch 550 Loss 2.3885 Accuracy 0.1209<br>Epoch 2 Batch 600 Loss 2.3758 Accuracy 0.1222<br>Epoch 2 Batch 650 Loss 2.3651 Accuracy 0.1235<br>Epoch 2 Batch 700 Loss 2.3557 Accuracy 0.1247<br>Epoch 2 Loss 2.3552 Accuracy 0.1247<br>Time taken for 1 epoch: 341.75365233421326 secs</p><p>Epoch 3 Batch 0 Loss 1.8798 Accuracy 0.1347<br>Epoch 3 Batch 50 Loss 2.1781 Accuracy 0.1438<br>Epoch 3 Batch 100 Loss 2.1810 Accuracy 0.1444<br>Epoch 3 Batch 150 Loss 2.1796 Accuracy 0.1452<br>Epoch 3 Batch 200 Loss 2.1759 Accuracy 0.1462<br>Epoch 3 Batch 250 Loss 2.1710 Accuracy 0.1471<br>Epoch 3 Batch 300 Loss 2.1625 Accuracy 0.1473<br>Epoch 3 Batch 350 Loss 2.1520 Accuracy 0.1476<br>Epoch 3 Batch 400 Loss 2.1411 Accuracy 0.1481<br>Epoch 3 Batch 450 Loss 2.1306 Accuracy 0.1484<br>Epoch 3 Batch 500 Loss 2.1276 Accuracy 0.1490<br>Epoch 3 Batch 550 Loss 2.1231 Accuracy 0.1497<br>Epoch 3 Batch 600 Loss 2.1143 Accuracy 0.1500<br>Epoch 3 Batch 650 Loss 2.1063 Accuracy 0.1508<br>Epoch 3 Batch 700 Loss 2.1034 Accuracy 0.1519<br>Epoch 3 Loss 2.1036 Accuracy 0.1519<br>Time taken for 1 epoch: 328.1187334060669 secs</p><p>Epoch 4 Batch 0 Loss 2.0632 Accuracy 0.1622<br>Epoch 4 Batch 50 Loss 1.9662 Accuracy 0.1642<br>Epoch 4 Batch 100 Loss 1.9674 Accuracy 0.1656<br>Epoch 4 Batch 150 Loss 1.9682 Accuracy 0.1667<br>Epoch 4 Batch 200 Loss 1.9538 Accuracy 0.1679<br>Epoch 4 Batch 250 Loss 1.9385 Accuracy 0.1683<br>Epoch 4 Batch 300 Loss 1.9296 Accuracy 0.1694<br>Epoch 4 Batch 350 Loss 1.9248 Accuracy 0.1705<br>Epoch 4 Batch 400 Loss 1.9178 Accuracy 0.1716<br>Epoch 4 Batch 450 Loss 1.9068 Accuracy 0.1724<br>Epoch 4 Batch 500 Loss 1.8983 Accuracy 0.1735<br>Epoch 4 Batch 550 Loss 1.8905 Accuracy 0.1745<br>Epoch 4 Batch 600 Loss 1.8851 Accuracy 0.1757<br>Epoch 4 Batch 650 Loss 1.8793 Accuracy 0.1768<br>Epoch 4 Batch 700 Loss 1.8742 Accuracy 0.1779<br>Epoch 4 Loss 1.8746 Accuracy 0.1780<br>Time taken for 1 epoch: 326.3032810688019 secs</p><p>Epoch 5 Batch 0 Loss 1.9596 Accuracy 0.1979<br>Epoch 5 Batch 50 Loss 1.7048 Accuracy 0.1961<br>Epoch 5 Batch 100 Loss 1.6949 Accuracy 0.1969<br>Epoch 5 Batch 150 Loss 1.6942 Accuracy 0.1986<br>Epoch 5 Batch 200 Loss 1.6876 Accuracy 0.1992<br>Epoch 5 Batch 250 Loss 1.6827 Accuracy 0.1994<br>Epoch 5 Batch 300 Loss 1.6776 Accuracy 0.2006<br>Epoch 5 Batch 350 Loss 1.6740 Accuracy 0.2013<br>Epoch 5 Batch 400 Loss 1.6706 Accuracy 0.2019<br>Epoch 5 Batch 450 Loss 1.6656 Accuracy 0.2028<br>Epoch 5 Batch 500 Loss 1.6599 Accuracy 0.2035<br>Epoch 5 Batch 550 Loss 1.6558 Accuracy 0.2040<br>Epoch 5 Batch 600 Loss 1.6519 Accuracy 0.2047<br>Epoch 5 Batch 650 Loss 1.6510 Accuracy 0.2053<br>Epoch 5 Batch 700 Loss 1.6453 Accuracy 0.2058<br>Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1<br>Epoch 5 Loss 1.6453 Accuracy 0.2058<br>Time taken for 1 epoch: 307.13636589050293 secs</p><p>Epoch 6 Batch 0 Loss 1.5280 Accuracy 0.2127<br>Epoch 6 Batch 50 Loss 1.5062 Accuracy 0.2214<br>Epoch 6 Batch 100 Loss 1.5121 Accuracy 0.2225<br>Epoch 6 Batch 150 Loss 1.5051 Accuracy 0.2216<br>Epoch 6 Batch 200 Loss 1.5014 Accuracy 0.2219<br>Epoch 6 Batch 250 Loss 1.4984 Accuracy 0.2222<br>Epoch 6 Batch 300 Loss 1.4966 Accuracy 0.2232<br>Epoch 6 Batch 350 Loss 1.4929 Accuracy 0.2231<br>Epoch 6 Batch 400 Loss 1.4900 Accuracy 0.2234<br>Epoch 6 Batch 450 Loss 1.4836 Accuracy 0.2237<br>Epoch 6 Batch 500 Loss 1.4792 Accuracy 0.2241<br>Epoch 6 Batch 550 Loss 1.4727 Accuracy 0.2245<br>Epoch 6 Batch 600 Loss 1.4695 Accuracy 0.2251<br>Epoch 6 Batch 650 Loss 1.4659 Accuracy 0.2256<br>Epoch 6 Batch 700 Loss 1.4625 Accuracy 0.2262<br>Epoch 6 Loss 1.4619 Accuracy 0.2262<br>Time taken for 1 epoch: 303.32839941978455 secs</p><p>Epoch 7 Batch 0 Loss 1.1667 Accuracy 0.2262<br>Epoch 7 Batch 50 Loss 1.3010 Accuracy 0.2407<br>Epoch 7 Batch 100 Loss 1.3009 Accuracy 0.2400<br>Epoch 7 Batch 150 Loss 1.2983 Accuracy 0.2414<br>Epoch 7 Batch 200 Loss 1.2959 Accuracy 0.2428<br>Epoch 7 Batch 250 Loss 1.2948 Accuracy 0.2436<br>Epoch 7 Batch 300 Loss 1.2928 Accuracy 0.2439<br>Epoch 7 Batch 350 Loss 1.2901 Accuracy 0.2442<br>Epoch 7 Batch 400 Loss 1.2831 Accuracy 0.2448<br>Epoch 7 Batch 450 Loss 1.2844 Accuracy 0.2458<br>Epoch 7 Batch 500 Loss 1.2832 Accuracy 0.2463<br>Epoch 7 Batch 550 Loss 1.2827 Accuracy 0.2469<br>Epoch 7 Batch 600 Loss 1.2786 Accuracy 0.2470<br>Epoch 7 Batch 650 Loss 1.2738 Accuracy 0.2473<br>Epoch 7 Batch 700 Loss 1.2737 Accuracy 0.2480<br>Epoch 7 Loss 1.2737 Accuracy 0.2480<br>Time taken for 1 epoch: 314.8111472129822 secs</p><p>Epoch 8 Batch 0 Loss 1.1562 Accuracy 0.2611<br>Epoch 8 Batch 50 Loss 1.1305 Accuracy 0.2637<br>Epoch 8 Batch 100 Loss 1.1262 Accuracy 0.2644<br>Epoch 8 Batch 150 Loss 1.1193 Accuracy 0.2639<br>Epoch 8 Batch 200 Loss 1.1210 Accuracy 0.2645<br>Epoch 8 Batch 250 Loss 1.1177 Accuracy 0.2651<br>Epoch 8 Batch 300 Loss 1.1182 Accuracy 0.2648<br>Epoch 8 Batch 350 Loss 1.1200 Accuracy 0.2653<br>Epoch 8 Batch 400 Loss 1.1212 Accuracy 0.2655<br>Epoch 8 Batch 450 Loss 1.1207 Accuracy 0.2653<br>Epoch 8 Batch 500 Loss 1.1222 Accuracy 0.2660<br>Epoch 8 Batch 550 Loss 1.1219 Accuracy 0.2664<br>Epoch 8 Batch 600 Loss 1.1229 Accuracy 0.2663<br>Epoch 8 Batch 650 Loss 1.1211 Accuracy 0.2664<br>Epoch 8 Batch 700 Loss 1.1206 Accuracy 0.2668<br>Epoch 8 Loss 1.1207 Accuracy 0.2668<br>Time taken for 1 epoch: 301.5652780532837 secs</p><p>Epoch 9 Batch 0 Loss 0.8384 Accuracy 0.2751<br>Epoch 9 Batch 50 Loss 0.9923 Accuracy 0.2793<br>Epoch 9 Batch 100 Loss 0.9958 Accuracy 0.2796<br>Epoch 9 Batch 150 Loss 0.9953 Accuracy 0.2787<br>Epoch 9 Batch 200 Loss 0.9937 Accuracy 0.2790<br>Epoch 9 Batch 250 Loss 0.9988 Accuracy 0.2800<br>Epoch 9 Batch 300 Loss 0.9999 Accuracy 0.2801<br>Epoch 9 Batch 350 Loss 1.0021 Accuracy 0.2800<br>Epoch 9 Batch 400 Loss 1.0001 Accuracy 0.2800<br>Epoch 9 Batch 450 Loss 1.0013 Accuracy 0.2800<br>Epoch 9 Batch 500 Loss 1.0027 Accuracy 0.2805<br>Epoch 9 Batch 550 Loss 1.0034 Accuracy 0.2804<br>Epoch 9 Batch 600 Loss 1.0071 Accuracy 0.2810<br>Epoch 9 Batch 650 Loss 1.0076 Accuracy 0.2810<br>Epoch 9 Batch 700 Loss 1.0075 Accuracy 0.2806<br>Epoch 9 Loss 1.0076 Accuracy 0.2806<br>Time taken for 1 epoch: 304.53144931793213 secs</p><p>Epoch 10 Batch 0 Loss 0.9130 Accuracy 0.3057<br>Epoch 10 Batch 50 Loss 0.8950 Accuracy 0.2966<br>Epoch 10 Batch 100 Loss 0.9066 Accuracy 0.2967<br>Epoch 10 Batch 150 Loss 0.9128 Accuracy 0.2958<br>Epoch 10 Batch 200 Loss 0.9099 Accuracy 0.2943<br>Epoch 10 Batch 250 Loss 0.9131 Accuracy 0.2935<br>Epoch 10 Batch 300 Loss 0.9155 Accuracy 0.2930<br>Epoch 10 Batch 350 Loss 0.9144 Accuracy 0.2922<br>Epoch 10 Batch 400 Loss 0.9148 Accuracy 0.2922<br>Epoch 10 Batch 450 Loss 0.9170 Accuracy 0.2916<br>Epoch 10 Batch 500 Loss 0.9164 Accuracy 0.2910<br>Epoch 10 Batch 550 Loss 0.9175 Accuracy 0.2908<br>Epoch 10 Batch 600 Loss 0.9193 Accuracy 0.2908<br>Epoch 10 Batch 650 Loss 0.9229 Accuracy 0.2907<br>Epoch 10 Batch 700 Loss 0.9245 Accuracy 0.2910<br>Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2<br>Epoch 10 Loss 0.9247 Accuracy 0.2910<br>Time taken for 1 epoch: 308.50231170654297 secs</p><p>Epoch 11 Batch 0 Loss 0.8796 Accuracy 0.3030<br>Epoch 11 Batch 50 Loss 0.8186 Accuracy 0.3025<br>Epoch 11 Batch 100 Loss 0.8268 Accuracy 0.3020<br>Epoch 11 Batch 150 Loss 0.8422 Accuracy 0.3026<br>Epoch 11 Batch 200 Loss 0.8453 Accuracy 0.3023<br>Epoch 11 Batch 250 Loss 0.8472 Accuracy 0.3020<br>Epoch 11 Batch 300 Loss 0.8478 Accuracy 0.3019<br>Epoch 11 Batch 350 Loss 0.8488 Accuracy 0.3018<br>Epoch 11 Batch 400 Loss 0.8509 Accuracy 0.3017<br>Epoch 11 Batch 450 Loss 0.8505 Accuracy 0.3012<br>Epoch 11 Batch 500 Loss 0.8505 Accuracy 0.3009<br>Epoch 11 Batch 550 Loss 0.8514 Accuracy 0.3005<br>Epoch 11 Batch 600 Loss 0.8541 Accuracy 0.3001<br>Epoch 11 Batch 650 Loss 0.8568 Accuracy 0.2998<br>Epoch 11 Batch 700 Loss 0.8581 Accuracy 0.2995<br>Epoch 11 Loss 0.8586 Accuracy 0.2996<br>Time taken for 1 epoch: 326.4959843158722 secs</p><p>Epoch 12 Batch 0 Loss 0.8353 Accuracy 0.3318<br>Epoch 12 Batch 50 Loss 0.7892 Accuracy 0.3161<br>Epoch 12 Batch 100 Loss 0.7778 Accuracy 0.3134<br>Epoch 12 Batch 150 Loss 0.7817 Accuracy 0.3132<br>Epoch 12 Batch 200 Loss 0.7845 Accuracy 0.3132<br>Epoch 12 Batch 250 Loss 0.7881 Accuracy 0.3124<br>Epoch 12 Batch 300 Loss 0.7903 Accuracy 0.3122<br>Epoch 12 Batch 350 Loss 0.7894 Accuracy 0.3107<br>Epoch 12 Batch 400 Loss 0.7889 Accuracy 0.3097<br>Epoch 12 Batch 450 Loss 0.7917 Accuracy 0.3089<br>Epoch 12 Batch 500 Loss 0.7947 Accuracy 0.3089<br>Epoch 12 Batch 550 Loss 0.7965 Accuracy 0.3087<br>Epoch 12 Batch 600 Loss 0.7990 Accuracy 0.3082<br>Epoch 12 Batch 650 Loss 0.8002 Accuracy 0.3077<br>Epoch 12 Batch 700 Loss 0.8026 Accuracy 0.3076<br>Epoch 12 Loss 0.8028 Accuracy 0.3076<br>Time taken for 1 epoch: 306.4404299259186 secs</p><p>Epoch 13 Batch 0 Loss 0.7718 Accuracy 0.3059<br>Epoch 13 Batch 50 Loss 0.7275 Accuracy 0.3206<br>Epoch 13 Batch 100 Loss 0.7308 Accuracy 0.3206<br>Epoch 13 Batch 150 Loss 0.7317 Accuracy 0.3186<br>Epoch 13 Batch 200 Loss 0.7342 Accuracy 0.3174<br>Epoch 13 Batch 250 Loss 0.7349 Accuracy 0.3171<br>Epoch 13 Batch 300 Loss 0.7374 Accuracy 0.3167<br>Epoch 13 Batch 350 Loss 0.7397 Accuracy 0.3166<br>Epoch 13 Batch 400 Loss 0.7410 Accuracy 0.3163<br>Epoch 13 Batch 450 Loss 0.7415 Accuracy 0.3154<br>Epoch 13 Batch 500 Loss 0.7434 Accuracy 0.3150<br>Epoch 13 Batch 550 Loss 0.7466 Accuracy 0.3148<br>Epoch 13 Batch 600 Loss 0.7490 Accuracy 0.3142<br>Epoch 13 Batch 650 Loss 0.7522 Accuracy 0.3142<br>Epoch 13 Batch 700 Loss 0.7552 Accuracy 0.3142<br>Epoch 13 Loss 0.7554 Accuracy 0.3142<br>Time taken for 1 epoch: 299.16382122039795 secs</p><p>Epoch 14 Batch 0 Loss 0.6654 Accuracy 0.3193<br>Epoch 14 Batch 50 Loss 0.6744 Accuracy 0.3277<br>Epoch 14 Batch 100 Loss 0.6809 Accuracy 0.3237<br>Epoch 14 Batch 150 Loss 0.6830 Accuracy 0.3238<br>Epoch 14 Batch 200 Loss 0.6875 Accuracy 0.3235<br>Epoch 14 Batch 250 Loss 0.6942 Accuracy 0.3238<br>Epoch 14 Batch 300 Loss 0.6976 Accuracy 0.3231<br>Epoch 14 Batch 350 Loss 0.7000 Accuracy 0.3230<br>Epoch 14 Batch 400 Loss 0.7019 Accuracy 0.3222<br>Epoch 14 Batch 450 Loss 0.7035 Accuracy 0.3212<br>Epoch 14 Batch 500 Loss 0.7077 Accuracy 0.3207<br>Epoch 14 Batch 550 Loss 0.7078 Accuracy 0.3201<br>Epoch 14 Batch 600 Loss 0.7095 Accuracy 0.3196<br>Epoch 14 Batch 650 Loss 0.7127 Accuracy 0.3197<br>Epoch 14 Batch 700 Loss 0.7148 Accuracy 0.3193<br>Epoch 14 Loss 0.7153 Accuracy 0.3194<br>Time taken for 1 epoch: 294.01167726516724 secs</p><p>Epoch 15 Batch 0 Loss 0.6159 Accuracy 0.3546<br>Epoch 15 Batch 50 Loss 0.6416 Accuracy 0.3339<br>Epoch 15 Batch 100 Loss 0.6477 Accuracy 0.3323<br>Epoch 15 Batch 150 Loss 0.6480 Accuracy 0.3300<br>Epoch 15 Batch 200 Loss 0.6518 Accuracy 0.3286<br>Epoch 15 Batch 250 Loss 0.6536 Accuracy 0.3283<br>Epoch 15 Batch 300 Loss 0.6576 Accuracy 0.3276<br>Epoch 15 Batch 350 Loss 0.6618 Accuracy 0.3274<br>Epoch 15 Batch 400 Loss 0.6657 Accuracy 0.3272<br>Epoch 15 Batch 450 Loss 0.6689 Accuracy 0.3269<br>Epoch 15 Batch 500 Loss 0.6693 Accuracy 0.3263<br>Epoch 15 Batch 550 Loss 0.6711 Accuracy 0.3255<br>Epoch 15 Batch 600 Loss 0.6740 Accuracy 0.3249<br>Epoch 15 Batch 650 Loss 0.6775 Accuracy 0.3250<br>Epoch 15 Batch 700 Loss 0.6796 Accuracy 0.3247<br>Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3<br>Epoch 15 Loss 0.6800 Accuracy 0.3247<br>Time taken for 1 epoch: 296.7416775226593 secs</p><p>Epoch 16 Batch 0 Loss 0.6764 Accuracy 0.3298<br>Epoch 16 Batch 50 Loss 0.6024 Accuracy 0.3335<br>Epoch 16 Batch 100 Loss 0.6089 Accuracy 0.3345<br>Epoch 16 Batch 150 Loss 0.6135 Accuracy 0.3315<br>Epoch 16 Batch 200 Loss 0.6191 Accuracy 0.3323<br>Epoch 16 Batch 250 Loss 0.6214 Accuracy 0.3324<br>Epoch 16 Batch 300 Loss 0.6230 Accuracy 0.3315<br>Epoch 16 Batch 350 Loss 0.6268 Accuracy 0.3313<br>Epoch 16 Batch 400 Loss 0.6294 Accuracy 0.3309<br>Epoch 16 Batch 450 Loss 0.6325 Accuracy 0.3306<br>Epoch 16 Batch 500 Loss 0.6350 Accuracy 0.3300<br>Epoch 16 Batch 550 Loss 0.6385 Accuracy 0.3298<br>Epoch 16 Batch 600 Loss 0.6405 Accuracy 0.3293<br>Epoch 16 Batch 650 Loss 0.6434 Accuracy 0.3291<br>Epoch 16 Batch 700 Loss 0.6472 Accuracy 0.3289<br>Epoch 16 Loss 0.6476 Accuracy 0.3290<br>Time taken for 1 epoch: 302.5653040409088 secs</p><p>Epoch 17 Batch 0 Loss 0.7453 Accuracy 0.3696<br>Epoch 17 Batch 50 Loss 0.5800 Accuracy 0.3427<br>Epoch 17 Batch 100 Loss 0.5841 Accuracy 0.3422<br>Epoch 17 Batch 150 Loss 0.5912 Accuracy 0.3409<br>Epoch 17 Batch 200 Loss 0.5911 Accuracy 0.3384<br>Epoch 17 Batch 250 Loss 0.5962 Accuracy 0.3389<br>Epoch 17 Batch 300 Loss 0.5997 Accuracy 0.3389<br>Epoch 17 Batch 350 Loss 0.6017 Accuracy 0.3383<br>Epoch 17 Batch 400 Loss 0.6042 Accuracy 0.3376<br>Epoch 17 Batch 450 Loss 0.6077 Accuracy 0.3375<br>Epoch 17 Batch 500 Loss 0.6106 Accuracy 0.3369<br>Epoch 17 Batch 550 Loss 0.6127 Accuracy 0.3361<br>Epoch 17 Batch 600 Loss 0.6148 Accuracy 0.3352<br>Epoch 17 Batch 650 Loss 0.6171 Accuracy 0.3346<br>Epoch 17 Batch 700 Loss 0.6195 Accuracy 0.3339<br>Epoch 17 Loss 0.6196 Accuracy 0.3339<br>Time taken for 1 epoch: 303.3943374156952 secs</p><p>Epoch 18 Batch 0 Loss 0.4733 Accuracy 0.3313<br>Epoch 18 Batch 50 Loss 0.5544 Accuracy 0.3395<br>Epoch 18 Batch 100 Loss 0.5637 Accuracy 0.3435<br>Epoch 18 Batch 150 Loss 0.5625 Accuracy 0.3421<br>Epoch 18 Batch 200 Loss 0.5686 Accuracy 0.3421<br>Epoch 18 Batch 250 Loss 0.5714 Accuracy 0.3413<br>Epoch 18 Batch 300 Loss 0.5727 Accuracy 0.3407<br>Epoch 18 Batch 350 Loss 0.5770 Accuracy 0.3406<br>Epoch 18 Batch 400 Loss 0.5759 Accuracy 0.3394<br>Epoch 18 Batch 450 Loss 0.5779 Accuracy 0.3390<br>Epoch 18 Batch 500 Loss 0.5810 Accuracy 0.3392<br>Epoch 18 Batch 550 Loss 0.5836 Accuracy 0.3388<br>Epoch 18 Batch 600 Loss 0.5870 Accuracy 0.3379<br>Epoch 18 Batch 650 Loss 0.5905 Accuracy 0.3378<br>Epoch 18 Batch 700 Loss 0.5945 Accuracy 0.3376<br>Epoch 18 Loss 0.5947 Accuracy 0.3376<br>Time taken for 1 epoch: 298.2541983127594 secs</p><p>Epoch 19 Batch 0 Loss 0.5082 Accuracy 0.3261<br>Epoch 19 Batch 50 Loss 0.5285 Accuracy 0.3451<br>Epoch 19 Batch 100 Loss 0.5336 Accuracy 0.3472<br>Epoch 19 Batch 150 Loss 0.5322 Accuracy 0.3440<br>Epoch 19 Batch 200 Loss 0.5355 Accuracy 0.3439<br>Epoch 19 Batch 250 Loss 0.5413 Accuracy 0.3441<br>Epoch 19 Batch 300 Loss 0.5461 Accuracy 0.3443<br>Epoch 19 Batch 350 Loss 0.5519 Accuracy 0.3441<br>Epoch 19 Batch 400 Loss 0.5548 Accuracy 0.3436<br>Epoch 19 Batch 450 Loss 0.5561 Accuracy 0.3427<br>Epoch 19 Batch 500 Loss 0.5595 Accuracy 0.3423<br>Epoch 19 Batch 550 Loss 0.5616 Accuracy 0.3416<br>Epoch 19 Batch 600 Loss 0.5658 Accuracy 0.3412<br>Epoch 19 Batch 650 Loss 0.5684 Accuracy 0.3407<br>Epoch 19 Batch 700 Loss 0.5707 Accuracy 0.3405<br>Epoch 19 Loss 0.5709 Accuracy 0.3406<br>Time taken for 1 epoch: 297.59109830856323 secs</p><p>Epoch 20 Batch 0 Loss 0.6551 Accuracy 0.3720<br>Epoch 20 Batch 50 Loss 0.5086 Accuracy 0.3527<br>Epoch 20 Batch 100 Loss 0.5160 Accuracy 0.3495<br>Epoch 20 Batch 150 Loss 0.5196 Accuracy 0.3495<br>Epoch 20 Batch 200 Loss 0.5210 Accuracy 0.3490<br>Epoch 20 Batch 250 Loss 0.5241 Accuracy 0.3487<br>Epoch 20 Batch 300 Loss 0.5287 Accuracy 0.3486<br>Epoch 20 Batch 350 Loss 0.5312 Accuracy 0.3477<br>Epoch 20 Batch 400 Loss 0.5337 Accuracy 0.3475<br>Epoch 20 Batch 450 Loss 0.5369 Accuracy 0.3469<br>Epoch 20 Batch 500 Loss 0.5377 Accuracy 0.3458<br>Epoch 20 Batch 550 Loss 0.5400 Accuracy 0.3453<br>Epoch 20 Batch 600 Loss 0.5441 Accuracy 0.3450<br>Epoch 20 Batch 650 Loss 0.5469 Accuracy 0.3445<br>Epoch 20 Batch 700 Loss 0.5507 Accuracy 0.3440<br>Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4<br>Epoch 20 Loss 0.5507 Accuracy 0.3440<br>Time taken for 1 epoch: 303.6011939048767 secs</p></blockquote><h2 id="9-6-评估"><a href="#9-6-评估" class="headerlink" title="9.6 评估"></a>9.6 评估</h2><p>评估过程包含以下步骤：</p><ul><li>使用<code>Portuguese tokenizer</code>对输入语句进行编码</li><li>解码输入<code>start token == tokenizer_en.vocab_size</code></li><li>计算<code>padding_mask</code>和<code>look_ahead_mask</code></li><li><code>decoder</code>输出预测结果</li><li>选择最后一个词，并且计算它的<code>argmax</code></li><li>将之前输出的词拼接起来，作为<code>deocder</code>的输入，用于预测后面的词</li><li>最后的到最终的预测结果</li></ul><blockquote><p>这个评估过程非常重要，实际上这也是模型训练好以后，我们使用模型进行翻译的过程。我们可以看到这个过程是一步一步进行的，专业术语叫做<em>Auto-Regression</em>。虽然transformer的训练很快，但是推理却很慢，主要原因就是它做的是<em>Auto-regression</em>，不能进行并行化推理，所以后续很多对transformer的改进工作都是在这上面做的改进，我会在后续的博客中详细介绍相关模型。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(inp_sentence)</span>:</span></span><br><span class="line">    start_token = [tokenizer_pt.vocab_size]</span><br><span class="line">    end_token = [tokenizer_pt.vocab_size + <span class="number">1</span>]</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># inp sentence is portuguese, hence adding the start and end token</span></span><br><span class="line">    inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token</span><br><span class="line">    encoder_input = tf.expand_dims(inp_sentence, <span class="number">0</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># as the target is english, the first word to the transformer should be the</span></span><br><span class="line">    <span class="comment"># english start token.</span></span><br><span class="line">    decoder_input = [tokenizer_en.vocab_size]</span><br><span class="line">    output = tf.expand_dims(decoder_input, <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(MAX_LENGTH):</span><br><span class="line">        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(</span><br><span class="line">            encoder_input, output)</span><br><span class="line">  </span><br><span class="line">        <span class="comment"># predictions.shape == (batch_size, seq_len, vocab_size)</span></span><br><span class="line">        predictions, attention_weights = transformer(encoder_input, </span><br><span class="line">                                                     output,</span><br><span class="line">                                                     <span class="literal">False</span>,</span><br><span class="line">                                                     enc_padding_mask,</span><br><span class="line">                                                     combined_mask,</span><br><span class="line">                                                     dec_padding_mask)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># select the last word from the seq_len dimension</span></span><br><span class="line">        predictions = predictions[: ,<span class="number">-1</span>:, :]  <span class="comment"># (batch_size, 1, vocab_size)</span></span><br><span class="line"></span><br><span class="line">        predicted_id = tf.cast(tf.argmax(predictions, axis=<span class="number">-1</span>), tf.int32)</span><br><span class="line">      </span><br><span class="line">        <span class="comment"># return the result if the predicted_id is equal to the end token</span></span><br><span class="line">        <span class="keyword">if</span> predicted_id == tokenizer_en.vocab_size+<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> tf.squeeze(output, axis=<span class="number">0</span>), attention_weights</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># concatentate the predicted_id to the output which is given to the decoder</span></span><br><span class="line">        <span class="comment"># as its input.</span></span><br><span class="line">        output = tf.concat([output, predicted_id], axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tf.squeeze(output, axis=<span class="number">0</span>), attention_weights</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_attention_weights</span><span class="params">(attention, sentence, result, layer)</span>:</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">16</span>, <span class="number">8</span>))</span><br><span class="line">  </span><br><span class="line">    sentence = tokenizer_pt.encode(sentence)</span><br><span class="line">  </span><br><span class="line">    attention = tf.squeeze(attention[layer], axis=<span class="number">0</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> head <span class="keyword">in</span> range(attention.shape[<span class="number">0</span>]):</span><br><span class="line">        ax = fig.add_subplot(<span class="number">2</span>, <span class="number">4</span>, head+<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># plot the attention weights</span></span><br><span class="line">        ax.matshow(attention[head][:<span class="number">-1</span>, :], cmap=<span class="string">'viridis'</span>)</span><br><span class="line"></span><br><span class="line">        fontdict = &#123;<span class="string">'fontsize'</span>: <span class="number">10</span>&#125;</span><br><span class="line">    </span><br><span class="line">        ax.set_xticks(range(len(sentence)+<span class="number">2</span>))</span><br><span class="line">        ax.set_yticks(range(len(result)))</span><br><span class="line">    </span><br><span class="line">        ax.set_ylim(len(result)<span class="number">-1.5</span>, <span class="number">-0.5</span>)</span><br><span class="line">        </span><br><span class="line">        ax.set_xticklabels(</span><br><span class="line">            [<span class="string">'&lt;start&gt;'</span>]+[tokenizer_pt.decode([i]) <span class="keyword">for</span> i <span class="keyword">in</span> sentence]+[<span class="string">'&lt;end&gt;'</span>], </span><br><span class="line">            fontdict=fontdict, rotation=<span class="number">90</span>)</span><br><span class="line">    </span><br><span class="line">        ax.set_yticklabels([tokenizer_en.decode([i]) <span class="keyword">for</span> i <span class="keyword">in</span> result </span><br><span class="line">                           <span class="keyword">if</span> i &lt; tokenizer_en.vocab_size], </span><br><span class="line">                           fontdict=fontdict)</span><br><span class="line">    </span><br><span class="line">        ax.set_xlabel(<span class="string">'Head &#123;&#125;'</span>.format(head+<span class="number">1</span>))</span><br><span class="line">  </span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">translate</span><span class="params">(sentence, plot=<span class="string">''</span>)</span>:</span></span><br><span class="line">    result, attention_weights = evaluate(sentence)</span><br><span class="line">  </span><br><span class="line">    predicted_sentence = tokenizer_en.decode([i <span class="keyword">for</span> i <span class="keyword">in</span> result </span><br><span class="line">                                              <span class="keyword">if</span> i &lt; tokenizer_en.vocab_size])  </span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Input: &#123;&#125;'</span>.format(sentence))</span><br><span class="line">    print(<span class="string">'Predicted translation: &#123;&#125;'</span>.format(predicted_sentence))</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> plot:</span><br><span class="line">        plot_attention_weights(attention_weights, sentence, result, plot)</span><br></pre></td></tr></table></figure><blockquote><p>translate(“este é um problema que temos que resolver.”)<br>print (“Real translation: this is a problem we have to solve .”)</p></blockquote><p><img src="https://www.tensorflow.org/beta/tutorials/text/transformer_files/output_t-kFyiOLH0xg_1.png" alt="png"></p><h1 id="10-参考资料"><a href="#10-参考资料" class="headerlink" title="10. 参考资料"></a>10. 参考资料</h1><p><a href="https://www.tensorflow.org/beta/tutorials/text/transformer" target="_blank" rel="noopener">Transformer model for language understanding</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer代码实现-Pytorch版</title>
      <link href="/2019/09/11/Transformer%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-pytorch/"/>
      <url>/2019/09/11/Transformer%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-pytorch/</url>
      
        <content type="html"><![CDATA[<p>前面介绍了Transformer的模型结构，最后也给出了<code>pytorch</code>版本的代码实现，但是始终觉得不够过瘾，有些话还没说清楚，因此，这篇文章专门用来讨论Transformer的代码细节。</p><a id="more"></a><p>本文主要参考了：<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">The Annotated Transformer</a>。这篇文章是哈佛大学OpenNMT团队的工作，所以在正式进入话题之前要先把代码环境搭建好。<code>Pytorch</code>的安装网上有详细的教程这里不再赘述，只简单提一点，直接下载安装的话可能会速度比较慢，甚至下载失败，可以使用国内清华大学的镜像进行安装：</p><ul><li>添加清华大学镜像：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --set show_channel_urls yes</span><br></pre></td></tr></table></figure><ul><li>添加<code>pytorch</code>镜像：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br></pre></td></tr></table></figure><h1 id="1-前期准备"><a href="#1-前期准备" class="headerlink" title="1. 前期准备"></a>1. 前期准备</h1><p>导入相应的包</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> math, copy, time</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn</span><br><span class="line">seaborn.set_context(context=<span class="string">"talk"</span>)</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h1 id="2-模型结构"><a href="#2-模型结构" class="headerlink" title="2. 模型结构"></a>2. 模型结构</h1><p>大多数有竞争力的神经序列转换模型都有<em>encoder-decoder</em>结构，其中<em>encoder</em>部分将输入序列$(x_1, x_2, …, x_n)$映射到一个连续表示的序列$\mathbf{z}=(z_1, z_2, …, z_n)$中。给定$\mathbf{z}$，<em>decoder</em>再生成一个输出序列$(y_1, y_2, …, y_m)$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. Base for this and many </span></span><br><span class="line"><span class="string">    other models.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line">        self.generator = generator</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Take in and process masked src and target sequences.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Define standard linear + softmax generation step.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img.vim-cn.com/3a/78ea12dca1ce0f99f9a9705466afc16c58c3cf.png" alt></p><p><em>encoder</em>和<em>decoder</em>都是由6个这样的结构堆叠而成的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span><span class="params">(module, N)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Produce N identical layers.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)])</span><br></pre></td></tr></table></figure><h2 id="2-1-Encoder"><a href="#2-1-Encoder" class="headerlink" title="2.1 Encoder"></a>2.1 Encoder</h2><p><em>encoder</em>是由6个相同的模块堆叠在一起的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Core encoder is a stack of N layers.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Pass the input (and mask) through each layer in turn.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><p>每个<em>encoder block</em>由<em>Multi-head Attention</em>和<em>Feed Forward</em>两个sub-layer组成，每个sub-layer后面会接一个layer normalization：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Construct a layer normalization module.</span></span><br><span class="line"><span class="string">    See https://arxiv.org/abs/1607.06450 for detail</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        mean = x.mwan(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure><p>sub-layer和layer normalization之间使用残差方式进行连接（进行残差连接之前都会先进行Dropout）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    See http://jmlr.org/papers/v15/srivastava14a.html for dropout detail</span></span><br><span class="line"><span class="string">    and https://arxiv.org/abs/1512.03385 for residual connection detail.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Apply residual connection to any sublayer with the same size.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure><p>模型中，为了使<code>x + self.dropout(sublayer(self.norm(x)))</code>能够正常运行，必须保证<code>x</code>和<code>dropout</code>的维度保持一致，论文中使用的$d_{model}=512$（包括embedding层）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Encoder block consist of two sub-layers (define below): </span></span><br><span class="line"><span class="string">    - multi-head attention (self-attention) </span></span><br><span class="line"><span class="string">    - feed forward.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Encoder block.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure><h2 id="2-2-Decoder"><a href="#2-2-Decoder" class="headerlink" title="2.2 Decoder"></a>2.2 Decoder</h2><p><em>Decoder</em>同样是由6个相同的模块堆叠在一起的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Generic N layer decoder with masking.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><p>与<em>encoder block</em>不同的是，在<em>decoder block</em>的<em>Multi-head attention</em>和<em>Feed forward</em>之间还会插入一个<em>Multi-head attention</em>，这个<em>attention</em>中的key和value来源于<em>encoder</em>的输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Encoder block consist of three sub-layers (define below): </span></span><br><span class="line"><span class="string">    - multi-head attention (self-attention) </span></span><br><span class="line"><span class="string">    - encoder multi-head attention </span></span><br><span class="line"><span class="string">    - feed forward.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn =  src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Decoder block.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure><p>为了保证解码过程中第$i$个位置的输出只依赖于前面已有的输出结果，在<em>decoder</em>中加入了<strong>Masking</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span><span class="params">(size)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Mask out subsequent position.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">'uint8'</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span></span><br></pre></td></tr></table></figure><p>下图的attention mask展示了每个目标词（行）可以看到的位置（列），黄色表示可以看到，紫色表示看不到。</p><p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_31_0.png" alt="png"></p><h1 id="3-模型细节"><a href="#3-模型细节" class="headerlink" title="3. 模型细节"></a>3. 模型细节</h1><p>上面我们实现了模型的整体结构，下面我们来实现其中的细节。前面我们提到，每个<em>encoder block</em>有两个sub-layer：<em>Multi-head attention</em>和<em>feed forward</em>，虽然<em>decoder block</em>有三个sub-layer，但是两个都是<em>Multi-head attention</em>，说到底还是只有<em>Multi-head attention</em>和<em>feed forward</em>。</p><h2 id="3-1-Multi-Head-Attention"><a href="#3-1-Multi-Head-Attention" class="headerlink" title="3.1 Multi-Head Attention"></a>3.1 Multi-Head Attention</h2><p>之前我们介绍的时候讲到所谓<em>Multi-Head Attention</em>是有两部分组成：<em>Multi-Head</em>和<em>Attention</em>。</p><p>结构如下：</p><p><img src="https://img.vim-cn.com/b1/e4bc841abc55d366813340f92f6696c5d59e95.png" alt></p><script type="math/tex; mode=display">\mathrm{MultiHead}(Q, K, V) = Concat(head_1, ..., head_h)\mathbf{W}^O</script><p>其中$head_i$就是Attention，即$head_i=\mathrm{Attention}(QW_i^Q, KW_i^K, VW_i^V)$具体结构如下图：</p><p><img src="https://img.vim-cn.com/ed/97e04d7d6067cb360e8fef1d29cf41978d353e.png" alt></p><p>先来看下<em>Scaled Dot-Product Attention</em>得出具体实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Compute `Scale Dot-Product Attention`.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        :params query: linear projected query maxtrix, Q in above figure right</span></span><br><span class="line"><span class="string">        :params key: linear projected key maxtrix, k in above figure right</span></span><br><span class="line"><span class="string">        :params value: linear projected value maxtrix, v in above figure right</span></span><br><span class="line"><span class="string">        :params mask: sub-sequence mask</span></span><br><span class="line"><span class="string">        :params dropout: rate of dropout</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        d_k = query.size(<span class="number">-1</span>)</span><br><span class="line">        scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) / math.sqrt(d_k)</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scores = scores.mask_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)</span><br><span class="line">        p_attn = F.softmax(scores, dim=<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            p_attn = self.dropout(p_attn)</span><br><span class="line">        <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure><p>下面我们就可以实现<em>Multi-head Attention</em>了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Build Multi-Head Attention sub-layer.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :params h: int, number of heads</span></span><br><span class="line"><span class="string">        :params d_model: model size</span></span><br><span class="line"><span class="string">        :params dropout: rate of dropout</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(MultiHeadAtention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># According to the paper, d_v always equals to d_k</span></span><br><span class="line">        <span class="comment"># and d_v = d_k = d_model / h = 64</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        <span class="comment"># following K, Q, V and `Concat`, so we need 4 linears</span></span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Implement Multi-Head Attention.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        :params query: query embedding matrix, Q in above figure left</span></span><br><span class="line"><span class="string">        :params key: key embedding matrix, K in above figure left</span></span><br><span class="line"><span class="string">        :params value value embedding matrix, V in above figure left</span></span><br><span class="line"><span class="string">        :params mask: sub-sequence mask</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># same mask applied to all heads</span></span><br><span class="line">            mask = mask.unsequeeze(<span class="number">1</span>)</span><br><span class="line">        n_batch = query.size(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 1. Do all the linear projections in batch from d_model to h x d_k</span></span><br><span class="line">        query, key, value = [l(x).view(n_batch, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">                             <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line">        <span class="comment"># 2. Apply attention on all the projected vectors in batch</span></span><br><span class="line">        x, self.attn = self.attention(query, key, value, mask=mask)</span><br><span class="line">        <span class="comment"># 3. `Concat` using a view and apply a final linear</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(n_batch, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)</span><br></pre></td></tr></table></figure><p><em>Transformer</em>中<em>Multi-Head Attention</em>有三种用法：</p><ol><li>在<em>decoder</em>层中，中间的<em>Multi-Head Attention</em>模块中query来源于前置<em>Masked Multi-Head Attention</em>模块，而key和value来源于<em>encoder</em>层的输出， 这一部分模仿了典型的<em>seq2seq</em>模型中的<em>encoder-decoder</em>注意力机制；</li><li>在<em>encoder</em>层中，所有的query, key, value都来源于前一个<em>encoder</em>层的输出；</li><li>类似的在<em>decoder</em>层中，有一个<em>Masked Multi-Head Attention</em>模块，其中<em>Masked</em>是因为在进行解码的过程中，我们是从左向右一步一步的进行解码，对于模型来说右侧的信息是缺失的，因此不应该对左侧的信息产生干扰，因此在模型中我们令相应位置的值为$\infty$。</li></ol><h2 id="3-2-Position-wise-Feed-Forward-Networks"><a href="#3-2-Position-wise-Feed-Forward-Networks" class="headerlink" title="3.2 Position-wise Feed-Forward Networks"></a>3.2 Position-wise Feed-Forward Networks</h2><p><em>Feed forward</em>部分是由两个<em>Relu</em>线性变换组成的，在同一个<em>block</em>内的的不同位置使用相同的参数，但是不同<em>block</em>使用不同的参数。</p><script type="math/tex; mode=display">\mathrm{FFN(x)} = \max(0, xW_1 + b_1)W_2 + b_2</script><p>这个操作类似于卷积核大小为1的卷积操作。              </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements FFN equation.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout)</span>:</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w2 = nn.Linear(d_ff, f_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.w2(self.dropout(F.relu(self.w1(x))))</span><br></pre></td></tr></table></figure><h2 id="3-3-Embedding和Softmax"><a href="#3-3-Embedding和Softmax" class="headerlink" title="3.3 Embedding和Softmax"></a>3.3 Embedding和Softmax</h2><p><em>Transformer</em>中使用预训练的word embeddings，并且输入和输出的word embedding保持一致。和其他模型不同的是word embedding并不是直接进入模型，而是乘上一个缩放因子$\sqrt{d_{model}}$：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure><h2 id="3-4-Position-Encoding"><a href="#3-4-Position-Encoding" class="headerlink" title="3.4 Position Encoding"></a>3.4 Position Encoding</h2><p>由于单纯的注意力机制没有有效的利用序列的顺序信息，因此作者在<em>Transformer</em>中加入了位置编码，用来抓住序列中的位置信息。</p><script type="math/tex; mode=display">PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})</script><script type="math/tex; mode=display">PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})</script><p>其中$pos$指得是位置索引，$i$是第$pos$个位置上对应向量的第$i$维。对序列的位置进行编码后，将输入序列和位置编码进行相加，得到一个新的输入序列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionEncoding</span><span class="params">(nn.module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements Position Encoding.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionEncoding, self).__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the position encodings once in log space</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * </span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">            x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], requires_grad=<span class="literal">False</span>)</span><br><span class="line">            <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure><p>至此， 整个模型各个模块我们已经搭建好了，最后进行总装。</p><h1 id="4-Full-Model"><a href="#4-Full-Model" class="headerlink" title="4. Full Model"></a>4. Full Model</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Construct Transformer model.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    :params src_vocab: source language vocabulary</span></span><br><span class="line"><span class="string">    :params tgt_vocab: target language vocabulary</span></span><br><span class="line"><span class="string">    :params N: number of encoder or decoder stacks</span></span><br><span class="line"><span class="string">    :params d_model: dimension of model input and output</span></span><br><span class="line"><span class="string">    :params d_ff: dimension of feed forward layer</span></span><br><span class="line"><span class="string">    :params h: number of attention head</span></span><br><span class="line"><span class="string">    :params dropout: rate of dropout</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff)</span><br><span class="line">    position = PositionEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">                Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout),N),   </span><br><span class="line">                Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),</span><br><span class="line">                nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">                nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">                Generator(d_model, tgt_vocab)</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code.</span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>至此Transformer模型已经完成了，下面介绍是整个模型的训练过程以及机器翻译过程的一些技巧和常用工具的介绍，没有兴趣的话到这里就可以结束了。</p><h1 id="5-模型训练"><a href="#5-模型训练" class="headerlink" title="5. 模型训练"></a>5. 模型训练</h1><p>本节快速介绍一些在训练<em>encoder-decoder</em>模型过程中常用的工具，首先定义一个<em>batch</em>对象用来获取训练所需的源句子和目标句子，以及构建<em>masking</em>。</p><h2 id="5-1-Batches-and-Masking"><a href="#5-1-Batches-and-Masking" class="headerlink" title="5.1 Batches and Masking"></a>5.1 Batches and Masking</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Batch</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Object for holding a batch of data with mask during training.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, src, tgt=None, pad=<span class="number">0</span>)</span>:</span></span><br><span class="line">        self.src = src</span><br><span class="line">        self.src_mask = (src != pad).unsequeeze(<span class="number">-2</span>)</span><br><span class="line">        <span class="keyword">if</span> tgt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.tgt = tgt[;, :<span class="number">-1</span>]</span><br><span class="line">            self.tgt_y = tgt[:, <span class="number">1</span>:]</span><br><span class="line">            self.tgtg_mask = self.make_std_mask(self.tgt, pad)</span><br><span class="line">            self.ntokens = (self.tgt_y != pad).data.sum()</span><br><span class="line">            </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_std_mask</span><span class="params">(tgt, pad)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Create a mask to hide padding and future words.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        tgt_mask = (tgt != pad).unsequeeze(<span class="number">-2</span>)</span><br><span class="line">        tgt_mask = tgt_mask &amp; Variable(</span><br><span class="line">            subsequent_mask(tgt.size(<span class="number">-1</span>)).type_as(tgt_mask.data)</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> tgt_mask</span><br></pre></td></tr></table></figure><p>接下来我们创建一个通用的训练和计算得分的函数用于跟踪损失。我们传入一个通用的用于更新权重的损失函数。</p><h2 id="5-2-Training-Loop"><a href="#5-2-Training-Loop" class="headerlink" title="5.2 Training Loop"></a>5.2 Training Loop</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_epoch</span><span class="params">(data_iter, model, loss_compute)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Standard training and logging function.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    total_tokens = <span class="number">0</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    tokens = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(data_iter):</span><br><span class="line">        out = model.forward(batch.src,</span><br><span class="line">                            batch.tgt,</span><br><span class="line">                            batch.src_mask,</span><br><span class="line">                            batch.tgt_mask)</span><br><span class="line">        loss = loss_compute(out, batch.tgt_y, batch.ntokens)</span><br><span class="line">        total_loss += loss</span><br><span class="line">        total_tokens += batch.ntokens</span><br><span class="line">        tokens += batch.ntokens</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">1</span>:</span><br><span class="line">            elapsed = time.time() - start_time</span><br><span class="line">            print(<span class="string">"Epoch Step: %d Loss: %f Tokens per Sec: %f"</span> </span><br><span class="line">                  % (i, loss / batch.ntokens, tokens / elapsed))</span><br><span class="line">            start_time = time.time()</span><br><span class="line">            token = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> total_loss / total_tokens</span><br></pre></td></tr></table></figure><h2 id="5-3-Training-Data-and-Batching"><a href="#5-3-Training-Data-and-Batching" class="headerlink" title="5.3 Training Data and Batching"></a>5.3 Training Data and Batching</h2><p>论文使用的数据集：</p><ul><li>WMT 2014 English-German dataset： 4.5 million sentence pairs</li><li>WMT 2014 English-French dataset： 36 M sentence pairs</li></ul><p>由于<em>Transformer</em>本身训练需要的资源较多，而且上面的数据集过多，本文只是从原理上实现算法，不需要训练一个实际可用的模型，因此并没有在这两个数据集上进行训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">global</span> max_src_in_batch, max_tgt_in_batch</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_size_fn</span><span class="params">(new, count, sofar)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Keeping augumenting batch and calculate total number of tokens + padding.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">global</span> max_src_in_batch, max_tgt_in_batch</span><br><span class="line">    <span class="keyword">if</span> count == <span class="number">1</span>:</span><br><span class="line">        max_src_in_batch = <span class="number">0</span></span><br><span class="line">        max_tgt_in_batch = <span class="number">0</span></span><br><span class="line">    max_src_in_batch = max(max_src_in_batch, len(new.src))</span><br><span class="line">    max_tgt_in_batch = max(max_tgt_in_batch, len(new.tgt) + <span class="number">2</span>)</span><br><span class="line">    src_elements = count * max_src_in_batch</span><br><span class="line">    tgt_elements = count * max_tgt_in_batch</span><br><span class="line">    <span class="keyword">return</span> max(src_elements, tgt_elements)</span><br></pre></td></tr></table></figure><h2 id="5-4-Optimizer"><a href="#5-4-Optimizer" class="headerlink" title="5.4 Optimizer"></a>5.4 Optimizer</h2><p>论文中使用<code>Adam</code>优化器，其中$\beta_1=0.9, \beta_2=0.98, \epsilon=10^{-9}$；学习率根据公式$l_{rate} = d_{model}^{-0.5} \cdot \min(step_num^{-0.5}, step_num \cdot warmup_steps^{-1.5})$来确定，该式意味着在开始的$warmup_steps$循环内学习率是线性增加的，达到一定程度后学习率开始下降， 论文中的$warmup_step=4000$，这是一个超参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NoamOpt</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Optimizer warp that implements rate.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model_size, factor, warmup, optimizer)</span>:</span></span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self._step = <span class="number">0</span></span><br><span class="line">        self.warmup = warmup</span><br><span class="line">        self.factor = factor</span><br><span class="line">        self.model_size = model_size</span><br><span class="line">        self._rate = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Update parameters and rate</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self._step += <span class="number">1</span></span><br><span class="line">        rate = self.rate()</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.optimizer.param_groups:</span><br><span class="line">            p[<span class="string">'lr'</span>] = rate</span><br><span class="line">        self._rate = rate</span><br><span class="line">        self.optmizer.step()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rate</span><span class="params">(self, step=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Implement `lrate` above</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> step <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            step = self._step</span><br><span class="line">        <span class="keyword">return</span> self.factor * self.model_size ** (<span class="number">-0.5</span>) * \</span><br><span class="line">               min(step ** (<span class="number">-0.5</span>), step * self.warmup ** (<span class="number">-1.5</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_std_opt</span><span class="params">(model)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> NoamOpt(model.src_embed[<span class="number">0</span>],d_model, <span class="number">2</span>, <span class="number">4000</span>, </span><br><span class="line">                       torch.optim.Adam(model.parameters(), </span><br><span class="line">                                        lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br></pre></td></tr></table></figure><p>下面给出了三组不同的优化器超参数的例子，直观的感受学习率的变化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">opts = [NoamOpt(<span class="number">512</span>, <span class="number">1</span>, <span class="number">4000</span>, <span class="literal">None</span>),</span><br><span class="line">        NoamOpt(<span class="number">512</span>, <span class="number">1</span>, <span class="number">8000</span>, <span class="literal">None</span>),</span><br><span class="line">        NoamOpt(<span class="number">256</span>, <span class="number">1</span>, <span class="number">4000</span>, <span class="literal">None</span>)]</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, <span class="number">20000</span>, [[opt.rate(i) <span class="keyword">for</span> opt <span class="keyword">in</span> opts] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">20000</span>)]))</span><br><span class="line">plt.legend([<span class="string">"512:4000"</span>, <span class="string">"512:8000"</span>, <span class="string">"256:4000"</span>])</span><br></pre></td></tr></table></figure><p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_69_0.png" alt="png"></p><h2 id="5-5-正则化"><a href="#5-5-正则化" class="headerlink" title="5.5 正则化"></a>5.5 正则化</h2><p>训练过程中作者使用了<em>label smoothing</em> $\epsilon_{ls}=0.1$， 虽然这样对<em>Perplexity</em>有所损伤， 但是提高了整体的BLEU值。这里我们用KL散度损失实现了<em>label smoothing</em>，并且使用分布式目标词分布用以替代<em>one-hot</em>分布。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelSmoothing</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements label smoothing.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, padding_idx, smoothing=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(LabelSmoothing, self).__init__()</span><br><span class="line">        self.criterion = nn.KLDivLoss(size_average=<span class="literal">False</span>)</span><br><span class="line">        self.padding_idx = padding_idx</span><br><span class="line">        self.confidence = <span class="number">1.0</span> = smoothing</span><br><span class="line">        self.size = size</span><br><span class="line">        self.true_dist = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, target)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x.size(<span class="number">1</span>) == self.size</span><br><span class="line">        true_dist = x.data.clone()</span><br><span class="line">        true_dist.fill_(self.smoothing / (self.size - <span class="number">2</span>))</span><br><span class="line">        true_dist.scatter_(<span class="number">1</span>, target.data.unsequeeze(<span class="number">1</span>), self.confidence)</span><br><span class="line">        true_dist[:, self.padding_idx] = <span class="number">0</span></span><br><span class="line">        mask = torch.nonzero(target.data == self.padding_idx)</span><br><span class="line">        <span class="keyword">if</span> mask_dim() &gt; <span class="number">0</span>:</span><br><span class="line">            true_dist.index_fill(<span class="number">0</span>, mask.sequeeze(), <span class="number">0.0</span>)</span><br><span class="line">        self.true_dist = true_dist</span><br><span class="line">        <span class="keyword">return</span> self.criterion(x, Variable(true_dist, requires_grad=<span class="literal">False</span>))</span><br></pre></td></tr></table></figure><p>下面给出一个例子说明基于置信度的词分布看起来是什么样的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.4</span>)</span><br><span class="line">predict = torch.FloatTensor([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>]</span><br><span class="line">])</span><br><span class="line">v = crit(Variable(predict.log()),</span><br><span class="line">         Variable(torch.LongTensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>])))</span><br><span class="line">plt.imshow(crit.true_dist)</span><br></pre></td></tr></table></figure><p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_74_0.png" alt="png"></p><p>如果模型对一个给定的选择给出非常大的置信度，<em>Label smoothing</em>就会开始对模型进行惩罚。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(x)</span>:</span></span><br><span class="line">    d = x + <span class="number">3</span> * <span class="number">1</span></span><br><span class="line">    predict = torch.FloatTensor([</span><br><span class="line">        [<span class="number">0</span>, x/d, <span class="number">1</span>/d, <span class="number">1</span>/d, <span class="number">1</span>/d]</span><br><span class="line">    ])</span><br><span class="line">    <span class="keyword">return</span> crit(Variable(predict.log()),</span><br><span class="line">                Variable(torch.LongTensor([<span class="number">1</span>]))).data[<span class="number">0</span>]</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, <span class="number">100</span>), [loss(x) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">100</span>)])</span><br></pre></td></tr></table></figure><p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_76_0.png" alt="png"></p><h1 id="6-第一个例子"><a href="#6-第一个例子" class="headerlink" title="6. 第一个例子"></a>6. 第一个例子</h1><p>在正式在真实的数据上做实验之前，我们可以先在一个随机生成的数据集上实验，目标是生成和源序列相同的序列，例如源序列是“I have a dream”，我们的目标是将序列输入到模型，然后输出这个序列。</p><h2 id="6-1-合成数据"><a href="#6-1-合成数据" class="headerlink" title="6.1 合成数据"></a>6.1 合成数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_gen</span><span class="params">(V, batch, nbatches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Generate random data for src-tgt copy task.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(nbatches):</span><br><span class="line">        data = torch.from_numpy(np.random.randint(<span class="number">1</span>, V, size=(batch, <span class="number">10</span>)))</span><br><span class="line">        data[:, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        src = Variable(data, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        tgt = Variable(data, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">yield</span> Batch(src, tgt, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="6-2-损失计算"><a href="#6-2-损失计算" class="headerlink" title="6.2 损失计算"></a>6.2 损失计算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimleLossCompute</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A simple loss compute and train function.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, generator, criterion, opt=None)</span>:</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        self.criterion = criterion</span><br><span class="line">        self.opt = opt</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x, y, norm)</span>:</span></span><br><span class="line">        x = self.generator(x)</span><br><span class="line">        loss = self.criterion(x.contiguous().view(<span class="number">-1</span>, x.size(<span class="number">-1</span>)),</span><br><span class="line">                              y.contiguous().view(<span class="number">-1</span>)) / norm</span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="keyword">if</span> self.opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.opt.step()</span><br><span class="line">            self.opt.optimizer.zero_grad()</span><br><span class="line">        <span class="keyword">return</span> loss.data[<span class="number">0</span>] * norm</span><br></pre></td></tr></table></figure><h2 id="6-3-Greedy-Decoding"><a href="#6-3-Greedy-Decoding" class="headerlink" title="6.3 Greedy Decoding"></a>6.3 Greedy Decoding</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">V = <span class="number">11</span></span><br><span class="line">criterion = LabelSmoothing(size=V, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.0</span>)</span><br><span class="line">model = make_model(V, V, N=<span class="number">2</span>)</span><br><span class="line">model_opt = NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">1</span>, <span class="number">400</span>,</span><br><span class="line">                    torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, </span><br><span class="line">                                     betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    run_epoch(data_gen(V, <span class="number">30</span>, <span class="number">20</span>), model, </span><br><span class="line">             SimpleLossCompute(model.generator, criterion, model_opt))</span><br><span class="line">    model.evel()</span><br><span class="line">    print(run_epoch(data_gen(V, <span class="number">30</span>, <span class="number">5</span>), model, </span><br><span class="line">                   SimpleLossCompute(model.generator, criterion, <span class="literal">None</span>)))</span><br></pre></td></tr></table></figure><p>实际的翻译模型一般使用<em>beam search</em>，这里为例简化代码，我们使用贪婪编码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greedy_decode</span><span class="params">(model, src, src_mask, max_len, start_symbol)</span>:</span></span><br><span class="line">    memory = model.encode(src, src_mask)</span><br><span class="line">    ys = torch.ones(<span class="number">1</span>, <span class="number">1</span>).fill_(start_symbol).type_as(src.data)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_len<span class="number">-1</span>):</span><br><span class="line">        out = model.decode(memory, src_mask, </span><br><span class="line">                          Variable(ys), </span><br><span class="line">                          Variable(subsequent_mask(ys.size(<span class="number">1</span>))</span><br><span class="line">                                   .type_as(src.data)))</span><br><span class="line">        prob = model.generator(out[:, <span class="number">-1</span>])</span><br><span class="line">        _, next_word = torch.max(prob, dim=<span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat([ys,</span><br><span class="line">                       torch.ones(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)],</span><br><span class="line">                       dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> ys</span><br><span class="line">    </span><br><span class="line">model.eval()</span><br><span class="line">src = Variable(torch.LongTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]]))</span><br><span class="line">src_mask = Variable(torch.ones(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>))</span><br><span class="line">print(greedy_decode(model, src, src_mask, max_len=<span class="number">10</span>, start_symbol=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><h1 id="7-A-Real-World-Example"><a href="#7-A-Real-World-Example" class="headerlink" title="7. A Real World Example"></a>7. A Real World Example</h1><p>这里我们使用IWSLT German-English Translation 数据集，这个数据集比论文中使用的数据集小得多，但是能够检验整个模型。下面我们还会演示怎样在多GPU上进行训练。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install torchtext spacy</span><br><span class="line">python -m spacy download en</span><br><span class="line">python -m spacy download de</span><br></pre></td></tr></table></figure><h2 id="7-1-数据加载"><a href="#7-1-数据加载" class="headerlink" title="7.1 数据加载"></a>7.1 数据加载</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data, datasets</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">import</span> spacy</span><br><span class="line">    spacy_de = spacy.load(<span class="string">'de'</span>)</span><br><span class="line">    spacy_en = spacy.load(<span class="string">'en'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize_de</span><span class="params">(text)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> spacy.tokenizer(text)]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize_en</span><span class="params">(text)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [tok.text <span class="keyword">for</span>  tok <span class="keyword">in</span> spcy.tokenizer(text)]</span><br><span class="line">    </span><br><span class="line">    BOS_WORD = <span class="string">'&lt;S&gt;'</span></span><br><span class="line">    EOS_WORD = <span class="string">'&lt;/S&gt;'</span></span><br><span class="line">    BLANK_WORD = <span class="string">'&lt;blank&gt;'</span></span><br><span class="line">    SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)</span><br><span class="line">    TGT = data.Field(tokenize=tokenize_en, init_token=BOS_WORD,</span><br><span class="line">                    eos_token=EOS_WORD, pad_token=BLANK_WORD)</span><br><span class="line">    </span><br><span class="line">    MAX_LEN = <span class="number">100</span></span><br><span class="line">    TRAIN, VAL, TEST = datasets.IWSLT.splits(</span><br><span class="line">        exts=(<span class="string">'.de'</span>, <span class="string">'.en'</span>), fields=(SRC, TGT),</span><br><span class="line">        filter_pred=<span class="keyword">lambda</span> x: len(vars(x)[<span class="string">'src'</span>]) &lt;= MAX_LEN <span class="keyword">and</span></span><br><span class="line">            len(vars(x)[<span class="string">'trg'</span>]) &lt;= MAX_LEN</span><br><span class="line">    )</span><br><span class="line">    MIN_FREQ = <span class="number">2</span></span><br><span class="line">    SRC.build_vocab(train.src, min_freq = MIN_FREQ)</span><br><span class="line">    TGT.build_vocab(train.trg, min_freq = MIN_FREQ)</span><br></pre></td></tr></table></figure><h2 id="7-2-Iterators"><a href="#7-2-Iterators" class="headerlink" title="7.2 Iterators"></a>7.2 Iterators</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyIterator</span><span class="params">(data.Iterator)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_batches</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">pool</span><span class="params">(d, random_shuffler)</span>:</span></span><br><span class="line">                <span class="keyword">for</span> p  <span class="keyword">in</span> data.batch(d, self.batch_size * <span class="number">100</span>):</span><br><span class="line">                    p_batch =  data.batch(</span><br><span class="line">                        sorted(p, key=self.sort_key),</span><br><span class="line">                        self.batch_size, self.batch_size_fn</span><br><span class="line">                    )</span><br><span class="line">                    <span class="keyword">for</span> b <span class="keyword">in</span> random_shuffler(list(p_batch)):</span><br><span class="line">                        <span class="keyword">yield</span> b</span><br><span class="line">             self.batches = pool(self.data(), self.random_shuffler)</span><br><span class="line">            </span><br><span class="line">         <span class="keyword">else</span>:</span><br><span class="line">             self.batches = []</span><br><span class="line">                <span class="keyword">for</span> b <span class="keyword">in</span> data.batch(self.data(), self.batch_size,</span><br><span class="line">                                   self.batch_size_fn):</span><br><span class="line">                    self.batches.append(sorted(b, key=self.sort_key))</span><br><span class="line">                    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rebatch</span><span class="params">(pad_idx, batch)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Fix order in torchtext to match ours.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        src, trg = batch.src.transpose(<span class="number">0</span>, <span class="number">1</span>), batch.trg.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> Batch(src, trg, pad_idx)</span><br></pre></td></tr></table></figure><h2 id="7-3-Multi-GPU-Training"><a href="#7-3-Multi-GPU-Training" class="headerlink" title="7.3 Multi-GPU Training"></a>7.3 Multi-GPU Training</h2><p>最后，为了加速训练，我们使用多GPU进行训练。方法就是在训练过程中将生成词的过程分成多份在多个GPU上并行处理。</p><p>我们使用pytorch的原生库来实现：</p><ul><li><code>replicate</code> - 将模块分割放进不同的GPU上；</li><li><code>scatter</code> - 将不同的batch放进不同的GPU上；</li><li><code>parallel_apply</code> - 将不同的batch放到对应的GPU中的模块中；</li><li><code>gather</code> - 将分散的数据重新集合到同一个GU上；</li><li><code>nn.DataParallel</code> - 一个特殊的模块集合，用来在评估模型之前调度上面那些模块</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Skip if not interested in multigpu.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiGPULossCompute</span>:</span></span><br><span class="line">    <span class="string">"A multi-gpu loss compute and train function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, generator, criterion, devices, opt=None, chunk_size=<span class="number">5</span>)</span>:</span></span><br><span class="line">        <span class="comment"># Send out to different gpus.</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        self.criterion = nn.parallel.replicate(criterion, </span><br><span class="line">                                               devices=devices)</span><br><span class="line">        self.opt = opt</span><br><span class="line">        self.devices = devices</span><br><span class="line">        self.chunk_size = chunk_size</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, out, targets, normalize)</span>:</span></span><br><span class="line">        total = <span class="number">0.0</span></span><br><span class="line">        generator = nn.parallel.replicate(self.generator, </span><br><span class="line">                                                devices=self.devices)</span><br><span class="line">        out_scatter = nn.parallel.scatter(out, </span><br><span class="line">                                          target_gpus=self.devices)</span><br><span class="line">        out_grad = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> out_scatter]</span><br><span class="line">        targets = nn.parallel.scatter(targets, </span><br><span class="line">                                      target_gpus=self.devices)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Divide generating into chunks.</span></span><br><span class="line">        chunk_size = self.chunk_size</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, out_scatter[<span class="number">0</span>].size(<span class="number">1</span>), chunk_size):</span><br><span class="line">            <span class="comment"># Predict distributions</span></span><br><span class="line">            out_column = [[Variable(o[:, i:i+chunk_size].data, </span><br><span class="line">                                    requires_grad=self.opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>)] </span><br><span class="line">                           <span class="keyword">for</span> o <span class="keyword">in</span> out_scatter]</span><br><span class="line">            gen = nn.parallel.parallel_apply(generator, out_column)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute loss. </span></span><br><span class="line">            y = [(g.contiguous().view(<span class="number">-1</span>, g.size(<span class="number">-1</span>)), </span><br><span class="line">                  t[:, i:i+chunk_size].contiguous().view(<span class="number">-1</span>)) </span><br><span class="line">                 <span class="keyword">for</span> g, t <span class="keyword">in</span> zip(gen, targets)]</span><br><span class="line">            loss = nn.parallel.parallel_apply(self.criterion, y)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Sum and normalize loss</span></span><br><span class="line">            l = nn.parallel.gather(loss, </span><br><span class="line">                                   target_device=self.devices[<span class="number">0</span>])</span><br><span class="line">            l = l.sum()[<span class="number">0</span>] / normalize</span><br><span class="line">            total += l.data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Backprop loss to output of transformer</span></span><br><span class="line">            <span class="keyword">if</span> self.opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                l.backward()</span><br><span class="line">                <span class="keyword">for</span> j, l <span class="keyword">in</span> enumerate(loss):</span><br><span class="line">                    out_grad[j].append(out_column[j][<span class="number">0</span>].grad.data.clone())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backprop all loss through transformer.            </span></span><br><span class="line">        <span class="keyword">if</span> self.opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            out_grad = [Variable(torch.cat(og, dim=<span class="number">1</span>)) <span class="keyword">for</span> og <span class="keyword">in</span> out_grad]</span><br><span class="line">            o1 = out</span><br><span class="line">            o2 = nn.parallel.gather(out_grad, </span><br><span class="line">                                    target_device=self.devices[<span class="number">0</span>])</span><br><span class="line">            o1.backward(gradient=o2)</span><br><span class="line">            self.opt.step()</span><br><span class="line">            self.opt.optimizer.zero_grad()</span><br><span class="line">        <span class="keyword">return</span> total * normalize</span><br></pre></td></tr></table></figure><p>下面我们就可以构造模型了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GPUs to use</span></span><br><span class="line">devices = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="keyword">if</span> <span class="literal">True</span>:</span><br><span class="line">    pad_idx = TGT.vocab.stoi[<span class="string">"&lt;blank&gt;"</span>]</span><br><span class="line">    model = make_model(len(SRC.vocab), len(TGT.vocab), N=<span class="number">6</span>)</span><br><span class="line">    model.cuda()</span><br><span class="line">    criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=<span class="number">0.1</span>)</span><br><span class="line">    criterion.cuda()</span><br><span class="line">    BATCH_SIZE = <span class="number">12000</span></span><br><span class="line">    train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=<span class="number">0</span>,</span><br><span class="line">                            repeat=<span class="literal">False</span>, sort_key=<span class="keyword">lambda</span> x: (len(x.src), len(x.trg)),</span><br><span class="line">                            batch_size_fn=batch_size_fn, train=<span class="literal">True</span>)</span><br><span class="line">    valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=<span class="number">0</span>,</span><br><span class="line">                            repeat=<span class="literal">False</span>, sort_key=<span class="keyword">lambda</span> x: (len(x.src), len(x.trg)),</span><br><span class="line">                            batch_size_fn=batch_size_fn, train=<span class="literal">False</span>)</span><br><span class="line">    model_par = nn.DataParallel(model, device_ids=devices)</span><br></pre></td></tr></table></figure><h2 id="7-4-训练模型"><a href="#7-4-训练模型" class="headerlink" title="7.4 训练模型"></a>7.4 训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="literal">False</span>:</span><br><span class="line">    model_opt = NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">1</span>, <span class="number">2000</span>,</span><br><span class="line">            torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        model_par.train()</span><br><span class="line">        run_epoch((rebatch(pad_idx, b) <span class="keyword">for</span> b <span class="keyword">in</span> train_iter), </span><br><span class="line">                  model_par, </span><br><span class="line">                  MultiGPULossCompute(model.generator, criterion, </span><br><span class="line">                                      devices=devices, opt=model_opt))</span><br><span class="line">        model_par.eval()</span><br><span class="line">        loss = run_epoch((rebatch(pad_idx, b) <span class="keyword">for</span> b <span class="keyword">in</span> valid_iter), </span><br><span class="line">                          model_par, </span><br><span class="line">                          MultiGPULossCompute(model.generator, criterion, </span><br><span class="line">                          devices=devices, opt=<span class="literal">None</span>))</span><br><span class="line">        print(loss)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model = torch.load(<span class="string">"iwslt.pt"</span>)</span><br></pre></td></tr></table></figure><p>模型一旦训练好了我们就可以用来翻译了。这里我们以验证集的第一个句子为例，进行翻译：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(valid_iter):</span><br><span class="line">    src = batch.src.transpose(<span class="number">0</span>, <span class="number">1</span>)[:<span class="number">1</span>]</span><br><span class="line">    src_mask = (src != SRC.vocab.stoi[<span class="string">"&lt;blank&gt;"</span>]).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">    out = greedy_decode(model, src, src_mask, </span><br><span class="line">                        max_len=<span class="number">60</span>, start_symbol=TGT.vocab.stoi[<span class="string">"&lt;s&gt;"</span>])</span><br><span class="line">    print(<span class="string">"Translation:"</span>, end=<span class="string">"\t"</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, out.size(<span class="number">1</span>)):</span><br><span class="line">        sym = TGT.vocab.itos[out[<span class="number">0</span>, i]]</span><br><span class="line">        <span class="keyword">if</span> sym == <span class="string">"&lt;/s&gt;"</span>: <span class="keyword">break</span></span><br><span class="line">        print(sym, end =<span class="string">" "</span>)</span><br><span class="line">    print()</span><br><span class="line">    print(<span class="string">"Target:"</span>, end=<span class="string">"\t"</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, batch.trg.size(<span class="number">0</span>)):</span><br><span class="line">        sym = TGT.vocab.itos[batch.trg.data[i, <span class="number">0</span>]]</span><br><span class="line">        <span class="keyword">if</span> sym == <span class="string">"&lt;/s&gt;"</span>: <span class="keyword">break</span></span><br><span class="line">        print(sym, end =<span class="string">" "</span>)</span><br><span class="line">    print()</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><h1 id="8-注意力可视化"><a href="#8-注意力可视化" class="headerlink" title="8. 注意力可视化"></a>8. 注意力可视化</h1><p>尽管贪婪编码的翻译看起来不错，但是我们还想看看每层注意力到底发生了什么：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">tgt_sent = trans.split()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span><span class="params">(data, x, y, ax)</span>:</span></span><br><span class="line">    seaborn.heatmap(data, </span><br><span class="line">                    xticklabels=x, square=<span class="literal">True</span>, yticklabels=y, vmin=<span class="number">0.0</span>, vmax=<span class="number">1.0</span>, </span><br><span class="line">                    cbar=<span class="literal">False</span>, ax=ax)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>):</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">4</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">    print(<span class="string">"Encoder Layer"</span>, layer+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        draw(model.encoder.layers[layer].self_attn.attn[<span class="number">0</span>, h].data, </span><br><span class="line">            sent, sent <span class="keyword">if</span> h ==<span class="number">0</span> <span class="keyword">else</span> [], ax=axs[h])</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>):</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">4</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">    print(<span class="string">"Decoder Self Layer"</span>, layer+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        draw(model.decoder.layers[layer].self_attn.attn[<span class="number">0</span>, h].data[:len(tgt_sent), :len(tgt_sent)], </span><br><span class="line">            tgt_sent, tgt_sent <span class="keyword">if</span> h ==<span class="number">0</span> <span class="keyword">else</span> [], ax=axs[h])</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"Decoder Src Layer"</span>, layer+<span class="number">1</span>)</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">4</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        draw(model.decoder.layers[layer].self_attn.attn[<span class="number">0</span>, h].data[:len(tgt_sent), :len(sent)], </span><br><span class="line">            sent, tgt_sent <span class="keyword">if</span> h ==<span class="number">0</span> <span class="keyword">else</span> [], ax=axs[h])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>Encoder Layer 2</p></blockquote><p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_119_1.png" alt="png"></p><blockquote><p>Encoder Layer 4</p></blockquote><p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_119_3.png" alt="png"></p><blockquote><p>Encoder Layer 6</p></blockquote><p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_119_5.png" alt="png"></p><blockquote><p>Decoder Self Layer 2</p></blockquote><p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_119_7.png" alt="png"></p><blockquote><p>Decoder Src Layer 2</p></blockquote><p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_119_9.png" alt="png"></p><blockquote><p>Decoder Self Layer 4</p></blockquote><p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_119_11.png" alt="png"></p><blockquote><p>Decoder Src Layer 4</p></blockquote><p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_119_13.png" alt="png"></p><blockquote><p>Decoder Self Layer 6</p></blockquote><p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_119_15.png" alt="png"></p><blockquote><p>Decoder Src Layer 6</p></blockquote><p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_119_17.png" alt="png"></p><h1 id="9-参考资料"><a href="#9-参考资料" class="headerlink" title="9. 参考资料"></a>9. 参考资料</h1><p><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">The Annotated Transformer</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于Transformer的分析</title>
      <link href="/2019/09/01/analyse-transformer/"/>
      <url>/2019/09/01/analyse-transformer/</url>
      
        <content type="html"><![CDATA[<p><em>Transformer</em> 的模型框架我们已经介绍完了，接下来这篇文章我们讨论一下更多关于 <em>Transformer</em> 的模型细节。比如多头注意力的头越多越好吗？自注意力为什么要进行归一化？训练的时候 <em>Warm-up</em> 有什么用？</p><a id="more"></a><h1 id="1-模型总览"><a href="#1-模型总览" class="headerlink" title="1. 模型总览"></a>1. 模型总览</h1><p><img src="https://img.vim-cn.com/3a/78ea12dca1ce0f99f9a9705466afc16c58c3cf.png" alt></p><p>在进入正题之前，我们先回顾一下 <em>Transformer</em>  的整体结构，大致包含以下几个部分：</p><ul><li><em>Input Embedding</em></li><li><em>Positional Encoding</em></li><li><em>Multi-head Attention</em></li><li><em>Add &amp; Norm</em></li><li><em>Feed Forward</em></li><li><em>Masked Multi-head Attention</em></li><li><em>Linear</em></li><li><em>softmax</em></li></ul><h1 id="2-Input-Embedding"><a href="#2-Input-Embedding" class="headerlink" title="2. Input Embedding"></a>2. Input Embedding</h1><p><em>Input Embedding</em> 实际上可以看成两部分：<em>Input</em> 和 <em>Embedding</em>。神经网络在处理自然语言问题的时候，一个基础的工作就是词向量化。要将自然语言问题转化成计算问题，首先就要将自然语言符号表示转化成数字表示，最简单的表示方法就是 <em>one-hot</em> ，但是 <em>one-hot</em> 有两个问题：① 数据稀疏；② 无法表示词语此之间的语义关系。所以后来人们发明了 <em>embedding</em> 表示法，就是将自然语言符号表示成低维连续的向量，这样可以同时解决上述两个问题。</p><p>向量化是自然语言处理的一个必不可少的工作，一般的词向量化有两种方式：</p><ul><li>通过一些算法（比如，<em>word2vec</em> 或者 <em>glov2vec</em>）预训练一个词向量表，之后通过查表的方式将句子转化成词向量表示。然后在模型的训练的时候词向量不参与训练；</li><li>在模型输入阶段随机初始化一张词向量表，同样是通过查表的方式将句子表示成向量形式，而这张词向量表随着模型一起训练。</li></ul><p>以上基本就是任意使用神经网络处理的自然语言问题的必经之路。其中的向量化过程我们不再赘述（在后续的系列文章中会专门讨论自然语言处理中的预训练技术），这里我们详细讨论一下 <strong>Input</strong> 部分。</p><p>上面我们说到我们需要一张词向量表将自然语言符号映射成向量，自然而然地我们同样需要一张词表，这张词表包含的元素就是自然语言符号。在训练模型的时候我们是使用一张固定大小的词表，也就是说模型只能处理词表中出现过的词。一个显然的问题是，大千世界，我们人类发明的自然语言符号虽然不是无穷无尽的，但是却是数量非常庞大的，而且还在不断的增长中，所以我们不可能将所有的符号都纳入到词表中。即使我们将当前所有的符号都纳入到词表中也会面临两个问题：</p><ul><li>过于庞大的词表就意味着庞大的词向量表，如此庞大的词向量表会使模型参数量爆炸，大参数量不仅意味着大计算量，同时还有可能意味着较高的错误率。比如机器翻译过程中最后的输出由 <em>softmax</em> 挑选最合适的词进行输出，一个简单的直觉就是从更大的词表中选择会有更大的概率出错；</li><li>在所有的符号中，有些是常用的，有些是非常罕见的。对于常用的符号，模型可以得到充分的训练，但对于稀有的符号模型得不到充分训练，实际的效果也不会好。</li></ul><p>这就意味着我们的模型只能使用一个有限的词表进行训练，但是当我们的模型训练好了以后，让它去处理实际问题的时候，它面临的却是开放的世界，它需要处理的也是所有的自然语言符号，一旦遇到词表之外的符号，模型就无能为力了。这就是自然语言处理过程中的 <em>OOV（out-of-vocabulary）</em> 问题。</p><p>为了解决 <em>OOV</em> 问题，研究人员展开了大量的研究工作，研究的核心就在于以什么样的方式表示自然语言符号。</p><ul><li><strong>以词为单位</strong>。通常，我们认为一个句子的基本元素是词（<em>word</em>），所以我们通常是使用一个个词组成一个序列，用来表示句子。而使用词作为自然语言符号表示法的话，就会面临上述的所有问题。而且以词为单位不利于模型学习词缀之间的关系，比如模型学到的“old”, “older”, and “oldest”之间的关系无法泛化到“smart”, “smarter”, and “smartest”。</li><li><strong>以字为单位</strong>。后来人们想到用字（<em>character</em>）来表示。虽然真实世界的词浩如烟海，但是组成词的字却是非常有限的，比如英文只有 26 个字母。但是这种处理方法粒度太细，丢失了词带给句子的潜藏语义关联，因此通常效果不如以词为基础单元的模型。</li><li><strong>以子词为单位</strong>。什么是子词<em>（sub-word）</em>？就是将一般的词，比如 <em>older</em> 分解成更小单元，<em>old+er</em>，这样这些小单元可以几个词共用，减小了词表大小，同时还能使模型学习到词缀之间的关系。因此子词可以很好的平衡 <em>OOV</em> 问题。所以下面我们详细介绍一下子词技术。</li></ul><h2 id="2-1-Byte-Pair-Encoding"><a href="#2-1-Byte-Pair-Encoding" class="headerlink" title="2.1 Byte Pair Encoding"></a>2.1 Byte Pair Encoding</h2><p><em>Byte Pair Encoding（BPE）</em> 算法最早是由 <a href="https://dl.acm.org/doi/10.5555/177910.177914" target="_blank" rel="noopener">Gage</a> 于1994 年提出的一种用于数据压缩的算法，而在 2015 年被 <a href="https://arxiv.org/abs/1508.07909" target="_blank" rel="noopener">Sennrich</a> 等人推广到了自然语言处理领域。</p><ul><li><strong>算法过程</strong></li></ul><p>算法过程如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. 准备足够大的训练语料;</span><br><span class="line">2. 确定期望的子词表大小；</span><br><span class="line">3. 将单词拆分为字符序列并在末尾添加后缀“&lt;/w&gt;”，统计单词频率；</span><br><span class="line">4. 统计每一个连续字节对的出现频率；</span><br><span class="line">5. 选择最高频者合并成新的子词；</span><br><span class="line">6. 重复 4 ~ 5 步直到达到第2步设定的子词表大小或下一个最高频的字节对出现频率为 1。</span><br></pre></td></tr></table></figure><p>举个例子：</p><blockquote><ul><li><p>假设我们准备了一份语料，语料中包含：$(low, lower, newest, widest)$ 这几个词;</p></li><li><p>将单词拆分成字符，并在后面添加 “&lt;/w&gt;”，然后统计词频，得到：</p><p>vocab = {‘l o w &lt;/w&gt;’: 5, ‘l o w e r &lt;/w&gt;’: 2, ‘n e w e s t &lt;/w&gt;’: 6, ‘w i d e s t &lt;/w&gt;’: 3}</p></li><li><p>统计连续字符对出现的频率：</p><p>{(‘l’, ‘o’): 7, (‘o’, ‘w’): 7, (‘w’, ‘&lt;/w&gt;’): 5, (‘w’, ‘e’): 8, (‘e’, ‘r’): 2, (‘r’, ‘&lt;/w&gt;’): 2, (‘n’, ‘e’): 6, (‘e’, ‘w’): 6, (‘e’, ‘s’): 9, (‘s’, ‘t’): 9, (‘t’, ‘&lt;/w&gt;’): 9, (‘w’, ‘i’): 3, (‘i’, ‘d’): 3, (‘d’, ‘e’): 3}</p></li><li><p>最高频连续字节对 (“e”, “s”)，合并成 “es”，得到：</p><p>{‘l o w &lt;/w&gt;’: 5, ‘l o w e r &lt;/w&gt;’: 2, ‘n e w es t &lt;/w&gt;’: 6, ‘w i d es t &lt;/w&gt;’: 3}</p></li><li><p>重复以上步骤</p></li></ul></blockquote><p><strong>注意：</strong>停止符”&lt;/w&gt;”的意义在于表示subword是词后缀。举例来说：”st”字词不加”&lt;/w&gt;”可以出现在词首如”st ar”，加了”&lt;/w&gt;”表明改字词位于词尾，如”wide st&lt;/w&gt;”，二者意义截然不同。</p><ul><li><strong>代码实现</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re, collections</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_stats</span><span class="params">(vocab)</span>:</span></span><br><span class="line">    pairs = collections.defaultdict(int)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> vocab.items():</span><br><span class="line">        symbols = word.split()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(symbols)<span class="number">-1</span>):</span><br><span class="line">            pairs[symbols[i],symbols[i+<span class="number">1</span>]] += freq</span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_vocab</span><span class="params">(pair, v_in)</span>:</span></span><br><span class="line">    v_out = &#123;&#125;</span><br><span class="line">    bigram = re.escape(<span class="string">' '</span>.join(pair))</span><br><span class="line">    p = re.compile(<span class="string">r'(?&lt;!\S)'</span> + bigram + <span class="string">r'(?!\S)'</span>)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> v_in:</span><br><span class="line">        w_out = p.sub(<span class="string">''</span>.join(pair), word)</span><br><span class="line">        v_out[w_out] = v_in[word]</span><br><span class="line">    <span class="keyword">return</span> v_out</span><br></pre></td></tr></table></figure><p>训练示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">vocab = &#123;<span class="string">'l o w &lt;/w&gt;'</span>: <span class="number">5</span>, </span><br><span class="line">         <span class="string">'l o w e r &lt;/w&gt;'</span>: <span class="number">2</span>, </span><br><span class="line">         <span class="string">'n e w e s t &lt;/w&gt;'</span>: <span class="number">6</span>, </span><br><span class="line">         <span class="string">'w i d e s t &lt;/w&gt;'</span>: <span class="number">3</span>&#125;</span><br><span class="line">num_merges = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_merges):</span><br><span class="line">    pairs = get_stats(vocab)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> pairs:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    best = max(pairs, key=pairs.get)</span><br><span class="line">    vocab = merge_vocab(best, vocab)</span><br><span class="line">    print(best)</span><br><span class="line">    print(vocab)</span><br><span class="line">    print(<span class="string">'='</span>*<span class="number">20</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">'e'</span>, <span class="string">'s'</span>)</span><br><span class="line">&#123;<span class="string">'l o w &lt;/w&gt;'</span>: <span class="number">5</span>, <span class="string">'l o w e r &lt;/w&gt;'</span>: <span class="number">2</span>, <span class="string">'n e w es t &lt;/w&gt;'</span>: <span class="number">6</span>, <span class="string">'w i d es t &lt;/w&gt;'</span>: <span class="number">3</span>&#125;</span><br><span class="line">====================</span><br><span class="line">(<span class="string">'es'</span>, <span class="string">'t'</span>)</span><br><span class="line">&#123;<span class="string">'l o w &lt;/w&gt;'</span>: <span class="number">5</span>, <span class="string">'l o w e r &lt;/w&gt;'</span>: <span class="number">2</span>, <span class="string">'n e w est &lt;/w&gt;'</span>: <span class="number">6</span>, <span class="string">'w i d est &lt;/w&gt;'</span>: <span class="number">3</span>&#125;</span><br><span class="line">====================</span><br><span class="line">(<span class="string">'est'</span>, <span class="string">'&lt;/w&gt;'</span>)</span><br><span class="line">&#123;<span class="string">'l o w &lt;/w&gt;'</span>: <span class="number">5</span>, <span class="string">'l o w e r &lt;/w&gt;'</span>: <span class="number">2</span>, <span class="string">'n e w est&lt;/w&gt;'</span>: <span class="number">6</span>, <span class="string">'w i d est&lt;/w&gt;'</span>: <span class="number">3</span>&#125;</span><br><span class="line">====================</span><br><span class="line">(<span class="string">'l'</span>, <span class="string">'o'</span>)</span><br><span class="line">&#123;<span class="string">'lo w &lt;/w&gt;'</span>: <span class="number">5</span>, <span class="string">'lo w e r &lt;/w&gt;'</span>: <span class="number">2</span>, <span class="string">'n e w est&lt;/w&gt;'</span>: <span class="number">6</span>, <span class="string">'w i d est&lt;/w&gt;'</span>: <span class="number">3</span>&#125;</span><br><span class="line">====================</span><br><span class="line">(<span class="string">'lo'</span>, <span class="string">'w'</span>)</span><br><span class="line">&#123;<span class="string">'low &lt;/w&gt;'</span>: <span class="number">5</span>, <span class="string">'low e r &lt;/w&gt;'</span>: <span class="number">2</span>, <span class="string">'n e w est&lt;/w&gt;'</span>: <span class="number">6</span>, <span class="string">'w i d est&lt;/w&gt;'</span>: <span class="number">3</span>&#125;</span><br><span class="line">====================</span><br><span class="line">(<span class="string">'n'</span>, <span class="string">'e'</span>)</span><br><span class="line">&#123;<span class="string">'low &lt;/w&gt;'</span>: <span class="number">5</span>, <span class="string">'low e r &lt;/w&gt;'</span>: <span class="number">2</span>, <span class="string">'ne w est&lt;/w&gt;'</span>: <span class="number">6</span>, <span class="string">'w i d est&lt;/w&gt;'</span>: <span class="number">3</span>&#125;</span><br><span class="line">====================</span><br><span class="line">(<span class="string">'ne'</span>, <span class="string">'w'</span>)</span><br><span class="line">&#123;<span class="string">'low &lt;/w&gt;'</span>: <span class="number">5</span>, <span class="string">'low e r &lt;/w&gt;'</span>: <span class="number">2</span>, <span class="string">'new est&lt;/w&gt;'</span>: <span class="number">6</span>, <span class="string">'w i d est&lt;/w&gt;'</span>: <span class="number">3</span>&#125;</span><br><span class="line">====================</span><br><span class="line">(<span class="string">'new'</span>, <span class="string">'est&lt;/w&gt;'</span>)</span><br><span class="line">&#123;<span class="string">'low &lt;/w&gt;'</span>: <span class="number">5</span>, <span class="string">'low e r &lt;/w&gt;'</span>: <span class="number">2</span>, <span class="string">'newest&lt;/w&gt;'</span>: <span class="number">6</span>, <span class="string">'w i d est&lt;/w&gt;'</span>: <span class="number">3</span>&#125;</span><br><span class="line">====================</span><br><span class="line">(<span class="string">'low'</span>, <span class="string">'&lt;/w&gt;'</span>)</span><br><span class="line">&#123;<span class="string">'low&lt;/w&gt;'</span>: <span class="number">5</span>, <span class="string">'low e r &lt;/w&gt;'</span>: <span class="number">2</span>, <span class="string">'newest&lt;/w&gt;'</span>: <span class="number">6</span>, <span class="string">'w i d est&lt;/w&gt;'</span>: <span class="number">3</span>&#125;</span><br><span class="line">====================</span><br><span class="line">(<span class="string">'w'</span>, <span class="string">'i'</span>)</span><br><span class="line">&#123;<span class="string">'low&lt;/w&gt;'</span>: <span class="number">5</span>, <span class="string">'low e r &lt;/w&gt;'</span>: <span class="number">2</span>, <span class="string">'newest&lt;/w&gt;'</span>: <span class="number">6</span>, <span class="string">'wi d est&lt;/w&gt;'</span>: <span class="number">3</span>&#125;</span><br><span class="line">====================</span><br><span class="line">(<span class="string">'wi'</span>, <span class="string">'d'</span>)</span><br><span class="line">&#123;<span class="string">'low&lt;/w&gt;'</span>: <span class="number">5</span>, <span class="string">'low e r &lt;/w&gt;'</span>: <span class="number">2</span>, <span class="string">'newest&lt;/w&gt;'</span>: <span class="number">6</span>, <span class="string">'wid est&lt;/w&gt;'</span>: <span class="number">3</span>&#125;</span><br><span class="line">====================</span><br><span class="line">(<span class="string">'wid'</span>, <span class="string">'est&lt;/w&gt;'</span>)</span><br><span class="line">&#123;<span class="string">'low&lt;/w&gt;'</span>: <span class="number">5</span>, <span class="string">'low e r &lt;/w&gt;'</span>: <span class="number">2</span>, <span class="string">'newest&lt;/w&gt;'</span>: <span class="number">6</span>, <span class="string">'widest&lt;/w&gt;'</span>: <span class="number">3</span>&#125;</span><br><span class="line">====================</span><br><span class="line">(<span class="string">'low'</span>, <span class="string">'e'</span>)</span><br><span class="line">&#123;<span class="string">'low&lt;/w&gt;'</span>: <span class="number">5</span>, <span class="string">'lowe r &lt;/w&gt;'</span>: <span class="number">2</span>, <span class="string">'newest&lt;/w&gt;'</span>: <span class="number">6</span>, <span class="string">'widest&lt;/w&gt;'</span>: <span class="number">3</span>&#125;</span><br><span class="line">====================</span><br><span class="line">(<span class="string">'lowe'</span>, <span class="string">'r'</span>)</span><br><span class="line">&#123;<span class="string">'low&lt;/w&gt;'</span>: <span class="number">5</span>, <span class="string">'lower &lt;/w&gt;'</span>: <span class="number">2</span>, <span class="string">'newest&lt;/w&gt;'</span>: <span class="number">6</span>, <span class="string">'widest&lt;/w&gt;'</span>: <span class="number">3</span>&#125;</span><br><span class="line">====================</span><br><span class="line">(<span class="string">'lower'</span>, <span class="string">'&lt;/w&gt;'</span>)</span><br><span class="line">&#123;<span class="string">'low&lt;/w&gt;'</span>: <span class="number">5</span>, <span class="string">'lower&lt;/w&gt;'</span>: <span class="number">2</span>, <span class="string">'newest&lt;/w&gt;'</span>: <span class="number">6</span>, <span class="string">'widest&lt;/w&gt;'</span>: <span class="number">3</span>&#125;</span><br><span class="line">====================</span><br></pre></td></tr></table></figure><h2 id="2-2-WordPiece"><a href="#2-2-WordPiece" class="headerlink" title="2.2 WordPiece"></a>2.2 WordPiece</h2><p><em>WordPiece</em> 算法可以看作是 <em>BPE</em> 的变种。不同点在于，<em>WordPiece</em> 基于概率生成新的子词而不是最高频字节对。</p><ul><li><strong>算法过程</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. 准备足够大的训练语料;</span><br><span class="line">2. 确定期望的子词表大小;</span><br><span class="line">3. 将单词拆分成字符序列;</span><br><span class="line">4. 基于第3步数据训练语言模型;</span><br><span class="line">5. 从所有可能的子词单元中选择加入语言模型后能最大程度地增加训练数据概率的单元作为新的单元;</span><br><span class="line">6. 重复第5步直到达到第2步设定的子词表大小或概率增量低于某一阈值;</span><br></pre></td></tr></table></figure><p><strong>注意：</strong>因为加入每个可能的词对都需重新训练语言模型，这样需要的计算资源会很大，因此作者通过以下策略降低计算量：</p><blockquote><ol><li>只测试语料中出现的词对；</li><li>只测试有很大可能（高优先）是最好词对的候选；</li><li>同时测试几个词对，只要它们互不影响；</li><li>重训语言模型（并不需要是神经网络型），只重新计算受影响的部分。</li></ol></blockquote><h2 id="2-3-Unigram-Language-Model"><a href="#2-3-Unigram-Language-Model" class="headerlink" title="2.3 Unigram Language Model"></a>2.3 Unigram Language Model</h2><p><em>ULM</em> 是另外一种子词分隔算法，它能够输出带概率的多个子词分段。它引入了一个假设：所有子词的出现都是独立的，并且子词序列由子词出现概率的乘积产生。</p><p><em>ULM</em> 和 <em>WordPiece</em> 一样都是利用语言模型建立子词表。与前两者都不相同的是，前两者构建词表的时候是增量的，而 <em>ULM</em> 是减量的。</p><ul><li><strong>算法过程</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1. 先建立一个足够大的种子词表，可以用所有字符的组合加上语料中常见的子字符串。</span><br><span class="line">   对于种子词表，也能用 BPE 法来生成。</span><br><span class="line">2. 固定词表，用 EM 算法来最优化当前词表在语料上的概率；</span><br><span class="line">3. 之后计算每个子词的 loss，对应的 loss 相当于该子词有多大可能使总的 loss 降低；</span><br><span class="line">4. 接着按照每个子词 loss 大小来排序，保留最大一定比例（比如说80%）的子词，</span><br><span class="line">   为了避免OOV，建议保留字符级的单元；</span><br><span class="line">6. 不断重复 2 ~ 4，直到词表量减少到限定范围。</span><br></pre></td></tr></table></figure><ul><li><strong>子词采样</strong></li></ul><p>相比于之前的两种方法，分词结果是确定的，<em>ULM</em> 会使用一定的概率对分词结果进行采样。这样给训练过程带来了随机性。结果表明相比起只用一种方案的确定性分词法，子词正则能够获得很大的提升。但同时也正如看到的，子词正则加上 <em>ULM</em> 法过于复杂，所以应用难度也相应增大，不像 <em>BPE</em> 应用广泛。</p><h2 id="2-4-BPE-Dropout"><a href="#2-4-BPE-Dropout" class="headerlink" title="2.4 BPE-Dropout"></a>2.4 BPE-Dropout</h2><p>该方法非常简单，采取和子词正则相同思路，对 <em>BPE</em> 算法训练做了些改进，加入了一定的随机性。具体在每次对训练数据进行处理分词时，设定一定概率（10%）让一些融合不通过，于是即使是相同词，每次用 <em>BPE dropout</em> 生成出来的子词也都不一样。</p><ul><li><strong>算法过程</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. 用 BPE 生成所有的融合候选子词；</span><br><span class="line">2. 根据一定的概率 p 删除一部分候选子词；</span><br></pre></td></tr></table></figure><h2 id="2-5-小结"><a href="#2-5-小结" class="headerlink" title="2.5 小结"></a>2.5 小结</h2><ol><li>子词可以平衡词汇量和对未知词的覆盖。 </li><li>对于包括中文在内的许多亚洲语言，单词不能用空格分隔。 因此，初始词汇量需要比英语大很多。</li><li>另外需要注意一点的是，对于中文这种不以空格分隔的语言，在应用子词技术的时候，需要先用分词算法进行分词。</li></ol><h1 id="3-Positional-Encoding"><a href="#3-Positional-Encoding" class="headerlink" title="3. Positional Encoding"></a>3. Positional Encoding</h1><h2 id="3-1-What-why-and-when？"><a href="#3-1-What-why-and-when？" class="headerlink" title="3.1 What, why and when？"></a>3.1 What, why and when？</h2><p>所谓位置编码指的是，在训练深度模型的时候，在输入训练数据的时候添加一种编码，使得模型的输出可以表征输入数据的时序特征。对于文本数据，词与词之间的顺序关系往往影响整个句子的含义。因此，在对文本进行建模的时候，词序是一个必须考虑的问题。但是否是所有的文本建模都需要使用位置编码呢？</p><p>答案是：<strong>No!</strong></p><p>只有当我们使用对位置不敏感（<em>position-insensitive</em>）的模型对文本数据建模的时候，才需要额外使用位置编码。那么什么是位置敏感模型？什么是位置不敏感模型呢？</p><blockquote><p>如果模型的输出会随着输入文本数据顺序的变化而变化，那么这个模型就是关于位置敏感的，反之则是位置不敏感的。</p></blockquote><p>用形式化的语言描述为：假设模型函数 $y = f(x)$，其中 $x = (x_1, …, x_n)$ 为输入序列，$y$ 为输出。如果将 $x$ 找那个任意元素的位置进行了置换， $x’ = (x_{k_1}, …x_{k_n})$。此时，如果 $f(x) = f(x’)$，那么我们称模型是位置不敏感的；反之则为位置敏感的。</p><p>传统的 <em>RNN</em> 模型是关于位置敏感的模型，我们在使用 <em>RNN</em> 相关模型进行文本建模的时候是不需要位置编码的。只有使用位置不敏感的模型进行文本建模的时候才需要位置编码。</p><h2 id="3-2-How"><a href="#3-2-How" class="headerlink" title="3.2 How?"></a>3.2 How?</h2><p>从直观上来讲，我们可以给序列中的第一个词的位置编码为 1，第二个词位置编码为 2 …… 依此类推。但是这种方法会带来两个问题：① 句子比较长的话，位置编码数字会变得很大；② 模型在推理的时候会遇到比训练数据更长的句子，这种编码会削弱模型的泛化性。</p><p>我们直接能想到的第二种方法是，将序列长度归一化到 $[0, 1]$ 范围内，0 表示第一个词， 1 表示最后一个词。但是这种编码方式也是有问题的：句子是变长的，我们不知道模型遇到的句子的长度是多少，对于不同长度的句子，每个词对应的位置编码是不一样的，也就是说这样的位置编码是无效编码。举个例子：</p><blockquote><p>I love you</p><p>I love you!</p></blockquote><p>这两句话，实际上意思相同，只不过第二句话多了一个句号，在用上面的方式进行位置编码的时候，两句话中对应的词的位置编码是不一样的，最后模型输出的结果也很难表征出相同的意思。因此，这种方式同样不可取。</p><p>理想的位置编码应该具备以下几个特征：</p><ul><li>每一个词的位置都有一个唯一的编码；</li><li>不同长度的句子中任意两个词之间的距离应该是相同的；</li><li>编码值应该是有界的，而且可以轻松泛化到更长的句子中；</li><li>必须具有确定性。</li></ul><p>目前主流的位置编码有三种：</p><ol><li>可学习的位置编码（<em>Learned Positional Embedding</em>）；</li><li>正余弦位置编码（<em>Sinusoidal Position Encoding</em>）；</li><li>相对位置编码（<em>Relative Position Representations</em>）。</li></ol><h3 id="3-2-1-可学习的位置编码"><a href="#3-2-1-可学习的位置编码" class="headerlink" title="3.2.1 可学习的位置编码"></a>3.2.1 可学习的位置编码</h3><p>这种编码方式相对简单也容易理解。它的做法是，随机初始化一个位置矩阵，然后加到（或者拼接）词向量矩阵上，输入给模型作为模型参数参与模型训练。</p><h3 id="3-2-2-正弦位置编码"><a href="#3-2-2-正弦位置编码" class="headerlink" title="3.2.2 正弦位置编码"></a>3.2.2 正弦位置编码</h3><p>这种位置编码方式就是 <em>Transformer</em> 中使用的位置编码。假设 $t$ 是输入序列的位置，$\vec{p}_t \in \mathbb{R}^d$ 表示其对应的编码。$\vec{p}_t$ 定义如下：</p><script type="math/tex; mode=display">\vec{p}_t^{(i)} = \begin{cases}\sin(\omega_k \cdot t),  i=2k \\\\\cos(\omega_k \cdot t),  i=2k+1\end{cases}</script><p>其中 $\omega_k = 1/10000^{2k/d}$。</p><p>有了位置编码以后，将词向量与位置向量相加得到带有位置信息的新序列。注意为了相加操作的正确，要保证此项来那个维度和位置向量维度一致：</p><script type="math/tex; mode=display">\psi'(\omega_t) = \psi(\omega_t) + \vec{p}_t</script><p>正弦位置编码的优势是，对于任意固定的偏移量 $k$，$\vec{p}_{t+k}$ 可以表示成 $\vec{p }_t$ 的线性函数。也就是说，模型可以很容易通过先验绝对位置编码学习到相对位置编码。通常对于句子中的词序，我们更关注的是词与词之间的相对位置。我们来证明一下吧。</p><ul><li><p><strong>问题描述</strong></p><p>令 $ \vec{p}_t$ 表示第 $t$ 个位置的位置编码：</p><script type="math/tex; mode=display">\vec{p}_t=\begin{bmatrix}\sin(\omega_1 \cdot t)\\\\\cos(\omega_1 \cdot t)\\\\\sin(\omega_2 \cdot t)\\\\\cos(\omega_2 \cdot t)\\\\\vdots \\\\\sin(\omega_{d_{model}/2} \cdot t)\\\\\cos(\omega_{d_{model}/2} \cdot t)\end{bmatrix}_{d_{model} \times 1}</script><p>我们希望能找到一个与 $t$ 无关的矩阵，使得 $\vec{p}_t$ 通过线性变换成为 $\vec{p}_{t+k}$。（因为如果是关于 $t$ 的矩阵， 那这个变换就是非线性的了）</p></li><li><p><strong>证明</strong></p><p>我们令</p><script type="math/tex; mode=display">\vec{p}_t=\begin{bmatrix}\begin{bmatrix}\sin(\omega_1 \cdot t)\\\\\cos(\omega_1 \cdot t)\end{bmatrix}\\\\\begin{bmatrix}\sin(\omega_2 \cdot t)\\\\\cos(\omega_2 \cdot t)\end{bmatrix}\\\\\vdots \\\\\begin{bmatrix}\sin(\omega_{d_{model}/2} \cdot t)\\\\\cos(\omega_{d_{model}/2} \cdot t)\end{bmatrix}\end{bmatrix}</script></li></ul><p>  我们希望找到一个 $M \in \mathbb{R}^{2 \times 2}$ ，使得 </p><script type="math/tex; mode=display">  M \cdot \begin{bmatrix}  \sin(\omega_i \cdot t)\\\\  \cos(\omega_i \cdot t)  \end{bmatrix}=  \begin{bmatrix}  \sin(\omega_i \cdot (t+k))\\\\  \cos(\omega_i \cdot (t+k))  \end{bmatrix}</script><p>  令 $M = \begin{bmatrix}u_1 &amp; v_1 \\\ u_2 &amp; v_2\end{bmatrix}$，代入上式，并分解等式右边得</p><script type="math/tex; mode=display">  \begin{bmatrix}  u_1 & v_1 \\\\   u_2 & v_2  \end{bmatrix} \cdot \begin{bmatrix}  \sin(\omega_i \cdot t)\\\\  \cos(\omega_i \cdot t)  \end{bmatrix}= \begin{bmatrix}  \sin(\omega_i \cdot t)\cos(\omega_i \cdot k) + \cos(\omega_i \cdot t)\sin(\omega_i \cdot k)\\\\  \cos(\omega_i \cdot t)\cos(\omega_i \cdot k) - \sin(\omega_i \cdot t)\sin(\omega_i \cdot k)  \end{bmatrix}</script><p>  由此可得：</p><script type="math/tex; mode=display">  u_1\sin(\omega_i \cdot t) + v_1 \cos(\omega_i \cdot t) = \quad \cos(\omega_i \cdot k)\sin(\omega_i \cdot t) + \sin(\omega_i \cdot k)\cos(\omega_i \cdot t)\\\\  u_2\sin(\omega_i \cdot t) + v_2 \cos(\omega_i \cdot t) = -\sin(\omega_i \cdot k)\sin(\omega_i \cdot t) + \cos(\omega_i \cdot k)\cos(\omega_i \cdot t)\\\\</script><p>  通过对比等式两边，我们可以得到 $u_1, v_1, u_2, v_2$ 的一组解：</p><script type="math/tex; mode=display">  u_1 = \quad \cos(\omega_i \cdot k) \quad v_1 = \sin(\omega_i \cdot k) \\\\  u_2 = -\sin(\omega_i \cdot k) \quad v_2 = \cos(\omega_i \cdot k)</script><p>  也就是说</p><script type="math/tex; mode=display">  M = \begin{bmatrix}  \cos(\omega_i \cdot k) &  \sin(\omega_i \cdot k) \\\\  -\sin(\omega_i \cdot k) &  \cos(\omega_i \cdot k)  \end{bmatrix}</script><p>  我们可以看到 $M$ 是一个与 $t$ 无关的矩阵。最后，我们令</p><script type="math/tex; mode=display">  T^{(k)} = \begin{bmatrix}  M_1^{(k)} & 0 & \cdots & 0 \\\\  0 & M_2^{(k)} & \cdots & 0 \\\\  \vdots & \vdots & \ddots & \vdots \\\\  0 & 0 & 0 & M_{d_{model}/2}^{(k)}  \end{bmatrix}</script><p>  其中 $0$ 表示 $2 \times 2$ 的全零矩阵。$T^{(k)}$ 即为我们想要的线性变换矩阵。</p><ul><li><p><strong>正弦位置编码的性质</strong></p><p>在计算自注意力的时候， <em>Transformer</em> 是计算序列中任意两个元素的注意力权重。因此，我们这里探究一下正弦位置编码的内积的性质。</p><p>① <strong>内积随相对位置递增而减小</strong></p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}\vec{p}_t \cdot \vec{p}_{t+k} &= \sum_{i=0}^{\frac{d_{model}}{2}-1} \mathrm{sin}(\omega_1\cdot t)\cdot \mathrm{sin}(\omega_i\cdot (t+k))+\cos(\omega_i\cdot t)\cdot \cos(\omega_i\cdot(t+k))\\\\&= \sum_{i=0}^{\frac{d_{model}}{2}-1} \cos(\omega_i\cdot(t-(t+k)))\\\\&= \sum_{i=0}^{\frac{d_{model}}{2}-1} \cos(\omega_i\cdot -k)\\\\&= \sum_{i=0}^{\frac{d_{model}}{2}-1} \cos(\omega_i\cdot k)\end{aligned}\end{equation}</script><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200522155328.png" alt></p><p>由此我们可以发现，随着相对位置的递增，正弦位置编码的内积会减小。如上图所示，点积的结果是对称的，并且随 $|k|$ 增加而减少（但并不单调）。</p><p>但是在 <em>Transformer</em> 中，由于需要经过映射，即两者间的点积实际是 $\vec{p}_t \cdot W \cdot \vec{p}_{t+k}$，下图展示了经过映射之后的位置向量点积：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200522155344.png" alt></p><p>我们可以看到，此时位置向量的点积并没有展现出明确的趋势。</p><p>② <strong>对称性</strong></p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}\vec{p}_t \cdot \vec{p}_{t-k} &= \sum_{i=0}^{\frac{d_{model}}{2}-1} \sin(\omega_1\cdot t)\cdot \mathrm{sin}(\omega_i\cdot (t-k))+\cos(\omega_i\cdot t)\cdot \cos(\omega_i\cdot(t-k))\\\\&= \sum_{i=0}^{\frac{d_{model}}{2}-1} \cos(\omega_i\cdot(t-(t-k)))\\\\&= \sum_{i=0}^{\frac{d_{model}}{2}-1} \cos(\omega_i\cdot k)\end{aligned}\end{equation}</script><p>由此我们发现，$ \vec{p}_t \cdot \vec{p}_{t+k} = \vec{p}_t \cdot \vec{p}_{t-k} $，这说明正弦位置编码无法区分词与词的前后关系。</p></li><li><p><strong>词向量和位置向量直接相加是否合理？</strong></p><p><em>Transformer</em> 对模型输入的词向量和位置向量的处理是直接相加，作者并没有给出一个理论解释，为什么要直接相加。但是如果我们读过足够多的的 <em>NLP</em> 论文，我们会发现，对位置向量的处理通常只有两种方法：相加和拼接。这两种处理方式在不同的模型中有不同的表现，但总体上并没有太大的差别。<em>Transformer</em> 之所以使用相加的方式，可能主要是考虑减少模型参数，毕竟自注意力矩阵的参数已经足够多了。</p><p>但是我们仔细考虑一下，无论是直接相加还是拼接，都隐含了一个假设：词向量和位置向量都是独立分布的。这个假设对词向量来说是成立的，因为词向量的 <em>index</em> 与我们使用的字典排序有关，而这个排序是任意的，任意的 <em>index</em> 与 <em>index+k</em> 或者 <em>index-k</em> 都没有任何依赖关系，所以这个假设是成立的。但是对于位置向量却并不满足这一假设，其顺序关系对模型理解文本有着重要的影响。我们称之为位置不敏感问题（<em>position-insensitive problem</em>）。</p><p>为了解决这一问题 <a href="https://openreview.net/pdf?id=Hke-WTVtwr" target="_blank" rel="noopener">Wang</a> 等人提出关于位置的连续函数来表征词在位置上的表示，即：</p><script type="math/tex; mode=display">f(j, t) = \pmb{g}_j(t) \in \mathbb{R}^D</script><p>其中 $\pmb{g}_j(t) = [g_{j,1}(t), g_{j, 2}(t), …, g_{j, D}(t)]$，$g_{j, d}(t)$ 是一个关于 $t$ 的函数。为了让这个函数更好的表征位置信息，$g_{j, d}(t)$ 必须要满足一下两个性质：</p><ol><li><p><strong>Position-free offset transformation</strong></p><p>存在一个线性函数 $\mathrm{Transform}_k(\cdot)=\mathrm{Transform}(g(t))$ 使得</p><script type="math/tex; mode=display">g(t+k) = \mathrm{Transform}(g(t))</script><p>也就是说在位置 $t+k$ 上的词 $w_{j,t+k}$ 可以通过位置 $t$ 上的词 $w_{i, t}$ 通过一个只和 $k$ 相关的变换得到，而与具体这个词无关。有点类似正弦位置编码的线性变换，只是这里不仅是位置的变换，还有词的变换。</p></li><li><p><strong>Boundedness</strong></p><p>这个线性函数必须是有界的。这是一个非常合理的限制，不做过多解释。</p></li></ol><p>最后，作者提出了 $g(t)$ 的函数形式：</p><script type="math/tex; mode=display">g(t) = r \cdot e^{i(\omega \cdot t +\theta)}</script><p>我们可以看到这是一个复数形式的函数，其中 $r$ 为振幅， $\omega$ 为角频率，$\theta$ 为初相，都是需要学习的参数。</p></li></ul><h3 id="3-2-3-相对位置编码"><a href="#3-2-3-相对位置编码" class="headerlink" title="3.2.3 相对位置编码"></a>3.2.3 相对位置编码</h3><p>无论是可学习位置编码还是正弦位置编码都是将位置编码作为额外的信息输入给模型，相对位置编码是将位置信息作为模型本身的属性，使模型不需要额外输入位置编码即可处理序列的位置信息。</p><p>这部分因为涉及到的都是模型架构的修改，因此我们会在后续的论文解读中解读相关的论文，这里不做解释。有兴趣的话可以看以下几篇论文（这几篇论文都会在后面详细解读）：</p><ol><li><a href="https://arxiv.org/pdf/1803.02155.pdf" target="_blank" rel="noopener">Self-Attention with Relative Position Representations</a></li><li><a href="https://arxiv.org/pdf/1901.02860.pdf" target="_blank" rel="noopener">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a></li><li><a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4614" target="_blank" rel="noopener">Gaussian Transformer: A Lightweight Approach for Natural Language Inference</a> </li></ol><h1 id="4-Multi-head-Attention"><a href="#4-Multi-head-Attention" class="headerlink" title="4. Multi-head Attention"></a>4. Multi-head Attention</h1><h2 id="4-1-为什么需要-K、Q、V，只用K、Q-K、V-Q、V行不行？"><a href="#4-1-为什么需要-K、Q、V，只用K、Q-K、V-Q、V行不行？" class="headerlink" title="4.1 为什么需要 K、Q、V，只用K、Q/K、V/Q、V行不行？"></a>4.1 为什么需要 K、Q、V，只用K、Q/K、V/Q、V行不行？</h2><p>实际上这个问题，我们在 <a href="https://rogerspy.gitee.io/2019/08/26/NLP中的注意力机制简介（一">NLP中的注意力机制简介（一）</a> 这篇文章中介绍过了。最初的注意力机制实际上是只用了两个值的，直到 <a href="https://arxiv.org/pdf/1503.08895.pdf" target="_blank" rel="noopener">Sukhbaatar et al., 2015</a> 将注意力机制引入到对话系统模型中才出现了 $k, q, v$ 三值的注意力形式。这样的注意力形式有两个好处：</p><ul><li>可复用性</li><li>灵活性</li></ul><p>更详细的内容可以参考上面的博客以及论文，这里不赘述。</p><h2 id="4-2-自注意力为什么scaled"><a href="#4-2-自注意力为什么scaled" class="headerlink" title="4.2 自注意力为什么scaled?"></a>4.2 自注意力为什么scaled?</h2><p>知乎上关于这个问题有很详细的讨论与解释，我就不自由发挥了，照抄过来。原文地址：<a href="https://www.zhihu.com/question/339723385" target="_blank" rel="noopener">transformer中的attention为什么scaled?</a> </p><h3 id="4-2-1-为什么比较大的输入会使得-softmax-的梯度变得很小？"><a href="#4-2-1-为什么比较大的输入会使得-softmax-的梯度变得很小？" class="headerlink" title="4.2.1 为什么比较大的输入会使得 softmax 的梯度变得很小？"></a>4.2.1 为什么比较大的输入会使得 softmax 的梯度变得很小？</h3><p>对于输入向量 $\pmb{x} \in \mathbb{R}^d$，<em>softmax</em> 函数将其映射/归一化到一个 $\pmb{\hat{y}} \in \mathbb{R}^d$。这个过程中，<em>softmax</em> 先用一个自然底数 $e$ 将输入的元素距离先拉大，然后归一化为一个分布。假设某个输入 $\pmb{x}_k$ 表示最大的元素，<strong>如果输入的数量级变大（每个元素都很大），那么 $\pmb{x}_k$ 对应的 $\pmb{\hat{y}}_k$ 会非常接近 1。</strong> </p><p>我们可以用一个小例子来看看。假定输入 $\pmb{x} = [a, a, 2a]^T$，</p><ul><li>$a = 1$ 时，$\hat{y}_2=0.5761168847658291$；</li><li>$a=10$ 时，$\hat{y}_2 = 0.999909208284341$；</li><li>$a = 100$ 时，$\hat{y}_2 \approx 1$ （计算机精度限制）。</li></ul><p>我们不妨把不同的 $a$ 对应的 $\hat{y}_2$ 绘制成一条曲线，更能清晰的看出问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> exp</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line">f = <span class="keyword">lambda</span> x: exp(x * <span class="number">2</span>) / (exp(x) + exp(x) + exp(x * <span class="number">2</span>))</span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">100</span>, <span class="number">100</span>)</span><br><span class="line">y_3 = [f(x_i) <span class="keyword">for</span> x_i <span class="keyword">in</span> x]</span><br><span class="line">plt.plot(x, y_3)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200521094625.png" alt></p><p>可以看到，数量级对 <em>softmax</em> 得到的分布影响非常大。<strong>在数量级较大时，softmax 将几乎全部的概率分布都分配给了最大值对应的标签</strong>。这里的数学证明请参考文章：<a href="https://zhuanlan.zhihu.com/p/79585726" target="_blank" rel="noopener">浅谈Softmax函数</a>（见A.补充）。</p><p>然后我们来看 <em>softmax</em> 的梯度。不妨简记 <em>softmax</em> 函数为 $g(\cdot)$，$\pmb{\hat{y}} = g(\pmb{x})$ 对 $\pmb{x}$ 的梯度为：</p><script type="math/tex; mode=display">\frac{\partial g({\pmb{x}})}{\partial \pmb{x}} = \begin{bmatrix}\hat{y}_0 & 0 & \cdots & 0 \\\\0 & \hat{y}_1 & \cdots & 0 \\\\\vdots & \vdots & \ddots & \vdots \\\\0 & 0 & \cdots & \hat{y}_{d-1}\end{bmatrix}-\begin{bmatrix}\hat{y}_0^2 & \hat{y}_0 \hat{y}_1 & \cdots & \hat{y}_0\hat{y}_{d-1} \\\\\hat{y}_1 \hat{y}_0 & \hat{y}_1^2 & \cdots & \hat{y}_1\hat{y}_{d-1} \\\\\vdots & \vdots & \ddots & \vdots \\\\\hat{y}_{d-1} \hat{y}_0 & \hat{y}_{d-1} \hat{y}_1 & \cdots & \hat{y}_{d-1}^2\end{bmatrix}</script><p>根据前面的讨论，当输入 $ \pmb{x}$ 的元素均较大时，<em>softmax</em> 会把大部分概率分布分配给最大的元素。假设我们的输入数量级很大，最大的元素是 $\pmb{x}_1$，那么就将产生一个接近 <em>one-hot</em> 的向量 $\pmb{\hat{y}} = [1, 0, \cdots, 0]^T$，此时此时上面的矩阵变为如下形式：</p><script type="math/tex; mode=display">\frac{\partial g({\pmb{x}})}{\partial \pmb{x}} = \begin{bmatrix}1 & 0 & \cdots & 0 \\\\0 & 0 & \cdots & 0 \\\\\vdots & \vdots & \ddots & \vdots \\\\0 & 0 & \cdots & 0\end{bmatrix}-\begin{bmatrix}1 & 0 & \cdots & 0 \\\\0 & 0 & \cdots & 0 \\\\\vdots & \vdots & \ddots & \vdots \\\\0 & 0  & \cdots & 0\end{bmatrix} = \pmb{0}</script><p>也就是说，在输入的数量级很大时，<strong>梯度消失为0，造成参数更新困难</strong>。</p><h3 id="4-2-2-维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩？"><a href="#4-2-2-维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩？" class="headerlink" title="4.2.2  维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩？"></a>4.2.2  维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩？</h3><p>假设向量 $\pmb{q}$ 和 $\pmb{k}$ 的各个分量是相互独立的随机变量，均值是 $0$，方差为 $1$，则点积 $\pmb{q \cdot k}$ 的均值为 $0$，方差为 $d_k$，$d_k$ 表示向量维度。这里给出一点详细的推导：</p><p>$\forall i = 1, …, d_k$，$q_i$ 和 $k_i$ 都是随机变量。为方便书写，不妨记 $X = q_i, Y=k_i$。已知：$D(X) = D(Y) = 1$ ， $E(X)=E(Y)=0$。</p><p>则有：</p><ol><li><p>$E(XY)=E(X)E(Y) = 0 \times 0 = 0$</p></li><li><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}D(XY) &= E(X^2Y^2)-[E(XY)]^2 \\\\&= E(X^2)E(Y^2) - [E(X)E(Y)] \\\\&= E(X^2 - 0^2)E(Y^2-0^2) - [E(X)E(Y)]^2 \\\\&= E(X^2-[E(X)]^2)\cdot E(Y^2-[E(Y)]^2) - [E(X)E(Y)]^2 \\\\&= D(X)D(Y)-[E(X)E(Y)]^2 \\\\&= 1 \times 1 - (0 \times 0)^2 \\\\&= 1\end{aligned}\end{equation}</script></li></ol><p>由期望和方差的性质， 对相互独立的分量 $Z_i$ 有：</p><script type="math/tex; mode=display">E(\sum_i Z_i) = \sum E(Z_i) \\\\D(\sum_i Z_i) = \sum D(Z_i)</script><p>所以 $\pmb{q} \cdot \pmb{k}$ 的均值 $E(\pmb{q \cdot k}) = 0$，方差 $D(\pmb{q \cdot k}) = d_k$。<strong>方差越大也就说明，点积的数量级越大（以越大的概率取大值）</strong>。那么一个自然的做法就是把方差稳定到 1，做法是将点积除以 $\sqrt{d_k}$：</p><script type="math/tex; mode=display">D(\frac{\pmb{q \cdot k}}{\sqrt{d_k}}) = \frac{d_k}{(\sqrt{d_k})^2} = 1</script><p><strong>将方差控制为 1，也就有效地控制了前面提到的梯度消失的问题</strong>。</p><h3 id="4-2-3-为什么在其他-softmax-的应用场景，不需要做-scaled"><a href="#4-2-3-为什么在其他-softmax-的应用场景，不需要做-scaled" class="headerlink" title="4.2.3 为什么在其他 softmax 的应用场景，不需要做 scaled?"></a>4.2.3 为什么在其他 softmax 的应用场景，不需要做 scaled?</h3><p>参考：<a href="https://arxiv.org/abs/1703.03906" target="_blank" rel="noopener">Massive Exploration of Neural Machine Translation Architectures</a> </p><p>具体来说，分为以下两个层面：</p><ul><li><p><strong>为什么在普通形式的 attention 中，使用非 scaled 的 softmax？</strong></p><p>最基础的注意力机制有两种形式， 一种是<a href="https://arxiv.org/abs/1409.0473v2" target="_blank" rel="noopener">加性的</a>，另一种是<a href="https://arxiv.org/abs/1508.04025" target="_blank" rel="noopener">乘性的</a>。数学描述为：</p><script type="math/tex; mode=display">\mathrm{score}(\pmb{v}_i, \pmb{u}) = \pmb{w}^T \tanh(\pmb{W}[\pmb{v}_i; \pmb{u}]) \\\\\mathrm{score}(\pmb{v}_i, \pmb{u}) = \pmb{u}^T \pmb{v}_i</script><p>在计算自注意力的时候之所以用乘性注意力机制，主要是为了<strong>计算更快</strong>。因为虽然矩阵加法的计算更简单，但是加性注意力包含 $\tanh$ ，相当于是一个完整的神经网络层。在整体计算复杂度上两者接近，但是矩阵乘法已经有了非常成熟的加速实现。在 $d_k$ 较小的时候两者效果接近，但随着 $d_k$ 增大，加性效果开始明显超越乘性注意力。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200521115516.png" alt></p><p>作者分析乘性注意力效果不佳的原因，认为是<strong>极大的点积值将整个 softmax 推向梯度平缓区，使得收敛困难</strong>。也就是上面讨论的梯度消失。这才有了 <em>scaled</em>。所以加性注意力机制天然不需要 <em>scaled</em>，在乘性注意力中，$d_k$ 较大的时候必须要做 <em>scaled</em>。</p><p>那么，极大的点积值是从哪里来的呢？</p><p>乘性注意力的极大点积值的来源上面讨论过，是由于两向量点积方差会从 $[0, 1]$ 范围扩散到 $[0, d_k]$ 范围。而加性的注意力，由于存在 $\tanh$ 将值限制在 $[-1, 1]$ 范围内，整体的方差和 $d_k$ 没有关系。</p></li><li><p><strong>为什么在分类层（最后一层），使用非 scaled 的 softmax？</strong></p><p>同上面部分，分类层的 <em>softmax</em> 也没有两个随机变量相乘的情况。此外，这一层的 <em>softmax</em> 通常和交叉熵联合求导，在某个目标类别 $i$ 上的整体梯度会变成 $\pmb{y}’_i-\pmb{y}_i$，即预测值与真值的差。当出现某个极大值时，<em>softmax</em>  的输出概率会集中在该类别上。如果预测正确，整体梯度接近于 0，如果类别错误，整体梯度会接近于 1，给出最大程度的负反馈。</p><p>也就是说，这个时候的梯度形式改变，不会出现极大值导致梯度消失的情况。</p></li></ul><h2 id="4-3-Transformer-为什么要用-Multi-head？"><a href="#4-3-Transformer-为什么要用-Multi-head？" class="headerlink" title="4.3 Transformer 为什么要用 Multi-head？"></a>4.3 Transformer 为什么要用 Multi-head？</h2><p><em>Multi-head</em>  应该是借鉴的 <em>Multi-dimension attention</em> 的思想。最早由 <a href="http://www.aaai.org/Conferences/AAAI/2017/PreliminaryPapers/15-Wang-W-14441.pdf" target="_blank" rel="noopener">Wang et al., 2017</a> 提出 <em>2D-attention</em>，希望模型能在不同的语境下，关注句子中不同的点。后来由 <a href="https://openreview.net/pdf?id=BJC_jUqxe" target="_blank" rel="noopener">Lin et al. 2017</a> 将 <em>2D-attention</em> 扩展到 <em>Multi-dimension attention</em> 。多维注意力的初衷是使注意力形成多个子空间，可以让模型去关注不同方面的信息，多头注意力也继承了这一想法。但是在实际的训练中，模型真的如我们的预期那样，去学习了不同方面的特征吗？</p><p>现在的研究表明，<em>Transformer</em> 的底层更偏向于关注语法，顶层更偏向于关注语义。虽然在同一层中多数头的关注模式是一样的，但是总有那么一两个头与众不同。这种模式是很普遍的，为什么会出现这种情况？目前还不清楚。</p><p>针对不同的头，我们思考以下几个问题：</p><ul><li>同一层中，不同头之间的差距有多少？（用 $h_i$ 度量）</li><li>同一层中，不同头的数量是否对 $h_i $ 有影响？</li><li>$h_i$ 是否随层数的变化而变化？</li><li>我们能否使用 <em>single-head</em> 达到 <em>multi-head</em> 的效果？</li></ul><blockquote><p>另外，初始化对 $h_i$ 的影响也是一个值得探究的问题，这个我们专门开一篇文章讲解，这里略过。</p></blockquote><p>论文 <a href="https://arxiv.org/pdf/1906.04341v1.pdf" target="_blank" rel="noopener">What Does BERT Look At? An Analysis of BERT’s Attention</a> 研究指出，头之间的差距随着所在层数变大而减少。换句话说，<strong>头之间的方差随着所在层数的增大而减小</strong>。</p><p>而在论文 <a href="https://arxiv.org/pdf/1905.09418.pdf" target="_blank" rel="noopener">Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned</a> 中，作者研究了翻译的质量在何种程度上依赖单个头以及能否去掉一些头而不失效果等问题。研究结果表明，<strong>只有一小部分头对翻译而言是重要的，其他的头都是次要的（可以丢掉）。</strong> 这说明注意力头并非越多越好，但要足够多，<em>single-head attention</em> 目前来看还是无法取代 <em>multi-head attention</em>。</p><p>另外，其实还有一个有意思的问题是，不同头之间的差距有什么作用？在 <a href="https://openreview.net/pdf?id=BJC_jUqxe" target="_blank" rel="noopener">Lin et al. 2017</a> 的这篇文章中，通过引入一个正则化项，将不同头之间的注意力差距变大。他给出的解释是，是模型尽可能多的关注到不同的点上。那么拉大注意力头之间的差距在 <em>Transformer</em> 中有什么效果，目前还没有相关的实验。</p><p>更多关于 <em>Multi-head attention</em> 的探讨可以参考：</p><ul><li><a href="https://blog.ml.cmu.edu/2020/03/20/are-sixteen-heads-really-better-than-one/" target="_blank" rel="noopener">Are Sixteen Heads Really Better than One?</a> （博客）</li><li><a href="https://arxiv.org/pdf/1905.10650.pdf" target="_blank" rel="noopener">Are Sixteen Heads Really Better than One?</a> （论文）</li><li><a href="https://www.aclweb.org/anthology/P18-1167.pdf" target="_blank" rel="noopener">How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures</a> </li></ul><h1 id="5-Add-amp-Norm"><a href="#5-Add-amp-Norm" class="headerlink" title="5. Add &amp; Norm"></a>5. Add &amp; Norm</h1><p>还记得我们在介绍 <em>Transformer</em>  的时候，模型训练的初期我们使用了 <em>warm-up</em> 的小技巧。那么问题就来了，<em>warm-up</em> 为什么有效？</p><p><a href="https://link.zhihu.com/?target=https%3A//openreview.net/forum%3Fid%3DB1x8anVFPr" target="_blank" rel="noopener">On Layer Normalization in the Transformer Architecture</a> 就试图从 <em>LayerNorm</em> 的角度去解释。</p><p>首先我们先思考：为什么 <em>warm-up</em> 是必须的？能不能把它去掉？本文的出发点是：既然 <em>warm-up</em> 是训练的初始阶段使用的，那肯定是训练的初始阶段优化有问题，包括模型的初始化。顺着这个思路，作者发现在训练的初始阶段，输出层附近的期望梯度非常大，如果没有 <em>warm-up</em>，模型优化过程就会炸裂，非常不稳定。使用 <em>warm-up</em> 既可以保持分布的平稳，也可以保持深层的稳定。</p><p>作者发现这一现象与 <em>LayerNorm</em> 的位置有关。在原始的 <em>Transformer</em> 中，<em>LayerNorm</em> 在跟在 <em>Add</em> 之后的（也就是跟在残差之后），我们把这个称为 <code>Post-LN Transformer</code>。那我们很自然的想到，如果我们把 <em>LayerNorm</em> 换个位置，比如放到残差计算过程中（称为<code>Pre-LN Transformer</code>）会怎么样呢？</p><p>下图是两种结构的示意图：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20200521170535.png" alt></p><p>作者通过理论证明了：</p><ul><li><code>Post-LN Transformer</code>的梯度范数在输出层附近很大，因此很可能随着后向传播的进行梯度越来越小；</li><li><code>Pre-LN Transformer</code>在每层的梯度范数都近似不变。</li></ul><p>因此， <em>warm-up</em> 对 <code>Post-LN Transformer</code> 来说是必不可少的，实验结果也证实了这一点。而对于 <code>Pre-LN Transformer</code> 来说，理论上 <em>warm-up</em> 是可有可无的。然后作者通过实验证明 <code>Pre-LN Transformer</code> 似乎的确不再需要 <em>warm-up</em> 了。</p><h1 id="6-Masked-Multi-head-Attention"><a href="#6-Masked-Multi-head-Attention" class="headerlink" title="6. Masked Multi-head Attention"></a>6. Masked Multi-head Attention</h1><p><em>Mask</em> 在 <em>NLP</em> 中是一个很常规的操作，也有多种应用的场景和形式。那么 <em>Mask</em> 到底有什么用呢？先上结论：</p><ul><li>处理非定长序列</li><li>防止标签泄露</li></ul><p>总的来说 <em>Mask</em> 无非就是这两种用途，下面我们详细解释一下。</p><h2 id="6-1-处理非定长序列"><a href="#6-1-处理非定长序列" class="headerlink" title="6.1 处理非定长序列"></a>6.1 处理非定长序列</h2><p>在 <em>NLP</em> 中，文本一般是不定长的，所以在进行训练之前，要先进行长度的统一，过长的句子可以通过截断到固定的长度，过短的句子可以通过 <em>padding</em> 增加到固定的长度，但是 <em>padding</em> 对应的字符只是为了统一长度，并没有实际的价值，因此希望在之后的计算中屏蔽它们，这时候就需要 <em>Mask</em>。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/1162239951.png" alt></p><p>上图中的 1 表示有效字，0 代表无效字。</p><ul><li><p><strong>RNN 中的 Mask</strong></p><p>对于 <em>RNN</em> 等模型，本身是可以直接处理不定长数据的，因此它不需要提前告知序列长度。但是在实践中，为了 批量训练，一般会把不定长的序列 <em>padding</em> 到相同长度，再用 <em>mask</em> 去区分非 <em>padding</em> 部分和 <em>padding</em> 部分。做这样区分的目的是使得 <em>RNN</em> 只作用到它实际长度的句子，而不会处理无用的 <em>padding</em> 部分，这样 <em>RNN</em> 的输出和隐状态都会是对应句子实际的最后一位。另外，对于 <em>token</em> 级别的任务，也可以通过 <em>mask</em> 去忽略 <em>padding</em> 部分对应的 <em>loss</em>。 </p></li><li><p><strong>Attention 中的 Mask</strong></p><p>在 <em>Transformer</em> 中，有两种不同的 <em>Mask</em>。其中作用在序列上的 <em>Mask</em> 同样是为了忽略 <em>padding</em> 部分的影响。</p></li></ul><h2 id="6-2-防止标签泄露"><a href="#6-2-防止标签泄露" class="headerlink" title="6.2 防止标签泄露"></a>6.2 防止标签泄露</h2><p>在一些任务中，模型输入的序列中可能包含需要预测的一些信息，为了防止模型“提前看答案”，我们需要将输入序列中的一些信息 <em>mask</em> 掉。</p><ul><li><p><strong>Transformer 中的 mask</strong></p><p><em>Transformer</em> 中 <em>masked multi-head attention</em> 中的 <em>mask</em> 目的就是掩盖输入序列中的一些信息。在进行机器翻译的时候，模型 <em>decoder</em> 的输入是目标序列（已经预测的部分），比如 $t$ 时刻，模型是不可能知道 $t+1$ 时刻的输入的，因此，在训练的时候我们要模拟这一过程。至于怎么样进行 <em>mask</em> 这个在 <em>Transformer</em> 的介绍中已经详细介绍过了，不再赘述。</p></li><li><p><strong>BERT 中的 Mask</strong></p><p>BERT 实际上是 <em>Transformer</em> 的 <em>Encoder</em>，为了在语言模型的训练中，使用上下文信息又不泄露标签信息，采用了 <em>Masked LM</em>，简单来说就是随机的选择序列的部分 <em>token</em> 用 <code>[Mask]</code> 标记代替。至于这么做为什么有效，实际上 <a href="https://arxiv.org/abs/1703.02573" target="_blank" rel="noopener">Data Noising as Smoothing in Neural Network Language Models</a> 就首次提出了此方法，而且给出了理论解释。这种替换其实本质上属于语言建模里面基于 <em>interpolation</em> 的平滑方式。</p></li><li><p><strong>XLNet 中的 Mask</strong></p><p><em>XLNet</em> 通过 <strong>Permutation Language Modeling</strong> 实现了不在输入中加 <code>[Mask]</code>，同样可以利用上下文信息。</p><p>为了更直观的解释，我们举个例子：</p><blockquote><p>假设输入的序列是<code>[1,2,3,4]</code>, 排列共有 4x3x2=24 种。</p><p>选其中的四种分别为<code>[3,2,4,1]</code>,<code>[2,4,3,1]</code>,<code>[1,4,2,3]</code>,<code>[4,3,1,2]</code>。</p><p>在预测位置3的单词时，</p><p>第一种排列看不到任何单词，第二种排列能看到<code>[2,4]</code></p><p>第三种排列能看到<code>[1,2,4]</code>,第四种排列能看到<code>[4]</code>,</p><p>所以预测位置 3 的单词时，不仅能看到上文<code>[1,2]</code>,也能看到下文的<code>[4]</code>。</p></blockquote></li></ul><p>关于 <em>Mask</em> 的讨论也就到此为止了。</p><p>同时关于 <em>Transformer</em> 中的一些细节，以及引申出来的一些讨论，我们也就介绍到这。</p><h1 id="7-Appendix"><a href="#7-Appendix" class="headerlink" title="7. Appendix"></a>7. Appendix</h1><h2 id="Appendix-A：-BPE-初始版本"><a href="#Appendix-A：-BPE-初始版本" class="headerlink" title="Appendix A： BPE 初始版本"></a>Appendix A： BPE 初始版本</h2><p>假设有一段序列 $aaabdaaabac$，我们想对其进行编码。通过观察我们会发现 $aa$ 的出现概率最高（只考虑字符对），那么我们用一个序列中没有出现过的字符 $Z$ 来代替 $aa$ ：</p><script type="math/tex; mode=display">Z = aa \\\\aaabdaaabac \rightarrow ZabdZabac</script><p>得到新序列之后我们会发现 $ab$ 字符对出现的概率最高，同样的我们使用一个新序列中没有出现过的字符 $Y$ 来代替 $ab$ ：</p><script type="math/tex; mode=display">Y = ab \\\\Z = aa \\\\ZabdZabac \rightarrow ZYdZYac</script><p>继续重复上面的步骤，我们得到：</p><script type="math/tex; mode=display">X=ZY \\\\Y=ab \\\\Z=aa \\\\ZYdZYac \rightarrow XdXac</script><p>最后，序列中的所有字符对出现的频率都是 1，<em>BPE</em> 编码结束。解码的时候按照相反的顺序更新替换即可。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/abs/1508.07909" target="_blank" rel="noopener">Neural machine translation of rare words with subword units.</a> <em>Rico Sennrich, Barry Haddow, Alexandra Birch. 2015. arXiv: 1508.07909</em></li><li><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf" target="_blank" rel="noopener">Japanese and Korean Voice Search.</a> <em>Schuster, Mike, and Kaisuke Nakajima. 2012.  IEEE</em></li><li><a href="https://arxiv.org/abs/1910.13267" target="_blank" rel="noopener">BPE-Dropout: Simple and Effective Subword Regularization.</a> <em>Ivan Provilkov, Dmitrii Emelianenko, Elena Voita. 2019. arXiv:1910.13267</em></li><li><a href="http://arxiv.org/abs/1804.10959" target="_blank" rel="noopener">Subword regularization: Improving neural network translation models with multiple subword candidates.</a> <em>Taku Kudo. 2018. arXiv: 1804.10959</em></li><li><a href="https://zhuanlan.zhihu.com/p/86965595" target="_blank" rel="noopener">深入理解NLP Subword算法：BPE、WordPiece、ULM</a> <em>Luke</em>. 知乎</li><li><a href="https://zhuanlan.zhihu.com/p/90151246" target="_blank" rel="noopener">子词技巧：The Tricks of Subword</a> <em>Andy Yang.</em> 知乎 </li><li><a href="https://mp.weixin.qq.com/s/FTmkkFJzCw2hQtDDEDPqnQ" target="_blank" rel="noopener">一分钟搞懂的算法之BPE算法</a></li><li><a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/#what-is-positional-encoding-and-why-do-we-need-it-in-the-first-place" target="_blank" rel="noopener">Transformer Architecture: The Positional Encoding</a></li><li><a href="https://timodenk.com/blog/linear-relationships-in-the-transformers-positional-encoding/" target="_blank" rel="noopener">Linear Relationships in the Transformer’s Positional Encoding</a> <em>Timo Denk’s Blog</em> </li><li><a href="https://openreview.net/pdf?id=Hke-WTVtwr" target="_blank" rel="noopener">Encoding Word Oder In Complex Embeddings</a> <em>Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, Jakob Grue Simonsen. 2020. ICLR</em></li><li><a href="https://mp.weixin.qq.com/s/DQvhw6gTJt2V_8CPD0jHEQ" target="_blank" rel="noopener">如何优雅地编码文本中的位置信息？三种positioanl encoding方法简述</a> 小鹿鹿 <em>lulu</em>, 微信公众号</li><li><a href="https://www.aclweb.org/anthology/P18-1167.pdf" target="_blank" rel="noopener">How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures</a> <em>Tobias Domhan. 2018. ACL</em></li><li><a href="https://arxiv.org/abs/1905.09418" target="_blank" rel="noopener">Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned</a> <em>Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov. 2019. arXiv: 1905.09418</em></li><li><a href="https://arxiv.org/abs/1905.10650" target="_blank" rel="noopener">Are Sixteen Heads Really Better than One?</a> <em>Paul Michel, Omer Levy, Graham Neubig. 2019. arXiv: 1905.10650</em> </li><li><a href="https://openreview.net/forum%3Fid%3DB1x8anVFPr" target="_blank" rel="noopener">On Layer Normalization in the Transformer Architecture</a> <em>Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Huishuai Zhang, Yanyan Lan, Liwei Wang, Tie-Yan Liu. 2020. ICLR (rejected)</em></li><li><a href="https://mp.weixin.qq.com/s/0hUNG6tC-hlfyTJtuzwU5w" target="_blank" rel="noopener">NLP中的Mask全解</a>，海晨威 <em>PaperWeekly</em> </li><li><a href="https://zhuanlan.zhihu.com/p/82391768?from_voters_page=true" target="_blank" rel="noopener">关于Transformer的若干问题整理记录</a> <em>Adherer</em> 知乎</li><li><a href="https://zhuanlan.zhihu.com/p/84614490" target="_blank" rel="noopener">香侬读 | Transformer中warm-up和LayerNorm的重要性探究</a> 香侬科技， 知乎</li><li><a href="https://www.zhihu.com/question/339723385" target="_blank" rel="noopener">transformer中的attention为什么scaled?</a> </li><li><a href="https://arxiv.org/abs/1703.03906" target="_blank" rel="noopener">Massive Exploration of Neural Machine Translation Architectures</a>, <em>Denny Britz, Anna Goldie, Minh-Thang Luong, Quoc Le. 2017. arXiv: 1703.03906</em> </li><li><a href="https://arxiv.org/pdf/1911.04474.pdf" target="_blank" rel="noopener">TENER: Adapting Transformer Encoder for Named Entity Recognition</a> <em>Hang Yan, Bocao Deng, Xiaonan Li, Xipeng Qiu. 2019. arXiv:1911.04474</em> </li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Attention </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP中的注意力机制简介（二）</title>
      <link href="/2019/08/27/NLP%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%AE%80%E4%BB%8B%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2019/08/27/NLP%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%AE%80%E4%BB%8B%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1>    <small>——Transformer专题篇</small></h1><h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><p>之前我们介绍了各种各样的注意力机制，如果仔细回想一下就可以发现无论是哪种注意力机制都不是单独出现的，都是伴随着<em>RNN</em>或者其他<em>RNN</em>的变种。这种基于<em>RNN</em>的注意力机制会面临一个问题就是，难以处理长序列的句子，因为无法实现并行计算，所以非常消耗计算资源。</p><a id="more"></a><p><em>CNN</em>虽然可以实现并行计算，但是它无法获得序列的位置信息，这样它也就难以获得远距离信息上的依赖关系。后来虽然有人提出了完全基于<em>CNN</em>的<em>seq2seq</em>模型，但是却非常消耗内存。</p><p>由于基于<em>RNN</em>的注意力机制遇到了计算资源上的瓶颈，<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Vaswani et al., 2017</a>提出了一个新的模型——<strong>Transformer！</strong>从目前的发展来看，这个模型对得起这个名字，因为它真的很能打，自从2018年基于<em>Transformer</em>的<em>BERT</em>预训练语言模型的横空出世到今天，几乎每一次<em>NLP</em>的重大进展都与它息息相关。因此，我们专门开一个专题篇来详细介绍一下这个模型。</p><p><strong>Transformer</strong>的创新点在于抛弃了之前传统的基于<em>CNN</em>或者<em>RNN</em>的<em>encoder-decoder</em>模型的固有模式，只用<em>Attention</em>实现<em>encoder-decoder</em>。<em>Transformer</em>的主要目的在于减少计算量和提高并行效率的同时不损害最终的实验结果。</p><h1 id="2-模型结构"><a href="#2-模型结构" class="headerlink" title="2. 模型结构"></a>2. 模型结构</h1><h2 id="2-1-模型结构总览"><a href="#2-1-模型结构总览" class="headerlink" title="2.1 模型结构总览"></a>2.1 模型结构总览</h2><p>在初始的论文中<em>Transformer</em>仍然是被用于机器翻译任务上：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/the_transformer_3.png" alt="img"></p><p>下面我们来打开擎天柱，看下它到底是怎么构成的？</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/The_transformer_encoders_decoders.png" alt="img"></p><p>可以看到<em>Transformer</em>仍然采用了<em>encoder-decoder</em>结构，原始论文中<em>encoder</em>是由6个相同的编码模块堆叠而成（这里的相同指的是结构相同，但是其中的权重却是不共享的， 下面的解码器与之相同），而<em>decoder</em>同样也是用6个解码器堆叠而成的。<strong>6</strong>这个数字并没有什么特殊之处，只是原始论文使用的层数，我们可以在实验过程中任意设置层数。如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/The_transformer_encoder_decoder_stack.png" alt="img"></p><p>注意一个细节，<em>encoder</em>和<em>decoder</em>的链接方式是<em>encoder</em>的最后一层输出与<em>decoder</em>的每一层相连。下面我们打开其中一个编码器和解码器看下里面是什么结构：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/Transformer_decoder.png" alt="img"></p><p><em>encoder</em>的输入先经过一个自注意力层对句子进行编码获取词的注意力权重，然后将自注意力输入给一个全连接层。</p><p>对于<em>decoder</em>中自注意力层和全连接层和<em>encoder</em>相同，但是在自注意力输出后要经过一个注意力编码层与<em>encoder</em>进行联合编码，然后再传入给全连接层。这个联合编码其实就类似于在<em>seq2seq</em>模型中解码过程中，<em>decoder</em>隐状态和注意力联合编码然后再输出的过程是类似的。</p><p>下面我们继续拆解擎天柱的零件，看看这个自注意力和全连接层下面埋藏着什么秘密。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/1566026177834.png" alt></p><p>原来所谓的<em>Self-attention</em>是一个叫做<em>Multi-Head Attention</em>的东西，这个就是擎天柱的核心部件了。其他的小零件比如<em>Add, Norm, Linear, softmax, Feed Forward</em>等等都是在五金店都能买到的小玩意儿。下面我们就详细看下这个能量块到底隐藏着什么秘密吧。</p><h2 id="2-2-Multi-Head-Attention"><a href="#2-2-Multi-Head-Attention" class="headerlink" title="2.2 Multi-Head Attention"></a>2.2 Multi-Head Attention</h2><p>从字面上就可以看出所谓<em>Multi-Head Attention</em>是由两部分组成——<em>Multi-Head</em>和<em>Attention</em>，而实际情况也是如此。其实严格来讲这是一个一般化的名称，如果具体到论文使用的自注意力机制的话，应该叫做<em>Self-Multi-Head Attention</em>。应该有三部分组成，除了上面提到的两个，还应该加上一个自注意力。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/e4bc841abc55d366813340f92f6696c5d59e95.png" alt></p><p>还记得我们上一篇介绍注意力机制的文章中提到一种使用$\mathbf{k}, \mathbf{q}, \mathbf{v}$计算注意力的注意力机制。而<em>Scaled Dot-Product</em>也是我们之前提到的一种计算<em>Alignment score function</em>的方法。也就是说，图中下半部分是在计算<em>Scaled-Dot Product Attention</em>，而图中上半部分的<em>Concat</em>操作就是拼接操作，拼接什么？仔细看下半部分计算注意力的时候，不是只计算一个<em>Scaled-Dot Product Attention</em>，而是在同时计算<em>h</em>个<em>Scaled-Dot Product Attention</em>。而<em>Concat</em>拼接的的就是这<em>h</em>个<em>Scaled-Dot Product Attention</em>。这就是所谓的<em>Multi-Head Attention</em>，每一个<em>Head</em>就是一个<em>Scaled-Dot Product Attention</em>。</p><p>形式化描述如下：</p><script type="math/tex; mode=display">MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h)W^O</script><script type="math/tex; mode=display">head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)</script><p>其中$W_i^Q \in \mathbb{R}^{d_{model}\times d_q}$，$W_i^K \in \mathbb{R}^{d_{model}\times d_k}$，$W_i^V \in \mathbb{R}^{d_{model}\times d_v}$，$W_i^O \in \mathbb{R}^{d_{model}\times d_o}$。在原始论文中$h=8$，$d_k=d_v=d_{model}/h=64$。由于对每一个<em>Head</em>进行了降维，所以总的计算量和使用一个单独的不降维的<em>Head</em>是一样的。本文涉及到的公式所用标记都与原论文保持一致，避免混淆。</p><p>我们仔细思考一下这个<em>Multi-Head Attention</em>，和我们提到的<em>Multi-dimensional Attention</em>有异曲同工之妙。论文里提到，相对于使用单个注意力而言，使用<em>Multi-Head</em>能获得更好的效果。但是论文并没有解释为什么。我们这里结合<em>Multi-dimensional Attention</em>做一个大胆的猜想：<em>Transformer</em>的强大之处正是由于这个<em>Multi-Head</em>！因为多维注意力机制能够获得一句话中在不同语境下的不同理解。而在语言模型中，词语和句子的歧义性一直是自然语言处理的难点，而<em>Transformer</em>在多维注意力机制的作用下能够很好的获取句子的多重含义，并且能根据上下文信息自动获取正确的语义，因此<em>Transformer</em>能够在预训练语言模型中大放异彩。</p><p>下面我们就应该看一下这个核心中的核心——<em>Scaled-Dot Product Attention</em>了。</p><h2 id="2-3-Scaled-Dot-Product-Attention"><a href="#2-3-Scaled-Dot-Product-Attention" class="headerlink" title="2.3 Scaled-Dot Product Attention"></a>2.3 Scaled-Dot Product Attention</h2><p><em>Scaled-Dot Product Attention</em>结构如图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/1566031501224.png" alt></p><p>首先我们先给出其形式化定义：</p><script type="math/tex; mode=display">Attention(K, Q, V) = softmax(\frac{QK^T}{\sqrt {d_k}})V</script><p>我们把图中结构分解开来，一步一步解释清楚：</p><ul><li>第一步<em>MatMul</em>：计算<em>Alignment score function</em></li></ul><script type="math/tex; mode=display">MatMul(Q, K_i) = QK_i^T</script><ul><li>第二步<em>Scale</em>：调节<em>Alignment score function</em>分数</li></ul><script type="math/tex; mode=display">Scale(Q, K_i) = \frac{Q,K_i^T}{\sqrt{d_k}}</script><ul><li>第三步<em>Mask</em>（可选）：<em>encoder</em>不需要这一步，<em>decoder</em>需要这一步。</li></ul><p>这里的masked就是要在训练语言模型的时候，不给模型看到未来的信息， 让模型自己预测下一个词。</p><ul><li>第四步<em>Softmax</em>：相似度归一化</li></ul><script type="math/tex; mode=display">\alpha_i = Softmax(Q, K_i) =softmax(\frac{QK_i^T}{\sqrt {d_k}})</script><ul><li>第五步<em>MatMul</em>：通过计算出来的权重与$V$加权求和得到最终的<em>Attention</em>向量</li></ul><script type="math/tex; mode=display">Attention(K_i, Q, V_i) = \sum_i \alpha_i V_i</script><p>下面我们从序列输入开始详细解释一下每一步到底是在做什么。</p><ul><li>第一步：计算<em>K, Q, V</em></li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/transformer_self_attention_vectors.png" alt="img"></p><p>将输入的每一个词转化成词向量，词向量可以是预训练好的（比如用<em>word2vec</em>）词向量，在网络训练过程中固定词向量矩阵不参与训练，也可以是随时初始化，然后随着网络的训练不断更新词向量矩阵。</p><p>将序列中每个元素对应的词向量分别与$W^Q, W^K, W^V$相乘计算得到<em>queries, keys, values</em>。计算得到的<em>queries, keys, values</em>的维度为64，当然维度缩减并非必须。</p><ul><li>第二步：计算自注意力的<em>alignment score function</em></li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/transformer_self_attention_score.png" alt="img"></p><p>所谓自注意力就是计算序列中两两元素之前的依赖关系，这个分数表示当前这个词需要把多少注意力放在句子中的其他部分上。</p><p>以上图举例，<em>Thinking</em>的得分是$\mathbf{q}_1\cdot \mathbf{k}_1=112$，<em>Machines</em>的得分是$\mathbf{q}_1\cdot \mathbf{k}_2=96$，后面的以此类推。</p><ul><li>第三步和第四步：对上一步的得分进行缩放，然后计算<em>softmax</em></li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/self-attention_softmax.png" alt="img"></p><p>上一步的得分除以8（因为前面我们提到<em>queries, keys, values</em>的维度为64，开方之后就是8）。之所以要做这个缩放论文给出的解释是防止这个得分过大或者过小，在做<em>softmax</em>的时候不是0就是1，这样的话就不够<em>soft</em>了。</p><p>得到放缩后的得分之后就是计算<em>softmax</em>了。</p><ul><li>第五步：在<em>decoder</em>中对句子进行<em>Mask</em>。比如输入是一句话 “i have a dream” 总共4个单词，这里就会形成一张4x4的注意力机制的图： </li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/1.png" alt></p><p>这里的<em>mask</em>就是要在做语言建模的时候，不给模型看到未来的信息，让模型自己预测后面的信息。比如上图中，“I”作为第一个词，只能和自己进行<em>Attention</em>；“have”作为第二个词，可以和“I”和“have”本身进行<em>Attention</em>；“a”作为第三个单词，可以和“I”，“have”，“a” 三个单词进行<em>Attention</em>；到了最后一个单词“dream”的时候，才有对整个句子4个单词的<em>attention</em>。</p><ul><li>第六步：用上面计算出来的<em>softmax</em>与<em>values</em>进行加权求和</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/self-attention-output.png" alt="img"></p><p>至此<em>Scaled-Dot Product Attention</em>就计算完了。</p><h2 id="2-4-矩阵计算-Scaled-Dot-Product-Attention"><a href="#2-4-矩阵计算-Scaled-Dot-Product-Attention" class="headerlink" title="2.4 矩阵计算 Scaled-Dot Product Attention"></a>2.4 矩阵计算 Scaled-Dot Product Attention</h2><p>前面我们说过，<em>Transformer</em>的初衷就是并行计算，所谓并行计算就是矩阵计算，上面的例子是通过一个一个向量进行计算的，如果我们把向量堆叠成矩阵，就可以实现并行运算了：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/self-attention-matrix-calculation.png" width="40%" align="left"><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/self-attention-matrix-calculation-2.png" width="60%" align="right"></p><h2 id="2-5-矩阵计算-Multi-Head"><a href="#2-5-矩阵计算-Multi-Head" class="headerlink" title="2.5 矩阵计算 Multi-Head"></a>2.5 矩阵计算 Multi-Head</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs//transformer_multi-headed_self-attention-recap.png" alt="img"></p><h2 id="2-6-Position-Encoding"><a href="#2-6-Position-Encoding" class="headerlink" title="2.6 Position Encoding"></a>2.6 Position Encoding</h2><p>到目前为止，擎天柱的能量核心结构我们介绍完了，但是我们还忽略了一个问题：句子是一个有序的序列，句子中两个词的位置互换的话，这个句子的意思完全不同了，因此在处理自然语言的时候词与词的绝对位置或者相对位置也是一个非常重要的信息。</p><p>为了解决位置信息的问题，<em>Transformer</em>在每一个输入向量中增加了一个位置向量，这个位置向量的维度为$d_{model}$，这样输入向量和位置向量就可以 直接相加了。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/transformer_positional_encoding_example.png" alt="img"></p><p>在NLP的很多模型中都有位置向量的使用，比如前面我们提到基于<em>CNN</em>的<em>seq2seq</em>模型（<a href="https://arxiv.org/abs/1705.03122" target="_blank" rel="noopener">Gehring et al., 2017</a>）。但是通常其他模型中的位置向量都是通过学习得来的，本文采用的是直接通过函数构造出来的：</p><script type="math/tex; mode=display">\left\{\begin{aligned}PE_{(pos, 2i)} & =  \sin(pos/10000^{2i/d_{model}}) \\\\PE_{(pos, 2i+1)} & =  \cos(pos/10000^{2i/d_{model}}) \end{aligned}\right.</script><p>其中$pos$表示位置索引，$i$表示维度索引。也就是说位置向量中的每一维都是一个余弦曲线，波长是一个从$2\pi$到$10000 \cdot 2\pi$的等比数列。之所以选择这个函数，是因为它允许模型能很容易的学习到相对位置信息，因为对于任意固定的偏置$k$，$PE_{pos+k}$能通过一个$PE_{pos} $的线性函数推理得到：</p><script type="math/tex; mode=display">\begin{aligned}\sin(\alpha+\beta) &= \sin(\alpha) \cos(\beta) + \cos(\alpha)\sin(\beta)\\\\\cos(\alpha+\beta) &= \cos(\alpha) \cos(\beta) - \sin(\alpha)\sin(\beta) \end{aligned}</script><p>另外，作者也做过实验，使用通过学习得到的位置向量，最后发现两者的效果差别不大。在效果差别不大的情况下使用直接构造的方法能够避免训练过程的权重更新，这样可以加速训练。另外一个很重要的原因就是，选择这个余弦版本的位置向量还可以处理比训练时遇到的更长的序列。</p><h2 id="2-6-残差结构"><a href="#2-6-残差结构" class="headerlink" title="2.6 残差结构"></a>2.6 残差结构</h2><p>如果我们仔细看模型结构图就会发现，数据的流向并不是从一层单项流向下一层的这种简单的串联结构，而是采用了类似残差网络的残差式连接。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/1566120593103.png" alt></p><ul><li>第一步：计算<em>Multi-Head Attention</em></li><li>第二步：原始的输入+<em>Multi-Head Attention</em></li><li>第三步：使用<em>LayerNorm</em>进行正则化</li><li>第四步：正则化后的数据经过全连接层，全连接层的激活函数使用<em>ReLU</em>函数。注意这里面的全连接层是每个位置每个位置单独进行计算的，其实更像是卷积核大小为1的卷积层。</li><li>第五步：第三步正则化的数据与全连接层后的数据相加</li><li>第六步：第五步相加后的数据再次正则化</li></ul><p>这就是<em>Transformer</em>的残差网络计算过程。</p><p>到目前为止，擎天柱身上的主要零部件我们都已经介绍完了，接下来就该把这些零部件再组装回去了。</p><h1 id="3-模型组装"><a href="#3-模型组装" class="headerlink" title="3. 模型组装"></a>3. 模型组装</h1><h2 id="3-1-Encoder"><a href="#3-1-Encoder" class="headerlink" title="3.1 Encoder"></a>3.1 Encoder</h2><p><em>Encoder</em>包含6个相同的层</p><ul><li>每一层包含2个<em>sub-layer</em>：<em>Multi-Head Attention</em>和全连接层。</li><li>每个<em>sub-layer</em>都要正则化</li><li><em>sub-layer</em>内部通过残差结构连接</li><li>每一层的输出维度为$d_{model}=512$</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/transformer_resideual_layer_norm_2.png" width="60%"></p><h2 id="3-2-Decoder"><a href="#3-2-Decoder" class="headerlink" title="3.2 Decoder"></a>3.2 Decoder</h2><p>Decoder也是6层</p><ul><li>每层包含3个<em>sub-layer</em>：<em>Multi-Head Attention</em>，<em>Encoder-Decoder Attention</em>和全连接层</li><li>其中<em>Encoder-Decoder Attention</em>的结构和其他的注意力相同，但是不同的是这一层的$K, V$都是来源于<em>Encoder</em>，而$Q$来源于上一层注意力产生的。</li><li><em>Decoder</em>中的<em>Multi-Head Attention</em>层需要进行修改，因为只能获取到当前时刻之前的输入，因此只对时刻 <em>t</em> 之前的时刻输入进行<em>Attention</em>计算，这也称为<em>Mask</em>操作</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/transformer_resideual_layer_norm_3.png" alt="img"></p><h2 id="3-3-最后的Linear层和Softmax层"><a href="#3-3-最后的Linear层和Softmax层" class="headerlink" title="3.3 最后的Linear层和Softmax层"></a>3.3 最后的Linear层和Softmax层</h2><p><em>Decoder</em>输出一个向量，我们怎么把这个向量转化成单词呢？这就是最后的<em>Linear</em>和<em>Softmax</em>层做的事情。</p><p>线性变换层是一个简单的全连接层，将<em>Decoder</em>输出的向量转化成一个<em>logits vector</em>。假设模型的词表中有10000个词，这个<em>logits vector</em>的维度就是10000，每一维对应词表中的一个词的得分。</p><p>然后<em>softmax</em>将这些得分进行归一化，将得分变成每个词的概率，然后选出概率最大的那个位置对应到词就是最后的输出了。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/1566125533498.png" width="60%"></p><h1 id="4-Transformer在机器翻译中的应用"><a href="#4-Transformer在机器翻译中的应用" class="headerlink" title="4. Transformer在机器翻译中的应用"></a>4. Transformer在机器翻译中的应用</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/transformer_decoding_1.gif" alt="img"> </p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/transformer_decoding_2.gif" alt="img"></p><h1 id="5-关于K-Q-V的讨论"><a href="#5-关于K-Q-V的讨论" class="headerlink" title="5. 关于K, Q, V的讨论"></a>5. 关于K, Q, V的讨论</h1><p>到这里我们关于整个<em>Transformer</em>的介绍就结束了，我们先从整体上介绍了<em>Transformer</em>也是一个基于<em>encoder-decoder</em>的结构，然后抽丝剥茧般的一层一层的剥开模型，看看它的每一部分到底长什么样子，然后我们了解了每个零件之后又重新把每个零件组装回去。但是还是有两个问题我们可以再细致的讨论一下的，比如为什么需要$V$？为什么$K, Q$使用不同的权重获得？</p><h2 id="5-1-我们为什么需要V？"><a href="#5-1-我们为什么需要V？" class="headerlink" title="5.1 我们为什么需要V？"></a>5.1 我们为什么需要V？</h2><p>注意力权重矩阵可以表示序列中任意两个元素的相似性，但是不能用来表示原始的序列，因为它缺少了词向量。注意力的作用是给原始序列中的不同位置上的元素以不同的权重，这样可以更好的获取到这个句子中哪一部分重要哪一部分不那么重要，或者说对于句子中的某个词来说，哪个词对它更有依赖关系，哪些词跟它关系没那么密切。所以说，注意力有两个重要的组成部分，一个是注意力权重，也就是词与词之间的相似性，另一个就是原始的句子序列。从模型结构就可以看出来，$K, Q$是用来计算相似性的，那么$V$其实就是用来表征句子序列特征的。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/1566186141115.png" alt></p><p>我们可以认为注意力权重矩阵是一个<em>过滤矩阵</em> ，把更多的注意力给重要的词，给那些不那么重要的词以更少的注意力。</p><h2 id="5-2-为什么使用两个不同的权重获得K-Q"><a href="#5-2-为什么使用两个不同的权重获得K-Q" class="headerlink" title="5.2 为什么使用两个不同的权重获得K, Q?"></a>5.2 为什么使用两个不同的权重获得K, Q?</h2><p>另一个问题就是，我们为什么要用两个不同的矩阵来获得$K, Q$？换句话说，就是我们为什么要用两个不同的矩阵来计算注意力呢？</p><p>正如我们前面所说的，注意力实际上是在计算两个词的相似度，如果使用相同的矩阵的话那就相当于计算自己与自己的相似度了，最后我们会得到一个对称矩阵，这样最后的模型的泛化性会大打折扣。</p><h2 id="5-3-Transformer是如何实现All-You-Need的？"><a href="#5-3-Transformer是如何实现All-You-Need的？" class="headerlink" title="5.3 Transformer是如何实现All You Need的？"></a>5.3 Transformer是如何实现<em>All You Need</em>的？</h2><p>回顾一下前一篇文章，我们介绍了各种各样的注意力机制：</p><ul><li>用于<em>seq2seq</em>的注意力机制</li><li>用于多语义理解的多维注意力机制</li><li>用于文本分类和语法纠正的层级注意力机制</li><li>用于阅读理解的基于记忆力的注意力机制</li><li>用于语言模型的自注意力机制</li><li>用于排序的指针网络</li><li>其他基于特定任务的注意力机制等等</li></ul><p>而<em>Transformer</em>本身就是基于<em>encoder-decoder</em>的结构，由于才用了<em>Multi-Head</em>这种类似多维度注意力机制，所以也能在多维度理解语义，另外由于本身是完全基于注意力的网络所以类层级注意力和类指针网络的的特性应该是<em>Transformer</em>的内秉属性。最后重要的两点：自注意力和基于记忆力的注意力机制在<em>Transformer</em>中表现尤为明显。所以说<em>Transformer</em>可以说是注意力机制的集大成者。</p><h1 id="6-代码实现（Pytorch）"><a href="#6-代码实现（Pytorch）" class="headerlink" title="6. 代码实现（Pytorch）"></a>6. 代码实现（Pytorch）</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> math, copy, time</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. Base for this and many </span></span><br><span class="line"><span class="string">    other models.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line">        self.generator = generator</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Take in and process masked src and target sequences."</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br><span class="line">    </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Define standard linear + softmax generation step."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=<span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span><span class="params">(module, N)</span>:</span></span><br><span class="line">    <span class="string">"Produce N identical layers."</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Core encoder is a stack of N layers"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"Pass the input (and mask) through each layer in turn."</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Construct a layernorm module (See citation for details)."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">        <span class="string">"Apply residual connection to any sublayer with the same size."</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Encoder is made up of self-attn and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (left) for connections."</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Generic N layer decoder with masking."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (right) for connections."</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span><span class="params">(size)</span>:</span></span><br><span class="line">    <span class="string">"Mask out subsequent positions."</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">'uint8'</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=None)</span>:</span></span><br><span class="line">    <span class="string">"Compute 'Scaled Dot Product Attention'"</span></span><br><span class="line">    d_k = query.size(<span class="number">-1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) \</span><br><span class="line">             / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim = <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"Take in model size and number of heads."</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="string">"Implements Figure 2"</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) "Concat" using a view and apply a final linear. </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implements FFN equation."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement the PE function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"Helper: Construct a model from hyperparameters."</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), </span><br><span class="line">                             c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h1 id="7-参考资料"><a href="#7-参考资料" class="headerlink" title="7. 参考资料"></a>7. 参考资料</h1><ol><li><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention is all you need</a></li><li><a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">The Illustrated Transformer</a></li><li><a href="http://super1peng.xyz/2018/11/26/Attention-is-All-You-Need/?nsukey=vdt8WL9eYHSqq%2F005akltYu4igB%2BuTF%2B2KWPULUX1nn8k91eO9sr%2BChTyLJXQ37Au2eWcaYldRdlOfl8pIQyv9ppfGptFfwwtw0efEQJ33aGvuOKBMUFWJPNFIVeVRYNqnUtVUSrIjo7nWkkOMH%2B%2FXGP%2BMomk8lN%2BVxi2o6ynULt3bVzxCufGq1rAYv8Q52P3ZN6poCGdX3jQTGpaLAt3g%3D%3D" target="_blank" rel="noopener">细讲 | Attention Is All You Need</a></li><li><a href="https://spaces.ac.cn/archives/4765" target="_blank" rel="noopener">《Attention is All You Need》浅读（简介+代码）</a></li><li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">The Annotated Transformer</a></li><li><a href="https://arxiv.org/abs/1705.03122" target="_blank" rel="noopener">Convolutional Sequence to Sequence Learning</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Attention </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP中的注意力机制简介（一）</title>
      <link href="/2019/08/26/NLP%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%AE%80%E4%BB%8B%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2019/08/26/NLP%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%AE%80%E4%BB%8B%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>传统的注意力机制是与<em>encoder-decoder</em>架构相结合的，其中编码器和解码器都是<em>RNN</em>。首先是一段文本序列输入到编码器当中，然后将编码器的最后一个隐状态单元作为解码器的初始状态，然后一个接一个地产生目标序列，如下所示：</p><a id="more"></a><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/99e5fb3a-b8f1-4b14-8c27-c4cefef15695.gif" alt></p><p><em>encoder-decoder</em>结构在机器翻译任务中被广泛使用，尽管这种方法相较之前的统计翻译方法对翻译效果有很大的提升，但是这种基于<em>RNN</em>的结构也存在两个非常严重的问题：</p><ol><li><em>RNN</em>是可遗忘的（forgetful），这就意味着随着信息的传递，在经历了多个时间步长之后，旧的信息可能会丢失；（<strong>长程依赖问题</strong>）</li><li>在解码过程中没有利用词对齐信息，也就是说在产生目标序列的过程中每产生一个词是基于整个源序列的信息的，由于注意力分散，所以在产生目标序列的过程中也容易出错。（<strong>注意力分散问题</strong>）</li></ol><p>为了解决以上两个问题， <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener"><em>Bahdanau et al., 2014</em></a>和 <a href="https://arxiv.org/abs/1508.04025" target="_blank" rel="noopener"><em>Luong et al., 2015</em></a>提出了注意力机制。论文中仍然使用基于<em>RNN</em>的<em>encoder-decoder</em>结构，但是在解码过程中每个时间步长都会计算注意力得分，然后再生成该隐状态下对应的输出。下面我们以<em>seq2seq with attention</em>模型为例，介绍传统注意力机制过程：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/48d0da24-2b86-49e7-a43b-beaab016bfa3.gif" alt></p><p>加入了注意力机制的<em>encoder-decoder</em>模型与标准的<em>encoder-encoder</em>有两点区别：</p><ol><li>传入<em>decoder</em>的数据不再是单纯的<em>encoder</em>的最后一个隐状态，而是<em>encoder</em>的所有隐状态；</li><li><em>decoder</em>在产生输出之前还会有额外的计算，用于确定当前<em>decoder</em>的隐状态应该对应到哪个<em>encoder</em>的隐状态，这样相当于集中注意力来产生与之对应的输出，这也是注意力机制得名的由来。</li></ol><p>注意力得分的计算过程如下所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/d97693da-0368-48e5-aa1f-a0e8f9a24e32.gif" alt></p><ol><li>准备好输入到<em>decoder</em>的所有<em>encoder</em>隐状态$\mathbf{h}_1,\mathbf{h}_2,\mathbf{h}_3$；</li><li>对每一个隐状态给定一个得分；</li><li>对得分使用<em>softmax</em>进行归一化，即为注意力得分；</li><li>使用注意力得分和每个<em>decoder</em>隐状态相乘，得到隐状态在当前<em>decoder</em>下的重要性；</li><li>最后对加权后的<em>encoder</em>隐状态进行求和，求和后的隐状态即相当于原始<em>encoder-decoder</em>中的单一隐状态。</li></ol><p>以上步骤在<em>decoder</em>中的每一个时间步长上都进行一次计算，仍然以翻译模型为例，对整个过程进行简单的介绍：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/5fe59340-18a3-49b8-89a7-47fe74dd2648.gif" alt></p><ol><li>首先是<em>encoder</em>中对输入的句子序列进行编码，得到$h_1, h_2, h_3$三个隐状态待用；</li><li>第一个<em>decoder</em>单元接收到源序列中的终止符<em><end></end></em>开始进行解码，<em>decoder</em>单元首先初始化一个隐状态权重，此时与接收到的源序列终止符<em><end></end></em>相互作用，但并不产生输出，而是产生一个新的隐状态$\mathbf{h}_4$；</li><li>利用上述计算注意力得分的方法对$\mathbf{h}_1,\mathbf{h}_2,\mathbf{h}_3$，进行注意力计算得到$C_4$；</li><li>将$C_4$和$\mathbf{h}_4$进行拼接得到一个新的向量；</li><li>将新向量传入到一个全连接层，全连接层的输出则是第一个<em>decoder</em>的输出，即目标序列的第一个元素；</li><li>将上述$\mathbf{h}_4$隐状态作为第二个<em>decoder</em>单元的初始隐状态，第一个<em>decoder</em>的输出向量（即全连接层的输出）作为第二个<em>decoder</em>的输入，再次重复上面的步骤$3-5$。</li></ol><p>从上面的过程可以看出，注意力机制的核心点在于注意力权重的计算，下面呢我们对这个计算过程进行公式化描述：</p><script type="math/tex; mode=display">e_{ji} = a(\mathbf{h}_i^{in}, \mathbf{h}_j^{out})</script><script type="math/tex; mode=display">\alpha_{ji} = \frac{exp(e_{ji})}{\sum_iexp(e_{ji})}</script><script type="math/tex; mode=display">\mathbf{c}_j = \sum_i\alpha_{ji}\mathbf{h}_i^{in}</script><p>其中$\mathbf{h}_i^{in}$表示<em>decoder</em>单元的输入隐状态（即<em>encoder</em>第$i$个隐状态），$\mathbf{h}_j^{out}$表示当前<em>decoder</em>单元的输出，$e_{ji}$表示第$i$个<em>encoder</em>隐藏层对当前<em>decoder</em>单元输出的影响权重（$j$表示第$j$个<em>decoder</em>时间步长，为了方便这里指当前<em>step</em>）,此即为上述注意力全忠计算过程中的步骤$2$。公式$(2)$即使用<em>softmax</em>进行权重的归一化，$\alpha_{ji}$就是归一化后的注意力权重，即为注意力权重计算过程步骤$3$。公式$(3)$表示的就是注意力权重计算过程步骤$4-5$。</p><p>得到注意力权重以后，与当前的<em>decoder</em>隐状态$h_j$和输入$y_{j-1}$结合最终输出目标元素（<strong>注意：</strong>这里当前<em>decoder</em>的输入是上一个<em>decoder</em>的输出，可参考前面的<em>seq2seq</em>模型的整个过程的介绍。）</p><script type="math/tex; mode=display">\mathbf{y}_j = f_y(\mathbf{h}_j^{out}, \mathbf{y}_{j-1},\mathbf{c}_j)</script><script type="math/tex; mode=display">\mathbf{h}_{j+1}^{out} = f_h(\mathbf{h}_j^{out}, \mathbf{y}_j)</script><p>其中$f_y$和$f_h$表示<em>RNN</em>的输出层和隐状态层。</p><p>将上述计算过程对每一个<em>RNN</em>时间步长进行重复计算就能解决最开始我们提出的单纯基于<em>RNN</em>的<em>encoder-decoder</em>模型所带来的问题：</p><ol><li>对于长程依赖问题，由于计算注意力过程每次都会通过输入序列的隐状态来计算，因此$\mathbf{c}_j$不会受到源序列长度的影响；</li><li>对于注意力分散的问题，由于我们在每一次进行<em>decode</em>的时候都会进行注意力加权，注意力只会集中在对当前影响较大的部分序列上，而不是把注意力分散到整个序列中，因此有效的解决了注意力分散的问题。</li></ol><p>注意力机制自从被提出来以后，在机器翻译领域取得了非常打的成功，随后注意力机制被广泛应用于NLP各个领域中，并且为了解决不同领域中各种问题，注意力机制也出现了各种各样的变种，下图总结了最近几年关于注意力机制的一些比较重要的研究。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/1565940458240.png" alt></p><p>下面我们就介绍一些比较重要的注意力模型。</p><h1 id="2-注意力模型的基本形式"><a href="#2-注意力模型的基本形式" class="headerlink" title="2. 注意力模型的基本形式"></a>2. 注意力模型的基本形式</h1><p>前面我们介绍了注意力机制在神经机器翻译中的应用，为了更一般化、形式化注意力机制，首先定义$V=\{\mathbf{v}_i\} \in \mathbb{R}^{n\times d_v}$，其中$\mathbf{v}_i$表示序列元素对应的向量（这里实际上应该是经过编码后的隐状态向量即前文所说的$\mathbf{h}_i$），重写之前的注意力模型：</p><script type="math/tex; mode=display">e_i = a(\mathbf{v}_i, \mathbf{u})</script><script type="math/tex; mode=display">\alpha_i = \frac{exp(e_i)}{\sum_i exp(e_i)}</script><script type="math/tex; mode=display">c = \sum \alpha_i \mathbf{v}_i</script><p>其中$\mathbf{u} \in \mathbb{R}^{d_v}$表示序列$\{\mathbf{v}_i\}$对应的输出（<em>pattern vector</em>）， 而$e_i$则表示序列$\{\mathbf{v}_i\}$中第$i$个元素对输出的影响，多数情况下$d_u=d_v=d$，而$a(\cdot)$函数（又叫<em>Alignment score function</em>）通常有一下几种选择：</p><div class="table-container"><table><thead><tr><th style="text-align:center">Name</th><th style="text-align:center">Alignment score function $a(\cdot )$</th><th style="text-align:center">Notes</th><th style="text-align:center">Citation</th></tr></thead><tbody><tr><td style="text-align:center">Content-base</td><td style="text-align:center">$a(\mathbf{v}_i, \mathbf{u})=cosine([\mathbf{v}_i, \mathbf{u}])$</td><td style="text-align:center">—</td><td style="text-align:center"><a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener">Graves2014</a></td></tr><tr><td style="text-align:center">Additive</td><td style="text-align:center">$a(\mathbf{v}_i, \mathbf{u}) = \mathbf{w}_2^{T}tanh(W_1[\mathbf{v}_i;\mathbf{u}])$</td><td style="text-align:center">—</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau2015</a></td></tr><tr><td style="text-align:center">Location-base</td><td style="text-align:center">$\alpha_i=softmax(W\mathbf{u})$</td><td style="text-align:center">只依赖目标输出，直接计算注意力权重</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a></td></tr><tr><td style="text-align:center">General</td><td style="text-align:center">$ a(\mathbf{v}_i, \mathbf{u}) = \mathbf{u}^TW\mathbf{v}_i$</td><td style="text-align:center">—</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a></td></tr><tr><td style="text-align:center">Dot-product</td><td style="text-align:center">$a(\mathbf{v}_i, \mathbf{u}) = \mathbf{u}^T\mathbf{v}_i$</td><td style="text-align:center">—</td><td style="text-align:center"><a href="https://arxiv.org/pdf/1508.4025.pdf" target="_blank" rel="noopener">Luong2015</a></td></tr><tr><td style="text-align:center">Scaled Dot-product</td><td style="text-align:center">$a(\mathbf{v}_i, \mathbf{u}) = \frac{\mathbf{u}^T\mathbf{v}_i}{\sqrt{n}}$</td><td style="text-align:center">其中$n$是$\mathbf{v}_i$的向量维度</td><td style="text-align:center"><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener">Vaswani2017</a></td></tr></tbody></table></div><p>以<em>Content-base</em>为例，计算出$e_i$在进行归一化，实际上可以认为这是一个相似性计算（类似$cos(\cdot)$）的操作，也就是说这里是在计算$\mathbf{v}_i$与输出之间的相似性。</p><p>为了避免混淆，我们先解释一下本文所采用的符号：</p><ul><li>小写符号表示标量，如$e_i$;</li><li>小写加粗符号表示向量， 如$\mathbf{v}_i$;</li><li>大写符号表示矩阵， 如$W$；</li><li>大写加粗符号表示张量，如$\mathbf{W}$；</li><li>$W, b$默认为待学习的权重矩阵和偏置</li></ul><h1 id="3-注意力机制的变种"><a href="#3-注意力机制的变种" class="headerlink" title="3. 注意力机制的变种"></a>3. 注意力机制的变种</h1><p>前面我们讨论了注意力机制的基本形式，由于其简单且有效使得注意力机制在NLP领域被广泛使用，但是通常在一些复杂性况下，这种简单形式的注意力机制仍然不够强大。随着注意力机制在不同场景下的应用，各种各样与之相关的注意力机制被提出来，总结起来可以概括为：<strong><em>基础注意力（Basic Attention）</em></strong>，<strong><em>多维注意力（Multi-dimensional Attention）</em></strong>，<strong><em>层级注意力（Hierarchical Attention）</em></strong>，<strong><em>自注意力（Self-Attention）</em></strong>，<strong><em>基于记忆的注意力（Memory-based Attention）</em></strong>，<strong><em>指针网络（Pointer Network）</em></strong>，<strong><em>特定任务下的注意力（Task-specific Attention）</em></strong>。</p><p>注意我们是根据不同的NLP任务应用场景对注意力机制进行分类的，在一些论文或者博客中通常见到的注意力模型分类将注意力分成<em>soft attention, hard attention, global attention, local attention, self-attention</em>等，与本文的分法不太相同，这是因为分类的依据不同。这里对这种分类方法的注意力机制做简单的介绍：</p><p><em>soft/hard attention</em>的区别类似于<em>word embedding</em>和<em>one-hot</em>的区别。<em>soft attention</em>的优势在于注意力平滑可微分，也就是说可以利用梯度下降进行权重更新，缺点就是如果输入较大（输入序列过长）比较消耗计算资源。<em>hard attention</em>的优势就是计算资源消耗少，但缺点就是不能微分，在进行权重更新的时候需要比较复杂的技术来处理这个问题。</p><p><em>global attention</em> 类似于<em>soft attention</em>将整个序列的信息融合进一个向量中，<em>local attention</em>类似于<em>hard attention</em>，它是在一个选定的窗口内进行<em>soft attention</em>，而在窗口之外仍然是<em>hard attention</em>。在特定任务下的注意力机制中我们会介绍一个<em>local attention</em>在机器翻译中的应用。</p><p>表1总结了不同注意力模型各自的特点：</p><div class="table-container"><table><thead><tr><th style="text-align:center">注意力机制类型</th><th style="text-align:center">特点</th></tr></thead><tbody><tr><td style="text-align:center">Basic Attention</td><td style="text-align:center">从一个序列中抽取出重要的元素</td></tr><tr><td style="text-align:center">Multi-dimensional Attention</td><td style="text-align:center">获得元素之间的多种操作类型</td></tr><tr><td style="text-align:center">Hierarchical Attention</td><td style="text-align:center">抽取全局和局部的重要信息</td></tr><tr><td style="text-align:center">Self-Attention</td><td style="text-align:center">抽取序列中隐含的上下文信息</td></tr><tr><td style="text-align:center">Memory-based Attention</td><td style="text-align:center">获取NLP任务中的隐藏的依赖关系</td></tr><tr><td style="text-align:center">Pointer Network</td><td style="text-align:center">对输入序列进行排序</td></tr><tr><td style="text-align:center">Task-specific Attention</td><td style="text-align:center">获取特定任务中的重要信息</td></tr></tbody></table></div><h2 id="3-1-多维注意力机制"><a href="#3-1-多维注意力机制" class="headerlink" title="3.1 多维注意力机制"></a>3.1 多维注意力机制</h2><p>前面我们介绍的基础注意力模型可以认为是一维的注意力机制（<em>1D-attention</em>），因为对于序列$V=\{\mathbf{v}_i\}$中的每一个元素计算出来的注意力权重$\alpha_i$都是一个标量，即$V=\{\mathbf{v}_i\}$对应的注意力权重为$\mathbf{\alpha}=\{\alpha_i\} \in \mathbb{R}^n$。但是在需要提取多种信息表示的情况下<em>1D-attention</em>就显得无能为力了，比如：</p><blockquote><p>Fish Burger is the best dish it tastes fresh.</p></blockquote><p>这样一句很简单的话，我们可以有不同的理解方式：</p><ol><li>晚餐之中哪道菜是最好的？</li><li>晚餐中<em>Fish Burger</em>这道菜和其他菜比起来怎么样？</li></ol><p>对于第一个理解方式，那么注意力应该在<em>Fish Burger</em>；而对于第二种理解方式，注意力应该在<em>the best</em>上面。也就是说针对不同语境（<em>representation space</em>），同一句话的注意力重点也是不同的。因此，我们需要多个注意力来处理这类情况，即所谓的<em>Multi-dimensional attention</em>。</p><p>最简单的方式就是多个<em>1D-attention</em>堆积成一个<em>Multi-dimensional attention</em>。例如<a href="http://www.aaai.org/Conferences/AAAI/2017/PreliminaryPapers/15-Wang-W-14441.pdf" target="_blank" rel="noopener">Wang et al., 2017</a>提出的<em>2D-attention</em>：给定一个输入序列$V=\{\mathbf{v}_i\}$，两个<em>representation space</em> $U = \{\mathbf{u}^a, \mathbf{u}^p\}$，权重张量$\mathbf{W}=\{W_a, W_p\}$，注意力权重的计算变为：</p><script type="math/tex; mode=display">\mathbf{e}_i = tanh([\mathbf{v}_i^TW_a\mathbf{u}^a:\mathbf{v}_i^TW_p\mathbf{u}^p])</script><p>其中$[:]$表示两个向量的拼接，$E = \{\mathbf{e}_i\}$， 经过<em>softmax</em>归一化之后得到注意力矩阵$\Lambda = \{\alpha_i\}$。结果如图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/1565763505669.png" alt></p><p><em>Aspect attention</em>和<em>Opinion attention</em>表示两个<em>representation space</em>，<em>Aspect attention</em>能把注意力集中在<em>Fish Burger</em>上，而<em>Opinion attention</em>能把注意力集中在<em>best</em>上，这样就解决了上面提到的问题。</p><p>然而这种多维注意力机制存在一个问题：</p><blockquote><p>如果每一维注意力都有相似的注意力，那么最后得到的信息会存在信息冗余的问题。</p></blockquote><p>针对这个问题<a href="https://openreview.net/pdf?id=BJC_jUqxe" target="_blank" rel="noopener">Lin et al. 2017</a>提出一种惩罚机制，即</p><script type="math/tex; mode=display">P = \|(\Lambda \Lambda^T-I)\|_F^2</script><p>其中$\Lambda$ 表示注意力矩阵，$I$表示单位矩阵，$|\cdot|_F$表示<em>Frobenius</em>范数。类似于添加一个$L_2$正则项，这个惩罚项会乘以一个系数（超参数）然后和原始的损失函数一同计算最小化。</p><p>下面我们讨论一下这个惩罚项的有效性（原始论文中作者最先考虑的是<em>KL</em>散度，但是在实际测试过程中发现效果并不理想，所以才考虑添加这样一个惩罚项）。</p><p>考虑 $\Lambda = \{\alpha_a, \alpha_p\}$，由于$\alpha_a$和$\alpha_p$都经过了<em>softmax</em>，因此$\alpha_a$和$\alpha_p$可以被认为是离散概率分布中的概率质量（<em>probability mass</em>），对于$\Lambda \Lambda^T$矩阵中任意非对角的元素$\alpha_{ij}(i\neq j)$，对应于两个分布的元素级乘积求和：</p><script type="math/tex; mode=display">0 \lt \alpha_{ij}=\sum_{k=1}^n\alpha_k^i\alpha_k^j \lt 1</script><p>其中$\alpha_k^i$和$\alpha_k^j$分别对应$\mathbf{\alpha}_a$和$\mathbf{\alpha}_p$的第$k$个元素。最极端情况下假设$\mathbf{\alpha}_a$和$\mathbf{\alpha}_p$没有任何交叉点，即$\alpha_{ij}=0$，否则$\alpha_{ij} \gt 0$；另一个极端情况是假设$\mathbf{\alpha}_a$和$\mathbf{\alpha}_p$完全相等，即两个注意力全部在同一个词上面，此时$\alpha_{ij}=1$。我们从$\Lambda \Lambda^T$中减去一个单位矩阵，相当于强迫$\Lambda \Lambda^T$的对角元素都约等于1，当$\alpha_{ij}=1(i=j)$时，$\alpha_{ij}=0(i \neq j) $，而我们的目的就是使每个维度上的注意力都不相同，所以我们需要$\alpha_{ij}=0(i \neq j)$，满足第一种极端假设。</p><p>实际上我们最小化这个惩罚项的时候是在将注意力矩阵$\Lambda $进行正交化，每一行都与其他行正交，即每一行都与其他行的注意力不同。</p><h2 id="3-2-层级注意力机制"><a href="#3-2-层级注意力机制" class="headerlink" title="3.2 层级注意力机制"></a>3.2 层级注意力机制</h2><h3 id="3-2-1-自下而上的层级注意力机制（Bottom-up-Hierarchical-Attention）"><a href="#3-2-1-自下而上的层级注意力机制（Bottom-up-Hierarchical-Attention）" class="headerlink" title="3.2.1 自下而上的层级注意力机制（Bottom-up Hierarchical Attention）"></a>3.2.1 自下而上的层级注意力机制（Bottom-up Hierarchical Attention）</h3><p>下面我们考虑文本分类任务。文本分类可以说是NLP中最基础的任务之一了。文本具有典型的层级结构：</p><blockquote><p>character =&gt; word =&gt; sentence =&gt; document</p></blockquote><p><em>document</em>的类别取决于构成它的<em>sentence</em>，而<em>sentence</em>的意思又取决于<em>word</em>以及词序，也就是说要想对文本很好的进行分类，需要抓住其中的<strong>关键词</strong>和<strong>关键句子</strong>（语序可以通过<em>RNN</em>等网络结构来解决，这里不讨论语序的问题）。</p><p>针对这个问题<a href="https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf" target="_blank" rel="noopener">Yang et al., 2016</a>提出了<em>Hierarchical Attention Networks (HAN)</em>，也就是层级注意力神经网络。<em>HAN</em>使用双向<em>GRU</em>网络对序列进行编码。给定一个序列$V_j=\{\mathbf{v}_i\}$表示文档中第$j$个句子，$\mathbf{v}_i$则表示句子中第$i$个词。给定一个序列集$\mathbf{D} = \{V_i\}$表示一篇文档。</p><ul><li><strong>Word Encoder</strong></li></ul><script type="math/tex; mode=display">\mathbf{h}_i^{word} = BiGRU(\mathbf{v}_i)</script><script type="math/tex; mode=display">\mathbf{u}_i^{word} = tanh(W_w\mathbf{h}_i^{word}+b_w)</script><p>这里<em>encoder</em>编码后的隐状态又经过了一层全连接层进行编码。</p><ul><li><strong>Word Attention</strong></li></ul><script type="math/tex; mode=display">e_i^{word} = a(\mathbf{u}_i^{word}, \mathbf{u}_w) = \mathbf{u}_w{^T}\mathbf{u}_i{^{word}}</script><script type="math/tex; mode=display">\alpha_i^{word} = softmax(e_i^{word})</script><script type="math/tex; mode=display">\mathbf{s}_i = \sum_t \alpha_i^{word}\mathbf{h}_i^{word}</script><p>其中$\mathbf{s}_i$表示$V_i$经过编码后的句向量，$t$表示句子长度，$\mathbf{u}_w$是随机初始化的可学习参数。</p><ul><li><strong>Sentence Encoder</strong></li></ul><script type="math/tex; mode=display">\mathbf{h}_i^{sent} = BiGRU(\mathbf{s}_i)</script><script type="math/tex; mode=display">\mathbf{u}_i^{sent} = tanh(W_s\mathbf{h}_i^{sent}+b_s)</script><ul><li><strong>Sentence Attention</strong></li></ul><script type="math/tex; mode=display">e_i^{sent} = a(\mathbf{u}_i^{sent}, \mathbf{u}_s) = \mathbf{u}_s{^T}\mathbf{u}_s^{sent}</script><script type="math/tex; mode=display">\alpha_i^{sent} = softmax(e_i^{sent})</script><script type="math/tex; mode=display">\mathbf{d} = \sum_i \alpha_i^{sent}\mathbf{h}_i^{sent}</script><p>其中$\mathbf{d}$就是<em>document</em>对应的向量。</p><h3 id="3-2-2-自上而下的层级注意力机制（Top-down-Hierarchical-Attention）"><a href="#3-2-2-自上而下的层级注意力机制（Top-down-Hierarchical-Attention）" class="headerlink" title="3.2.2 自上而下的层级注意力机制（Top-down Hierarchical Attention）"></a>3.2.2 自上而下的层级注意力机制（Top-down Hierarchical Attention）</h3><p><em>HAN</em>是一种<em>bottom-up</em>类型的层级注意力机制，除此之外还有<em>top-down</em>类型的层级注意力机制，比如<a href="https://pdfs.semanticscholar.org/6ed8/0c434270bfb09c37a0ac87e0b3f554becbe3.pdf" target="_blank" rel="noopener">Ji et al., 2017</a>提出使用层级注意力机制进行语法错误改正（<em>grammatical error correction</em>）。在这篇文章中，在编码阶段使用<em>GRU</em>进行编码，但是为了解决<em>OOV (out-of-vocabulary)</em>问题，作者提出了所谓的<em>Hybrid encoder</em>，即对于不在词表中的词（<em>UNK</em>），使用字母编码向量代替<em>UNK</em>，这个并非本文讨论的重点，因此下面我们集中讨论这篇文章中<em>top-down</em>层级注意力机制：<em>word-level attention</em>和<em>character-level attention</em>。</p><p>语法错误改正的特点正好和文本分类相反，文本分类是需要从词到句再到文档一层一层抓住文本所描述的语义，而语法错误改正是需要先理解文本的语义，然后根据语义对其中的词进行修改。词的修改也并不是修改全词，有可能只是其中几个字母的错误，因此需要用<em>top-down</em>层级注意力机制。</p><p>目标词的确定是通过以下两种方式：</p><ol><li>如果目标词在词表内，则使用<em>word-level attention</em></li><li>如果目标词不在词表内，则使用<em>character-level attention</em></li></ol><p>具体来说就是：</p><ol><li><p>以词为基础利用<em>GRU</em>进行句子编码（此处的编码指的是<em>Hybrid encoder</em>），得到隐状态$\mathbf{h}_1, \mathbf{h}_2, …, \mathbf{h}_T$；</p></li><li><p>对于源词和目标词都在词表中的情况，直接使用<em>word-level</em>注意力权重进行解码，生成目标序列；</p></li><li><p>对于目标词不在词表的情况（模型预测出来的目标词为<em>UNK</em>），$p(UNK|\mathbf{x}_c)\cdot p(char_seq|\mathbf{x}_c)$；</p></li><li><p>其中<em>character-sequence</em>的生成方法如下：</p><p>初始化<em>character-decoder</em>隐状态：$\mathbf{d}_s^\sim = Relu(W[\mathbf{c}_s;\mathbf{d}_s])$，其中$\mathbf{c}_s=\sum_T\alpha_j\mathbf{h}_j$，$\mathbf{d_s}$表示当前<em>word-decoder</em>的隐状态；</p><ul><li>若目标词对应的源词在词表中，直接使用<em>character-decoder</em>生成一个词；</li><li>若目标词对应的源词也不在此表中（即源词和目标词都为<em>UNK</em>），首先使用<em>character-decoder</em>生成一个词，然后利用编码时使用的<em>character-encoder</em>得到的<em>UNK</em>源词编码与生成的词进行加权求和即得到目标词</li></ul><script type="math/tex; mode=display">\mathbf{d}_n^{cc} = Relu(W_c[\mathbf{c}_n^c;\mathbf{d}_n^c])</script></li></ol><p>另外，这种自上而下的注意力机制还被应用于为唱片挑选合适的海报<a href="https://arxiv.org/abs/1708.02977" target="_blank" rel="noopener">Yu et al., 2017</a>以及文本生成，这里就不详细介绍了。总的来说想这种需要先从全局信息来确定局部信息的场景都可以使用这种从上而下的层级注意力机制。</p><h2 id="3-3-自注意力机制"><a href="#3-3-自注意力机制" class="headerlink" title="3.3 自注意力机制"></a>3.3 自注意力机制</h2><p>回顾一下基础的注意力机制：给定一个序列$V=\{\mathbf{v}_i\}$和模式向量$\mathbf{u}$，对于每一个$\mathbf{v}_i$我们都可以计算一个注意力权重$\alpha_i=softmax(a(\mathbf{v}_i, \mathbf{u}))$。实际上这种注意力机制是一种外部注意力，正如前面我们讨论的，这种注意力实际上有点类似于计算$\mathbf{v}_i$与外部模式向量$ \mathbf{u}$的匹配度（或者相似度），注意力权重依赖于外部模式。</p><p>所谓自注意力机制指的是$\mathbf{u}$是输入序列自身的一部分，即：</p><script type="math/tex; mode=display">e_{ij} = a(\mathbf{v}_i, \mathbf{v}_j)</script><script type="math/tex; mode=display">\alpha_{ij} = softmax(e_{ij})</script><p>通常情况下，</p><script type="math/tex; mode=display">\alpha_{ij} = softmax(tanh(\mathbf{w}^T[\mathbf{v}_i;\mathbf{v}_j]+b))</script><p>那么自注意力机制通常在什么场景下使用呢？</p><ul><li><p>获取序列内部元素之间的相互依赖关系。比如”<em>Volleyball match is in progress between ladies</em>“，这句话中其他词都是在围绕<em>match</em>进行描述的。从上面的形式描述也可以看出，自注意力机制是在计算序列内部元素之间的匹配度，因此当我们需要获取序列内部的依赖关系的时候自注意力机制就可发挥作用。</p></li><li><p>通过语义获取词义，类似于词义消歧。举个例子：</p><blockquote><p>I arrived at the bank after crossing the street.</p><p>I arrived at the bank after crossing the river.</p></blockquote><p>其中<em>bank</em>再第一句中最可能的意思应该是<em>银行</em>， 而第二句中的<em>bank</em>最可能的语义应该是<em>岸边</em>。因为自注意力能使第一句中的<em>bank</em>注意到<em>street</em>，而第二句中的<em>bank</em>注意到<em>river</em>。</p></li></ul><p>自注意力机制最成功的的案例应该属于最近大火的<em>Transformer</em>了，下面我们会对<em>Transformer</em>进行单独的介绍，这里就先跳过。</p><h2 id="3-4-基于记忆力的注意力机制"><a href="#3-4-基于记忆力的注意力机制" class="headerlink" title="3.4 基于记忆力的注意力机制"></a>3.4 基于记忆力的注意力机制</h2><p>为了介绍基于记忆力的注意力机制，我们重新构建旧的注意力机制。</p><p>假设有一组键值对$V =\{(\mathbf{k}_i, \mathbf{v}_i)\}$和一个检索向量（<em>query vector</em>）$\mathbf{q}$，重新定义注意力权重计算过程：</p><script type="math/tex; mode=display">e_i = a(\mathbf{k}_i, \mathbf{q})</script><script type="math/tex; mode=display">\alpha_i = softmax(e_i)</script><script type="math/tex; mode=display">\mathbf{c} = \sum_i \alpha_i \mathbf{v}_i</script><p>这种注意力机制有点类似信息检索，给定一些文本和一个检索词，根据这些文本与检索词之间的匹配度（注意力权重$\{\alpha_i\}$）提取文本中的信息。当然信息检索是根据文本与检索词之间的匹配度进行文本排序，这里不做讨论。需要注意的是，如果所有的$\mathbf{k}_i=\mathbf{v}_i$的话，基于记忆力的注意力机制又转化成基础的注意力机制。更多关于$\mathbf{k}$和$\mathbf{v}$的内容我们会在<em>Transformer</em>专题里面进行讨论。</p><p>下面我们从两个方面详细讨论基于记忆力的注意力机制相对于基础记忆力机制的优势：</p><ul><li><p>可复用性（<em>Reusability</em>）</p><p>对于问答系统来说，一个基本的问题是答案与问题并不是直接相关的，举个例子：</p><blockquote><ol><li>Sam walks into the kitchen.</li><li>Sam picks up an apple.</li><li>Sam walks into bedroom.</li><li>Sam drops the apple. </li></ol><p>Q: Where is the apple?</p></blockquote><p>上面四个句子与<em>apple</em>匹配度比较高的应该是第2和第4句话，在用基础注意力机制的时候，注意力权重会集中在这两句话上，但是我们知道正确答案却在第3句话。因此，基础注意力机制没有办法解决这种间接相关（或者说需要进行一定程度上的推理）的任务。</p><p>但是如果我们通过迭代更新记忆信息来模拟时序推理的话，这个问题就可以得到解决。<a href="https://arxiv.org/pdf/1503.08895.pdf" target="_blank" rel="noopener">Sukhbaatar et al., 2015</a>通过结合记忆神经网络和注意力机制，为这类问题提出了一种解决方案。这里我们只讨论<em>Sukhbaatar et al., 2015</em>的论文中的注意力机制和记忆力神经网络结合的部分，其他的细节问题不做讨论，过程如下：</p><ol><li>初始化$\mathbf{q}_t=\phi_q(question)$；</li><li>计算句向量$\mathbf{k}_i=\phi_k(x_i )$；</li><li>$e_i=\mathbf{q}_t^T\mathbf{k}_i$；</li><li>$\alpha_i=softmax(e_i)$；</li><li>$\mathbf{v}_i=\phi_v(x_i)$；</li><li>$\mathbf{c}_i=\sum_i \alpha_i \mathbf{v}_i$；</li><li>更新问题向量$\mathbf{q}_{t+1}=\mathbf{q}_{t}+\mathbf{c}_i$；</li><li>重复$2-7$</li></ol><p>我们对上面的过程做一个解释：</p><p>第一步：初始化问题，将问题转化成向量，需要注意这里是一个一维向量，不是多个词向量序列构成的句矩阵，包括下面$x_i$指的是一个句子而不是词。$\phi_q, \phi_k, \phi_v$都是将句子转化成一维的向量，具体怎么转化的在文章中有具体的介绍，这里不是我们讨论的重点。</p><p>第二步：将第$i$个句子转化成句向量。</p><p>第三至第六步：就是标准的注意力加权求和。</p><p>第七步：更新问题向量。</p><p>整个推理过程如下：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/1565860574005.png" alt></p><p>另外有很多人在这方面也做了很多工作，比如<em><a href="https://arxiv.org/pdf/1506.07285.pdf" target="_blank" rel="noopener">Kumar et al. 2016</a>，<a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener">Graves et al., 2014</a></em>这里不详细介绍了。下面我们继续介绍第二点优势。</p></li><li><p>灵活性（<em>Flexibility</em>）</p><p>由于<em>键</em> 和<em>值</em> 有不同的向量表示，我们就可以自由的设计相应的嵌入层更好的获得相应的信息，比如分别设计针对问题和答案的向量表示，比如<a href>Miller et al., 2016</a>提出的<em>key-value memory network</em>：设计一个窗口结构，窗口中心的词为值向量，而窗口周围的词为键向量，用上面的例子则<em>apple</em>和<em>bedroom</em>为值向量，他们周围的词为键向量。</p><p>如果我们希望使用基础的注意力机制只需要令$\mathbf{k}=\mathbf{v}$即可，这也是其灵活性的表现。另外还有一些其他的键值向量的设计方式，这里我们不再过多介绍了。</p></li></ul><h2 id="3-5-指针网络"><a href="#3-5-指针网络" class="headerlink" title="3.5 指针网络"></a>3.5 指针网络</h2><p>对于一些排序的问题（<em>sorting</em>）或者“巡回推销员”（<em>travelling salesman</em>）问题输出并不是确定的，而是随着输入的变化输出也随之改变。面对这类问题前面提到的各种注意力模型都无能为力了，因此 <a href="https://arxiv.org/abs/1506.03134" target="_blank" rel="noopener">Vinyals, et al. 2015</a>提出了<strong>指针网络</strong>的方法。不同于其他注意力模型将上下文信息糅合进一个向量里面，指针网络是在解码阶段通过注意力机制一个一个从输入序列中选择元素进行输出。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/1565925694004.png" alt></p><p>给定一个输入序列$V=\{\mathbf{v}_i\}$，指针网络输出一个序列$\mathbf{c}=\{c_i\}$，其中$c_i$是$V$中元素的索引。</p><script type="math/tex; mode=display">\begin{align}y_i &= p(c_i|c_1,...c_{i-1}, V) \nonumber\\     &= \sigma(a(\mathbf{v}_i, \mathbf{u})) \nonumber\\    &= \sigma(\mathbf{w}_2^{T}tanh(W_1[\mathbf{v}_i;\mathbf{u}])) \nonumber\end{align}</script><h2 id="3-6-特定任务下的注意力机制"><a href="#3-6-特定任务下的注意力机制" class="headerlink" title="3.6 特定任务下的注意力机制"></a>3.6 特定任务下的注意力机制</h2><ul><li><strong>文本摘要</strong></li></ul><p><a href="https://www.aclweb.org/anthology/P17-1108" target="_blank" rel="noopener">Tan et al., 2017</a>在文本摘要任务中提出了一种类似于<em>PageRank</em>的<em>graph-based attention</em>。给定一篇文档$V=\{\mathbf{v}_i\}$，其中$\mathbf{v}_i$表示句子向量， 假设注意力分布为$\mathbf{\alpha}=\{\alpha_i\}$，则$\mathbf{\alpha}$满足一下条件：</p><script type="math/tex; mode=display">\mathbf{\alpha}(t+1)=\lambda WD^{-1}\mathbf{\alpha}(t)+(1-\lambda)\mathbf{y}</script><p>其中$W$表示方阵$W(i,j)=\mathbf{v}_i^TM\mathbf{v}_j$，其中M是一个待学习的权重矩阵。$D$是一个对角矩阵，对角线上的元素$d_i=W(i,i)$，这是为了保证$WD^{-1}=1$，$\lambda$是一个阻尼系数，$\mathbf{y} \in \mathbb{R}^n$，并且$\mathbf{y} $中的所有元素都是$1/n$。</p><p>我们来考虑下这个注意力的物理意义是什么？忽略阻尼系数$\lambda$，我们来看下$WD^{-1}$表示什么：</p><script type="math/tex; mode=display">WD^{-1} =(\mathbf{v}_i^TM\mathbf{v}_j)D^{-1}</script><p>其中的$W$与之前的$e_i$何其相似（公式$(9) $），而$D^{-1}$的存在是为了保证这一项能各元素相加为$1$，这又与$softmax(e_i)$何其相似。也就是说$WD^{-1}$我们也可以看成是一个注意力权重，而这个注意力权重是把注意力放在另一个注意力上。从公式$(32)$我们可以看出$\alpha_i(t)$注意力越大$\alpha_i(t+1)$也就会越大。也就是说，如果一个句子与其他重要的句子有更多的关联性的话，那么这个句子也会更重要。这个思想如同我们前面提到的和<em>PageRank</em>非常相似。</p><ul><li><strong>机器翻译</strong></li></ul><p><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong et al., 2015</a>为了解决机器翻译过程中，对长序列的所有元素都计算注意力太过消耗资源，因此提出一个叫做局部注意力机制（<em>local attention</em>）的方法。</p><p>所谓局部注意力机制其实很简单，就是从整个序列中设定一个窗口$D$，选取$[p_t-D, p_t+D]$范围内的序列计算注意力。其中$p_t$有两种确定方法：</p><ol><li>假设源序列和目标序列是一一对应的，则$p_t=t$，即以步长为1沿着序列依次向后滑动窗口取子序列；</li><li>假设源序列和目标序列并不是一一对应的，则$p_t=S\cdot sigmoid(\mathbf{v}_p^Ttanh(W_p\mathbf{h}_t))$，其中$S$表示序列的源总长度。然后计算注意力权重，最后视同高斯平滑得到输出。</li></ol><ul><li><strong>其他</strong></li></ul><p>除了以上介绍的各种各样的注意力机制以外，<a href="https://arxiv.org/pdf/1702.00887.pdf" target="_blank" rel="noopener">Kim et al., 2017</a>还提出一种叫做结构化注意力机制的方法。这种注意力机制可用于处理序列选择性问题，例如从一个序列中选出一个子序列，或者从语法树中选取一个子树。基本方法是将注意力模型看成是条件随机场<em>（CRF）</em>，引入一个隐变量$Z={\mathbf{z}_i}$，然后对序列进行编码-解码。这里不再详细介绍这种方法了，如有兴趣可以看论文原文。</p><h1 id="4-小结"><a href="#4-小结" class="headerlink" title="4. 小结"></a>4. 小结</h1><p>从上面的介绍我们可以看到，注意力机制的强大之处几乎在NLP领域的各个任务中都有相当出色的发挥：</p><ul><li>机器翻译，作为最先引入注意力机制的任务，注意力机制使得机器翻译的水平比之前有一个大的飞跃；</li><li>多模式理解，使用多维注意力机制使我们能从不同角度理解自然语言</li><li>文本分类，使用自下而上的层级注意力机制提升文本分类的能力</li><li>语法纠正，自上而下的注意力机制帮助我们完成这类人物</li><li>语言模型，自注意力机制，尤其是<em>Transformer</em>的横空出世，使得语言模型的水准达到了前所未有的高度</li><li>阅读理解，基于记忆力的注意力机制使得注意力机制具有了推理能力，帮助我们完成阅读理解的任务</li><li>文本摘要，在基于图的注意力机制下，完成了类似<em>PageRank</em>的抽取式文本摘要，虽然目前没有看到其在信息检索方面的研究，但是我相信把它应用于信息检索领域也会有不俗的表现</li><li>序列排序，指针网络的出现补足了注意力机制在排序方面的应用空白</li><li>另外对于分词，分块，语法解析等基础任务同样有相关的注意力机制的研究成果</li></ul><p>从上面的总结我们不难得出一个结论：<strong><em>Attention is all you need</em></strong> 此言非虚！</p><h1 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5. 参考资料"></a>5. 参考资料</h1><ol><li><a href="http://www.researchgate.net/publication/328953535_An_Introductory_Survey_on_Attention_Mechanisms_in_NLP_Problems" target="_blank" rel="noopener">An Introductory Survey on Attention Mechanisms in NLP Problems, </a><em>Dichao Hu</em>, 2018</li><li><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate, </a><em>Bahdanau et al.</em>, 2014</li><li><a href="https://arxiv.org/abs/1508.04025" target="_blank" rel="noopener">Effective Approaches to Attention-based Neural Machine Translation, </a><em>Luong et al.,</em> 2015</li><li><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank" rel="noopener">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention), </a><em>Jay Alammar</em></li><li><a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener">Neural Turing Machines, </a><em>Graves et al.,</em> 2014</li><li><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE, </a><em>Bahdanau et al.,</em> 2015</li><li><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener">Attention is all you need, </a><em>Vaswani et al.</em>, 2017</li><li><a href="http://www.aaai.org/Conferences/AAAI/2017/PreliminaryPapers/15-Wang-W-14441.pdf" target="_blank" rel="noopener">Coupled multi-layer attentions for co-extraction of aspect and opinion terms, </a><em>Wang et al.,</em> 2017</li><li><a href="https://openreview.net/pdf?id=BJC_jUqxe" target="_blank" rel="noopener">A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING,</a> <em>Lin et al.,</em> 2017</li><li><a href="https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf" target="_blank" rel="noopener">Hierarchical attention networks for document classification, </a><em>Yang et al.,</em> 2016</li><li><a href="https://pdfs.semanticscholar.org/6ed8/0c434270bfb09c37a0ac87e0b3f554becbe3.pdf" target="_blank" rel="noopener"> A nested attention neural hybrid model for grammatical error correction, </a><em>Ji et al.,</em> 2017</li><li><a href="https://arxiv.org/abs/1708.02977" target="_blank" rel="noopener"> Hierarchicallyattentive rnn for album summarization and storytelling, </a><em>Yu et al.,</em> 2017</li><li><a href="https://arxiv.org/pdf/1503.08895.pdf" target="_blank" rel="noopener">End-toend memory networks, </a><em>Sukhbaatar et al.,</em> 2015</li><li><a href="https://arxiv.org/pdf/1506.07285.pdf" target="_blank" rel="noopener">Ask me anything: Dynamic memory networks for natural language processing, </a><em>Kumar et al.,</em> 2016</li><li><a href="https://arxiv.org/abs/1606.03126" target="_blank" rel="noopener"> Key-value memory networks for directly reading documents, </a><em>Miller et al.,</em> 2016</li><li><a href="https://arxiv.org/abs/1506.03134" target="_blank" rel="noopener">Pointer Networks, </a><em>Vinyals, et al.,</em> 2015</li><li><a href="https://www.aclweb.org/anthology/P17-1108" target="_blank" rel="noopener"> Abstractive document summarization with a graph-based attentional neural model, </a><em>Tan et al.,</em> 2017</li><li><a href="https://arxiv.org/pdf/1702.00887.pdf" target="_blank" rel="noopener"> Structured attention networks, </a><em>Kim et al.,</em> 2017</li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
