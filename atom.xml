<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Rogerspy&#39;s Home</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://rogerspy.gitee.io/"/>
  <updated>2022-01-12T13:27:13.064Z</updated>
  <id>https://rogerspy.gitee.io/</id>
  
  <author>
    <name>Rogerspy</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Dijkstra&#39;s Algorithm in 5 steps with Python</title>
    <link href="https://rogerspy.gitee.io/2022/01/11/dijkstra-algorithm-python/"/>
    <id>https://rogerspy.gitee.io/2022/01/11/dijkstra-algorithm-python/</id>
    <published>2022-01-11T08:01:34.000Z</published>
    <updated>2022-01-12T13:27:13.064Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><p>Dijkstra’s 是最广为人知的图算法之一，同时也是最难发音和拼写的图算法。Dijkstra’s 算法是最短路径算法，在它的基础上还衍生出很多其他变种。本文将介绍两种 Dijkstra’s 算法，并以邻接表为例用 python 实现。</p><a id="more"></a><p>Dijkstra’s 算法伪代码如下：</p><blockquote><ol><li>创建一个“距离”列表，元素个数等于图节点数。每个元素初始化无穷大；</li><li>将起始节点的“距离”设置为 0；</li><li>创建一个“访问”列表，同样将元素个数设定为图节点数。将每个元素设置成 Fasle，因为我们还没有开始访问节点；</li><li>遍历所有节点：<ul><li>再次遍历所有节点，然后从还没有访问的节点中挑选出距离最小的节点；</li><li>将节点设置成已访问；</li><li>将“距离”列表中的距离设置成相应的距离数值。</li></ul></li><li>原始的“距离”列表现在应该已经包含了到每个节点的最短路径，或者如果节点无法到达的话距离为无穷大。</li></ol></blockquote><h1 id="2-邻接表图"><a href="#2-邻接表图" class="headerlink" title="2. 邻接表图"></a>2. 邻接表图</h1><blockquote><p>假设你已经装了 <code>numpy</code>。</p></blockquote><p>首先创建一个有 5 个节点的邻接表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">graph = &#123;</span><br><span class="line">    <span class="number">0</span>: [(<span class="number">1</span>, <span class="number">1</span>)],</span><br><span class="line">    <span class="number">1</span>: [(<span class="number">0</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">3</span>)],</span><br><span class="line">    <span class="number">2</span>: [(<span class="number">1</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">1</span>), (<span class="number">4</span>, <span class="number">5</span>)],</span><br><span class="line">    <span class="number">3</span>: [(<span class="number">1</span>, <span class="number">3</span>), (<span class="number">2</span>, <span class="number">1</span>), (<span class="number">4</span>, <span class="number">1</span>)],</span><br><span class="line">    <span class="number">4</span>: [(<span class="number">2</span>, <span class="number">5</span>), (<span class="number">3</span>, <span class="number">1</span>)]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/pasted_image_0.png" alt></p><h1 id="3-用-python-实现原生-Dijkstra’s"><a href="#3-用-python-实现原生-Dijkstra’s" class="headerlink" title="3. 用 python 实现原生 Dijkstra’s"></a>3. 用 python 实现原生 Dijkstra’s</h1><p>首先先实现原生的 Dijkstra’s 算法，这种实现的算法复杂度是 $O(n^2)$。创建一个函数接收两个参数：邻接表和根节点。</p><p>首先创建一个距离列表，初始化为无穷大：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_dijkstras</span><span class="params">(graph, root)</span>:</span></span><br><span class="line">    n = len(graph)</span><br><span class="line">    <span class="comment"># 将距离列表中的所有元素初始化成无穷大</span></span><br><span class="line">    <span class="comment"># 这里无穷大使用的是 np.Inf，而不是设置成一个很大的数</span></span><br><span class="line">    <span class="comment"># 因为一个很大的数可能造成内存泄露</span></span><br><span class="line">    dist = [np.Inf <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br></pre></td></tr></table></figure><p>第二步，将根节点的距离设置成 0：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dist[root] = <span class="number">0</span></span><br></pre></td></tr></table></figure><p>第三步，创建一个“访问”列表，将所有元素初始化为 False</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">visited = [<span class="literal">False</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br></pre></td></tr></table></figure><p>第四步有三部分：</p><p>① 遍历所有节点然后挑选出距离最近的节点。如果遍历了所有的可用节点还没有找到最近的那个，那就跳出循环：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 遍历所有节点</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(n):</span><br><span class="line">    u = <span class="number">-1</span>  <span class="comment"># 初始节点设置成 -1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="comment"># 如果节点 i 还没有被访问，我们不需要对它进行处理</span></span><br><span class="line">        <span class="comment"># 或者如果它的距离小于 “start” 节点的时候</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> visited[i] <span class="keyword">and</span> (u == <span class="number">-1</span> <span class="keyword">or</span> dist[i] &lt; dist[u]):</span><br><span class="line">            u = i</span><br><span class="line">        <span class="comment"># 访问了所有节点或者该节点无法到达</span></span><br><span class="line">        <span class="keyword">if</span> dist[u] == np.Inf:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>② 将距离最近的节点添加到“访问”列表中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">visited[u] = <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>③ 将已访问的节点的距离设置成可用的最短距离：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> v, l <span class="keyword">in</span> graph(u):</span><br><span class="line">    <span class="keyword">if</span> dist[u] + <span class="number">1</span> &lt; dist[v]:</span><br><span class="line">        dist[v] = dist[u] + <span class="number">1</span></span><br></pre></td></tr></table></figure><p>最后，返回“距离”列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> dist</span><br></pre></td></tr></table></figure><p>完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_dijkstras</span><span class="params">(graph, root)</span>:</span></span><br><span class="line">    n = len(graph)</span><br><span class="line">    <span class="comment"># 将距离列表中的所有元素初始化成无穷大</span></span><br><span class="line">    <span class="comment"># 这里无穷大使用的是 np.Inf，而不是设置成一个很大的数</span></span><br><span class="line">    <span class="comment"># 因为一个很大的数可能造成内存泄露</span></span><br><span class="line">    dist = [np.Inf <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">    <span class="comment"># 将根节点的距离设置成 0</span></span><br><span class="line">    dist[root] = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 创建一个“访问”列表，将所有元素初始化为 False</span></span><br><span class="line">    visited = [<span class="literal">False</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">    <span class="comment"># 遍历所有节点</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(n):</span><br><span class="line">        u = <span class="number">-1</span>  <span class="comment"># 初始节点设置成 -1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="comment"># 如果节点 i 还没有被访问，我们不需要对它进行处理</span></span><br><span class="line">            <span class="comment"># 或者如果它的距离小于 “start” 节点的时候</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> visited[i] <span class="keyword">and</span> (u == <span class="number">-1</span> <span class="keyword">or</span> dist[i] &lt; dist[u]):</span><br><span class="line">                u = i</span><br><span class="line">        <span class="comment"># 访问了所有节点或者该节点无法到达</span></span><br><span class="line">        <span class="keyword">if</span> dist[u] == np.Inf:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 将节点设置成已访问</span></span><br><span class="line">        visited[u] = <span class="literal">True</span></span><br><span class="line">        <span class="comment"># 将已访问的节点的距离设置成可用的最短距离</span></span><br><span class="line">        <span class="keyword">for</span> v, l <span class="keyword">in</span> graph(u):</span><br><span class="line">            <span class="keyword">if</span> dist[u] + l &lt; dist[v]:</span><br><span class="line">                dist[v] = dist[u] + l</span><br><span class="line">    <span class="keyword">return</span> dist</span><br></pre></td></tr></table></figure><p>运行上面的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(naive_dijkstras(graph,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果为</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br></pre></td></tr></table></figure><h1 id="4-用-python-实现-Lazy-Dijkstra’s"><a href="#4-用-python-实现-Lazy-Dijkstra’s" class="headerlink" title="4. 用 python 实现 Lazy Dijkstra’s"></a>4. 用 python 实现 Lazy Dijkstra’s</h1><p>原生版的 Dijkstra’s 我们已经实现了，现在我们来尝试 Lazy Dijkstra’s。为什么叫 “Lazy Dijkstra’s”?因为我们不再遍历所有的节点（上面第四步），这样我们可以更加高效的处理稀疏图（所谓稀疏图就是并非图中的每个点都与其他点相连）。这种实现的算法复杂度是 $O(n\times\log(n))$。</p><blockquote><p>假设你已经装了 <code>heapq</code>。</p></blockquote><p>前三步和之前是一样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lazy_dijkstras</span><span class="params">(graph, root)</span>:</span></span><br><span class="line">    n = len(graph)</span><br><span class="line">    dist = [Inf <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">    dist[root] = <span class="number">0</span></span><br><span class="line">    visited = [<span class="literal">False</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br></pre></td></tr></table></figure><p>从第四步开始就与之前不同了：</p><p>首先给根节点插入一个距离 0：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pq = [(<span class="number">0</span>, root)]</span><br></pre></td></tr></table></figure><p>将前面第四步的①和②合并：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> len(pq) &gt; <span class="number">0</span>:</span><br><span class="line">    _, u = heapq.heappop(pq)</span><br><span class="line">    <span class="keyword">if</span> visited[u]:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    visited[u] = <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>第四步的第三部分基本与之前一致：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> v, l <span class="keyword">in</span> graph[u]:</span><br><span class="line">    <span class="keyword">if</span> dist[u] + l &lt; dist[v]:</span><br><span class="line">        dist[v] = dist[u] + l</span><br><span class="line">        heapq.heappush(pq, (dist[v], v))</span><br></pre></td></tr></table></figure><p>最后，返回“距离”列表。</p><p>完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lazy_dijkstras</span><span class="params">(graph, root)</span>:</span></span><br><span class="line">    n = len(graph)</span><br><span class="line">    dist = [Inf <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">    dist[root] = <span class="number">0</span></span><br><span class="line">    visited = [<span class="literal">False</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">    pq = [(<span class="number">0</span>, root)]</span><br><span class="line">    <span class="keyword">while</span> len(pq) &gt; <span class="number">0</span>:</span><br><span class="line">        _, u = heapq.heappop(pq)</span><br><span class="line">        <span class="keyword">if</span> visited[u]:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        visited[u] = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> v, l <span class="keyword">in</span> graph[u]:</span><br><span class="line">            <span class="keyword">if</span> dist[u] + l &lt; dist[v]:</span><br><span class="line">                dist[v] = dist[u] + l</span><br><span class="line">                heapq.heappush(pq, (dist[v], v))</span><br><span class="line">    <span class="keyword">return</span> dist</span><br></pre></td></tr></table></figure><h1 id="5-Reference"><a href="#5-Reference" class="headerlink" title="5. Reference"></a>5. Reference</h1><p><a href="https://pythonalgos.com/dijkstras-algorithm-in-5-steps-with-python/" target="_blank" rel="noopener">Dijkstra’s Algorithm in 5 Steps with Python</a> </p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-前言&quot;&gt;&lt;a href=&quot;#1-前言&quot; class=&quot;headerlink&quot; title=&quot;1. 前言&quot;&gt;&lt;/a&gt;1. 前言&lt;/h1&gt;&lt;p&gt;Dijkstra’s 是最广为人知的图算法之一，同时也是最难发音和拼写的图算法。Dijkstra’s 算法是最短路径算法，在它的基础上还衍生出很多其他变种。本文将介绍两种 Dijkstra’s 算法，并以邻接表为例用 python 实现。&lt;/p&gt;
    
    </summary>
    
      <category term="博客转载" scheme="https://rogerspy.gitee.io/categories/%E5%8D%9A%E5%AE%A2%E8%BD%AC%E8%BD%BD/"/>
    
    
      <category term="Dijkstra&#39;s Algorithm" scheme="https://rogerspy.gitee.io/tags/dijkstra-s-algorithm/"/>
    
      <category term="Graph Algorithm" scheme="https://rogerspy.gitee.io/tags/graph-algorithm/"/>
    
  </entry>
  
  <entry>
    <title>预训练语言模型：CVT</title>
    <link href="https://rogerspy.gitee.io/2021/09/30/ptm-cvt/"/>
    <id>https://rogerspy.gitee.io/2021/09/30/ptm-cvt/</id>
    <published>2021-09-30T14:06:02.000Z</published>
    <updated>2022-01-17T10:51:02.433Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220117183503.png" alt></p><p>之前介绍的预训练模型都是将预训练过程和下游特定任务分成两阶段进行训练， <a href="https://arxiv.org/pdf/1809.08370.pdf" target="_blank" rel="noopener">Cross-View Training</a> 将着来年各个阶段合并成一个统一的半监督学习过程：bi-LSTM 编码器通过有标注数据的监督学习和无标注数据的无监督学习同时训练。</p><a id="more"></a><h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><p>同时使用标注数据和无标注数据进行模型训练，这种训练方式是典型的半监督学习。CVT 引入半监督学习中的自学习机制（self-training）用于神经网络序列建模。</p><p>假设标注数据集和无标注数据集分别为 $D_{l}$ 和 $D_u$，经典的自学习机制过程如下：</p><ul><li>首先从 $D_l$ 中训练一个模型 $M$；</li><li>然后将从 $D_u$ 中抽取部分数据 $D’_u$ 出来，用 $M$ 进行预测：$y’=M(D’_u)$；</li><li>将 $M$ 的预测结果 $y’$ 与 $D’_u$ 视为新的标注数据 $(D’_u, y’)$ 放进 $D_l$ 中；</li><li>重复上面的步骤，知道所有数据都变成 $D_l$。</li></ul><p>自学习机制的缺点也很明显：一旦在第二步中出错，在以后的训练中错误会越来越深。在 CV 领域人们提出给无标注数据加噪声的方法，使模型泛化性得到了有效的提升。但是对于 NLP 这种离散序列数据来说，如何加噪声就变得很棘手了。</p><p>受<a href="https://arxiv.org/pdf/1304.5634.pdf" target="_blank" rel="noopener">多视角学习（multi-view learning）</a> 的启发，CVT 增加了额外的预测模块用来对无标注数据进行预测。</p><h1 id="2-Cross-View-Training"><a href="#2-Cross-View-Training" class="headerlink" title="2. Cross-View Training"></a>2. Cross-View Training</h1><h2 id="2-1-Method"><a href="#2-1-Method" class="headerlink" title="2.1 Method"></a>2.1 Method</h2><ul><li>$D_l = \{(x_1, y_1), (x_2, y_2),…,(x_N, y_N)\}$ 表示标注数据；</li><li>$D_u = \{x_1, x_2,…,x_M\}$ 表示无标注数据；</li><li><p>$p_\theta(y|x_i)$ 表示模型参数为 $\theta$ 时，输入 $x_i$ 模型的输出结果。</p></li><li><p>在有标注的数据上，所有的模型参数都以标准的监督学习方式进行更新，模型的损失函数是标准的交叉熵损失函数：</p><script type="math/tex; mode=display">\mathcal{L}_{\text{sup}}(\theta) = \frac{1}{|D_l|} \sum_{x_i,y_i \in D_l} \text{Cross-Entropy}(y_i, p_\theta(y|x_i))</script><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220117154228.png" style="zoom:80%;"></p></li><li><p>在无标注的数据上：</p><p>① 首先用原始预测模块（primary prediction models）对无标注数据进行预测得：$p_\theta(y|x_i)$；</p><p>② 然后用 $k$ 个附加预测模块（auxiliary prediction modules）将 $p_\theta(y|x_i)$ 作为 ground truth，用加噪声的无标注数据进行预测，然后计算损失：</p><script type="math/tex; mode=display">\mathcal{L}_{\text{CVT}}(\theta) = \frac{1}{D_u} \sum_{x_i\in D_u} \sum_{j=1}^k KL(p_\theta(y|x_i), p_\theta^j(y|x_i^j))</script></li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220117154257.png" style="zoom:80%;"></p><ul><li>最后，整个模型的损失为：<script type="math/tex; mode=display">\mathcal{L} = \mathcal{L}_{\text{sup}} + \mathcal{L}_{\text{CVT}}</script></li></ul><h2 id="2-2-Multi-Task-Learning"><a href="#2-2-Multi-Task-Learning" class="headerlink" title="2.2 Multi-Task Learning"></a>2.2 Multi-Task Learning</h2><p>对多任务同时进行训练时，CVT 增加了几个相应的原始预测模块。所有原始预测模块共享句子表示编码器。进行监督学习时，模型随机选择一个任务进行训练。此时，模型的句子表示编码器和任务预测器的参数会随着训练更新，与选定任务无关的预测器的参数不会更新。比如，同时训练分类和序列标注时，当模型随机选择分类任务进行训练时，序列标注的预测器参数不会随训练更新。</p><p>当进行 CVT 学习时，模型的所有参数都会更新。比如训练分类任务和序列标注任务时，首先用原始预测模块分别预测类别 $c_1$ 和输出序列 $s_1$，然后以此为 ground truth，对 $x_i$ 进行加噪声，利用加噪声后的 $x_i^j$ 再分别预测类别 $c_1^j$ 和输出序列 $s_i^j$，然后分别计算 $(c_1, c_1^j)$ 和 $(s_1, s_1^j)$ 的损失，然后利用后向传播，对参数进行更新。</p><p>多任务学习可以使模型泛化能力得到加强，同时可以得到一个副产物：将所有无标注数据进行标注得到标注数据。</p><h1 id="3-Cross-View-Training-Models"><a href="#3-Cross-View-Training-Models" class="headerlink" title="3. Cross-View Training Models"></a>3. Cross-View Training Models</h1><p>由于 CVT 依赖的附加预测模块需要对输入进行加噪声，即限制输入的“视角”。下面我们介绍一些特定任务下，加噪声的方法。</p><blockquote><p>需要注意的是：当原始预测模块有 dropout 的时候，在进行监督学习时可以让 dropout 正常工作，但是在进行无监督训练时 dropout 需要关闭。</p></blockquote><h2 id="3-1-Bi-LSTM-句子编码"><a href="#3-1-Bi-LSTM-句子编码" class="headerlink" title="3.1 Bi-LSTM 句子编码"></a>3.1 Bi-LSTM 句子编码</h2><ul><li>输入： $x_i = [x_i^1, x_i^2, …, x_i^T]$；</li><li>词向量：$v = \text{word embedding}(x_i) + \text{CNN}(\text{char}(x_i))$：</li><li>$\text{layer}-1 \text{bi-LSTM}$：$h_1=\text{bi-LSTM}(v)=[\overrightarrow{h}{_1^1} \oplus \overleftarrow{h}{_1^1}, \overrightarrow{h}{_1^2} \oplus \overleftarrow{h}{_1^2} …, \overrightarrow{h}{_1^T} \oplus \overleftarrow{h}{_1^T}]$ ；</li><li>$\text{layer}-2 \text{bi-LSTM}$：$h_2=\text{bi-LSTM}(h_1)=[\overrightarrow{h}_2^1 \oplus \overleftarrow{h}{_2^1}, \overrightarrow{h}_2^2 \oplus \overleftarrow{h}{_2^2} …, \overrightarrow{h}{_2^T} \oplus \overleftarrow{h}{_2^T}]$；</li></ul><h2 id="3-2-序列标注"><a href="#3-2-序列标注" class="headerlink" title="3.2 序列标注"></a>3.2 序列标注</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220117173747.png" alt></p><p>序列标注任务（比如词性标注、命名实体识别）中，模型对序列中的每个词进行分类，预测模块包含一个全连接层和一个 softmax 层：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}p(y^t|x_i) &= \text{NN}(h_1^t \oplus h_2^t) \\           &= \text{softmax}(U\cdot \text{ReLU}(W(h_1^t \oplus h_2^t)) + b)\end{aligned}\end{equation}</script><p>在进行额外预测任务时，只给第一层 bi-LSTM 输入单向序列。因为这样的话，模型只观察到部分序列，就必须像语言模型那样对“未来”词进行预测：</p><script type="math/tex; mode=display">p{_\theta}^\text{fwd}(y^t|x_i) = \text{NN}^\text{fwd}(\overrightarrow h{_1^t}(x_i))\\p{_\theta}^\text{bwd}(y^t|x_i) = \text{NN}^\text{bwd}(\overleftarrow h{_1^t}(x_i))\\p{_\theta}^\text{futher}(y^t|x_i) = \text{NN}^\text{future}(\overrightarrow h{_1^t}(x_i))\\p{_\theta}^\text{past}(y^t|x_i) = \text{NN}^\text{past}(\overleftarrow h{_1^t}(x_i))</script><p>其中 forward 表示模型还没看到右侧的信息做出的预测，future 表示该词没有右侧或者该词本身信息做出的预测，两者的区别在于 forward 表示待预测的词右侧是有下文的，而 future 表示的是待预测的词右侧没有下文。</p><h2 id="3-3-Dependency-Parsing"><a href="#3-3-Dependency-Parsing" class="headerlink" title="3.3 Dependency Parsing"></a>3.3 Dependency Parsing</h2><p>依存句法分析任务中，句子中的词被当做是图的节点。词与词之间用有向边进行连接，形成一棵用来描述语法结构的树。$y_i^t = (u, t, r)$ 表示 $x_i^u$ 与 $x_i^t$ 相连，他们的关系是 $r$。</p><script type="math/tex; mode=display">p_\theta((u,t,r)|x_i) \propto e^{s(h_1^u(x_i) \oplus h_2^u(x_i), h_1^t(x_i) \oplus h_2^t(x_i), r)}</script><p>其中</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}s(z_1, z_2, r) = \text{ReLU}(W_\text{head}z_1 + b_\text{head})\cdot (W_r+W) \cdot \text{ReLU}(W_\text{dep}z_2+b_\text{dep})\end{aligned}\end{equation}</script><p>额外预测任务：</p><script type="math/tex; mode=display">p_\theta^\text{fwd-fwd}((u,t,r)|x_i) \propto e^{s^\text{fwd-fwd}(\overrightarrow h{_1^u}(x_i), \overrightarrow h{_1^t}(x_i), r)}\\p_\theta^\text{fwd-bwd}((u,t,r)|x_i) \propto e^{s^\text{fwd-bwd}(\overrightarrow h{_1^u}(x_i), \overleftarrow h{_1^t}(x_i), r)}\\p_\theta^\text{bwd-fwd}((u,t,r)|x_i) \propto e^{s^\text{bwd-fwd}(\overleftarrow h{_1^u}(x_i), \overrightarrow h{_1^t}(x_i), r)}\\p_\theta^\text{bwd-bwd}((u,t,r)|x_i) \propto e^{s^\text{bwd-bwd}(\overleftarrow h{_1^u}(x_i), \overleftarrow h{_1^t}(x_i), r)}\\</script><p>每一个句子都会丢失一部分上下文。</p><h2 id="3-4-Sequence-to-Sequence-Learning"><a href="#3-4-Sequence-to-Sequence-Learning" class="headerlink" title="3.4 Sequence-to-Sequence Learning"></a>3.4 Sequence-to-Sequence Learning</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/151034571.png" alt></p><ul><li>源序列：$x_i = x_i^1,…x_i^T$；</li><li>目标序列：$y_i=y_i^1,…,y_i^K$；</li><li>注意力得分：$\alpha_j \propto e^{h^jW_\alpha h^t}$；</li><li>注意力加权的源序列编码：$c_t = \sum_j\alpha_jh^j$；</li><li>隐状态：$a_t=\tanh(W_a[c_t, h_t])$；</li><li>预测输出：$p(y_i^t|y_i^{&lt;t}, x_i)=\text{softmax}(W_sa_t)$。</li></ul><p>两个附加预测解码器，LSTM 权重共享，但是注意力权重和 softmax 权重是不同的：</p><ol><li><p>对第一个解码器的注意力权重采用 dropout;</p></li><li><p>让第二个解码器预测目标序列的下一个词，而不是当前词：</p><script type="math/tex; mode=display">p_\theta^\text{future}(y_i^t|y_i^{<t}, x_i) = \text{softmax}(W_s^\text{future}a_{t-1}^\text{future})</script></li></ol><h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220117184738.png" alt></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/pdf/1809.08370.pdf" target="_blank" rel="noopener">Semi-Supervised Sequence Modeling with Cross-View Training</a>, <em>Kevin Clark, Minh-Thang Luong, Christopher D. Manning, Quoc V. Le. 2018. arxiv: 1809.08370</em></li><li><a href="https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#cross-view-training" target="_blank" rel="noopener">Generalized Language Models</a>，<em>Lil’Log</em></li><li><a href="https://zhuanlan.zhihu.com/p/32922326" target="_blank" rel="noopener">Lecture13 - Semi-Supervised Learning</a>, <em>幻想家皮霸霸丶</em> </li><li><a href="https://www.cs.cmu.edu/~avrim/Papers/cotrain.pdf" target="_blank" rel="noopener">Combining labeled and unlabeled data with co-training.</a> <em>Avrim Blum and Tom Mitchell. 1998. In COLT. ACM.</em></li><li><a href="https://arxiv.org/pdf/1304.5634.pdf" target="_blank" rel="noopener">A survey on multi-view learning</a>. <em>Chang Xu, Dacheng Tao, and Chao Xu. 2013. arXiv preprint arXiv:1304.5634.</em> </li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220117183503.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;之前介绍的预训练模型都是将预训练过程和下游特定任务分成两阶段进行训练， &lt;a href=&quot;https://arxiv.org/pdf/1809.08370.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Cross-View Training&lt;/a&gt; 将着来年各个阶段合并成一个统一的半监督学习过程：bi-LSTM 编码器通过有标注数据的监督学习和无标注数据的无监督学习同时训练。&lt;/p&gt;
    
    </summary>
    
      <category term="语言模型" scheme="https://rogerspy.gitee.io/categories/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="Language Model" scheme="https://rogerspy.gitee.io/tags/language-model/"/>
    
      <category term="CVT" scheme="https://rogerspy.gitee.io/tags/cvt/"/>
    
  </entry>
  
  <entry>
    <title>预训练语言模型：Pre-trained seq2seq</title>
    <link href="https://rogerspy.gitee.io/2021/09/17/ptm-pre-trained-seq2seq/"/>
    <id>https://rogerspy.gitee.io/2021/09/17/ptm-pre-trained-seq2seq/</id>
    <published>2021-09-17T07:57:24.000Z</published>
    <updated>2022-01-14T13:53:22.228Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/151034571.png" alt></p><p>之前我们介绍过 <a href="https://rogerspy.github.io/2019/08/26/NLP%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%AE%80%E4%BB%8B%EF%BC%88%E4%B8%80%EF%BC%89/" target="_blank" rel="noopener">seq2seq 模型</a>，通常用作机器翻译，通过编码器（encoder）对源语言进行编码，然后通过解码器（decoder）对编码器的结果进行解码，得到目标语言。原始的 seq2seq 模型是使用平行语料对模型从头开始进行训练，这种训练方式需要大量的平行语料。<a href="https://arxiv.org/pdf/1611.02683.pdf" target="_blank" rel="noopener">Prajit Ramachandran</a> 提出一种方法，可以大幅降低平行语料的需求量：先分别使用源语言和目标语言预训练两个语言模型，然后将语言模型的权重用来分别初始化编码器和解码器，最终取得了 SOTA 的结果。</p><a id="more"></a><h1 id="1-Method"><a href="#1-Method" class="headerlink" title="1. Method"></a>1. Method</h1><h2 id="1-1-Basic-Procedure"><a href="#1-1-Basic-Procedure" class="headerlink" title="1.1 Basic Procedure"></a>1.1 Basic Procedure</h2><p>给定输入序列 $x_1, x_2, …, x_m$，seq2seq 的目的是最大化：</p><script type="math/tex; mode=display">p(y_n, y_{n-1}, ..., y_1|x_1, x_2,...x_m) = \prod_{t=1}^n p(y_t|y_{t-1},...,y_1;x_1, x_2,...,x_m)</script><p>seq2seq 模型是使用编码器（RNN）将 $x_1, x_2, …, x_m$ 表示成一个隐向量，然后将隐向量传递给解码器进行序列解码。我们的方法是将编码器和解码器都当做 RNN 语言模型进行使用大量的语料进行预训练。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220114180418.png" style="zoom:80%;"></p><p>两个语言模型训练完成以后，将两个语言模型的权重用来初始化编码器和解码器。为了方便起见，解码器的 $\text{softmax}$ 使用目标语言的语言模型的 $\text{softmax}$ 进行初始化。</p><h2 id="1-2-Monolingual-language-modeling-losses"><a href="#1-2-Monolingual-language-modeling-losses" class="headerlink" title="1.2 Monolingual language modeling losses"></a>1.2 Monolingual language modeling losses</h2><p>使用语言模型初始化 seq2seq 以后，再用平行语料进行 fine-tuning。根据 <a href="https://arxiv.org/pdf/1312.6211.pdf" target="_blank" rel="noopener">Goodfellow et al. 2013</a> 的研究，fine-tuning 过程很容易造成灾难性遗忘（catastrophic forgetting），使得模型在语言模型上的性能急剧下降，损害模型的泛化能力。</p><p>为了保证模型在平行语料上不会过拟合，在fine-tuning 阶段继续训练语言模型任务，seq2seq 和 语言模型任务的损失等权相加作为最终损失。</p><h2 id="1-3-Other-improvements-to-the-model"><a href="#1-3-Other-improvements-to-the-model" class="headerlink" title="1.3 Other improvements to the model"></a>1.3 Other improvements to the model</h2><p>预训练和损失叠加机制能大幅提升模型性能，但是我们发现另外两个可以小幅提升模型能力的技巧：</p><ol><li>残差连接；</li><li>多层注意力。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220114214211.png" alt></p><h1 id="2-Experiments"><a href="#2-Experiments" class="headerlink" title="2. Experiments"></a>2. Experiments</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220114215019.png" alt></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/pdf/1611.02683.pdf" target="_blank" rel="noopener">Unsupervised Pretraining for Sequence to Sequence Learning</a>, <em>Prajit Ramachandran, Peter J. Liu and Quoc V. Le</em> 2017, arxiv: 1611.02683</li><li><a href="https://arxiv.org/pdf/1312.6211.pdf" target="_blank" rel="noopener">An empirical investigation of catastrophic forgetting in gradient-based neural networks</a>, <em>Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. 2013. arXiv preprint arXiv:1312.6211</em></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/151034571.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;之前我们介绍过 &lt;a href=&quot;https://rogerspy.github.io/2019/08/26/NLP%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%AE%80%E4%BB%8B%EF%BC%88%E4%B8%80%EF%BC%89/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;seq2seq 模型&lt;/a&gt;，通常用作机器翻译，通过编码器（encoder）对源语言进行编码，然后通过解码器（decoder）对编码器的结果进行解码，得到目标语言。原始的 seq2seq 模型是使用平行语料对模型从头开始进行训练，这种训练方式需要大量的平行语料。&lt;a href=&quot;https://arxiv.org/pdf/1611.02683.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Prajit Ramachandran&lt;/a&gt; 提出一种方法，可以大幅降低平行语料的需求量：先分别使用源语言和目标语言预训练两个语言模型，然后将语言模型的权重用来分别初始化编码器和解码器，最终取得了 SOTA 的结果。&lt;/p&gt;
    
    </summary>
    
      <category term="语言模型" scheme="https://rogerspy.gitee.io/categories/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="Language Model" scheme="https://rogerspy.gitee.io/tags/language-model/"/>
    
      <category term="pre-trained seq2seq" scheme="https://rogerspy.gitee.io/tags/pre-trained-seq2seq/"/>
    
  </entry>
  
  <entry>
    <title>预训练语言模型：Data noising smoothing</title>
    <link href="https://rogerspy.gitee.io/2021/09/15/ptm-data-noising-as-smoothing/"/>
    <id>https://rogerspy.gitee.io/2021/09/15/ptm-data-noising-as-smoothing/</id>
    <published>2021-09-15T13:12:22.000Z</published>
    <updated>2022-01-14T07:55:02.504Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220112213334.png" alt></p><p>数据噪化（data noising）是一种非常有效的神经网络正则化的有段，通常被用在语音和视觉领域，但是在离散序列化数据（比如语言模型）上很少应用。本文尝试探讨给神经网络语言模型加噪声与 n-gram 语言模型中的平滑之间的联系，然后利用这种联系设计出一种噪声机制，帮助我们对语言进行建模。</p><a id="more"></a><h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><p>给定一个序列：$X=(x_1, x_2, …, x_T)$，词表 $V$。我们可以对序列进行建模：</p><script type="math/tex; mode=display">p(X)=\prod_{t=1}^T p(x_t|x_{<t})</script><p>传统的 n-gram 模型很难对 $t$ 很大的序列建模，因为随着 $t$ 的增加，$x_{&lt;t}$ 的数量是以指数增加的。而神经网络可以处理更长的序列，因为神经网络处理的是隐状态而不是序列个数（参考之前的 RNN/CNN语言模型）。</p><p>以一个 $L$ 层的 LSTM 语言模型为例，第 $l$ 层的隐状态为 $h_t^{(l)}=f_\theta(h_{t-1}^{(l)}, h_t^{(t-1)})$。令 $h^{(0)}$ 为 $X$ 的 one-hot 编码，$t$ 时刻的模型输出为：</p><script type="math/tex; mode=display">p_\theta(x_t|x_{<t}) = \text{softmax}(g_\theta(h_t^{(L)}))</script><p>其中 $g_\theta: \mathbb{R}^{|h|} \rightarrow \mathbb{R}^{|V|}$。然后通过优化交叉熵损失函数最大化似然估计 $p_\theta(X)$:</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - \sum_t \log p_\theta (x_t|x_{<t})</script><p>另外，我们还考虑另一个序列任务——seq2seq，输入序列 $X$ 输出序列 $Y$：</p><script type="math/tex; mode=display">p(Y|X) = \prod_{t=1}^{T_Y} p(y_t|X, y_{<t})</script><p>损失函数：</p><script type="math/tex; mode=display">\mathcal{L}_\theta = \sum_t \log p_\theta(y_t|X, y_{<t})</script><h1 id="2-Smoothing-amp-Noising"><a href="#2-Smoothing-amp-Noising" class="headerlink" title="2. Smoothing &amp; Noising"></a>2. Smoothing &amp; Noising</h1><p>神经网络语言模型和 n-gram 语言模型一样，都是通过给定序列去预测下一个位置的元素，使用最大似然估计使模型参数达到最优。因此，两者其实有异曲同工之妙，但是神经网络容易过拟合，现有的正则化方法（比如 L2，dropout 等）都是从模型权重着手，并没有有效地分析和利用序列本身的特征。而 n-gram 模型则充分利用了序列本身的性质，因此分析 n-gram 的序列特征并将这些特征融入到神经网络中，对神经网络序列建模会有很大的帮助。</p><h2 id="2-1-n-gram-中的平滑"><a href="#2-1-n-gram-中的平滑" class="headerlink" title="2.1 n-gram 中的平滑"></a>2.1 n-gram 中的平滑</h2><p>之前介绍<a href="https://rogerspy.github.io/2021/03/16/ptm_probabilistic_language_model/" target="_blank" rel="noopener">统计语言模型</a>的时候，我们介绍过由于 n-gram 语言模型存在稀疏化问题，所以平滑技术至关重要。这里我们考虑插值平滑，比如 bi-gram：</p><script type="math/tex; mode=display">p_{\text{inter}}(x_t|x_{t-1}) = \lambda \cdot p(x_t | x_{t-1}) + (1-\lambda) \cdot p(x_t)</script><p>其中 $0 \le \lambda \le 1$。</p><h2 id="2-2-RNN-中的噪声"><a href="#2-2-RNN-中的噪声" class="headerlink" title="2.2 RNN 中的噪声"></a>2.2 RNN 中的噪声</h2><p>要想将 n-gram 中的平滑技术直接应用于 RNN 中会有一个问题，就是 RNN 中没有明确的计数。所以我们设计了两种加噪声的方法：</p><ul><li><p><strong>unigram noising</strong>：对于 $x_i \in x_{&lt;t}$，以 $\gamma$ 概率从样本中采样一个 unigram 来代替 $x_i$。</p><blockquote><p>原句：张三今年20岁。</p><p>unigram noising：李四今年20岁。/ 老虎今年20岁。</p></blockquote></li><li><p><strong>blank noising</strong>: 对于 $x_i \in x_{&lt;t}$ ，以 $\gamma$ 概率将 $x_i$ 替换成 “_”。</p><blockquote><p>原句：张三今年20岁。</p><p>blank noising：_今年20岁。</p></blockquote></li></ul><p>接下来我们就分析一下，这两种噪声与插值平滑之间的关系。</p><h2 id="2-3-unigram-noising-as-interpolation"><a href="#2-3-unigram-noising-as-interpolation" class="headerlink" title="2.3 unigram noising as interpolation"></a>2.3 unigram noising as interpolation</h2><p>我们先考虑最简单的情形——bigram。令 $c(x)$ 表示 $x$ 在原始数据中的个数，$c_\gamma(x)=\mathbb{E}_\tilde{x}[c(\tilde{x})]$ 表示在 unigram noising 情况下 $x$ 的期望个数，我们可得：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}p_\gamma(x_t|x_{t-1}) &= \frac{c_\gamma(x_{t-1}, x_t)}{c_\gamma(x_{t-1})} \\\\                      &= \frac{(1-\gamma)c(x_{t-1}, x_t) + \gamma p(x_{t-1}) c(x_t)}{c(x_{t-1})} \\\\                      &= (1-\gamma) p(x_t|x_{t-1})  +\gamma p(x_t)\end{aligned}\end{equation}</script><p>其中 $c_\gamma(x) = c(x)$， 因为 $q(x)$ 是 unigram 分布。另外，最后一行是由于：</p><script type="math/tex; mode=display">p(x_t) = \frac{c(x_t)}{n} \quad \& \quad p(x_{t-1}) = \frac{c(x_{t-1})}{n} \\\\n = \frac{c(x_t)}{p(x_t)} = \frac{c(x_{t-1})}{p(x_{t-1})}</script><p> $n$ 表示训练集中总的 token。则最后一行的第二项为：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}\frac{\gamma p(x_{t-1}) c(x_t)}{c(x_{t-1})} &= \gamma c(x_t) \cdot \frac{1}{n} \\\\                                            &= \gamma \frac{c(x_t)}{n} \\\\                                            &= \gamma p(x_t)\end{aligned}\end{equation}</script><p>我们可以看到 $p_\gamma(x_t|x_{t-1})$ 的加噪声形式与插值平滑数学形式上非常相似。我们还可以推导出更一般的形式，令 $\tilde{x_{&lt;t}}$ 表示加噪声后的序列， 则：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}p_\gamma(x_t|x_{<t}) &= \mathbb{E}_{\tilde{x_{<t}}}[p(x_t|\tilde{x}_{<t})] \\\\                      &= \sum_J \underbrace{\pi(|J|)}_{p(|J| \text{swaps})} \sum_{x_K} \underbrace{p(x_t|x_J, x_K)}_{p(x_t|\text{noised context})} \prod_{z\in x_K} \underbrace{p(z)}_{p(\text{drawing z})}\end{aligned}\end{equation}</script><p>其中 $\pi(|J|)=(1-\gamma)^{|J|} \gamma^{t-1-|J|}$， 且 $\sum_J \pi(|J|)=1, J \in \{1,2,…,t-1\}$ 表示 token 没有发生变化的索引，$K$ 表示 token 被替换的索引。</p><h2 id="2-4-blank-noising-as-interpolation"><a href="#2-4-blank-noising-as-interpolation" class="headerlink" title="2.4 blank noising as interpolation"></a>2.4 blank noising as interpolation</h2><p>Blank noising 可以也解释为 “word-dropout”（<a href="https://arxiv.org/pdf/1506.07285.pdf" target="_blank" rel="noopener">Kumar et al. 2015</a>，<a href="https://arxiv.org/pdf/1511.01432.pdf" target="_blank" rel="noopener">Dai &amp; Le 2015</a>，<a href="https://arxiv.org/pdf/1511.06349.pdf" target="_blank" rel="noopener">Bowman et al. 2015</a>）。令 $\tilde{x}_{&lt;t}$ 表示别替换成 “_” 的序列，$x_J$ 表示没有替换的序列：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}p_\gamma(x_t|x_{<t}) &= \mathbb{E}_{\tilde{x_{<t}}}[p(x_t|\tilde{x}_{<t})] \\\\                      &= \sum_J \underbrace{\pi(|J|)}_{p(|J| \text{swaps})}\underbrace{p(x_t|x_J)}_{p(x_t|\text{noised context})} \end{aligned}\end{equation}</script><p>其中 $J \in \{1,2,…,t-1\}$，比如对于 3-gram:</p><script type="math/tex; mode=display">p_\gamma(x_3|x_1, x_2) = \pi(2)p(x_3|x_1, x_2) + \pi(1)p(x_3|x_1, \_)+\pi(1)p(x_3|\_, x_2)+\pi(0)p(x_3|\_, \_)</script><p>其中 $\pi(i)=(1-\gamma)^i\gamma^{2-i}$。</p><h1 id="3-他山之石，可以攻玉"><a href="#3-他山之石，可以攻玉" class="headerlink" title="3. 他山之石，可以攻玉"></a>3. 他山之石，可以攻玉</h1><p>我们已经证明了噪化与平滑有异曲同工之妙，现在我们就可以从以下两个方面考虑如何提升噪声机制;</p><ol><li>自适应计算噪声概率 $\gamma$，用来反应特定输入子序列的置信度；</li><li>利用更高阶的 n-gram 统计信息，选择一个比 unigram 更简单的分布 $q(x)$。</li></ol><h2 id="3-1-Noising-Probability"><a href="#3-1-Noising-Probability" class="headerlink" title="3.1 Noising Probability"></a>3.1 Noising Probability</h2><p>假设有下面两个 bigram:</p><script type="math/tex; mode=display">\text{“and the”} \quad \text{“Humpty Dumpty”}</script><p>第一个二元组在英语语料中非常常见，它的概率非常容易估计，因此不应该用低阶的分布进行插值。直观上来说，我们希望定义一个 $\gamma(x_{1:t})$ 对于常见的二元组尽可能少地被噪化。</p><p>第二个二元组就比较罕见了，但是这个二元组非常特殊，因为在英语语料中 “Humpty” 后面通常跟着的就是 “Dumpty”，同样的 “Dumpty” 前面的通常也是 “Humpty”，即这两个单词通常是成对出现的，这样的二元组我们称之为 “sticky pair”。构成 sticky pair 的词之间有很强的互信息，这样的二元组更类似于 unigram，我们希望可以避免二元组向一元组逼近。 </p><p>令 $N_{1+}(x_1,\cdot)=|\{x_2:c(x_1, x_2)&gt;0\}|$ 表示以 $x_1$ 为开头的二元组的种类，比如 $\{张三: 3, 张四: 4\}$ 其中以“张” 为前缀的二元组总数为 $3+4=7$，而以“张”为前缀的二元组的种类为 $2$（“张三”，“张四”）。根据对上面两个二元组的分析，我们可以设计噪声概率 $\gamma$：</p><script type="math/tex; mode=display">\gamma_{AD}(x_1) = \gamma_0\frac{N_{1+}(x_1, \cdot)}{\sum_{x_2}c(x_1, x_2)}</script><p>其中 $0 \le \gamma_0 \le 1$，因此 $0 \le \gamma_{AD} \le 1$。如果我们忽略掉句子结束符的影响，则 $\sum_{x_2} c(x_1, x_2)=c(x_1)$。</p><ul><li>当以 $x_1$ 为前缀的二元组总数固定，其不同组合的中类越少的时候，$x_1$ 被噪化的概率越小。对应上面第一个分析，当总数一定，但是组合种类越少，那么其中某一种的组合就越常见，$x_1$ 就越不应该被噪化。<script type="math/tex; mode=display">\gamma_{AD}(\text{and}) = \gamma_0 \frac{N_+(\text{and}, \cdot)}{c(\text{and})} \quad</script>假设 “and” 组成的二元组是平均分布的，则 $c(\text{and})= \mathbb{E}(c(\text{and, the})) \times \mathbb{E}(N_+(\text{and, the}))$ ，当 $\mathbb{E}(c(\text{and, the}))$ 越大的时候，$\mathbb{E}(N_+(\text{and, the}))$ 就会越小，则 $\gamma_{AD}(\text{and})$ 就越小。</li></ul><h2 id="3-2-Proposal-distribution"><a href="#3-2-Proposal-distribution" class="headerlink" title="3.2 Proposal distribution"></a>3.2 Proposal distribution</h2><p>假设有下面两个二元组：</p><script type="math/tex; mode=display">\text{“San Francisco”} \quad \text{“New York”}</script><p>这两个二元组在语料中都非常常见，所以 “Francisco” 和 “York” 也非常常见。但是 “Francisco” 和 “York” 通常是跟在 “San” 和 “New” 后面，所以当使用 unigram 频率时它们也不应该有很高的噪声概率。相反，最好增加具有不同历史的一元组的提议概率，或者更准确地说是完成大量二元组类型的一元组。因此，我们令：</p><script type="math/tex; mode=display">q(x) \propto N_{1+}(\cdot, x)</script><ul><li>当以 $x$ 为结尾的二元组种类越少，被采样到的概率就会越低。假设语料中 “New York” 有 1 万条，但是 “York” 只与 “New” 组成二元组，即 $N_+(\cdot, \text{York})=1$，则 $q(x) \sim 1/10000$。 </li></ul><p>注意这个时候，我们除了会对 $x_{1:t-1}$ 进行噪化，同样也会对预测值 $x_t$ 进行噪化。结合 $q(x)$ 和 $\gamma_{AD}(x_1)$ 我们就可以得到   Kneser-Ney 平滑的噪化模拟了。</p><blockquote><ol><li><p>我们以 $\gamma_{AD}(x)$ 的概率决定 $x$ 是否会被噪化（替换成 “_” 或者其他 token）;</p></li><li><p>然后如果我们选择 ngram noising 的话，以 $q(x)$ 的概率对替他 token 进行采样，用来替换被 $\gamma_{AD}(x)$ 选中的 token。</p></li></ol></blockquote><p>下表总结了不同的噪化机制：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220114110524.png" alt></p><h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h1><h2 id="4-1-Language-Model"><a href="#4-1-Language-Model" class="headerlink" title="4.1 Language Model"></a>4.1 Language Model</h2><ul><li><p>Penn Treebank</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220114155146.png" alt></p></li><li><p>Text8</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220114155302.png" alt></p></li></ul><h2 id="4-2-Machine-Translation"><a href="#4-2-Machine-Translation" class="headerlink" title="4.2 Machine Translation"></a>4.2 Machine Translation</h2><ul><li>IWSLT 2015（English-German）</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220114155401.png" alt></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/pdf/1703.02573.pdf" target="_blank" rel="noopener">Data Noising as Smoothing in Neural Network Language Models</a>, <em>Ziang Xie, Sida I. Wang, Jiwei Li, Daniel Lévy, Aiming Nie, Dan Jurafsky, Andrew Y. Ng.</em>  ICLR, 2017</li><li><a href="https://arxiv.org/pdf/1506.07285.pdf" target="_blank" rel="noopener">Ask me anything: Dynamic memory networks for natural language processing</a>, <em>Ankit Kumar, Ozan Irsoy, Jonathan Su, James Bradbury, Robert English, Brian Pierce, Peter Ondruska, Ishaan Gulrajani, and Richard Socher.</em> <em>arXiv preprint arXiv:1506.07285</em>, 2015.</li><li><a href="https://arxiv.org/pdf/1511.06349.pdf" target="_blank" rel="noopener">Generating sentences from a continuous space.</a>, Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio. <em>arXiv preprint arXiv:1511.06349</em>, 2015.</li><li><a href> Semi-supervised sequence learning.</a><a href="https://arxiv.org/pdf/1511.01432.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1511.01432.pdf</a> <em>Andrew M Dai and Quoc V Le.</em>  In <em>Advances in Neural Information Processing Systems</em>, pp. 3061–3069, 2015.</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220112213334.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;数据噪化（data noising）是一种非常有效的神经网络正则化的有段，通常被用在语音和视觉领域，但是在离散序列化数据（比如语言模型）上很少应用。本文尝试探讨给神经网络语言模型加噪声与 n-gram 语言模型中的平滑之间的联系，然后利用这种联系设计出一种噪声机制，帮助我们对语言进行建模。&lt;/p&gt;
    
    </summary>
    
      <category term="语言模型" scheme="https://rogerspy.gitee.io/categories/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="Language Model" scheme="https://rogerspy.gitee.io/tags/language-model/"/>
    
      <category term="data noising" scheme="https://rogerspy.gitee.io/tags/data-noising/"/>
    
  </entry>
  
  <entry>
    <title>深入理解 einsum：实现多头注意力机制</title>
    <link href="https://rogerspy.gitee.io/2021/09/12/einsum-mhsa/"/>
    <id>https://rogerspy.gitee.io/2021/09/12/einsum-mhsa/</id>
    <published>2021-09-12T02:45:06.000Z</published>
    <updated>2022-01-12T08:37:38.374Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/einsum-attention.png" alt></p><p>Einsum 表示法是对张量的复杂操作的一种优雅方式，本质上是使用特定领域的语言。 一旦理解并掌握了 einsum，可以帮助我们更快地编写更简洁高效的代码。</p><a id="more"></a><p>Einsum 是爱因斯坦求和（Einstein summation）的缩写，是一种求和的方法，在处理关于坐标的方程式时非常有效。在 numpy、TensorFlow 和 Pytorch 中都有相关实现，本文通过 Pytorch 实现 Transformer 中的多头注意力来介绍 einsum 在深度学习模型中的应用。</p><h1 id="1-矩阵乘法"><a href="#1-矩阵乘法" class="headerlink" title="1. 矩阵乘法"></a>1. 矩阵乘法</h1><p>假设有两个矩阵：</p><script type="math/tex; mode=display">A = \left[\begin{matrix}1 & 2 & 3 \\4 & 5 & 6\end{matrix} \right],\quadB = \left[\begin{matrix}7 & 8  \\9 & 10 \\11 & 12\end{matrix} \right]</script><p>我们想求两个矩阵的乘积。</p><ul><li>第一步：</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210912131703.png" style="zoom:67%;"></p><ul><li>第二步：</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210912132039.png" style="zoom:67%;"></p><ul><li>第三步：</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210912132541.png" style="zoom:67%;"></p><ul><li>第四步：</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210912132929.png" style="zoom:67%;"></p><h1 id="2-Einstein-Notation"><a href="#2-Einstein-Notation" class="headerlink" title="2. Einstein Notation"></a>2. Einstein Notation</h1><p>爱因斯坦标记法又称爱因斯坦求和约定（Einstein summation convention），基本内容是：</p><blockquote><p>当两个变量具有相同的角标时，则遍历求和。在此情况下，求和号可以省略。</p></blockquote><p>比如，计算两个向量的乘积， $\color{red}{a}, \color{blue}{b} \in \mathbb{R}^I$：</p><script type="math/tex; mode=display">\color{green}{c} = \sum_i \color{red}{a_i}\color{blue}{b_i}=\color{red}{a_i}\color{blue}{b_i}</script><p>计算两个矩阵的乘积， <font color="red">$A$</font> $\in \mathbb{R}^{I\times K}$，<font color="blue">$B$</font> $\in \mathbb{R}^{K\times J}$。用爱因斯坦求和符号表示，可以写成：</p><script type="math/tex; mode=display">\color{green}{c}_{ij} \color{black}= \sum_k\color{red}{A_{ik}}\color{blue}{B_{kj}}=\color{red}{A_{ik}}\color{blue}{B_{kj}}</script><p>在深度学习中，通常使用的是更高阶的张量之间的变换。比如在一个 batch 中包含 $N$ 个训练样本的，最大长度是 $T$，词向量维度为 $K$ 的张量，即 $\color{red}{\mathcal{T}}\in \mathbb{R}^{N\times T \times K}$，如果想让词向量的维度映射到 $Q$ 维，则定义一个 $\color{blue}{W} \in \mathbb{R}^{K\times Q}$:</p><script type="math/tex; mode=display">\color{green}{C_{ntq}} = \sum_k\color{red}{\mathcal{T}_{ntk}}\color{blue}{W_{kq}}=\color{red}{\mathcal{T}_{ntk}}\color{blue}{W_{kq}}</script><p>在图像处理中，通常在一个 batch 的训练样本中包含 $N$ 张图片，每张图片长为 $T$，宽为 $K$，颜色通道为 $M$，即 $\color{red}{\mathcal{T}}\in \mathbb{R}^{N\times T \times K \times M}$ 是一个 4d 张量。如果我想进行三个操作：</p><ul><li>将 $K$ 投影成 $Q$ 维；</li><li>对 $T$ 进行求和；</li><li>将 $M$ 和 $N$ 进行转置。</li></ul><p>用爱因斯坦标记法可以表示成：</p><script type="math/tex; mode=display">\color{green}{C_{mqn}}=\sum_t \sum_k \color{red}{\mathcal{T}_{ntkm}} \color{blue}{W_{kq}} = \color{red}{\mathcal{T}_{ntkm}} \color{blue}{W_{kq}}</script><p>需要注意的是，爱因斯坦标记法是一种书写约定，是为了将复杂的公式写得更加简洁。它本身并不是某种运算符，具体运算还是要回归到各种算子上。</p><h1 id="3-einsum"><a href="#3-einsum" class="headerlink" title="3. einsum"></a>3. einsum</h1><ul><li>Numpy：<code>np.einsum</code></li><li>Pytorch：<code>torch.einsum</code></li><li>TensorFlow：<code>tf.einsum</code></li></ul><p>以上三种 <code>einsum</code> 都有相同的特性 <code>einsum(equation, operands)</code>：</p><ul><li><code>equation</code>：字符串，用来表示爱因斯坦求和标记法的；</li><li><code>operands</code>：一些列张量，要运算的张量。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210912232701.png" alt></p><p>其中 <code>口</code> 是一个占位符，代表的是张量维度的字符。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.einsum(&apos;ij,jk-&gt;ik&apos;, A, B)</span><br></pre></td></tr></table></figure><p><code>A</code> 和 <code>B</code> 是两个矩阵，将 <code>ij,jk-&gt;ik</code> 分成两部分：<code>ij, jk</code> 和 <code>ik</code>，那么 <code>ij</code> 代表的是输入矩阵 <code>A</code> 的第 <code>i</code> 维和第 <code>j</code> 维，<code>jk</code> 代表的是 <code>B</code> 第 <code>j</code> 维和第 <code>k</code> 维，<code>ik</code> 代表的是输出矩阵的第 <code>i</code> 维和第 <code>k</code> 维。注意 <code>i, j, k</code> 可以是任意的字符，但是必须保持一致。换句话说，<code>einsum</code> 实际上是直接操作了矩阵的维度（角标）。上例中表示的是， <code>A</code> 和 <code>B</code> 的乘积。</p><p><img src="https://obilaniu6266h16.files.wordpress.com/2016/02/einsum-matrixmul.png?w=676" alt></p><h2 id="3-1-矩阵转置"><a href="#3-1-矩阵转置" class="headerlink" title="3.1 矩阵转置"></a>3.1 矩阵转置</h2><script type="math/tex; mode=display">B_{ji} = A_{ij}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ij-&gt;ji'</span>, [a])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">3.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">4.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">5.</span>]])</span><br></pre></td></tr></table></figure><h2 id="3-2-求和"><a href="#3-2-求和" class="headerlink" title="3.2 求和"></a>3.2 求和</h2><script type="math/tex; mode=display">b = \sum_i\sum_j A_{ij}=A_{ij}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ij-&gt;'</span>, [a])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor(<span class="number">15.</span>)</span><br></pre></td></tr></table></figure><h2 id="3-3-列求和"><a href="#3-3-列求和" class="headerlink" title="3.3 列求和"></a>3.3 列求和</h2><script type="math/tex; mode=display">b_j=\sum_iA_{ij}=A_{ij}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ij-&gt;j'</span>, [a])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([ <span class="number">3.</span>,  <span class="number">5.</span>,  <span class="number">7.</span>])</span><br></pre></td></tr></table></figure><h2 id="3-4-行求和"><a href="#3-4-行求和" class="headerlink" title="3.4 行求和"></a>3.4 行求和</h2><script type="math/tex; mode=display">b_i=\sum_jA_{ij}=A_{ij}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ij-&gt;i'</span>, [a])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([  <span class="number">3.</span>,  <span class="number">12.</span>])</span><br></pre></td></tr></table></figure><h2 id="3-5-矩阵-向量乘积"><a href="#3-5-矩阵-向量乘积" class="headerlink" title="3.5 矩阵-向量乘积"></a>3.5 矩阵-向量乘积</h2><script type="math/tex; mode=display">c_i=\sum_kA_{ik}b_k=A_{ik}b_k</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b = torch.arange(<span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ik,k-&gt;i'</span>, [a, b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([  <span class="number">5.</span>,  <span class="number">14.</span>])</span><br></pre></td></tr></table></figure><h2 id="3-6-矩阵-矩阵乘积"><a href="#3-6-矩阵-矩阵乘积" class="headerlink" title="3.6 矩阵-矩阵乘积"></a>3.6 矩阵-矩阵乘积</h2><script type="math/tex; mode=display">C_{ij}=\sum_kA_{ik}B_{kj}=A_{ik}B_{kj}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b = torch.arange(<span class="number">15</span>).reshape(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">torch.einsum(<span class="string">'ik,kj-&gt;ij'</span>, [a, b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">tensor([[  <span class="number">25.</span>,   <span class="number">28.</span>,   <span class="number">31.</span>,   <span class="number">34.</span>,   <span class="number">37.</span>],</span><br><span class="line">        [  <span class="number">70.</span>,   <span class="number">82.</span>,   <span class="number">94.</span>,  <span class="number">106.</span>,  <span class="number">118.</span>]])</span><br></pre></td></tr></table></figure><h2 id="3-7-点积"><a href="#3-7-点积" class="headerlink" title="3.7 点积"></a>3.7 点积</h2><script type="math/tex; mode=display">c = \sum_ia_ib_i=a_ib_i</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">3</span>)</span><br><span class="line">b = torch.arange(<span class="number">3</span>, <span class="number">6</span>)</span><br><span class="line">torch.einsum(<span class="string">'i,i-&gt;'</span>, [a, b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">tensor(<span class="number">14.</span>)</span><br></pre></td></tr></table></figure><h2 id="3-8-Hardamard-积"><a href="#3-8-Hardamard-积" class="headerlink" title="3.8 Hardamard 积"></a>3.8 Hardamard 积</h2><script type="math/tex; mode=display">C_{ij} = A_{ij}B_{ij}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b = torch.arange(<span class="number">6</span>,<span class="number">12</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ij,ij-&gt;ij'</span>, [a, b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">tensor([[  <span class="number">0.</span>,   <span class="number">7.</span>,  <span class="number">16.</span>],</span><br><span class="line">        [ <span class="number">27.</span>,  <span class="number">40.</span>,  <span class="number">55.</span>]])</span><br></pre></td></tr></table></figure><h2 id="3-9-外积"><a href="#3-9-外积" class="headerlink" title="3.9 外积"></a>3.9 外积</h2><script type="math/tex; mode=display">C_{ij}=a_ib_j</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">3</span>)</span><br><span class="line">b = torch.arange(<span class="number">3</span>, <span class="number">7</span>)</span><br><span class="line">torch.einsum(<span class="string">'i, j-&gt;ij'</span>, [a, b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">tensor([[  <span class="number">0.</span>,   <span class="number">0.</span>,   <span class="number">0.</span>,   <span class="number">0.</span>],</span><br><span class="line">        [  <span class="number">3.</span>,   <span class="number">4.</span>,   <span class="number">5.</span>,   <span class="number">6.</span>],</span><br><span class="line">        [  <span class="number">6.</span>,   <span class="number">8.</span>,  <span class="number">10.</span>,  <span class="number">12.</span>]])</span><br></pre></td></tr></table></figure><h2 id="3-10-Batch-矩阵乘积"><a href="#3-10-Batch-矩阵乘积" class="headerlink" title="3.10 Batch 矩阵乘积"></a>3.10 Batch 矩阵乘积</h2><script type="math/tex; mode=display">C_{ijl}=\sum_kA_{ijk}B_{ikl}=A_{ijk}B_{ikl}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>,<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">b = torch.randn(<span class="number">3</span>,<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">torch.einsum(<span class="string">'ijk, jkl-&gt;ijl'</span>, [a, b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line">tensor([[[ <span class="number">1.0886</span>,  <span class="number">0.0214</span>,  <span class="number">1.0690</span>],</span><br><span class="line">         [ <span class="number">2.0626</span>,  <span class="number">3.2655</span>, <span class="number">-0.1465</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-6.9294</span>,  <span class="number">0.7499</span>,  <span class="number">1.2976</span>],</span><br><span class="line">         [ <span class="number">4.2226</span>, <span class="number">-4.5774</span>, <span class="number">-4.8947</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">-2.4289</span>, <span class="number">-0.7804</span>,  <span class="number">5.1385</span>],</span><br><span class="line">         [ <span class="number">0.8003</span>,  <span class="number">2.9425</span>,  <span class="number">1.7338</span>]]])</span><br></pre></td></tr></table></figure><h2 id="3-11-张量收缩"><a href="#3-11-张量收缩" class="headerlink" title="3.11 张量收缩"></a>3.11 张量收缩</h2><p>假设有两个张量 $\mathcal{A}\in \mathbb{R}^{I_1\times \dots\times I_n}$ 和 $\mathcal{B} \in \mathbb{R}^{J_1\times \dots \times J_m}$。比如 $n=4, m=5$，且 $I_2=J_3$ 和 $I_3=J_5$。我们可以计算两个张量的乘积，得到新的张量 $\mathcal{C}\in\mathbb{R}^{I_1\times I_4 \times J_1 \times J_2 \times J_4}$：</p><script type="math/tex; mode=display">C_{pstuv}=\sum_q\sum_r A_{pqrs}B_{tuqvr} = A_{pqrs}B_{tuqvr}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>)</span><br><span class="line">b = torch.randn(<span class="number">11</span>,<span class="number">13</span>,<span class="number">3</span>,<span class="number">17</span>,<span class="number">5</span>)</span><br><span class="line">torch.einsum(<span class="string">'pqrs,tuqvr-&gt;pstuv'</span>, [a, b]).shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">13</span>, <span class="number">17</span>])</span><br></pre></td></tr></table></figure><h2 id="3-12-双线性变换"><a href="#3-12-双线性变换" class="headerlink" title="3.12 双线性变换"></a>3.12 双线性变换</h2><script type="math/tex; mode=display">D_{ij}=\sum_k\sum_lA_{ik}B_{jkl}C_{il} = A_{ik}B_{jkl}C_{il}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.randn(<span class="number">5</span>,<span class="number">3</span>,<span class="number">7</span>)</span><br><span class="line">c = torch.randn(<span class="number">2</span>,<span class="number">7</span>)</span><br><span class="line">torch.einsum(<span class="string">'ik,jkl,il-&gt;ij'</span>, [a, b, c])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([[ <span class="number">3.8471</span>,  <span class="number">4.7059</span>, <span class="number">-3.0674</span>, <span class="number">-3.2075</span>, <span class="number">-5.2435</span>],</span><br><span class="line">        [<span class="number">-3.5961</span>, <span class="number">-5.2622</span>, <span class="number">-4.1195</span>,  <span class="number">5.5899</span>,  <span class="number">0.4632</span>]])</span><br></pre></td></tr></table></figure><h1 id="4-einops"><a href="#4-einops" class="headerlink" title="4. einops"></a>4. einops</h1><p>尽管 <code>einops</code> 是一个通用的包，这里哦我们只介绍 <code>einops.rearrange</code> 。同 <code>einsum</code> 一样，<code>einops.rearrange</code> 也是操作矩阵的角标的，只不过函数的参数正好相反，如下图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210914003018.png" alt></p><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • NOTE            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            如果 <code>rearrange</code> 传入的参数是一个张量列表，那么后面字符串的第一维表示列表的长度。        </div>        </div>    </div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">qkv = torch.rand(<span class="number">2</span>,<span class="number">128</span>,<span class="number">3</span>*<span class="number">512</span>) <span class="comment"># dummy data for illustration only</span></span><br><span class="line"><span class="comment"># We need to decompose to n=3 tensors q, v, k</span></span><br><span class="line"><span class="comment"># rearrange tensor to [3, batch, tokens, dim] and cast to tuple</span></span><br><span class="line">q, k, v = tuple(rearrange( qkv , <span class="string">'b t (d n) -&gt; n b t d '</span>, n=<span class="number">3</span>))</span><br></pre></td></tr></table></figure><h1 id="5-Scale-dot-product-self-attention"><a href="#5-Scale-dot-product-self-attention" class="headerlink" title="5. Scale dot product self-attention"></a>5. Scale dot product self-attention</h1><ul><li><p><strong>第一步</strong>：创建一个线性投影。给定输入 $X\in \mathbb{R}^{b\times t\times d}$，其中 $b$ 表示 $\text{batch size}$，$t$ 表示 $\text{sentence length}$，$d$ 表示 $\text{word dimension}$。</p><script type="math/tex; mode=display">Q=XW_Q, \quad K=XW_K, \quad V=XW_V</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">to_qvk = nn.Linear(dim, dim * <span class="number">3</span>, bias=<span class="literal">False</span>) <span class="comment"># init only</span></span><br><span class="line"><span class="comment"># Step 1</span></span><br><span class="line">qkv = to_qvk(x)  <span class="comment"># [batch, tokens, dim*3 ]</span></span><br><span class="line"><span class="comment"># decomposition to q,v,k</span></span><br><span class="line">q, k, v = tuple(rearrange(qkv, <span class="string">'b t (d k) -&gt; k b t d '</span>, k=<span class="number">3</span>))</span><br></pre></td></tr></table></figure></li><li><p><strong>第二步</strong>：计算点积，mask，最后计算 softmax。</p><script type="math/tex; mode=display">\text{dot_score} = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} \right)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Step 2</span></span><br><span class="line"><span class="comment"># Resulting shape: [batch, tokens, tokens]</span></span><br><span class="line">scaled_dot_prod = torch.einsum(<span class="string">'b i d , b j d -&gt; b i j'</span>, q, k) * self.scale_factor</span><br><span class="line"><span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">assert</span> mask.shape == scaled_dot_prod.shape[<span class="number">1</span>:]</span><br><span class="line">    scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)</span><br><span class="line">attention = torch.softmax(scaled_dot_prod, dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>第三步</strong>：计算注意力得分与 $V$ 的乘积。</p><script type="math/tex; mode=display">\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} \right)V</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.einsum(<span class="string">'b i j , b j d -&gt; b i d'</span>, attention, v)</span><br></pre></td></tr></table></figure></li></ul><p>将上面三步综合起来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SelfAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of plain self attention mechanism with einsum operations</span></span><br><span class="line"><span class="string">    Paper: https://arxiv.org/abs/1706.03762</span></span><br><span class="line"><span class="string">    Blog: https://theaisummer.com/transformer/</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dim)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            dim: for NLP it is the dimension of the embedding vector</span></span><br><span class="line"><span class="string">            the last dimension size that will be provided in forward(x),</span></span><br><span class="line"><span class="string">            where x is a 3D tensor</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="comment"># for Step 1</span></span><br><span class="line">        self.to_qvk = nn.Linear(dim, dim * <span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># for Step 2</span></span><br><span class="line">        self.scale_factor = dim ** <span class="number">-0.5</span>  <span class="comment"># 1/np.sqrt(dim)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask=None)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x.dim() == <span class="number">3</span>, <span class="string">'3D tensor must be provided'</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 1</span></span><br><span class="line">        qkv = self.to_qvk(x)  <span class="comment"># [batch, tokens, dim*3 ]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># decomposition to q,v,k</span></span><br><span class="line">        <span class="comment"># rearrange tensor to [3, batch, tokens, dim] and cast to tuple</span></span><br><span class="line">        q, k, v = tuple(rearrange(qkv, <span class="string">'b t (d k) -&gt; k b t d '</span>, k=<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2</span></span><br><span class="line">        <span class="comment"># Resulting shape: [batch, tokens, tokens]</span></span><br><span class="line">        scaled_dot_prod = torch.einsum(<span class="string">'b i d , b j d -&gt; b i j'</span>, q, k) * self.scale_factor</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> mask.shape == scaled_dot_prod.shape[<span class="number">1</span>:]</span><br><span class="line">            scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)</span><br><span class="line"></span><br><span class="line">        attention = torch.softmax(scaled_dot_prod, dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 3</span></span><br><span class="line">        <span class="keyword">return</span> torch.einsum(<span class="string">'b i j , b j d -&gt; b i d'</span>, attention, v)</span><br></pre></td></tr></table></figure><h1 id="6-Multi-Head-Self-Attention"><a href="#6-Multi-Head-Self-Attention" class="headerlink" title="6. Multi-Head Self-Attention"></a>6. Multi-Head Self-Attention</h1><ul><li><p><strong>第一步</strong>：为每一个头创建一个线性投影 $Q, K, V$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">to_qvk = nn.Linear(dim, dim_head * heads * <span class="number">3</span>, bias=<span class="literal">False</span>) <span class="comment"># init only</span></span><br><span class="line">qkv = self.to_qvk(x)</span><br></pre></td></tr></table></figure></li><li><p><strong>第二步</strong>：将 $Q, K, V$ 分解，并分配给每个头。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Step 2</span></span><br><span class="line"><span class="comment"># decomposition to q,v,k and cast to tuple</span></span><br><span class="line"><span class="comment"># [3, batch, heads, tokens, dim_head]</span></span><br><span class="line">q, k, v = tuple(rearrange(qkv, <span class="string">'b t (d k h) -&gt; k b h t d '</span>, k=<span class="number">3</span>, h=self.heads))</span><br></pre></td></tr></table></figure></li><li><p><strong>第三步</strong>：计算注意力得分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Step 3</span></span><br><span class="line"><span class="comment"># resulted shape will be: [batch, heads, tokens, tokens]</span></span><br><span class="line">scaled_dot_prod = torch.einsum(<span class="string">'b h i d , b h j d -&gt; b h i j'</span>, q, k) * self.scale_factor</span><br><span class="line"><span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">assert</span> mask.shape == scaled_dot_prod.shape[<span class="number">2</span>:]</span><br><span class="line">    scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)</span><br><span class="line">attention = torch.softmax(scaled_dot_prod, dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>第四步</strong>：注意力得分与 $V$ 相乘</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Step 4. Calc result per batch and per head h</span></span><br><span class="line">out = torch.einsum(<span class="string">'b h i j , b h j d -&gt; b h i d'</span>, attention, v)</span><br></pre></td></tr></table></figure></li><li><p><strong>第五步</strong>：将所有的头合并</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out = rearrange(out, <span class="string">"b h t d -&gt; b t (h d)"</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>第六步</strong>：线性变换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.W_0 = nn.Linear( _dim, dim, bias=<span class="literal">False</span>) <span class="comment"># init only</span></span><br><span class="line"><span class="comment"># Step 6. Apply final linear transformation layer</span></span><br><span class="line">self.W_0(out)</span><br></pre></td></tr></table></figure></li></ul><p>最终实现 MHSA：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadSelfAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dim, heads=<span class="number">8</span>, dim_head=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Implementation of multi-head attention layer of the original transformer model.</span></span><br><span class="line"><span class="string">        einsum and einops.rearrange is used whenever possible</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            dim: token's dimension, i.e. word embedding vector size</span></span><br><span class="line"><span class="string">            heads: the number of distinct representations to learn</span></span><br><span class="line"><span class="string">            dim_head: the dim of the head. In general dim_head&lt;dim.</span></span><br><span class="line"><span class="string">            However, it may not necessary be (dim/heads)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dim_head = (int(dim / heads)) <span class="keyword">if</span> dim_head <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> dim_head</span><br><span class="line">        _dim = self.dim_head * heads</span><br><span class="line">        self.heads = heads</span><br><span class="line">        self.to_qvk = nn.Linear(dim, _dim * <span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_0 = nn.Linear( _dim, dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.scale_factor = self.dim_head ** <span class="number">-0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask=None)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x.dim() == <span class="number">3</span></span><br><span class="line">        <span class="comment"># Step 1</span></span><br><span class="line">        qkv = self.to_qvk(x)  <span class="comment"># [batch, tokens, dim*3*heads ]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2</span></span><br><span class="line">        <span class="comment"># decomposition to q,v,k and cast to tuple</span></span><br><span class="line">        <span class="comment"># the resulted shape before casting to tuple will be:</span></span><br><span class="line">        <span class="comment"># [3, batch, heads, tokens, dim_head]</span></span><br><span class="line">        q, k, v = tuple(rearrange(qkv, <span class="string">'b t (d k h) -&gt; k b h t d '</span>, k=<span class="number">3</span>, h=self.heads))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 3</span></span><br><span class="line">        <span class="comment"># resulted shape will be: [batch, heads, tokens, tokens]</span></span><br><span class="line">        scaled_dot_prod = torch.einsum(<span class="string">'b h i d , b h j d -&gt; b h i j'</span>, q, k) * self.scale_factor</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> mask.shape == scaled_dot_prod.shape[<span class="number">2</span>:]</span><br><span class="line">            scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)</span><br><span class="line"></span><br><span class="line">        attention = torch.softmax(scaled_dot_prod, dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 4. Calc result per batch and per head h</span></span><br><span class="line">        out = torch.einsum(<span class="string">'b h i j , b h j d -&gt; b h i d'</span>, attention, v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 5. Re-compose: merge heads with dim_head d</span></span><br><span class="line">        out = rearrange(out, <span class="string">"b h t d -&gt; b t (h d)"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 6. Apply final linear transformation layer</span></span><br><span class="line">        <span class="keyword">return</span> self.W_0(out)</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p><a href="https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/" target="_blank" rel="noopener">Einstein Summation in Numpy</a>, <em>OLEXA BILANIUK</em></p></li><li><p><a href="https://ajcr.net/Basic-guide-to-einsum/" target="_blank" rel="noopener">A basic introduction to NumPy’s einsum</a>, <em>Alex Riley</em></p></li><li><a href="https://rockt.github.io/2018/04/30/einsum" target="_blank" rel="noopener">EINSUM IS ALL YOU NEED - EINSTEIN SUMMATION IN DEEP LEARNING</a>, <em>Tim Rocktäschel</em> </li><li><a href="https://theaisummer.com/einsum-attention/" target="_blank" rel="noopener">Understanding einsum for Deep learning: implement a transformer with multi-head self-attention from scratch</a>, <em>Nikolas Adaloglou</em></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/einsum-attention.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;Einsum 表示法是对张量的复杂操作的一种优雅方式，本质上是使用特定领域的语言。 一旦理解并掌握了 einsum，可以帮助我们更快地编写更简洁高效的代码。&lt;/p&gt;
    
    </summary>
    
      <category term="博客转载" scheme="https://rogerspy.gitee.io/categories/%E5%8D%9A%E5%AE%A2%E8%BD%AC%E8%BD%BD/"/>
    
    
      <category term="einsum" scheme="https://rogerspy.gitee.io/tags/einsum/"/>
    
      <category term="MHSA" scheme="https://rogerspy.gitee.io/tags/mhsa/"/>
    
  </entry>
  
  <entry>
    <title>预训练语言模型：context2vec</title>
    <link href="https://rogerspy.gitee.io/2021/09/09/ptm-context2vec/"/>
    <id>https://rogerspy.gitee.io/2021/09/09/ptm-context2vec/</id>
    <published>2021-09-09T15:31:32.000Z</published>
    <updated>2022-01-12T08:37:38.399Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210909234144.png" alt></p><p>上下文的向量表示在许多 NLP 任务中都有至关重要的作用，比如词义消歧、命名实体识别、指代消解等等。以前的方法多是直接用离散上下文词向量组合，缺乏对上下文整体的优化表示方法。本文提出一种双向 LSTM 模型，有效学习句子上下文表征。</p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>通常词向量能获得单个词的语义语法信息，在训练词向量的时候是通过优化与任务无关的目标函数。为了推理出一个具体的词，好的上下文向量表示也是必须的。比如：</p><blockquote><p>我找不到【星期五】了。</p></blockquote><p>其中【星期五】可能是个人，可能是一个宠物等等。我们必须借助“我找不到【】了”才能确定“星期五”并不是一个表示日期的词。</p><p>通常上下文的表示有两种方式：</p><ol><li>无监督。使用上下文的词向量组成一个序列输入到模型，或者直接使用上下文词向量相加求平均。这种方式缺乏对上下文整体表征的优化。</li><li>监督学习。通过标注数据根据具体的任务训练上下文表征。这种方式有两个缺点：① 依赖标注数据，通常标注数据是很难得的；② 训练出来的上下文表征依赖具体的任务，很可能并没有学习到目标词与上下文的依赖关系。</li></ol><p>context2vec 通过在大规模的无标注数据上训练神经网络模型，直接对整个上下文和目标词进行编码，能够获得他们的依赖关系。将训练好的模型应用于下游的任务也获得了很好的表现。</p><h1 id="2-Context2vec-模型"><a href="#2-Context2vec-模型" class="headerlink" title="2. Context2vec 模型"></a>2. Context2vec 模型</h1><table><tr>    <td><img width="600" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210910000919.png"></td>    <td><img width="600" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210910000943.png"></td></tr></table>            <p>Context2vec 的主要目标是学习一个通用的与任务无关嵌入模型，用来表示目标词上下文的变长序列向量表示。我们借鉴了 word2vec 的 CBOW 模型，利用上下文来预测目标词。与 CBOW 不同的是，我们将原来的上下文向量求平均操作替换成了双向 LSTM 模型，如上右图所示。</p><blockquote><p>John [submitted] a paper</p></blockquote><ol><li><p>用双向 LSTM 作为特征抽取器；</p></li><li><p>一个 LSTM 输入句子序列是从左向右；另一个 LSTM 输入序列是从右向左；</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210911121347.png" style="zoom:67%;"></p></li><li><p>将目标词左侧（“John”）的 left-to-right 特征与目标词右侧（“a paper”）的 right-to-left 特征拼接起来；</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210911122227.png" style="zoom:67%;"></p></li><li><p>将拼接后的特征输入到 MLP 中，我们的目标是让 MLP 的输出等于 [submitted] 的向量。</p></li><li><p>采用 Word2vec 中的负采样方法训练神经网络参数，这样就能学到上下文向量和目标词向量。</p></li></ol><h1 id="3-形式化分析"><a href="#3-形式化分析" class="headerlink" title="3. 形式化分析"></a>3. 形式化分析</h1><p>定义：lLS 表示 left-to-right LSTM，rLS 表示 right-to-left LSTM。给定句子 $w_{1:n}$ 和目标词 $w_i$，那么双向 LSTM 的输出为：</p><script type="math/tex; mode=display">biLS(w_{1:n}, i)=\text{lLS}(l_{1:i-1})\oplus\text{rLS}(r_{n:i+1})</script><p>其中 $l$ 和 $r$ 分别表示句子中从左到右和从右到左的词向量。注意在本模型中句子的第 $0$ 个位置和第 $n+1$ 个位置分别表示 $\text{BOS}$ 和 $\text{EOS}$。我们并没有将目标词传入到 LSTM 中去。接下来：</p><script type="math/tex; mode=display">\text{MLP}(x) = L_2(\text{ReLU}(L_1(x)))</script><p>其中 $\text{ReLU}$ 表示激活函数，$L_i$  表示线性变换。令 $c=(w_1, …, w_{i-1}, w_{i+1}, …, w_n)$ 表示句子的上下文词向量。</p><script type="math/tex; mode=display">\vec{c}=\text{MLP}(\text{biLS}(w_{1:n}, i))</script><p>令目标词 $w_i$ 的词向量为 $\vec{t}$：</p><script type="math/tex; mode=display">S=\sum_{t,c}\left( \log\sigma(\vec{t}\cdot \vec{c})+\sum_{i=1}^k\log\sigma(-\vec{t}\cdot\vec{c})\right)</script><p>其中 $\sum_{c,t}$ 表示对训练语料中的每个 $(t,c)$  对求和，$t_1, …, t_k$ 表示负采样的样本。负采样的概率分布为：</p><script type="math/tex; mode=display">p_\alpha(t) \propto (\#t)^\alpha</script><p>$0\le\alpha\le1$ 表示一个平滑系数，$\alpha$ 越大越容易采样到罕见词。$#$ 表示统计个数。</p><p><a href="https://proceedings.neurips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf" target="_blank" rel="noopener">Levy &amp; Goldberg (2014)</a> 证明了将上式用于单字上下文时是可以优化的，当</p><script type="math/tex; mode=display">\vec{t}\cdot\vec{c}=\text{PMI}_\alpha(t,c)-\log(k)</script><p>其中 $\text{PMI}(t,c)=\log\frac{p(t,c)}{p_\alpha(t)p(c)}$ 表示目标词 $t$ 与 上下文 $c$ 的点互信息。 Levy &amp; Goldberg (2014) 的分析适用于两个随机变量的共现矩阵。在我们这里，上下文不是单字而是一个目标词的完整句子表达。据此，我们可以将模型得到的上下文向量视作所有可能的目标词与可能句子上下文的 $\text{PMI}$ 的矩阵分解。</p><p>最终我们注意到 $\alpha$ 越大，则目标词越偏向罕见词。</p><h1 id="4-模型验证"><a href="#4-模型验证" class="headerlink" title="4. 模型验证"></a>4. 模型验证</h1><p>为了验证模型的质量，我们提出三种相似度矩阵：</p><ol><li>target-to-context</li><li>context-to-context</li><li>target-to-target</li></ol><p>所有的相似度都用 $\cos(\cdot)$ 计算。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210911134102.png" style="zoom:67%;"></p><h2 id="4-1-target-to-context"><a href="#4-1-target-to-context" class="headerlink" title="4.1 target-to-context"></a>4.1 target-to-context</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210911134426.png" alt></p><p>当 $\alpha$ 取不同的值的时候，目标词的结果：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210911134541.png" alt></p><h2 id="4-2-context-to-context"><a href="#4-2-context-to-context" class="headerlink" title="4.2 context-to-context"></a>4.2 context-to-context</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210911134650.png" alt></p><h2 id="4-3-target-to-target"><a href="#4-3-target-to-target" class="headerlink" title="4.3 target-to-target"></a>4.3 target-to-target</h2><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210911134743.png" alt></p><h1 id="5-与语言模型的关系"><a href="#5-与语言模型的关系" class="headerlink" title="5. 与语言模型的关系"></a>5. 与语言模型的关系</h1><p>从我们对模型的介绍，以及 target-to-context 实验结果的分析可以看出，我们的模型和基于 LSTM 的语言模型很像。主要的区别在于 LSTM 语言模型给定目标词，优化模型的联合概率。然而 context2vec 的目标是学习通用的向量表示。我们采用了 Word2vec 的学习框架，但是我们利用 $\vec{t}\cdot\vec{v}$ 近似点互信息，而不是 $\log p(t|c)$。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://aclanthology.org/K16-1006.pdf" target="_blank" rel="noopener">context2vec: Learning Generic Context Embedding with Bidirectional LSTM</a>, <em>Oren Melamud, Jacob Goldberger, Ido Dagan. 2016</em></li><li><a href="https://proceedings.neurips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf" target="_blank" rel="noopener">Neural Word Embedding as Implicit Matrix Factorization</a>, <em>Omer Levy, Yoav Goldberg. 2014</em> </li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210909234144.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;上下文的向量表示在许多 NLP 任务中都有至关重要的作用，比如词义消歧、命名实体识别、指代消解等等。以前的方法多是直接用离散上下文词向量组合，缺乏对上下文整体的优化表示方法。本文提出一种双向 LSTM 模型，有效学习句子上下文表征。&lt;/p&gt;
    
    </summary>
    
      <category term="语言模型" scheme="https://rogerspy.gitee.io/categories/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="context2vec" scheme="https://rogerspy.gitee.io/tags/context2vec/"/>
    
  </entry>
  
  <entry>
    <title>预训练语言模型-Semi-supervised Sequence Learning</title>
    <link href="https://rogerspy.gitee.io/2021/09/07/ptm-semi-supervised-sequence-learning/"/>
    <id>https://rogerspy.gitee.io/2021/09/07/ptm-semi-supervised-sequence-learning/</id>
    <published>2021-09-07T15:01:19.000Z</published>
    <updated>2022-01-12T08:37:38.403Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210907233207.png" alt></p><p>之前我们介绍了 Word Embedding，将词转换成稠密向量。词向量中包含了大量的自然语言中的先验知识，word2vec 的成功证明了我们可以通过无监督学习获得这些先验知识。随后很多工作试图将句子、段落甚至文档也表示成稠密向量。其中比较有代表性的，比如：</p><a id="more"></a><ul><li>有监督学习<ol><li><a href="https://www.researchgate.net/publication/336156611_Convolutional_Recurrent_Neural_Networks_for_Text_Classification" target="_blank" rel="noopener">Recurrent Convolutional Neural Networks for Text Classification</a> </li><li><a href="https://arxiv.org/pdf/1408.5882.pdf" target="_blank" rel="noopener">Convolutional Neural Networks for Sentence Classification</a> </li></ol></li><li>无监督学习<ol><li><a href="https://arxiv.org/pdf/1405.4053.pdf" target="_blank" rel="noopener">Distributed Representations of Sentences and Documents</a></li><li><a href="https://www.researchgate.net/publication/279068396_Skip-Thought_Vectors" target="_blank" rel="noopener">Skip-Thought</a> 、<a href="https://arxiv.org/pdf/1803.02893.pdf" target="_blank" rel="noopener">Quick-thoughts</a>、<a href="https://rogerspy.github.io/2020/10/13/ptm-introduction/" target="_blank" rel="noopener">InferSent</a> </li></ol></li></ul><p>等等。纯粹的有监督学习是通过分类任务去学习网络参数，最终得到句子向量表示。纯粹的无监督学习是通过预测上下文，比如 skip-thought 利用了 word2vec  的思想，通过预测上下文句子来学习句子表示。</p><p>本文要介绍的这篇论文则是首先尝试使用大规模无标注数据进行预训练，然后将整个句子的向量序列作为有监督任务的初始化值的方法。该方法开创了后来的与训练语言模型+微调下游任务的 NLP 模型训练模式。</p><h1 id="1-Sequence-autoencoders"><a href="#1-Sequence-autoencoders" class="headerlink" title="1.  Sequence autoencoders"></a>1.  Sequence autoencoders</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210907233207.png" alt></p><p>序列自编码器与机器翻译的 seq2seq 架构很相似，主要有两点不同：</p><ol><li>seq2seq 是有监督模型，序列自编码器是无监督模型</li><li>seq2seq 输出是目标语言序列，而序列自编码器输出是输入的句子本身，所以叫做自编码器。</li></ol><p>这个模型中，编码器（绿色部分）和解码器（红色部分）的权重是一样的。</p><p>序列自编码器的一个重要性质就是可以使用大量无标注的数据训练语言模型，这对有限标注数据任务非常有帮助。</p><h1 id="2-Recurrent-language-models"><a href="#2-Recurrent-language-models" class="headerlink" title="2. Recurrent language models"></a>2. Recurrent language models</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210908002430.png" alt></p><p>将序列自编码器，去掉编码器我们就可以得到 LSTM。在我们的任务中，我们使用序列自编码器对 LSTM 的权重进行初始化，我们将使用语言模型初始化后的 LSTM 称之为 LM-LSTM。</p><p>我们再将 LM-LSTM 用于下游的分类任务。通常情况下，LSTM 使用最后一个隐层的输出来预测输入的标签。但是在我们的实验中也尝试了使用 LSTM 每一步输出线性递增组合的方式预测标签，这样我们可以将梯度传递到更靠前的位置上，减轻梯度消失带来的问题。</p><p>另外，我们还尝试了将序列自编码器和下游监督学习模型一起训练的方法，称之为“联合训练”。</p><h1 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3. Experiments"></a>3. Experiments</h1><ul><li><p>IMDB 数据集实验结果</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210908005116.png" alt></p></li><li><p>Rotten Tomatoes 数据集实验结果</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210908004411.png" alt></p></li><li><p>20 newsgroups 数据集实验结果</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210908004438.png" alt></p></li><li><p>DBpedia character level classification</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210908004455.png" alt></p></li><li><p>CIFAR-10 object classification</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210908004510.png" alt></p></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/pdf/1511.01432.pdf" target="_blank" rel="noopener">Semi-supervised Sequence Learning</a>, <em>Andrew M. Dai, Quoc V. Le, 2015, arxiv:1511.01432</em></li><li><a href="https://zhuanlan.zhihu.com/p/21313501" target="_blank" rel="noopener">Semi-supervised Sequence Learning</a>, <em>PaperWeekly, Zhihu</em> </li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210907233207.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;之前我们介绍了 Word Embedding，将词转换成稠密向量。词向量中包含了大量的自然语言中的先验知识，word2vec 的成功证明了我们可以通过无监督学习获得这些先验知识。随后很多工作试图将句子、段落甚至文档也表示成稠密向量。其中比较有代表性的，比如：&lt;/p&gt;
    
    </summary>
    
      <category term="语言模型" scheme="https://rogerspy.gitee.io/categories/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="半监督语言模型" scheme="https://rogerspy.gitee.io/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>数据结构与算法：双端队列（deque）</title>
    <link href="https://rogerspy.gitee.io/2021/09/05/ds-deque/"/>
    <id>https://rogerspy.gitee.io/2021/09/05/ds-deque/</id>
    <published>2021-09-04T17:12:57.000Z</published>
    <updated>2022-01-12T08:37:38.348Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png" alt></p><p>本文介绍双端队列，并用 Python 实现。</p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>双端队列（deque），顾名思义指的是队列的前端和后端都可以进行插入和删除。因此，它不遵循 FIFO 原则。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905144730.png" alt></p><p>双端队列有两种：</p><ul><li>输入限制型双端队列：这种队列中输入被限制在一端，而删除则可以两端同时进行；</li><li>输出限制型双端队列：这种队列只能在一端进行删除，而插入元素则可以两端同时进行。</li></ul><h1 id="2-双端队列的基本操作"><a href="#2-双端队列的基本操作" class="headerlink" title="2. 双端队列的基本操作"></a>2. 双端队列的基本操作</h1><p>下面我们以循环队列实现的双端队列为例尽心介绍。在循环队列中，如果队列是满的，那么我们就从头开始。</p><p>但是，用线性队列实现的双端队列中，如果队列是满的，队列就不允许再往里插入元素了。</p><p>展示双端队列基本操作之前，我们有一些预备工作：</p><ol><li>设置队列的大小 <code>n</code>；</li><li>定义两个指针 <code>FRONT=-1</code> 和 <code>REAR=0</code>。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905150638.png" alt></p><h2 id="2-1-在头部插入元素"><a href="#2-1-在头部插入元素" class="headerlink" title="2.1 在头部插入元素"></a>2.1 在头部插入元素</h2><ol><li><p>检查前端位置</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905181644.png" alt></p></li><li><p>如果 <code>FRONT &lt; 1</code>，重置 <code>FRONT=n-1</code>（最后一个索引）</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905181844.png" alt></p></li></ol><ol><li><p>否则， <code>FRONT-1</code></p></li><li><p>在 <code>FRONT</code> 的位置添加新元素</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905193549.png" alt></p></li></ol><h2 id="2-2-在尾部添加元素"><a href="#2-2-在尾部添加元素" class="headerlink" title="2.2 在尾部添加元素"></a>2.2 在尾部添加元素</h2><ol><li><p>检查队列是否是满队列</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905193906.png" alt></p></li><li><p>如果队列是满的，重置 <code>REAR=0</code></p></li><li><p>否则，<code>REAR=REAR+1</code></p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905194245.png" alt></p></li><li><p>在 <code>REAR</code> 的位置添加新元素</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905194423.png" alt></p></li></ol><h2 id="2-3-从头部删除元素"><a href="#2-3-从头部删除元素" class="headerlink" title="2.3 从头部删除元素"></a>2.3 从头部删除元素</h2><ol><li><p>检查队列是否为空</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905194650.png" alt></p></li><li><p>如果队列是空（即 <code>FRONT=-1</code>），不能删除元素。</p></li><li><p>如果队列只有一个元素（即 <code>FRONT=REAR</code>），设置 <code>FRONT=-1</code> 以及 <code>REAR=-1</code>。</p></li><li><p>否则如果 <code>FORNT=n-1</code>，则令 <code>FRONT</code>去到首位，即令 <code>FRONT=0</code>。</p></li><li><p>否则 <code>FRONT=FORNT+1</code>。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905195221.png" alt></p></li></ol><h2 id="2-4-从尾部删除元素"><a href="#2-4-从尾部删除元素" class="headerlink" title="2.4 从尾部删除元素"></a>2.4 从尾部删除元素</h2><ol><li><p>检查队列是否为空。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905195459.png" alt></p></li><li><p>如果队列为空（<code>FRONT=-1</code>），不能删除元素。</p></li><li><p>如果队列只有一个元素（即 <code>FRONT=REAR</code>），设置 <code>FRONT=-1</code> 以及 <code>REAR=-1</code>。</p></li><li><p>如果 <code>REAR</code> 在前面（<code>REAR=0</code>），则令 <code>REAR=n-1</code>。</p></li><li><p>否则 <code>REAR=REAR-1</code>。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905195900.png" alt></p></li></ol><h2 id="2-5-检查队列是否为空"><a href="#2-5-检查队列是否为空" class="headerlink" title="2.5 检查队列是否为空"></a>2.5 检查队列是否为空</h2><p>检查 <code>FRONT=-1</code>，如果为真则队列为空。</p><h2 id="2-6-检查队列是否为满"><a href="#2-6-检查队列是否为满" class="headerlink" title="2.6 检查队列是否为满"></a>2.6 检查队列是否为满</h2><p>如果 <code>FRONT=0</code> 以及 <code>REAR=n-1</code> 或者 <code>FRONT=REAR+1</code> 则队列为满。</p><h1 id="3-Python-实现双端队列"><a href="#3-Python-实现双端队列" class="headerlink" title="3. Python 实现双端队列"></a>3. Python 实现双端队列</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Deque implementaion in python</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Deque</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.items = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isEmpty</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.items == []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addRear</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        self.items.append(item)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addFront</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        self.items.insert(<span class="number">0</span>, item)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeFront</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.items.pop(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeRear</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.items.pop()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.items)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">d = Deque()</span><br><span class="line">print(d.isEmpty())</span><br><span class="line">d.addRear(<span class="number">8</span>)</span><br><span class="line">d.addRear(<span class="number">5</span>)</span><br><span class="line">d.addFront(<span class="number">7</span>)</span><br><span class="line">d.addFront(<span class="number">10</span>)</span><br><span class="line">print(d.size())</span><br><span class="line">print(d.isEmpty())</span><br><span class="line">d.addRear(<span class="number">11</span>)</span><br><span class="line">print(d.removeRear())</span><br><span class="line">print(d.removeFront())</span><br><span class="line">d.addFront(<span class="number">55</span>)</span><br><span class="line">d.addRear(<span class="number">45</span>)</span><br><span class="line">print(d.items)</span><br></pre></td></tr></table></figure><h1 id="4-时间复杂度"><a href="#4-时间复杂度" class="headerlink" title="4. 时间复杂度"></a>4. 时间复杂度</h1><p>上述操作的时间复杂度为 $O(1)$。</p><h1 id="5-双端队列的应用"><a href="#5-双端队列的应用" class="headerlink" title="5. 双端队列的应用"></a>5. 双端队列的应用</h1><ol><li>软件的撤销操作</li><li>浏览器存储浏览历史</li><li>用来实现队列和栈</li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.programiz.com/dsa/deque" target="_blank" rel="noopener">Deque Data Structure</a> </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;本文介绍双端队列，并用 Python 实现。&lt;/p&gt;
    
    </summary>
    
      <category term="数据结构与算法" scheme="https://rogerspy.gitee.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据结构" scheme="https://rogerspy.gitee.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="queue" scheme="https://rogerspy.gitee.io/tags/queue/"/>
    
  </entry>
  
  <entry>
    <title>数据结构与算法：优先队列（priority queue）</title>
    <link href="https://rogerspy.gitee.io/2021/09/05/ds-priority-queue/"/>
    <id>https://rogerspy.gitee.io/2021/09/05/ds-priority-queue/</id>
    <published>2021-09-04T17:12:16.000Z</published>
    <updated>2022-01-12T08:37:38.364Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png" alt></p><p>本文介绍优先队列，并用 Python 实现。</p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>优先队列是一种特殊的队列类型，队列中每个元素都包含一个优先级值。每个元素根据优先级的大小进行处理，即优先级越高，对应的元素越早处理。</p><p>但是，如果两个元素的优先级一样的话，根据他们在队列中的先后进行处理。</p><h2 id="1-1-分配优先级"><a href="#1-1-分配优先级" class="headerlink" title="1.1 分配优先级"></a>1.1 分配优先级</h2><p>通常情况下，元素值本身就是优先级。比如，元素值越高则优先级越高，或者元素值越低优先级越高。当然，我们也可以根据具体需要来设置优先级。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905125711.png" alt></p><h2 id="1-2-优先队列与常规队列的区别"><a href="#1-2-优先队列与常规队列的区别" class="headerlink" title="1.2 优先队列与常规队列的区别"></a>1.2 优先队列与常规队列的区别</h2><p>在常规队列中，遵守先进先出规则；而在优先队列中，根据优先级删除值，首先删除优先级最高的元素。</p><h2 id="1-3-优先队列的实现方式"><a href="#1-3-优先队列的实现方式" class="headerlink" title="1.3 优先队列的实现方式"></a>1.3 优先队列的实现方式</h2><p>优先队列的实现有多种方式，比如数组、链表、堆以及二叉树等。其中堆更加高效，所以下面我们以堆实现的优先队列为例进行介绍。因此，在此之前需要先了解堆数据结构：<a href="https://www.programiz.com/dsa/heap-sort#heap" target="_blank" rel="noopener">max-heap and mean-heap</a>。</p><p>不同实现方式的复杂度对比：</p><div class="table-container"><table><thead><tr><th>Operations</th><th>peek</th><th>insert</th><th>delete</th></tr></thead><tbody><tr><td>Linked List</td><td><code>O(1)</code></td><td><code>O(n)</code></td><td><code>O(1)</code></td></tr><tr><td>Binary Heap</td><td><code>O(1)</code></td><td><code>O(log n)</code></td><td><code>O(log n)</code></td></tr><tr><td>Binary Search Tree</td><td><code>O(1)</code></td><td><code>O(log n)</code></td><td><code>O(log n)</code></td></tr></tbody></table></div><h1 id="2-优先队列的基本操作"><a href="#2-优先队列的基本操作" class="headerlink" title="2. 优先队列的基本操作"></a>2. 优先队列的基本操作</h1><p>优先队列的基本操作包括：插入、删除、查询。</p><h2 id="2-1-插入"><a href="#2-1-插入" class="headerlink" title="2.1 插入"></a>2.1 插入</h2><p>通过下面的步骤向优先队列中插入元素（max-heap）:</p><ul><li><p>在树的末尾插入元素</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905131519.png" alt></p></li><li><p>将树进行<a href="https://www.programiz.com/dsa/heap-data-structure#heapify" target="_blank" rel="noopener">堆化</a></p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905131744.png" alt></p><p>在优先队列中（max-heap）插入元素的算法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">If there is no node, </span><br><span class="line">  create a newNode.</span><br><span class="line">else (a node is already present)</span><br><span class="line">  insert the newNode at the end (last node from left to right.)</span><br><span class="line">  </span><br><span class="line">heapify the array</span><br></pre></td></tr></table></figure><p>对于 Min heap，上面的算法中 <code>parentNode</code> 永远小于 <code>newNode</code>。</p></li></ul><h2 id="2-2-删除"><a href="#2-2-删除" class="headerlink" title="2.2 删除"></a>2.2 删除</h2><p>通过下面的步骤从优先队列中删除元素（max heap）：</p><ul><li><p>选择要删除的元素</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905132805.png" alt></p></li><li><p>与最后一个元素位置进行交换</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905132935.png" alt></p></li><li><p>删除最后一个元素</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905133111.png" alt></p></li><li><p>将树进行堆化</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905133211.png" alt></p></li></ul><p>从优先队列中删除元素的算法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">If nodeToBeDeleted is the leafNode</span><br><span class="line">  remove the node</span><br><span class="line">Else swap nodeToBeDeleted with the lastLeafNode</span><br><span class="line">  remove noteToBeDeleted</span><br><span class="line">   </span><br><span class="line">heapify the array</span><br></pre></td></tr></table></figure><p>对于 Min Heap，上面算法中的 <code>childNodes</code> 一直 <code>currentNode</code>。</p><h2 id="2-3-查询"><a href="#2-3-查询" class="headerlink" title="2.3 查询"></a>2.3 查询</h2><p>对于 Max heap，返回最大元素；对于 Min heap，返回最小值。</p><p>对于 Max heap 和 Min heap 来说，都是返回根节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">return rootNode</span><br></pre></td></tr></table></figure><h2 id="2-4-选取最大值最小值"><a href="#2-4-选取最大值最小值" class="headerlink" title="2.4 选取最大值最小值"></a>2.4 选取最大值最小值</h2><p>抽取最大值返回从最大堆中删除后具有最大值的节点，而抽取最小值则返回从最小堆中删除后具有最小值的节点。</p><h1 id="3-Python-实现优先队列"><a href="#3-Python-实现优先队列" class="headerlink" title="3. Python 实现优先队列"></a>3. Python 实现优先队列</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"># Priority Queue implementation in Python</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Function to heapify the tree</span><br><span class="line">def heapify(arr, n, i):</span><br><span class="line">    # Find the largest among root, left child and right child</span><br><span class="line">    largest = i</span><br><span class="line">    l = 2 * i + 1</span><br><span class="line">    r = 2 * i + 2</span><br><span class="line"></span><br><span class="line">    if l &lt; n and arr[i] &lt; arr[l]:</span><br><span class="line">        largest = l</span><br><span class="line"></span><br><span class="line">    if r &lt; n and arr[largest] &lt; arr[r]:</span><br><span class="line">        largest = r</span><br><span class="line"></span><br><span class="line">    # Swap and continue heapifying if root is not largest</span><br><span class="line">    if largest != i:</span><br><span class="line">        arr[i], arr[largest] = arr[largest], arr[i]</span><br><span class="line">        heapify(arr, n, largest)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Function to insert an element into the tree</span><br><span class="line">def insert(array, newNum):</span><br><span class="line">    size = len(array)</span><br><span class="line">    if size == 0:</span><br><span class="line">        array.append(newNum)</span><br><span class="line">    else:</span><br><span class="line">        array.append(newNum)</span><br><span class="line">        for i in range((size // 2) - 1, -1, -1):</span><br><span class="line">            heapify(array, size, i)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Function to delete an element from the tree</span><br><span class="line">def deleteNode(array, num):</span><br><span class="line">    size = len(array)</span><br><span class="line">    i = 0</span><br><span class="line">    for i in range(0, size):</span><br><span class="line">        if num == array[i]:</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">    array[i], array[size - 1] = array[size - 1], array[i]</span><br><span class="line"></span><br><span class="line">    array.remove(size - 1)</span><br><span class="line"></span><br><span class="line">    for i in range((len(array) // 2) - 1, -1, -1):</span><br><span class="line">        heapify(array, len(array), i)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">arr = []</span><br><span class="line"></span><br><span class="line">insert(arr, 3)</span><br><span class="line">insert(arr, 4)</span><br><span class="line">insert(arr, 9)</span><br><span class="line">insert(arr, 5)</span><br><span class="line">insert(arr, 2)</span><br><span class="line"></span><br><span class="line">print (&quot;Max-Heap array: &quot; + str(arr))</span><br><span class="line"></span><br><span class="line">deleteNode(arr, 4)</span><br><span class="line">print(&quot;After deleting an element: &quot; + str(arr))</span><br></pre></td></tr></table></figure><h1 id="5-优先队列的应用"><a href="#5-优先队列的应用" class="headerlink" title="5. 优先队列的应用"></a>5. 优先队列的应用</h1><ul><li>Dijkstra 算法</li><li>实现栈结构</li><li>操作系统中的负载平衡和中断处理</li><li>Huffman 编码的数据压缩</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.programiz.com/dsa/priority-queue" target="_blank" rel="noopener">Priority Queue</a> </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;本文介绍优先队列，并用 Python 实现。&lt;/p&gt;
    
    </summary>
    
      <category term="数据结构与算法" scheme="https://rogerspy.gitee.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据结构" scheme="https://rogerspy.gitee.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="queue" scheme="https://rogerspy.gitee.io/tags/queue/"/>
    
  </entry>
  
  <entry>
    <title>数据结构与算法：循环队列（circular-queue）</title>
    <link href="https://rogerspy.gitee.io/2021/09/05/ds-circular-queue/"/>
    <id>https://rogerspy.gitee.io/2021/09/05/ds-circular-queue/</id>
    <published>2021-09-04T16:00:33.000Z</published>
    <updated>2022-01-12T08:37:38.347Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png" alt></p><p>本文介绍循环队列，并用 Python 实现循环队列。</p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>循环队列是常规队列（简单队列）的变种，是将队列中最后一个元素与第一个元素相连。因此，循环队列看起来像下图的样子：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210905000636.png" alt></p><p>循环队列是为了解决简单队列的限制。在常规队列中，经过一系列的出队入队操作之后，会有一些空位置。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210904230231.png" alt></p><p>上图中 0 和 1 的位置会被空置，除非等到队列重置。</p><h1 id="2-循环队列的基本操作"><a href="#2-循环队列的基本操作" class="headerlink" title="2. 循环队列的基本操作"></a>2. 循环队列的基本操作</h1><p>循环队列通过循环递增的方式工作，即当我们递增指针并到达队列的末尾时，我们又从队列的开头开始。其中递增是通过模除队列的尺寸，即：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">if REAR + 1 == 5 (overflow!), REAR = (REAR + 1)%5 = 0 (start of queue)</span><br></pre></td></tr></table></figure><p>具体过程如下：</p><ul><li>两个指针 <code>FRONT</code> 和 <code>REAR</code></li><li><code>FRONT</code> 追踪队列中第一个元素</li><li><code>REAR</code> 追踪队列中最后一个元素</li><li>初始化 <code>FRONT</code> 和 <code>REAR</code> 为 -1</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/circular-queue-program.png" style="zoom:50%;"></p><h2 id="2-1-Enqueue"><a href="#2-1-Enqueue" class="headerlink" title="2.1 Enqueue"></a>2.1 Enqueue</h2><ul><li>检查队列是否是满队列</li><li>对于第一个元素，设置 <code>FRONT</code> 的值为 0</li><li>循环增加 <code>REAR</code> ，如果 <code>REAR</code> 到末尾，下一步就从头开始</li><li>在 <code>REAR</code> 指向的位置添加新元素</li></ul><h2 id="2-2-Dequeue"><a href="#2-2-Dequeue" class="headerlink" title="2.2 Dequeue"></a>2.2 Dequeue</h2><ul><li>检查队列是否为空</li><li>返回 <code>FRONT</code> 指向的值</li><li><code>FRONT</code> 循环加 1</li><li>对于最后一个元素，重置 <code>FRONT</code> 和 <code>REAR</code> 为 -1</li></ul><p>然而，检查满队列的时候，有一个新问题：</p><ul><li>第一种情况：<code>FRONT=0</code> &amp;&amp; <code>REAR=size-1</code></li><li>第二种情况：<code>FRONT=REAR+1</code></li></ul><p>第二种情况下，<code>REAR</code> 因为循环递增而从 0  开始，并且其值只比 <code>FRONT</code> 时，队列已满。</p><h1 id="3-Python实现循环队列"><a href="#3-Python实现循环队列" class="headerlink" title="3. Python实现循环队列"></a>3. Python实现循环队列</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Circular Queue implementation in Python</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCircularQueue</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k)</span>:</span></span><br><span class="line">        self.k = k</span><br><span class="line">        self.queue = [<span class="literal">None</span>] * k</span><br><span class="line">        self.head = self.tail = <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Insert an element into the circular queue</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">enqueue</span><span class="params">(self, data)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> ((self.tail + <span class="number">1</span>) % self.k == self.head):</span><br><span class="line">            print(<span class="string">"The circular queue is full\n"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> (self.head == <span class="number">-1</span>):</span><br><span class="line">            self.head = <span class="number">0</span></span><br><span class="line">            self.tail = <span class="number">0</span></span><br><span class="line">            self.queue[self.tail] = data</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.tail = (self.tail + <span class="number">1</span>) % self.k</span><br><span class="line">            self.queue[self.tail] = data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Delete an element from the circular queue</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dequeue</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> (self.head == <span class="number">-1</span>):</span><br><span class="line">            print(<span class="string">"The circular queue is empty\n"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> (self.head == self.tail):</span><br><span class="line">            temp = self.queue[self.head]</span><br><span class="line">            self.head = <span class="number">-1</span></span><br><span class="line">            self.tail = <span class="number">-1</span></span><br><span class="line">            <span class="keyword">return</span> temp</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            temp = self.queue[self.head]</span><br><span class="line">            self.head = (self.head + <span class="number">1</span>) % self.k</span><br><span class="line">            <span class="keyword">return</span> temp</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">printCQueue</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span>(self.head == <span class="number">-1</span>):</span><br><span class="line">            print(<span class="string">"No element in the circular queue"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> (self.tail &gt;= self.head):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.head, self.tail + <span class="number">1</span>):</span><br><span class="line">                print(self.queue[i], end=<span class="string">" "</span>)</span><br><span class="line">            print()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.head, self.k):</span><br><span class="line">                print(self.queue[i], end=<span class="string">" "</span>)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, self.tail + <span class="number">1</span>):</span><br><span class="line">                print(self.queue[i], end=<span class="string">" "</span>)</span><br><span class="line">            print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Your MyCircularQueue object will be instantiated and called as such:</span></span><br><span class="line">obj = MyCircularQueue(<span class="number">5</span>)</span><br><span class="line">obj.enqueue(<span class="number">1</span>)</span><br><span class="line">obj.enqueue(<span class="number">2</span>)</span><br><span class="line">obj.enqueue(<span class="number">3</span>)</span><br><span class="line">obj.enqueue(<span class="number">4</span>)</span><br><span class="line">obj.enqueue(<span class="number">5</span>)</span><br><span class="line">print(<span class="string">"Initial queue"</span>)</span><br><span class="line">obj.printCQueue()</span><br><span class="line"></span><br><span class="line">obj.dequeue()</span><br><span class="line">print(<span class="string">"After removing an element from the queue"</span>)</span><br><span class="line">obj.printCQueue()</span><br></pre></td></tr></table></figure><h1 id="4-循环队列时间复杂度"><a href="#4-循环队列时间复杂度" class="headerlink" title="4. 循环队列时间复杂度"></a>4. 循环队列时间复杂度</h1><p>基于数组实现的循环队列，其 enqueue 和 dequeue 时间复杂度都是 $O(1)$。</p><h1 id="5-循环队列的应用"><a href="#5-循环队列的应用" class="headerlink" title="5. 循环队列的应用"></a>5. 循环队列的应用</h1><ul><li>CPU 任务调度</li><li>内存管理</li><li>任务堵塞管理</li></ul><h1 id="Refernece"><a href="#Refernece" class="headerlink" title="Refernece"></a>Refernece</h1><p><a href="https://www.programiz.com/dsa/circular-queue" target="_blank" rel="noopener">Circular Queue Data Structure</a> </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;本文介绍循环队列，并用 Python 实现循环队列。&lt;/p&gt;
    
    </summary>
    
      <category term="数据结构与算法" scheme="https://rogerspy.gitee.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据结构" scheme="https://rogerspy.gitee.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="queue" scheme="https://rogerspy.gitee.io/tags/queue/"/>
    
  </entry>
  
  <entry>
    <title>数据结构与算法：队列（queue）</title>
    <link href="https://rogerspy.gitee.io/2021/09/04/ds-queue/"/>
    <id>https://rogerspy.gitee.io/2021/09/04/ds-queue/</id>
    <published>2021-09-04T14:21:26.000Z</published>
    <updated>2022-01-12T08:37:38.364Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png" alt></p><p>本文介绍队列数据结构，并用 Python 代码实现。</p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>队列是一个非常有用的数据结构。它与电影院外排队买票是一样的，先排先买。队列也是如此，遵循先进先出（First In First Out，FIFO）原则。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210904222908.png" alt></p><p>如上图所示，1 排在 2 前面，也会在 2 之前被删除。</p><p>在编程的术语中，将元素添加进队列中的操作叫做 “<em>enqueue</em>”，从队列中删除的操作叫做 “<em>dequeue</em>”。</p><h1 id="2-队列的基本操作"><a href="#2-队列的基本操作" class="headerlink" title="2. 队列的基本操作"></a>2. 队列的基本操作</h1><ul><li><strong>Enqueue</strong>：向队列中添加元素</li><li><strong>Dequeue</strong>：从队列中删除元素</li><li><strong>IsEmpty</strong>：判断队列是否为空</li><li><strong>IsFull</strong>：判断队列是否为满队列</li><li><strong>Peek</strong>：获取队列最前面的元素而不删除该元素</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/Queue-program-enqueue-dequeue.png" style="zoom: 50%;"></p><ol><li>定义两个指针 <code>FRONT</code> 和 <code>REAR</code></li><li><code>FRONT</code> 追踪队列中第一个元素</li><li><code>REAR</code> 追踪队列中最后一个元素 </li><li>初始化 <code>FRONT</code> 和 <code>REAR</code> 都为 -1</li></ol><h2 id="2-1-Enqueue-操作"><a href="#2-1-Enqueue-操作" class="headerlink" title="2.1 Enqueue 操作"></a>2.1 Enqueue 操作</h2><ul><li>检查队列是否为满序列</li><li>对于第一个元素，设置 <code>FRONT</code> 为 0</li><li><code>REAR</code> 索引加 1</li><li>在 <code>REAR</code> 指向的位置处添加新元素</li></ul><h2 id="2-2-Dequeue"><a href="#2-2-Dequeue" class="headerlink" title="2.2 Dequeue"></a>2.2 Dequeue</h2><ul><li>检查队列是否为空</li><li>返回 <code>FRONT</code> 指向的元素</li><li><code>FRONT</code> 的索引加 1</li><li>对于最后一个元素，重新设置 <code>FRONT</code> 和 <code>REAR</code> 为  -1</li></ul><h1 id="3-Python-实现队列"><a href="#3-Python-实现队列" class="headerlink" title="3. Python 实现队列"></a>3. Python 实现队列</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Queue implementation in Python</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Queue</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.queue = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add an element</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">enqueue</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        self.queue.append(item)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Remove an element</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dequeue</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(self.queue) &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> self.queue.pop(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Display  the queue</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">display</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(self.queue)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.queue)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">q = Queue()</span><br><span class="line">q.enqueue(<span class="number">1</span>)</span><br><span class="line">q.enqueue(<span class="number">2</span>)</span><br><span class="line">q.enqueue(<span class="number">3</span>)</span><br><span class="line">q.enqueue(<span class="number">4</span>)</span><br><span class="line">q.enqueue(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">q.display()</span><br><span class="line"></span><br><span class="line">q.dequeue()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"After removing an element"</span>)</span><br><span class="line">q.display()</span><br></pre></td></tr></table></figure><h1 id="4-队列的限制"><a href="#4-队列的限制" class="headerlink" title="4. 队列的限制"></a>4. 队列的限制</h1><p>如下图所示，经过一系列的入队和出队，队列的尺寸减小了。但是我们只能在队列重置（所有的元素都出队）的时候设置 0 和 1 索引。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210904230231.png" alt></p><p>对于队列的一种变种——循环队列来说，由于入队时尾指针向前追赶头指针；出队时头指针向前追赶尾指针，造成队空和队满时头尾指针均相等。因此，无法通过条件front==rear来判别队列是”空”是”满”。</p><h1 id="5-队列的时间复杂度"><a href="#5-队列的时间复杂度" class="headerlink" title="5. 队列的时间复杂度"></a>5. 队列的时间复杂度</h1><p>Enqueue 和 dequeue 操作在使用数组实现的队列中复杂度都是 $O(1)$。如果你用python中的 <code>pop(n)</code> 方法，那时间复杂度可能是 $O(n)$，取决于你要删除的元素的位置。</p><h1 id="6-队列的应用"><a href="#6-队列的应用" class="headerlink" title="6. 队列的应用"></a>6. 队列的应用</h1><ul><li>CPU  调度，硬盘调度。</li><li>当两个进程之间异步传输数据时，队列用于消息同步。</li><li>处理实时系统的中断。</li><li>呼叫中心电话系统使用队列将呼叫他们的人按顺序排列。</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.programiz.com/dsa/queue" target="_blank" rel="noopener">Queue Data Structure</a> </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;本文介绍队列数据结构，并用 Python 代码实现。&lt;/p&gt;
    
    </summary>
    
      <category term="数据结构与算法" scheme="https://rogerspy.gitee.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据结构" scheme="https://rogerspy.gitee.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>数据结构与算法：队列类型</title>
    <link href="https://rogerspy.gitee.io/2021/09/04/ds-types-queue/"/>
    <id>https://rogerspy.gitee.io/2021/09/04/ds-types-queue/</id>
    <published>2021-09-04T05:15:12.000Z</published>
    <updated>2022-01-12T08:37:38.373Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png" alt></p><p>本文介绍不同类型的队列数据结构。</p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>队列就像排队买票，先到先得。有四种不同的队列：</p><ul><li>简单队列（simple queue）</li><li>循环队列（circular queue）</li><li>优先队列（priority queue）</li><li>双端队列（double ended queue，deque）</li></ul><h1 id="2-简单队列"><a href="#2-简单队列" class="headerlink" title="2. 简单队列"></a>2. 简单队列</h1><p>在一个简单的队列中，插入发生在后面，移除发生在前面。 它严格遵循 FIFO（先进先出）规则。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210904233021.png" alt></p><p>更详细内容，查看 <a href="https://rogerspy.github.io//2021/09/05/ds-queue/" target="_blank" rel="noopener">数据结构与算法：队列（queue）</a>。</p><h1 id="3-循环队列"><a href="#3-循环队列" class="headerlink" title="3. 循环队列"></a>3. 循环队列</h1><p>循环队列是指，最后一个元素指向第一个元素，形成一个循环链。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210904233239.png" alt></p><p>与简单队列相比，循环队列的主要优点是更好的内存利用率。 如果最后一个位置已满而第一个位置为空，我们可以在第一个位置插入一个元素。 此操作在简单队列中是不可能的。</p><p>更详细的内容，查看 <a href="https://rogerspy.github.io/2021/09/05/ds-circular-queue/" target="_blank" rel="noopener">数据结构与算法：循环队列（circular-queue）</a>。</p><h1 id="4-优先队列"><a href="#4-优先队列" class="headerlink" title="4. 优先队列"></a>4. 优先队列</h1><p>优先级队列是一种特殊类型的队列，其中每个元素都与一个优先级相关联，并根据其优先级进行处理。 如果出现具有相同优先级的元素，则按照它们在队列中的顺序进行处理。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210904233542.png" alt></p><p>更详细的内容，查看 <a href="https://rogerspy.github.io/2021/09/05/ds-priority-deque/" target="_blank" rel="noopener">数据结构与算法：优先队列（priority queue）</a>。</p><h1 id="5-双端队列"><a href="#5-双端队列" class="headerlink" title="5. 双端队列"></a>5. 双端队列</h1><p>在双端队列中，可以从前面或后面执行元素的插入和删除。 因此，它不遵循 FIFO（先进先出）规则。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210904235805.png" alt></p><p>更详细的内容，查看 <a href="https://rogerspy.github.io/2021/09/05/ds-deque/" target="_blank" rel="noopener">数据结构与算法：双端队列（deque）</a>。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.programiz.com/dsa/types-of-queue" target="_blank" rel="noopener">Types of Queues</a> </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;本文介绍不同类型的队列数据结构。&lt;/p&gt;
    
    </summary>
    
      <category term="数据结构与算法" scheme="https://rogerspy.gitee.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据结构" scheme="https://rogerspy.gitee.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="queue" scheme="https://rogerspy.gitee.io/tags/queue/"/>
    
  </entry>
  
  <entry>
    <title>数据结构与算法：栈（stack）</title>
    <link href="https://rogerspy.gitee.io/2021/09/04/ds-stack/"/>
    <id>https://rogerspy.gitee.io/2021/09/04/ds-stack/</id>
    <published>2021-09-04T04:16:52.000Z</published>
    <updated>2022-01-12T08:37:38.365Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png" alt></p><p>本文介绍栈（stack）数据结构，并用 python 代码实现。</p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>栈是一个线性数据结构，遵循后进先出（Last In First Out，LIFO）的原则。这就意味着最后插入的元素会先被删除。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210904122325.png" style="zoom:80%;"></p><p>就像叠盘子一样，</p><ul><li>你可以在最上面放一个新盘子</li><li>拿盘子的时候也是从最上面开始拿</li></ul><p>如果想要最下面的盘子，你就必须先把上面所有的盘子先拿走。这就是栈的基本工作方式。</p><h1 id="2-栈的-LIFO-原则"><a href="#2-栈的-LIFO-原则" class="headerlink" title="2. 栈的 LIFO 原则"></a>2. 栈的 LIFO 原则</h1><p>用编程的术语来说，在栈最上面放置一个元素称之为 “<em>push</em>”，删除元素叫做 “<em>pop</em>”。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210904122919.png" style="zoom: 80%;"></p><h1 id="3-栈的基本操作"><a href="#3-栈的基本操作" class="headerlink" title="3. 栈的基本操作"></a>3. 栈的基本操作</h1><ul><li><strong>push</strong>：在栈上面添加一个元素 </li><li><strong>pop</strong>：从栈中删除一个元素</li><li><strong>isEmpty</strong>：判断栈是否为空</li><li><strong>isFull</strong>：判断栈是否是一个满栈</li><li><strong>peek</strong>：获取栈最上层的元素而不删除它</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210904124034.png" alt></p><ol><li>用 <code>TOP</code> 指针来追踪栈中最上层的元素</li><li>初始化栈的时候，我们设置设置指针为 -1，这样我们就可以通过判断 <code>TOP==-1</code> 来检查栈是否为空</li><li>当往栈里 push 数据的时候，我峨嵋你增加 <code>TOP</code> 的值，将新元素放置在 <code>TOP</code> 指定的位置</li><li>删除元素的时候，返回 <code>TOP</code> 指向的值，然后减小 <code>TOP</code> 值</li><li>向栈 push 数据的时候应该先检查栈是否已满</li><li>删除数据的时候，应该检查栈是否为空</li></ol><h1 id="4-用-Python-实现栈"><a href="#4-用-Python-实现栈" class="headerlink" title="4. 用 Python 实现栈"></a>4. 用 Python 实现栈</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Stack implementation in python</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Creating a stack</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_stack</span><span class="params">()</span>:</span></span><br><span class="line">    stack = []</span><br><span class="line">    <span class="keyword">return</span> stack</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Creating an empty stack</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_empty</span><span class="params">(stack)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> len(stack) == <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Adding items into the stack</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(stack, item)</span>:</span></span><br><span class="line">    stack.append(item)</span><br><span class="line">    print(<span class="string">"pushed item: "</span> + item)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Removing an element from the stack</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(stack)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> (check_empty(stack)):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"stack is empty"</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> stack.pop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">stack = create_stack()</span><br><span class="line">push(stack, str(<span class="number">1</span>))</span><br><span class="line">push(stack, str(<span class="number">2</span>))</span><br><span class="line">push(stack, str(<span class="number">3</span>))</span><br><span class="line">push(stack, str(<span class="number">4</span>))</span><br><span class="line">print(<span class="string">"popped item: "</span> + pop(stack))</span><br><span class="line">print(<span class="string">"stack after popping an element: "</span> + str(stack))</span><br></pre></td></tr></table></figure><h1 id="5-栈的时间复杂度"><a href="#5-栈的时间复杂度" class="headerlink" title="5. 栈的时间复杂度"></a>5. 栈的时间复杂度</h1><p>对于基于数组的栈实现，push 和 pop 操作都是常数时间，即 $O(1)$。</p><h1 id="6-栈的应用"><a href="#6-栈的应用" class="headerlink" title="6. 栈的应用"></a>6. 栈的应用</h1><p>尽管栈是非常简单的数据结构，但是它非常有用，最常见的应用如：</p><ul><li>词倒置。将词中的每个字符方法栈中，然后一个一个删除就可以了。因为栈是 LIFO 的，删除的时候就可以将词中的字符倒置过来。</li><li>编译器中，计算比如 <code>2+4/5*(7-9)</code> 的表达式的时候，用栈将表达式转化成前缀或者后缀的形式。</li><li>浏览器中，后退按钮用栈存储了所有你浏览过的网址（URL），每次你浏览一个新的网站的时候，它就会被加入到栈中，当你回退的时候，现在的网页就会被删除，然后回到倒数第二个页面。</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.programiz.com/dsa/stack" target="_blank" rel="noopener">Stack Data Structure</a> </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210820161802.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;本文介绍栈（stack）数据结构，并用 python 代码实现。&lt;/p&gt;
    
    </summary>
    
      <category term="数据结构与算法" scheme="https://rogerspy.gitee.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据结构" scheme="https://rogerspy.gitee.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="stack" scheme="https://rogerspy.gitee.io/tags/stack/"/>
    
  </entry>
  
  <entry>
    <title>知识图谱：知识建模（三）RDFS/OWL 词汇表</title>
    <link href="https://rogerspy.gitee.io/2021/08/26/kg-rdf-vocabulary/"/>
    <id>https://rogerspy.gitee.io/2021/08/26/kg-rdf-vocabulary/</id>
    <published>2021-08-26T14:11:35.000Z</published>
    <updated>2022-01-12T08:37:38.382Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/kgicon.png" alt></p><p>前面的文章介绍了知识建模，我们提到知识建模使用的是 RDF 知识表示法，而 RDFS 本质上是一个标准化语义词汇表。所以本文总结一些常用的 RDFS/OWL 的语义词汇。</p><a id="more"></a><h1 id="0-絮絮叨叨"><a href="#0-絮絮叨叨" class="headerlink" title="0. 絮絮叨叨"></a>0. 絮絮叨叨</h1><h2 id="0-1-RDF-XML"><a href="#0-1-RDF-XML" class="headerlink" title="0.1 RDF/XML"></a>0.1 RDF/XML</h2><p>在正式介绍 RDFS/OWL 词汇之前，相信很多小伙伴在看知识建模的时候就会有很多疑问。为什么一定要用 RDF？XML 好像也能胜任这份工作，RDF 和 XML 的区别是什么？RDF 标榜的让计算机理解语义体现在哪里？等等一系列的疑问。当然回答这些问题并不是本文的目的，本文只是总结 RDFS/OWL 的词汇。要想弄明白 RDF 到底是怎么一回事，这里推荐一些必读的书籍/文献，希望能帮助到有疑问的人。</p><ul><li><a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=08AF5BC897E861F62FE3800830B02E22?doi=10.1.1.91.8164&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener"> Where are the Semantics in the Semantic Web?</a>, <em>Michael Uschold</em></li><li><a href="https://www.w3.org/DesignIssues/RDF-XML.html" target="_blank" rel="noopener">Why RDF model is different from the XML model</a>, <em>Tim Berners-Lee</em></li><li><a href="https://www.researchgate.net/publication/279393307_A_Developer%27s_Guide_to_the_Semantic_Web" target="_blank" rel="noopener">A developer’s guide to semantic web</a>, <em>Liyang Yu</em></li><li><a href="http://www-900.ibm.com/developerWorks/cn/xml/x-xmlrdf/index.shtml#authorname" target="_blank" rel="noopener">XML+RDF——实现Web数据基于语义的描述</a>, <em>周竞涛、王明微</em></li></ul><h2 id="0-2-IRI-URI-URL-URN-的区别"><a href="#0-2-IRI-URI-URL-URN-的区别" class="headerlink" title="0.2 IRI, URI, URL, URN 的区别"></a>0.2 IRI, URI, URL, URN 的区别</h2><ul><li><p>URL</p><p><strong>Uniform Resource Locator</strong>，统一资源定位符。是用于在网络中传播和访问互联网资源的一个地址，只有通过特定的地址才能够访问的指定的资源或者网页，简而言之就是我们所说的网址，当然这样有些不太恰当，但是确实最容易被理解的，就如你必须通过 <code>https://www.baidu.com</code> 才能访问百度搜索页面，通过其他的链接都是不行的，这个地址就被称之为 URL。</p></li><li><p>URI</p><p><strong>Uniform Resource Identifier</strong>，统一资源标识符。也可以理解为标识、定位资源的字符串。字符集仅限于 US-ASCII 中（额外加一个百分号 %）， 不包括某些保留字符。URI 可用作定位器、名称或两者。 如果 URI 是定位器，则它描述了资源的主要访问机制。 如果一个 URI 是一个名称，它通过给它一个唯一的名称来标识一个资源。 许多人容易将 URL 和 URI 两者混淆，其实两者非常相似，但是也有所不同。URL 包含相对地址和绝对地址，URI 就属于绝对地址，所以 URL 包含 URI，简单的举例就是很多的网站可能都有 <code>/about</code> 这个路径，但是不同的域名或者 IP 访问到的就是不同的资源页面，所以这就只是一个标识，并不能标识其具体未知或者唯一性。</p></li><li><p>IRI</p><p><strong>Internationalized Resource Identifier</strong>，国际化资源标识符。和 URI 类似，区别在于 URI 使用的字符集有限制，所以没有办法兼容不同的文字语言，所以 IRI 就引入了 Unicode 字符来解决这个兼容问题，最后就有了国际化资源标识符（IRI）。</p></li><li><p>URN</p><p><strong>Uniform Resource Name</strong>，统一资源名称。旨在用作持久的，与位置无关的资源标识符。URN 可以提供一种机制，用于查找和检索定义特定命名空间的架构文件。尽管普通的 URL 可以提供类似的功能，但是在这方面，URN 更加强大并且更容易管理，因为 URN 可以引用多个 URL。子凡举个最简单的例子大家就明白了，那就是：磁力链接，它就是 URN 的一种实现，可以持久化的标识一个 BT 资源，资源分布式的存储在 P2P 网络中，无需中心服务器用户即可找到并下载它。</p></li></ul><p>总结一下：</p><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • SUMMARY            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            • IRI ⊃ URI <br>            • URI ⊃ URL <br>            • URI ⊃ URN <br>            • URL ∩ URN = ∅         </div>        </div>    </div><p>下面就进入今天的正题——RDFS/OWL 词汇表。本文摘自 <em>《语义网技术体系》 [瞿裕忠，胡伟，程龚 编著] 2015年版</em> 这本书。想查看完整版词汇表，可前往这两个网页：</p><ul><li><a href="https://www.w3.org/TR/rdf-schema/" target="_blank" rel="noopener">RDF Schema 1.1</a> </li><li><a href="https://www.w3.org/TR/2004/REC-owl-ref-20040210/" target="_blank" rel="noopener">OWL Web Ontology Language</a></li></ul><p>再啰嗦一句，除了以上两个 RDF 词汇表，还有一个 <a href="http://xmlns.com/foaf/spec/" target="_blank" rel="noopener">FOAF</a> 词汇表在对人物进行建模的时候通常会用到。但是这里就不再介绍，想了解更多可自行前往。</p><h1 id="1-序言"><a href="#1-序言" class="headerlink" title="1. 序言"></a>1. 序言</h1><p>RDF Schema（下文简称 RDFS） 是 RDF 词汇表的一个扩展版本（RDF 本身是一个知识表示模型，但同时也是一个词汇表）。RDFS 承认有许多技术可以用来描述类和属性的含义，例如 OWL。</p><p>本文中定义的语言由一组 RDF 资源组成，这些资源可用于在特定于应用程序的 RDF 词汇表中描述其他 RDF 资源。核心词汇 <code>rdfs</code> 非正式地称为命名空间中定义。该命名空间由 IRI 标识：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://www.w3.org/2000/01/rdf-schema#</span><br></pre></td></tr></table></figure><p>并且通常与前缀相关联 <code>rdfs:</code>。本规范还使用前缀 <code>rdf:</code>来指代 RDF 命名空间：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://www.w3.org/1999/02/22-rdf-syntax-ns#</span><br></pre></td></tr></table></figure><p>为了方便和可读性，本规范使用缩写形式来表示 IRI。形式 <code>prefix:suffix</code> 的名称应该被解释为一个 IRI，它由与后缀连接的前缀相关联的 IRI 组成。</p><h1 id="2-RDFS"><a href="#2-RDFS" class="headerlink" title="2. RDFS"></a>2. RDFS</h1><p>资源可以被分成不同的组，这些组称之为“类”（classes）。每个类别下包含的成员称之为“实例”。比如“人”是一个类，“张三”是一个“人”的实例。通常我们把 RDF 和 RDFS 合写成 RDF(S) 或 RDF/S。</p><p>下面分别介绍 RDF(S) 的核心词汇。</p><h2 id="2-1-Classes"><a href="#2-1-Classes" class="headerlink" title="2.1 Classes"></a>2.1 Classes</h2><p>资源可以分成不同的组，这些组就称之为“类”，组内的成员就称之为类的“实例”。我们用 IRI 来标识类，然后用 RDF 属性来描述类。两个不同的类可能有相同的实例，比如“张三”既可以是“导演”这个类，也可以是“演员”这个类。一个类也可能是他自己的实例。</p><blockquote><p>名词解释：“类的外延”</p><p>与一个类别相关的集合，我们称之为类的外延。类的外延集合中的每个成员都是类的实例。举个例子：</p><p>类：食物</p><p>类的外延：a = {鸡，鸭，鱼，肉}</p><p>类的实例：鸡，鸭，鱼，肉</p><p>例子中，“食物”作为一个类别，表示一个抽象概念。跟这个类别相关的一个集合 a 表示“食物”的外延，相对类来说类的外延是具体的概念。但是要注意 a 作为一个集合整体出现。而 a 中的每一个元素称之为实例。</p><p>当我们说“鸡肉是一种食物”的时候，实际上是表明“鸡肉”是“食物”这个概念的外延集合中的一员。</p><script type="math/tex; mode=display">\text{instance} \in a \rightarrow class</script></blockquote><h3 id="2-1-1-rdf-Resource"><a href="#2-1-1-rdf-Resource" class="headerlink" title="2.1.1 rdf:Resource"></a>2.1.1 rdf:Resource</h3><p>所有 RDF 描述的事物都是资源，即都是 <code>rdfs:Resource</code> 的实例。这是所有事物的类，其他所有类都是它的子类。<code>rdfs:Resource</code> 也是 <code>rdfs:Class</code> 的实例。</p><h3 id="2-1-2-rdf-Class"><a href="#2-1-2-rdf-Class" class="headerlink" title="2.1.2 rdf:Class"></a>2.1.2 rdf:Class</h3><p>对应“类”的概念，即资源的类。当定义一个新类的时候，表示该类的资源必须有一个 <code>rdf:type</code> 属性，属性值是 <code>rdfs:Class</code>。比如定义“导演”是一个新类，那么我们必须定义：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">导演 rdf:type rdfs:Class</span><br></pre></td></tr></table></figure><p>注意，如上所述，一个实例可能属于多个类，所以类成员是不互斥的。<code>rdfs:Class</code> 是 <code>rdfs:Class</code> 是实例。</p><h3 id="2-1-3-rdf-Literal"><a href="#2-1-3-rdf-Literal" class="headerlink" title="2.1.3 rdf:Literal"></a>2.1.3 rdf:Literal</h3><p><code>rdf:Literal</code> 表示类或属性的字面量类型，比如数字、字符串等。<code>rdfs:Literal</code> 是 <code>rdfs:Class</code> 的实例，同时也是 <code>rdfs:Resource</code> 的子类。</p><h3 id="2-1-4-rdfs-Property"><a href="#2-1-4-rdfs-Property" class="headerlink" title="2.1.4 rdfs:Property"></a>2.1.4 rdfs:Property</h3><p><code>rdfs:Property</code> 是 RDF 属性类，同时也是 <code>rdfs:Class</code> 的实例。</p><h2 id="2-2-Properties"><a href="#2-2-Properties" class="headerlink" title="2.2.  Properties"></a>2.2.  Properties</h2><p>在 RDF 中，RDF 属性表示 subject 资源和 object 资源之间的关系。为了下文解释方便，我们这里写下三元组的一般形式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">subject predicate object</span><br></pre></td></tr></table></figure><h2 id="2-2-1-rdfs-range"><a href="#2-2-1-rdfs-range" class="headerlink" title="2.2.1 rdfs:range"></a>2.2.1 rdfs:range</h2><p><code>rdfs:range</code> 是 <code>rdfs:Property</code> 的一个实例，用来指明一个属性的值域。例如三元组：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p rdfs:range c</span><br></pre></td></tr></table></figure><p>表示 p 是 <code>rdfs:range</code> 的一个实例， c 是 <code>rdfs:Class</code>  的一个实例。上面的三元组描述的是一个 predicate 是 p 的 object 是 c 的实例。</p><h3 id="2-2-2-rdfs-domain"><a href="#2-2-2-rdfs-domain" class="headerlink" title="2.2.2 rdfs:domain"></a>2.2.2 rdfs:domain</h3><p><code>rdfs:domain</code> 是 <code>rdfs:Property</code> 的一个实例，用来指明一个属性的定义域。例如三元组：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p rdfs:domain c</span><br></pre></td></tr></table></figure><p>表示 p 是 <code>rdfs:Property</code> 的一个实例，c 是 <code>rdfs:Class</code> 的实例。上面的三元组描述的是一个 predicate 是 p 的 subject 是 c 的实例。</p><p>其中，如果 p 有不止一个 <code>rdfs:domain</code> ，那么其对应的所有 subject 都是 c 的实例。</p><p>举个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">人 吃 食物  </span><br><span class="line"></span><br><span class="line">吃 rdf:type rdfs:Property</span><br><span class="line">吃 rdfs:domain 人</span><br><span class="line">吃 rdfs:range 食物</span><br></pre></td></tr></table></figure><p>翻译过来就是，“吃”表示一种属性（关系），它的主语是“人”，宾语是“食物”。</p><h3 id="2-2-3-rdf-type"><a href="#2-2-3-rdf-type" class="headerlink" title="2.2.3 rdf:type"></a>2.2.3 rdf:type</h3><p><code>rdf:type</code> 是 <code>rdf:Property</code> 的一个实例，用于描述一个资源是类的实例，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">R rdf:type C</span><br></pre></td></tr></table></figure><p>表示 C 是 <code>rdfs:Class</code> 的子类，并且 R 是 C 的实例。用一句通俗易懂的话就是，R 是一种 C，比如 <code>人 rdf:type 生物</code> 表示“人是一种动物”。实际上 <code>rdf:type</code> 表示 “is-a” 的关系，可以简写成 <code>a</code>。</p><h3 id="2-2-4-rdfs-subClassOf"><a href="#2-2-4-rdfs-subClassOf" class="headerlink" title="2.2.4 rdfs:subClassOf"></a>2.2.4 rdfs:subClassOf</h3><p><code>rdfs:subClassOf</code> 是 <code>rdfs:Property</code>  的一个实例，用来指明一个类的所有实例也是另一个类的实例，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C1 rdfs:subClassOf C2</span><br></pre></td></tr></table></figure><p>描述的是，C1 是 <code>rdfs:Class</code> 的一个实例，C2 是 <code>rdfs:Class</code> 的一个实例，并且 C1 是 C2 的一个子类。<code>rdfs:subClassOf</code> 是可传递的，即如果 a 是 b 的子类，b 是 c 的子类，那么 a 也是 c 的子类。</p><p><code>rdfs:subClassOf</code> 的 <code>rdfs:domain</code> 是 <code>rdfs:Class</code>。<code>rdfs:subClassOf</code> 的 <code>rdfs:range</code> 是 <code>rdfs:Class</code>。</p><h3 id="2-2-5-rdfs-subPropertyOf"><a href="#2-2-5-rdfs-subPropertyOf" class="headerlink" title="2.2.5 rdfs:subPropertyOf"></a>2.2.5 rdfs:subPropertyOf</h3><p><code>rdfs:subPropertyOf</code> 是 <code>rdfs:Property</code> 的一个实例，用来指明与一个资源相关的所有属性也与另一个资源相关，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">P1 rdfs:subPropertyOf P2</span><br></pre></td></tr></table></figure><p>描述了 P1 是 <code>rdfs:Property</code> 的一个实例，P2 也是 <code>rdfs:Property</code> 的一个实例，并且 P1 是 P2 的一个子属性。<code>rdfs:subPropertyOf</code> 是可传递性的。</p><p><code>rdfs:subPropertyOf</code> 的 <code>rdfs:domain</code> 是 <code>rdf:Property</code>。<code>rdfs:subPropertyOf</code> 的 <code>rdfs:range</code> 是 <code>rdf:Property</code>。</p><p>除了上面介绍的词之外， RD(S) 还有很多其他有用的词汇，这里不一一列举。下图展示了 RDF(S) 各个词汇之间的关系：</p><p> <img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20210830172323.png" alt></p><h3 id="2-2-6-RDFS-词汇总结"><a href="#2-2-6-RDFS-词汇总结" class="headerlink" title="2.2.6 RDFS 词汇总结"></a>2.2.6 RDFS 词汇总结</h3><h4 id="2-2-6-1-Classes"><a href="#2-2-6-1-Classes" class="headerlink" title="2.2.6.1 Classes"></a>2.2.6.1 Classes</h4><div class="table-container"><table><thead><tr><th>Class name</th><th>comment</th></tr></thead><tbody><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_resource" target="_blank" rel="noopener">rdfs:Resource</a></td><td>The class resource, everything.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_literal" target="_blank" rel="noopener">rdfs:Literal</a></td><td>The class of literal values, e.g. textual strings and integers.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_langstring" target="_blank" rel="noopener">rdf:langString</a></td><td>The class of language-tagged string literal values.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_html" target="_blank" rel="noopener">rdf:HTML</a></td><td>The class of HTML literal values.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_xmlliteral" target="_blank" rel="noopener">rdf:XMLLiteral</a></td><td>The class of XML literal values.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_class" target="_blank" rel="noopener">rdfs:Class</a></td><td>The class of classes.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_property" target="_blank" rel="noopener">rdf:Property</a></td><td>The class of RDF properties.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_datatype" target="_blank" rel="noopener">rdfs:Datatype</a></td><td>The class of RDF datatypes.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_statement" target="_blank" rel="noopener">rdf:Statement</a></td><td>The class of RDF statements.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_bag" target="_blank" rel="noopener">rdf:Bag</a></td><td>The class of unordered containers.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_seq" target="_blank" rel="noopener">rdf:Seq</a></td><td>The class of ordered containers.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_alt" target="_blank" rel="noopener">rdf:Alt</a></td><td>The class of containers of alternatives.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_container" target="_blank" rel="noopener">rdfs:Container</a></td><td>The class of RDF containers.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_containermembershipproperty" target="_blank" rel="noopener">rdfs:ContainerMembershipProperty</a></td><td>The class of container membership properties, rdf:_1, rdf:_2, …, all of which are sub-properties of ‘member’.</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_list" target="_blank" rel="noopener">rdf:List</a></td><td>The class of RDF Lists.</td></tr></tbody></table></div><h4 id="2-2-6-2-Properties"><a href="#2-2-6-2-Properties" class="headerlink" title="2.2.6.2 Properties"></a>2.2.6.2 Properties</h4><div class="table-container"><table><thead><tr><th>Property name</th><th>comment</th><th>domain</th><th>range</th></tr></thead><tbody><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_type" target="_blank" rel="noopener">rdf:type</a></td><td>The subject is an instance of a class.</td><td>rdfs:Resource</td><td>rdfs:Class</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_subclassof" target="_blank" rel="noopener">rdfs:subClassOf</a></td><td>The subject is a subclass of a class.</td><td>rdfs:Class</td><td>rdfs:Class</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_subpropertyof" target="_blank" rel="noopener">rdfs:subPropertyOf</a></td><td>The subject is a subproperty of a property.</td><td>rdf:Property</td><td>rdf:Property</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_domain" target="_blank" rel="noopener">rdfs:domain</a></td><td>A domain of the subject property.</td><td>rdf:Property</td><td>rdfs:Class</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_range" target="_blank" rel="noopener">rdfs:range</a></td><td>A range of the subject property.</td><td>rdf:Property</td><td>rdfs:Class</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_label" target="_blank" rel="noopener">rdfs:label</a></td><td>A human-readable name for the subject.</td><td>rdfs:Resource</td><td>rdfs:Literal</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_comment" target="_blank" rel="noopener">rdfs:comment</a></td><td>A description of the subject resource.</td><td>rdfs:Resource</td><td>rdfs:Literal</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_member" target="_blank" rel="noopener">rdfs:member</a></td><td>A member of the subject resource.</td><td>rdfs:Resource</td><td>rdfs:Resource</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_first" target="_blank" rel="noopener">rdf:first</a></td><td>The first item in the subject RDF list.</td><td>rdf:List</td><td>rdfs:Resource</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_rest" target="_blank" rel="noopener">rdf:rest</a></td><td>The rest of the subject RDF list after the first item.</td><td>rdf:List</td><td>rdf:List</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_seealso" target="_blank" rel="noopener">rdfs:seeAlso</a></td><td>Further information about the subject resource.</td><td>rdfs:Resource</td><td>rdfs:Resource</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_isdefinedby" target="_blank" rel="noopener">rdfs:isDefinedBy</a></td><td>The definition of the subject resource.</td><td>rdfs:Resource</td><td>rdfs:Resource</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_value" target="_blank" rel="noopener">rdf:value</a></td><td>Idiomatic property used for structured values.</td><td>rdfs:Resource</td><td>rdfs:Resource</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_subject" target="_blank" rel="noopener">rdf:subject</a></td><td>The subject of the subject RDF statement.</td><td>rdf:Statement</td><td>rdfs:Resource</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_predicate" target="_blank" rel="noopener">rdf:predicate</a></td><td>The predicate of the subject RDF statement.</td><td>rdf:Statement</td><td>rdfs:Resource</td></tr><tr><td><a href="https://www.w3.org/TR/rdf-schema/#ch_object" target="_blank" rel="noopener">rdf:object</a></td><td>The object of the subject RDF statement.</td><td>rdf:Statement</td><td>rdfs:Resource</td></tr></tbody></table></div><h1 id="3-OWL"><a href="#3-OWL" class="headerlink" title="3. OWL"></a>3. OWL</h1><p>由于 RDFS 的表达能力较弱，W3C 2004 年又发布了 Web Ontology Language（OWL）进一步提供更加丰富的知识表示和推理能力。OWL 以描述逻辑为理论基础，可以将概念和属于用结构化的形式表示出来。通过 RDF 中的链接可以是本体分布在不同的系统中，充分体现了其标准化，开放性，扩展性以及适应性。现在 OWL 已经是 W3C 推荐的本体建模标准。OWL 的命名空间是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://www.w3.org/2002/07/owl#</span><br></pre></td></tr></table></figure><p>OWL 提供 3 中表达能力不同的子语言：OWL Full，OWL DL，OWL Lite。其中任意一个都可以映射成一个完整的 RDF 图。</p><ul><li>OWL Full。完全兼容 RDFS，但超出经典一阶逻辑的范畴。与 OWL Full 相关的推理工具现在还在探索中。</li><li>OWL DL。是 OWL Full 的一个子集，表达能力相对较强，可以有效的支持逻辑推理，但不是完全兼容 RDFS。</li><li>OWL Lite。在 OWL DL 的基础上对允许使用公理做了进一步的限制。</li></ul><p>到了 2012 年，W3C 对原先版本的 OWL 进行了修订，发布新的 OWL 版本——OWL 2。OWL 2 对 OWL 向后兼容，包含了 3 个指定的概图：</p><ul><li>OWL 2 EL。允许以高效的多项式时间算法对类型的可满足性检查、分类和实例检查并进行推理，特别适合使用含有大量属性或类本体的应用。</li><li>OWL 2 QL。允许使用传统的关系数据库实现查询问答，特别适合使用大量实例数据并且以查询问答作为主要推理任务的应用。</li><li>OWL 2 RL。允许以一种比较直接的方式，使用基于规则的推理引擎，在不牺牲太多的表达能力的情况下实现大规模推理。</li></ul><h2 id="3-1-OWL-Document"><a href="#3-1-OWL-Document" class="headerlink" title="3.1 OWL Document"></a>3.1 OWL Document</h2><p>一般情况下，描述本体的文档都包含本体本身的信息。一个本体是一个资源，可以采用 OWL 和其他命名空间属性进行描述。这些描述被称为本体头部，通常位于本体文档的开始部分。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;rdf:RDF xmlns=&quot;http://www.semanticweb.org/qiuji/ontologies/2017/9/untitled-ontology-2#&quot;</span><br><span class="line">     xml:base=&quot;http://www.semanticweb.org/qiuji/ontologies/2017/9/untitled-ontology-2&quot;</span><br><span class="line">     xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;</span><br><span class="line">     xmlns:owl=&quot;http://www.w3.org/2002/07/owl#&quot;</span><br><span class="line">     xmlns:xml=&quot;http://www.w3.org/XML/1998/namespace&quot;</span><br><span class="line">     xmlns:untitled-ontology-22=&quot;http://www.semanticweb.org/ontologies/2017/9/untitled-ontology-2#&quot;</span><br><span class="line">     xmlns:xsd=&quot;http://www.w3.org/2001/XMLSchema#&quot;</span><br><span class="line">     xmlns:untitled-ontology-2=&quot;http://www.semanticweb.org/qiuji/ontologies/2017/9/untitled-ontology-2#&quot;</span><br><span class="line">     xmlns:rdfs=&quot;http://www.w3.org/2000/01/rdf-schema#&quot;&gt;</span><br><span class="line">    &lt;owl:Ontology rdf:about=&quot;http://www.semanticweb.org/ontologies/2017/9/untitled-ontology-2&quot;/&gt;</span><br></pre></td></tr></table></figure><h3 id="3-1-1-owl-imports"><a href="#3-1-1-owl-imports" class="headerlink" title="3.1.1 owl:imports"></a>3.1.1 owl:imports</h3><p>允许引用另一个包含定义的 OWL 本体，并将其含义作为定义本体的一部分。每个引用都包含一个 URI，它指向被导入的本体。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">@prefix : &lt;http://example.com/pwl/families/&gt; .</span><br><span class="line">@prefix otherOnt: &lt;http/example.org/otherOntologies/families/&gt; .</span><br><span class="line">@prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt; .</span><br><span class="line">@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .</span><br><span class="line">@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;</span><br><span class="line"></span><br><span class="line">&lt;http://example.com/pwl/families/&gt;</span><br><span class="line">    rdf:type owlOntology ;</span><br><span class="line">    owl:imports &lt;http/example.org/otherOntologies/families.owl&gt; .</span><br></pre></td></tr></table></figure><p>另外，还可以在本体头部添加有关版本的一些信息。相关属性包括：<code>owl:versionInfo</code>，<code>owl:priorVersion</code>，<code>owl:backwardCompatibleWith</code>，<code>owl:incompatibleWith</code> 等等。</p><h2 id="3-2-OWL-Classes"><a href="#3-2-OWL-Classes" class="headerlink" title="3.2 OWL Classes"></a>3.2 OWL Classes</h2><p>与 RDFS 类似， OWL 也有“类”的概念，也是表示我们对资源的分组，也有“类的外延”等概念。需要注意的是，在OWL 中，类的外延中的元素称之为个体（individual），和在 Protege 建模工具菜单栏中的 individual 是同一概念，都表示实例。</p><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • NOTE            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            OWL DL 和 OWL DL 中的资源不能同时是个体（individual）和类（class），即 class 和 individual 是互斥的。            另外，rdfs:Class 和 rdfs:Property 是被禁止使用的。        </div>        </div>    </div><p>从上面的介绍来看，OWL 被设计出来主要是对 RDFS  的逻辑推理能力进行补强。要进行推理我们首先要有一些公理。在 OWL 中采用“类描述”对 OWL 类进行解释描述，然后将 OWL 组合成类公理。</p><h3 id="3-2-1-类描述（class-description）"><a href="#3-2-1-类描述（class-description）" class="headerlink" title="3.2.1 类描述（class description）"></a>3.2.1 类描述（class description）</h3><p>类描述通过类名或通过指定未命名匿名类的类外延来描述 OWL 类。OWL 中有 6 中不同的类描述：</p><ol><li>类标识符（URI）</li><li>穷举组成一个类的个体（enumeration）</li><li>属性限制（property restriction）</li><li>多个类描述的交集（intersection）</li><li>多个类描述的并集（union）</li><li>一个类描述的补集（complement）</li></ol><p>类标识符相当于通过类名（URI）来描述一个类；穷举表示一个类包含可穷举的个体；一个类中的所有个体都要满足特定的属性限制。对于 4、5、6 来说，可以认为是逻辑与（AND）或（OR）非（NOT）操作。</p><h4 id="3-2-1-1-owl-Class"><a href="#3-2-1-1-owl-Class" class="headerlink" title="3.2.1.1 owl:Class"></a>3.2.1.1 owl:Class</h4><p><code>owl:Class</code> 表示一个明明资源是一个类别，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ex：Human rdf:type owl:Class .</span><br></pre></td></tr></table></figure><p>其中 <code>ex</code> 表示本体的命名空间。下面的例子我们都用 RDF/XML 语法进行举例，所以上面的例子改写成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Class rdf:ID=&quot;Human&quot;/&gt;</span><br></pre></td></tr></table></figure><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • NOTE            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            OWL Lite 和 OWL DL 中 <code>owl:Class</code> 必须用在所有的类描述上。        </div>        </div>    </div><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • NOTE            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            在 OWL Lite 和 OWL DL 中<code>owl:Class</code> 是 <code>rdfs:Class</code> 的子类。这个关系说明            在 RDFS 中，并不是所有的类在 OWL DL(Lite) 都是合法的。但是在 OWL Full 中二者是等价的。        </div>        </div>    </div><p>OWL 类标识符是预先定义好的，即 <code>owl:Thing</code> / <code>owl:Nothing</code>。<code>owl:Thing</code> 是所有 OWL 类的父类，而 <code>owl:Nothing</code> 是所有类的子类（可以认为就是空集）。</p><h4 id="3-2-1-2-owl-oneOf"><a href="#3-2-1-2-owl-oneOf" class="headerlink" title="3.2.1.2 owl:oneOf"></a>3.2.1.2 owl:oneOf</h4><p><code>owl:oneOf</code> 用来表示类描述中的穷举，它的值必须是类的实例。为了方便，我们可以用 <code>rdfs:parseType=&quot;Collection&quot;</code> ，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Class&gt;</span><br><span class="line">  &lt;owl:oneOf rdf:parseType=&quot;Collection&quot;&gt;</span><br><span class="line">    &lt;owl:Thing rdf:about=&quot;#Eurasia&quot;/&gt;</span><br><span class="line">    &lt;owl:Thing rdf:about=&quot;#Africa&quot;/&gt;</span><br><span class="line">    &lt;owl:Thing rdf:about=&quot;#NorthAmerica&quot;/&gt;</span><br><span class="line">    &lt;owl:Thing rdf:about=&quot;#SouthAmerica&quot;/&gt;</span><br><span class="line">    &lt;owl:Thing rdf:about=&quot;#Australia&quot;/&gt;</span><br><span class="line">    &lt;owl:Thing rdf:about=&quot;#Antarctica&quot;/&gt;</span><br><span class="line">  &lt;/owl:oneOf&gt;</span><br><span class="line">&lt;/owl:Class&gt;</span><br></pre></td></tr></table></figure><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • NOTE            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            OWL Lite 没有穷举。        </div>        </div>    </div><h4 id="3-2-1-3-owl-Restriction"><a href="#3-2-1-3-owl-Restriction" class="headerlink" title="3.2.1.3 owl:Restriction"></a>3.2.1.3 owl:Restriction</h4><p>属性限制是一类特殊的类描述。它用来描述所有个体都满足一定限制条件的匿名类。OWL 有两种属性限制：值限制和基数限制。</p><ul><li>所谓值限制指的是，限制属性的值域。</li><li>所谓基数限制指的是，限制属性的个数。</li></ul><p>OWL 还提供了全局基数限制：<code>owl:FunctionalProperty</code> 和 <code>owl:InverseFunctionalProperty</code>。</p><p><code>owl:Restriction</code> 是 <code>owl:Class</code> 的子类。一个限制类应该有一个三元组用 <code>owl:onProperty</code> 来连接属性和限制。</p><ul><li><p><strong>值限制</strong></p><ol><li><p><code>owl:allValuesFrom</code>：用来限制一个类的所有个体是否在指定的值域内。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Restriction&gt;</span><br><span class="line">  &lt;owl:onProperty rdf:resource=&quot;#hasParent&quot; /&gt;</span><br><span class="line">  &lt;owl:allValuesFrom rdf:resource=&quot;#Human&quot;  /&gt;</span><br><span class="line">&lt;/owl:Restriction&gt;</span><br></pre></td></tr></table></figure></li><li><p><code>owl:someValuesFrom</code>：用来限制一个类的所有个体中，至少有一个个体来源于指定的值域。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Restriction&gt;</span><br><span class="line">  &lt;owl:onProperty rdf:resource=&quot;#hasParent&quot; /&gt;</span><br><span class="line">  &lt;owl:someValuesFrom rdf:resource=&quot;#Physician&quot; /&gt;</span><br><span class="line">&lt;/owl:Restriction&gt;</span><br></pre></td></tr></table></figure></li><li><p><code>owl:hasValue</code>：用来限制一个类的所有个体中，至少有一个（语义上）等于指定的值。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Restriction&gt;</span><br><span class="line">  &lt;owl:onProperty rdf:resource=&quot;#hasParent&quot; /&gt;</span><br><span class="line">  &lt;owl:hasValue rdf:resource=&quot;#Clinton&quot; /&gt;</span><br><span class="line">&lt;/owl:Restriction&gt;</span><br></pre></td></tr></table></figure><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • NOTE            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            “语义上等价于”的意思是，V 不一定是指定的值，但是 V 和指定的值 V1 之间有一个 <code>owl:sameAs</code>的关系。        </div>        </div>    </div></li></ol></li><li><p><strong>基数限制</strong></p><ol><li><p><code>owl:maxCardinality</code>：用来限制一个类包含了最多 N 个语义不同的个体，其中 N 就是基数限制的值。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Restriction&gt;</span><br><span class="line">  &lt;owl:onProperty rdf:resource=&quot;#hasParent&quot; /&gt;</span><br><span class="line">  &lt;owl:maxCardinality rdf:datatype=&quot;&amp;xsd;nonNegativeInteger&quot;&gt;2&lt;/owl:maxCardinality&gt;</span><br><span class="line">&lt;/owl:Restriction&gt;</span><br></pre></td></tr></table></figure></li><li><p><code>owl:minCardinality</code>：用来限制一个类至少包含 N 个语义不同的个体，其中 N 就是基数限制的值。</p><p>比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Restriction&gt;</span><br><span class="line">  &lt;owl:onProperty rdf:resource=&quot;#hasParent&quot; /&gt;</span><br><span class="line">  &lt;owl:minCardinality rdf:datatype=&quot;&amp;xsd;nonNegativeInteger&quot;&gt;2&lt;/owl:minCardinality&gt;</span><br><span class="line">&lt;/owl:Restriction&gt;</span><br></pre></td></tr></table></figure><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • NOTE            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            一个类中的所有实例都要有 N 个属性。        </div>        </div>    </div></li><li><p><code>owl:cardinality</code>：用来限制一个类必须要有 N 个语义不同的个体，不能多也不能少。其中 N 就是基数限制的值。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Restriction&gt;</span><br><span class="line">  &lt;owl:onProperty rdf:resource=&quot;#hasParent&quot; /&gt;</span><br><span class="line">  &lt;owl:cardinality rdf:datatype=&quot;&amp;xsd;nonNegativeInteger&quot;&gt;2&lt;/owl:cardinality&gt;</span><br><span class="line">&lt;/owl:Restriction&gt;</span><br></pre></td></tr></table></figure></li></ol></li></ul><h4 id="3-2-1-4-Intersection-union-and-complement"><a href="#3-2-1-4-Intersection-union-and-complement" class="headerlink" title="3.2.1.4 Intersection, union and complement"></a>3.2.1.4 Intersection, union and complement</h4><ul><li><p><code>owl:intersectionOf</code>：连接一个类和一个类描述的列表，表示这个类的外延中的个体同时也是列表中所有类描述的外延成员。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Class&gt;</span><br><span class="line">  &lt;owl:intersectionOf rdf:parseType=&quot;Collection&quot;&gt;</span><br><span class="line">    &lt;owl:Class&gt;</span><br><span class="line">      &lt;owl:oneOf rdf:parseType=&quot;Collection&quot;&gt;</span><br><span class="line">        &lt;owl:Thing rdf:about=&quot;#Tosca&quot; /&gt;</span><br><span class="line">        &lt;owl:Thing rdf:about=&quot;#Salome&quot; /&gt;</span><br><span class="line">      &lt;/owl:oneOf&gt;</span><br><span class="line">    &lt;/owl:Class&gt;</span><br><span class="line">    &lt;owl:Class&gt;</span><br><span class="line">      &lt;owl:oneOf rdf:parseType=&quot;Collection&quot;&gt;</span><br><span class="line">        &lt;owl:Thing rdf:about=&quot;#Turandot&quot; /&gt;</span><br><span class="line">        &lt;owl:Thing rdf:about=&quot;#Tosca&quot; /&gt;</span><br><span class="line">      &lt;/owl:oneOf&gt;</span><br><span class="line">    &lt;/owl:Class&gt;</span><br><span class="line">  &lt;/owl:intersectionOf&gt;</span><br><span class="line">&lt;/owl:Class&gt;</span><br></pre></td></tr></table></figure><p><code>owl:intersectionOf</code> 可以看成逻辑连词。</p></li><li><p><code>owl:unionOf</code>：表示一个个体至少会出现在列表中的一个类中。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Class&gt;</span><br><span class="line">  &lt;owl:unionOf rdf:parseType=&quot;Collection&quot;&gt;</span><br><span class="line">    &lt;owl:Class&gt;</span><br><span class="line">      &lt;owl:oneOf rdf:parseType=&quot;Collection&quot;&gt;</span><br><span class="line">        &lt;owl:Thing rdf:about=&quot;#Tosca&quot; /&gt;</span><br><span class="line">        &lt;owl:Thing rdf:about=&quot;#Salome&quot; /&gt;</span><br><span class="line">      &lt;/owl:oneOf&gt;</span><br><span class="line">    &lt;/owl:Class&gt;</span><br><span class="line">    &lt;owl:Class&gt;</span><br><span class="line">      &lt;owl:oneOf rdf:parseType=&quot;Collection&quot;&gt;</span><br><span class="line">        &lt;owl:Thing rdf:about=&quot;#Turandot&quot; /&gt;</span><br><span class="line">        &lt;owl:Thing rdf:about=&quot;#Tosca&quot; /&gt;</span><br><span class="line">      &lt;/owl:oneOf&gt;</span><br><span class="line">    &lt;/owl:Class&gt;</span><br><span class="line">  &lt;/owl:unionOf&gt;</span><br><span class="line">&lt;/owl:Class&gt;</span><br></pre></td></tr></table></figure></li><li><p><code>owl:complementOf</code>：连接一个类和一个类描述，表示类外延中的个体不属于类描述的外延。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Class&gt;</span><br><span class="line">  &lt;owl:complementOf&gt;</span><br><span class="line">    &lt;owl:Class rdf:about=&quot;#Meat&quot;/&gt;</span><br><span class="line">  &lt;/owl:complementOf&gt;</span><br><span class="line">&lt;/owl:Class&gt;</span><br></pre></td></tr></table></figure></li></ul><h3 id="3-2-2-类公理"><a href="#3-2-2-类公理" class="headerlink" title="3.2.2 类公理"></a>3.2.2 类公理</h3><p>类描述通过类公理组合在一起用来定义一个类。这句话说起来很拗口，其实描述的道理很简单。就相当于我们要炒一盘菜，需要一些原材料（类描述），然后通过一些原则（类公理）将这些原料组合在一起形成一盘菜（类）。</p><p>OWL 提供了 3 个词汇，将类描述组合起来：</p><ul><li><code>rdfs:subClassOf</code></li><li><code>owl:equivalentClass</code></li><li><code>owl:disjointWith</code></li></ul><h4 id="3-2-2-1-rdfs-subClassOf"><a href="#3-2-2-1-rdfs-subClassOf" class="headerlink" title="3.2.2.1 rdfs:subClassOf"></a>3.2.2.1 rdfs:subClassOf</h4><script type="math/tex; mode=display">class\ description \quad \text{rdfs:subClassOf} \quad class\ description</script><p>这里的 <code>rdfs:subClassOf</code> 和 RDFS 中的一样。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Class rdf:ID=&quot;Opera&quot;&gt;</span><br><span class="line">  &lt;rdfs:subClassOf rdf:resource=&quot;#MusicalWork&quot; /&gt;</span><br><span class="line">&lt;/owl:Class&gt;</span><br></pre></td></tr></table></figure><h4 id="3-2-2-2-owl-equivalentClass"><a href="#3-2-2-2-owl-equivalentClass" class="headerlink" title="3.2.2.2 owl:equivalentClass"></a>3.2.2.2 owl:equivalentClass</h4><script type="math/tex; mode=display">class\ description\quad \text{owl:equivalentClass}\quad class\ description</script><p><code>owl:equivalentClass</code> 表示两个类描述有相同的类外延。最简单的形式是，两个命名类别是等价的。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Class rdf:about=&quot;#US_President&quot;&gt;</span><br><span class="line">  &lt;equivalentClass rdf:resource=&quot;#PrincipalResidentOfWhiteHouse&quot;/&gt;</span><br><span class="line">&lt;/owl:Class&gt;</span><br></pre></td></tr></table></figure><div class="container" style="margin-top:40px;margin-bottom:20px;">    <div style="background-color:#54c7ec;height:36px;line-height:36px;vertical-align:middle;">        <div style="margin-left:10px">            <font color="white" size="4">                • NOTE            </font>        </div>    </div>    <div style="background-color:#F3F4F7">        <div style="padding:15px 10px 15px 20px;line-height:1.5;">            <code>owl:equivalentClass</code> 的两个类并不表示两个类是等价的。        </div>        </div>    </div><p>比如上例中，“美国总统” 这个概念和“白宫的主要居民”这个概念并不一样。真正的语义等价应该用 <code>owl:sameAs</code>。</p><h4 id="3-2-2-3-owl-disjointWith"><a href="#3-2-2-3-owl-disjointWith" class="headerlink" title="3.2.2.3 owl:disjointWith"></a>3.2.2.3 owl:disjointWith</h4><script type="math/tex; mode=display">class\ description\quad \text{owl:disjointWith}\quad class\ description</script><p><code>owl:disjointWith</code> 表示两个类描述没有公共的个体，或者说两个类描述是互斥的。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:Class rdf:about=&quot;#Man&quot;&gt;</span><br><span class="line">  &lt;owl:disjointWith rdf:resource=&quot;#Woman&quot;/&gt;</span><br><span class="line">&lt;/owl:Class&gt;</span><br></pre></td></tr></table></figure><h2 id="3-3-Properties"><a href="#3-3-Properties" class="headerlink" title="3.3 Properties"></a>3.3 Properties</h2><p> OWL 有两种属性：对象属性（object property）和数据类型属性（datatype property）。对象属性用来连接两个实例，而数据类型属性连接一个实例和寿哥数据类型的字面量。换成比较容易理解的话就是，对象属性表示两个实体之间的关系，数据类型属性就是实体和属性之间的关系。比如</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">小明 父亲 大明</span><br><span class="line">小明 生日 1990/1/1</span><br></pre></td></tr></table></figure><p>其中“父亲”就是对象属性，“生日”就是数据类型属性。</p><p>OWL 中支持的属性机构包括：</p><ul><li>RDFS ：<code>rdfs:subPropertyOf</code>, <code>rdfs:domain</code> 和 <code>rdfs:range</code></li><li>与其他属性相关的： <code>owl:equivalentProperty</code> 和 <code>owl:inverseOf</code></li><li>全局基数限制：<code>owl:FunctionalProperty</code> 和 <code>owl:InverseFunctionalProperty</code></li><li>逻辑属性： <code>owl:SymmetricProperty</code> 和 <code>owl:TransitiveProperty</code></li></ul><h3 id="3-3-1-owl-equivalentProperty"><a href="#3-3-1-owl-equivalentProperty" class="headerlink" title="3.3.1 owl:equivalentProperty"></a>3.3.1 owl:equivalentProperty</h3><p><code>owl:equivalentProperty</code> 表示两个属性有相同的属性外延。类似 <code>owl:equivalentClass</code>。</p><h3 id="3-3-2-owl-inverseOf"><a href="#3-3-2-owl-inverseOf" class="headerlink" title="3.3.2 owl:inverseOf"></a>3.3.2 owl:inverseOf</h3><p>属性是有方向的，从定义域指向值域。<code>owl:inverseOf</code> 表示反向属性，即原属性的定义域和值域互换。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:ObjectProperty rdf:ID=&quot;hasChild&quot;&gt;</span><br><span class="line">  &lt;owl:inverseOf rdf:resource=&quot;#hasParent&quot;/&gt;</span><br><span class="line">&lt;/owl:ObjectProperty&gt;</span><br></pre></td></tr></table></figure><h3 id="3-3-3-owl-FunctionalProperty"><a href="#3-3-3-owl-FunctionalProperty" class="headerlink" title="3.3.3 owl:FunctionalProperty"></a>3.3.3 owl:FunctionalProperty</h3><p><code>owl:FunctionalProperty</code> 表示对于实例 $x$ 来说，只有唯一的 $y$ 值。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:ObjectProperty rdf:ID=&quot;husband&quot;&gt;</span><br><span class="line">  &lt;rdfs:domain rdf:resource=&quot;#Woman&quot; /&gt;</span><br><span class="line">  &lt;rdfs:range  rdf:resource=&quot;#Man&quot; /&gt;</span><br><span class="line">&lt;/owl:ObjectProperty&gt;</span><br><span class="line"></span><br><span class="line">&lt;owl:FunctionalProperty rdf:about=&quot;#husband&quot; /&gt;</span><br></pre></td></tr></table></figure><h3 id="3-3-4-owl-InverseFunctionalProperty"><a href="#3-3-4-owl-InverseFunctionalProperty" class="headerlink" title="3.3.4 owl:InverseFunctionalProperty"></a>3.3.4 owl:InverseFunctionalProperty</h3><p><code>owl:InverseFunctionalProperty</code> 表示与 <code>owl:FunctionalProperty</code> 相反的意思，即对于值 $y$ 只能有一个 实例 $x$ 与之对应。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:InverseFunctionalProperty rdf:ID=&quot;biologicalMotherOf&quot;&gt;</span><br><span class="line">  &lt;rdfs:domain rdf:resource=&quot;#Woman&quot;/&gt;</span><br><span class="line">  &lt;rdfs:range rdf:resource=&quot;#Human&quot;/&gt;</span><br><span class="line">&lt;/owl:InverseFunctionalProperty&gt;</span><br></pre></td></tr></table></figure><h3 id="3-3-5-owl-TransitiveProperty"><a href="#3-3-5-owl-TransitiveProperty" class="headerlink" title="3.3.5 owl:TransitiveProperty"></a>3.3.5 owl:TransitiveProperty</h3><p><code>owl:TransitiveProperty</code> 表示属性的可传递性。如果 $(x,y)$ 是 P 的实例，$(y,z)$ 也是 P 的实例，那么 $(x,z)$ 也是 P 的实例。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:TransitiveProperty rdf:ID=&quot;subRegionOf&quot;&gt;</span><br><span class="line">  &lt;rdfs:domain rdf:resource=&quot;#Region&quot;/&gt;</span><br><span class="line">  &lt;rdfs:range  rdf:resource=&quot;#Region&quot;/&gt;</span><br><span class="line">&lt;/owl:TransitiveProperty&gt;</span><br></pre></td></tr></table></figure><h3 id="3-3-6-owl-SymmetricProperty"><a href="#3-3-6-owl-SymmetricProperty" class="headerlink" title="3.3.6 owl:SymmetricProperty"></a>3.3.6 owl:SymmetricProperty</h3><p><code>owl:SymmetricProperty</code> 表示如果 $(x,y)$ 是 P 的实例，那么 $(y,x)$ 也是 P 的实例。比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:SymmetricProperty rdf:ID=&quot;friendOf&quot;&gt;</span><br><span class="line">  &lt;rdfs:domain rdf:resource=&quot;#Human&quot;/&gt;</span><br><span class="line">  &lt;rdfs:range  rdf:resource=&quot;#Human&quot;/&gt;</span><br><span class="line">&lt;/owl:SymmetricProperty&gt;</span><br></pre></td></tr></table></figure><h2 id="3-4-Individuals"><a href="#3-4-Individuals" class="headerlink" title="3.4 Individuals"></a>3.4 Individuals</h2><p>个体分为两种：</p><ol><li>类的成员和个体的属性值</li><li>个体身份</li></ol><h3 id="3-4-1-类的成员和个体属性值"><a href="#3-4-1-类的成员和个体属性值" class="headerlink" title="3.4.1 类的成员和个体属性值"></a>3.4.1 类的成员和个体属性值</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;Opera rdf:ID=&quot;Tosca&quot;&gt;</span><br><span class="line">  &lt;hasComposer rdf:resource=&quot;#Giacomo_Puccini&quot;/&gt;</span><br><span class="line">  &lt;hasLibrettist rdf:resource=&quot;#Victorien_Sardou&quot;/&gt;</span><br><span class="line">  &lt;hasLibrettist rdf:resource=&quot;#Giuseppe_Giacosa&quot;/&gt;</span><br><span class="line">  &lt;hasLibrettist rdf:resource=&quot;#Luigi_Illica&quot;/&gt;</span><br><span class="line">  &lt;premiereDate rdf:datatype=&quot;&amp;xsd;date&quot;&gt;1900-01-14&lt;/premiereDate&gt;</span><br><span class="line">  &lt;premierePlace rdf:resource=&quot;#Roma&quot;/&gt;</span><br><span class="line">  &lt;numberOfActs rdf:datatype=&quot;&amp;xsd;positiveInteger&quot;&gt;3&lt;/numberOfActs&gt; </span><br><span class="line">&lt;/Opera&gt;</span><br></pre></td></tr></table></figure><h3 id="3-4-2-个体身份"><a href="#3-4-2-个体身份" class="headerlink" title="3.4.2 个体身份"></a>3.4.2 个体身份</h3><p>通常我们会给不同的事物取不同的名字，但是我们并不能保证不重名。比如“苹果”既可以是电子产品，也可以是水果。为了对个体的身份进行区分或合并，OWL 也设计了一套词汇：</p><ul><li><code>owl:sameAs</code>：表明是相同的个体，只是名字不同</li><li><code>owl:differentFrom</code>：表明是两个不同的个体</li><li><code>owl:AllDifferent</code>：表明列表中所有的个体都不相同</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;rdf:Description rdf:about=&quot;#William_Jefferson_Clinton&quot;&gt;</span><br><span class="line">  &lt;owl:sameAs rdf:resource=&quot;#BillClinton&quot;/&gt;</span><br><span class="line">&lt;/rdf:Description&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;Opera rdf:ID=&quot;Don_Giovanni&quot;/&gt;</span><br><span class="line"></span><br><span class="line">&lt;Opera rdf:ID=&quot;Nozze_di_Figaro&quot;&gt;</span><br><span class="line">  &lt;owl:differentFrom rdf:resource=&quot;#Don_Giovanni&quot;/&gt;</span><br><span class="line">&lt;/Opera&gt;</span><br><span class="line"></span><br><span class="line">&lt;Opera rdf:ID=&quot;Cosi_fan_tutte&quot;&gt;</span><br><span class="line">  &lt;owl:differentFrom rdf:resource=&quot;#Don_Giovanni&quot;/&gt;</span><br><span class="line">  &lt;owl:differentFrom rdf:resource=&quot;#Nozze_di_Figaro&quot;/&gt;</span><br><span class="line">&lt;/Opera&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;owl:AllDifferent&gt;</span><br><span class="line">  &lt;owl:distinctMembers rdf:parseType=&quot;Collection&quot;&gt;</span><br><span class="line">    &lt;Opera rdf:about=&quot;#Don_Giovanni&quot;/&gt;</span><br><span class="line">    &lt;Opera rdf:about=&quot;#Nozze_di_Figaro&quot;/&gt;</span><br><span class="line">    &lt;Opera rdf:about=&quot;#Cosi_fan_tutte&quot;/&gt;</span><br><span class="line">    &lt;Opera rdf:about=&quot;#Tosca&quot;/&gt;</span><br><span class="line">    &lt;Opera rdf:about=&quot;#Turandot&quot;/&gt;</span><br><span class="line">    &lt;Opera rdf:about=&quot;#Salome&quot;/&gt;</span><br><span class="line">  &lt;/owl:distinctMembers&gt;</span><br><span class="line">&lt;/owl:AllDifferent&gt;</span><br></pre></td></tr></table></figure><h1 id="4-结语"><a href="#4-结语" class="headerlink" title="4. 结语"></a>4. 结语</h1><p>关于 RDFS 和 OWL 的词汇总结我们就介绍这么多。当然，这些都只是一小部分，要想看完整版的推荐看 w3c 的官方文档。我们总结出来的这些词汇是比较常用的，同时也是有助于帮助不了解本体，不了解知识建模的同学对这些东西有一个大体的概念。其实本体建模就是在构建一套逻辑体系，这套逻辑体系帮助计算机进行逻辑推理。而无论是 RDFS 还是 OWL 亦或是其他众多我们没有介绍的词汇表都是在尝试将这样一个逻辑体系进行标准化。先阶段计算机的逻辑推理能力仍然处于很弱的阶段，说明我们现在的工作仍然很初级。我们这里总结的相关内容也许在不久的将来就会过期，失效甚至被推翻。但是了解这些知识也有助于我们对未来的发展有一个清晰的认知。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.w3.org/TR/rdf-schema/" target="_blank" rel="noopener">RDF Schema 1.1</a></li><li><a href="https://www.w3.org/TR/2004/REC-owl-ref-20040210/" target="_blank" rel="noopener">OWL Web Ontology Language</a> </li><li><a href="https://fusion.cs.uni-jena.de/fusion/blog/2016/11/18/iri-uri-url-urn-and-their-differences/" target="_blank" rel="noopener">IRI, URI, URL, URN and their differences</a>, <em>JAN MARTIN KEIL</em> </li><li><a href="https://zhangzifan.com/t/7393.html" target="_blank" rel="noopener">浅谈什么是 URL、URI、IRI、URN 及之间的区别</a>, <em>张子凡</em> </li><li>语义网技术体系 [瞿裕忠，胡伟，程龚 编著] 2015年版</li><li><a href="https://www.jianshu.com/p/9e2bfa9a5a06" target="_blank" rel="noopener">知识图谱-浅谈RDF、OWL、SPARQL</a>, <em>吕不韦</em> </li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/kgicon.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;前面的文章介绍了知识建模，我们提到知识建模使用的是 RDF 知识表示法，而 RDFS 本质上是一个标准化语义词汇表。所以本文总结一些常用的 RDFS/OWL 的语义词汇。&lt;/p&gt;
    
    </summary>
    
      <category term="知识图谱" scheme="https://rogerspy.gitee.io/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
    
      <category term="KG" scheme="https://rogerspy.gitee.io/tags/kg/"/>
    
      <category term="knowledge-modelling" scheme="https://rogerspy.gitee.io/tags/knowledge-modelling/"/>
    
  </entry>
  
  <entry>
    <title>预训练语言模型：CoVe</title>
    <link href="https://rogerspy.gitee.io/2021/08/25/ptm-cove/"/>
    <id>https://rogerspy.gitee.io/2021/08/25/ptm-cove/</id>
    <published>2021-08-25T13:55:11.000Z</published>
    <updated>2022-01-17T02:20:37.593Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://aylien.com/images/uploads/general/tumblr_inline_o8tinsmw081u37g00_540.png" alt></p><p>上一篇文章我们介绍了预训练词向量，它的缺点很明显：一旦训练完，每个词的词向量都固定下来了。而我们平时生活中面临的情况却复杂的多，一个最重要的问题就是一词多义，即同一个词在不同语境下有不同的含义。<a href="https://arxiv.org/pdf/1708.00107.pdf" target="_blank" rel="noopener">CoVe（Contextual Word Vectors）</a>同样是用来表示词向量的模型，但不同于 <a href="https://rogerspy.github.io/2021/08/11/ptm-word-embedding/" target="_blank" rel="noopener">word emebdding</a>，它是将整个序列作为输入，根据不同序列得到不同的词向量输出的函数。也就是说，CoVe 会根据不同的上下文得到不同的词向量表示。</p><a id="more"></a><h1 id="1-神经网络机器翻译"><a href="#1-神经网络机器翻译" class="headerlink" title="1. 神经网络机器翻译"></a>1. 神经网络机器翻译</h1><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/nmt-recap.png" alt></p><p>上图是一个经典的 attention seq2seq 模型：</p><ul><li><p>源语言 $x = [x_1, x_2, …, x_n]$;</p></li><li><p>目标语言：$y = [y_1, y_2, …, y_m]$;</p></li><li><p>用 <a href="https://rogerspy.github.io/2021/08/11/ptm-word-embedding/" target="_blank" rel="noopener">GloVe</a> 将源语言的词转换成词向量；</p></li><li><p>编码器是 bi-LSTM，输出一个隐状态序列：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}h &= [h_1, h_2, ..., h_n] \\  &= \text{bi-LSTM}(\text{GloVe}(x))\end{aligned}\end{equation}</script><p>其中 $h_t = [\overrightarrow{h_t}; \overleftarrow{h_t}]$，$\overrightarrow{h_t}=\text{LSTM}(x_t, \overrightarrow{h}_{t-1})$；$\overleftarrow{h_t}=\text{LSTM}(x_t, \overleftarrow{h}_{t-1})$。</p></li><li><p>注意力加持的解码器：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}\text{decoder hidden state:} \quad s_t &= \text{LSTM}([z_{t-1};\tilde{h}_{t-1}], s_{t-1}) \\\text{attention weights:} \quad \alpha_t &= \text{softmax}(H(W_1s_t+b_1)) \\\text{context-adjusted hidden state:} \quad \tilde{h}_t &= \tanh(W_2[H^\top \alpha_t; s_t]+b_2) \\\text{decoder output: } \quad p(y_t|H, y_1, ..., y_{t-1}) &=\text{softmax}(W_{out}\tilde{h}_t+b_{out})\end{aligned} \end{equation}</script></li></ul><p>seq2seq 训练完成之后，将编码器的输出作为 CoVe 用于下游任务。</p><h1 id="2-CoVe-在下游任务中的应用"><a href="#2-CoVe-在下游任务中的应用" class="headerlink" title="2. CoVe 在下游任务中的应用"></a>2. CoVe 在下游任务中的应用</h1><p>seq2seq 编码器的隐状态作为下游任务的语义向量：</p><script type="math/tex; mode=display">\text{CoVe}(x) = \text{bi-LSTM}(\text{GloVe}(x))</script><p>论文中提出将 GloVe 和 CoVe 进行拼接用于问答和分类任务。GloVe 是通过词共现比例学习到的向量，因此它没有句子上下文。而 CoVe 是通过处理文本序列学习到的向量，本身就具有上下文信息：</p><script type="math/tex; mode=display">v = [\text{GloVe}(x);\text{CoVe}(x)]</script><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/20220117101548.png" alt></p><p>给定下游任务，首先将输入的词用上面的方法转化成向量，然后输入到特定任务模型中进行训练。</p><h1 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h1><p>CoVe 的缺点是显而易见的：</p><ol><li>因为预训练过程是有监督训练，所以训练效果严重依赖标注数据（平行语料）；</li><li>CoVe 的性能受限于特定任务的模型结构。</li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/pdf/1708.00107.pdf" target="_blank" rel="noopener">Learned in Translation: Contextualized Word Vectors</a>， <em>Bryan McCann，James Bradbury，Caiming Xiong，Richard Socher. 2017</em></li><li><a href="https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html" target="_blank" rel="noopener">Generalized Language Models</a>, <em>Jan 31, 2019 by Lilian Weng</em></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://aylien.com/images/uploads/general/tumblr_inline_o8tinsmw081u37g00_540.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;上一篇文章我们介绍了预训练词向量，它的缺点很明显：一旦训练完，每个词的词向量都固定下来了。而我们平时生活中面临的情况却复杂的多，一个最重要的问题就是一词多义，即同一个词在不同语境下有不同的含义。&lt;a href=&quot;https://arxiv.org/pdf/1708.00107.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CoVe（Contextual Word Vectors）&lt;/a&gt;同样是用来表示词向量的模型，但不同于 &lt;a href=&quot;https://rogerspy.github.io/2021/08/11/ptm-word-embedding/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;word emebdding&lt;/a&gt;，它是将整个序列作为输入，根据不同序列得到不同的词向量输出的函数。也就是说，CoVe 会根据不同的上下文得到不同的词向量表示。&lt;/p&gt;
    
    </summary>
    
      <category term="语言模型" scheme="https://rogerspy.gitee.io/categories/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="Language Model" scheme="https://rogerspy.gitee.io/tags/language-model/"/>
    
      <category term="词向量" scheme="https://rogerspy.gitee.io/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>知识图谱：知识建模（二）构建本体的方法论</title>
    <link href="https://rogerspy.gitee.io/2021/08/23/kg-build-ontology-method/"/>
    <id>https://rogerspy.gitee.io/2021/08/23/kg-build-ontology-method/</id>
    <published>2021-08-23T02:52:56.000Z</published>
    <updated>2022-01-12T08:37:38.379Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/kgicon.png" alt></p><h1 id="1-什么是本体？"><a href="#1-什么是本体？" class="headerlink" title="1. 什么是本体？"></a>1. 什么是本体？</h1><p>“本体”（<em>ontology</em>）的概念来源于哲学对本体论的研究。随着人工智能（AI）的发展，科学家们将“本体”这一概念引入到计算机领域。不同的文献对本体有着不同的定义，甚至有些定义是相互矛盾的。为了方便起见，我们将本体定义为：<strong>本体是一系列词汇，这些词汇包括机器可读的概念定义和概念之间的关系</strong>。</p><a id="more"></a><ul><li><em>Classes</em>： 类别，或者概念（<em>concepts</em>）表示具体领域内的一些抽象概念，比如“酒”，“人”等。<em>Class</em> 是本体的核心。</li><li><em>Subclasses</em>：表示，大类别下面的子类。比如“酒”的子类包括“白酒”、“红酒”等。</li><li><em>Instances</em>：实例，表示抽象概念下的具体事物。比如“白酒”的实例包括“红星二锅头”、“飞天茅台”等。</li><li><em>Properties</em>：属性，表示的是概念的不同特征和属性。<em>OWL</em> 中的 <em>property</em> 实际上表示的就是关系。主要包括两种关系：<em>object property</em> 和 <em>data property</em> 。<em>Object property</em> 表示两个实体之间的关系，比如小明和小红是兄妹关系，其中“兄妹”就是“小明”和“小红”两个实体的 <em>object property</em>；<em>data property</em> 表示实体属性，比如小明的年龄是12岁，其中“姓名”就是“小明”这个实体的 <em>data property</em>。除此以外， W3C 还规定了一种标注属性（<em>annotation property</em>），它表示一些实体的注释元信息，用来对实体进行注释说明。</li><li><em>slots</em>：槽，可以认为就是实体具体属性，比如“小明的年龄是12岁”，其中 “年龄” 就是一个 <em>slot</em>，而 “12岁” 就是 <em>slot value</em>。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/image-20210729171714117.png" alt></p><p>总结一下，本体需要包含以下要素：</p><ul><li>定义本体类别</li><li>构建各类别之间的层级关系（父类 -&gt; 子类）</li><li>定义 <em>slot</em> 以及可接受的值，比如年龄 <em>slot</em> 必须是数字</li><li>在实例中对 <em>slot</em> 进行填充</li></ul><h1 id="2-为什么需要本体？"><a href="#2-为什么需要本体？" class="headerlink" title="2. 为什么需要本体？"></a>2. 为什么需要本体？</h1><ul><li>共享人类或者计算机理解的信息结构。假设多个的网页包含不同的医疗信息，如果这些网页能够共享相同的本体，那么计算机可以轻易从这些网页抽取有用信息提供给用户。不仅如此，计算机还可以聚合不同来源的信息，准确回答用户的问题。</li><li>领域知识的复用。举个例子，很多地方都会需要时间信息，包括时间段、时间点、相对时间等等。如果有人能够构建一个关于时间的本体，那么其他人就可以轻易将这个本体应用到自己的领域。另外，当我们自己构建本体的时候，也可以使用已有的本体知识在上面进行扩充或者缩减等。</li><li>明确领域假设。相当于我们将某一领域内的知识点利用一些假设关系相互串联起来，使我们对整个领域有更加清晰准确的认识，尤其是对一些新人。</li><li>将领域知识与可操作性的知识分离。这就有点类似于我们在设计一款产品的时候，我们将具体的产品和组件分离开来（模块化）。比如手机，多年前的手机充电器，基本上是一个品牌甚至同一个品牌的不同型号手机就有一个充电器，充电器不同共用，相当于手机和充电器是深度绑定的。后来为了解决这种深度绑定带来的各种问题，业内开始制定统一标准实现充电器与手机分离，一个充电器可以使用不同的充电器，而一个充电器可以给不同的手机充电。其中充电器标准就可以认为是领域知识本体，而手机就是可操作数据。</li><li>领域知识分析。当我们要复用和扩展领域知识的时候，这些领域知识元素就会变得非常有价值，说白了其实还是避免重复造轮子。</li></ul><p>通常定义领域的本体并不是我们的最终目的。开发本体类似于定义一组数据及其结构以供其他程序使用。 解决问题的方法、独立于领域的应用程序和软件代理使用从本体构建的本体和知识库作为数据。 例如，在本文中，我们开发了葡萄酒和食物的本体以及葡萄酒与膳食的适当组合。 然后，该本体可以用作一套餐厅管理工具中某些应用程序的基础：一个应用程序可以为当天的菜单创建葡萄酒建议或回答服务员和顾客的查询。 另一个应用程序可以分析酒窖的库存清单，并建议扩展哪些葡萄酒类别以及为即将到来的菜单或食谱购买哪些特定的葡萄酒。</p><h1 id="3-如何构建本体？"><a href="#3-如何构建本体？" class="headerlink" title="3. 如何构建本体？"></a>3. 如何构建本体？</h1><p>现实中，并没有一个标准的、统一的本体构建方法。本文只是讨论一种比较通用的方法：先构建比较粗糙、大粒度的本体，然后不断的迭代细化。</p><p>在详细介绍构建本体的流程之前，我们先强调本体设计时的一些规则，虽然看起来有些教条，但是在很多情况下这些规则确实能帮助我们。</p><ul><li>没有一个标准的、统一的本体构建方法。最好的方法就是根据实际业务需求去构建。</li><li>本体构建是一个需要不断迭代的过程</li><li>本体中的概念和关系必须是一些比较相近的对象（无论是物理上还是逻辑上）。这些对象可能是某领域内描述性句子中的名词或者动词。</li></ul><p>接下来，我们以构建酒类和食物领域的本体为例，介绍构建本体的方法。</p><h2 id="3-1-第一步、确定本体的领域和范围"><a href="#3-1-第一步、确定本体的领域和范围" class="headerlink" title="3.1 第一步、确定本体的领域和范围"></a>3.1 第一步、确定本体的领域和范围</h2><p>构建本体的第一步是确定领域和范围，因此我们需要回答下面几个问题：</p><blockquote><ul><li>我们要构建什么领域的本体？</li><li>这些本体用来做什么？</li><li>这些本体可以回答什么问题？</li><li>谁会使用这些本体？</li></ul></blockquote><p>在本体设计过程中，这些问题的答案可能会发生变化，但是任何时候这些问题都可以帮助我们限定本体范围。</p><p>考虑酒类和食物的例子。首先我们已经确定要构建酒类和食物的本体了，我们的目的是用这些本体来推荐一些好的食物和酒的搭配。</p><p>那么显然，不同酒类的概念，食物类型的概念，以及酒和食物的搭配就必须包含在我们的本体中。同时，在我们的本体中不太可能包括管理酒厂库存或餐厅员工的概念，即使这些概念与酒和食物的概念有些相关。</p><p>如果我们的本体是用来帮助酒类杂志文章进行自然语言处理，那么词性、同义词等自然语言信息可能就会变得非常重要。如果本体用于帮助餐厅顾客决定订购哪种酒，我们需要包括零售定价信息。如果用于酒品买家储存酒窖，则可能需要批发定价和可用性信息。如果本体描述语言不同于本体用户语言，我们可能需要提供语言之间的映射。</p><h3 id="能力问题（competency-questions）"><a href="#能力问题（competency-questions）" class="headerlink" title="能力问题（competency questions）"></a>能力问题（competency questions）</h3><p>确定本体范围的方法之一就是勾勒出基于本体的知识库能够回答的问题（<a href="https://www.semanticscholar.org/paper/Methodology-for-the-Design-and-Evaluation-of-Gruninger/497abc0ddace6a7772a5f5a3edb3d7b751476755" target="_blank" rel="noopener">Gruninger and Fox 1995</a>）。这些问题能帮助我们确定我们是否有足够的信息去回答这些问题，本体粒度都不够，覆盖的范围全不全。只需要一些大致的问题即可，无需穷举。</p><p>在我们的例子中，我们可能包含以下能力问题：</p><blockquote><ul><li>当我挑选酒品的时候应该考虑什么？</li><li>Cabernet Sauvignon 适合搭配海鲜吗？</li><li>什么酒与烤肉最配？</li><li>酒类的哪些特点会影响与食物的搭配？</li><li>特定的酒的香气或者酒本身会随着时间发生变化吗？</li><li>Napa Zinfandel 最好的年份？</li></ul></blockquote><p>从上面的问题中可以总结出，在我们的本体中石少应该包含：不同酒的特点、酒的类型、年份（及其品质的好坏）、食物分类以及酒和食物的搭配。</p><h2 id="3-2-第二步、现有本体的复用"><a href="#3-2-第二步、现有本体的复用" class="headerlink" title="3.2 第二步、现有本体的复用"></a>3.2 第二步、现有本体的复用</h2><p>程序员的圣经之一就是“不要重复造轮子”。查找已有的可用本体是一件非常重要的事情。网上有很多相关的资源，下面列举一些比较重要的资源（大多是英文的资源，中文开放本体资源目前还比较少）：</p><blockquote><ul><li><p>OpenKG</p><p>OpenKG是最大的中文开放知识图谱库，其中包含了很多本体。</p><p>地址：<a href="http://openkg.cn/home" target="_blank" rel="noopener">http://openkg.cn/home</a></p></li><li><p>Protege Ontology Library</p><p>地址：<a href="https://protegewiki.stanford.edu/wiki/Protege_Ontology_Library" target="_blank" rel="noopener">https://protegewiki.stanford.edu/wiki/Protege_Ontology_Library</a></p></li><li><p>Ontolingua ontology library</p><p>地址：<a href="http://www.ksl.stanford.edu/software/ontolingua/" target="_blank" rel="noopener">http://www.ksl.stanford.edu/software/ontolingua/</a></p></li><li><p>DAML ontology library</p><p>地址：<a href="http://www.daml.org/ontologies/" target="_blank" rel="noopener">http://www.daml.org/ontologies/</a></p></li><li><p>UNSPSC</p><p>地址：<a href="https://www.unspsc.org" target="_blank" rel="noopener">https://www.unspsc.org</a></p></li><li><p>RosettaNet</p><p>地址：<a href="https://www.rosettanet.org" target="_blank" rel="noopener">https://www.rosettanet.org</a></p></li><li><p>DMOZ</p><p>地址：<a href="https://www.dmoz.org" target="_blank" rel="noopener">https://www.dmoz.org</a></p></li></ul></blockquote><p>实际上现在确实有酒类开放实体可用，但是我们假设不存在，从头构建一个酒类本体。</p><h2 id="3-3-第三步、枚举本体中的重要对象"><a href="#3-3-第三步、枚举本体中的重要对象" class="headerlink" title="3.3 第三步、枚举本体中的重要对象"></a>3.3 第三步、枚举本体中的重要对象</h2><p>当提到一个对象的时候你会讨论些什么？这些对象有什么属性？关于这个对象你会说些什么？思考这些问题对我们构建本体是非常有用的。我们可以把这些对象写成一个列表记录下来，比如提到“酒”，你会想到“葡萄”、“酿酒厂”、“原产地”、“酒的颜色”、“口感”、“含糖量”、“酒精含量”等等。而提到“食物”，我们通常会想到“鱼”、“虾“、”肉“、”蛋“、”奶“等等。起初，对于对象的一个综合理解更重要，无需过于关注概念和关系之间的相互覆盖。</p><h2 id="3-4-第四步、定义类和类的层级结构"><a href="#3-4-第四步、定义类和类的层级结构" class="headerlink" title="3.4 第四步、定义类和类的层级结构"></a>3.4 第四步、定义类和类的层级结构</h2><p>有几种不同的方法定义类的层级结构（<a href="https://www.cambridge.org/core/journals/knowledge-engineering-review/article/abs/ontologies-principles-methods-and-applications/2443E0A8E5D81A144D8C611EF20043E6" target="_blank" rel="noopener">Uschold &amp; Gruninger 1996</a>）：</p><ul><li><strong>自上而下</strong>的方法是先定义领域内最通用的顶层概念，然后依次向下扩展。比如，我们先定义两个类别：“酒”和“食物”。然后定义酒的子类：白酒、红酒、玫瑰酒等等。然后对红酒进一步分类：<em>Syrah</em>，<em>Red Burgundy</em>，<em>Cabernet Sauvignon</em> 等等。对“食物”也是如此。</li><li><strong>自下而上</strong>的方法是先定义一些具体的类别，然后讲这些类别聚合成更加通用的类别。比如“衡水老白干”和“红星二骨头”可以聚合成“白酒”。另外，“拉菲”，“桃乐丝”可以聚合成“红酒”。而“白酒”和“红酒”可以聚合成“酒”。</li><li><strong>上下结合</strong>的方法是先定义一些比较重要的类，然后向上聚合和向下扩展。相当于将上面两种方法结合在一起。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/image-20210802113435286.png" alt></p><p>这三种方法中，没有一种是一定比其他两种更好的方法。最终采取哪种方法取决于开发人对领域的认知。如果开发人员对领域有着系统性的了解，那么采取自上而下的方法应该是首选。通常对于大多数的开发人员来说，上下结合的方法是比较适合的，因为多数人对领域都是有一定的了解而又了解不深。所以可以通过先构建一些比较通用的本体，然后再从实例中进行总结向上补充的方法会比较合适。</p><p>比如，多数人都知道酒可以分成“白酒”、“红酒”、“鸡尾酒”等，经常看广告也可以知道“白酒”有“酱香型”、“浓香型”对于其他的香型不太了解。而对于具体的酒进行总结归类，就可以发现，原来“酒鬼酒”是馥郁香型的，那么我们就可以将“馥郁香型”补充到酒类香型的层级上去。</p><p>无论是那种方法，都是从定义类开始的。第三步中，我们列举出了一些对象，现在我们可以从列表中的选择那些用于描述独立存在的对象作为类别，以这些对象作为锚点构建层级关系。一个重要的假设如下：</p><blockquote><p>  <em>如果类别 A 是类别 B 的父类，那么属于 B 类的所有实例也同样是类别 A 的实例。</em></p></blockquote><h2 id="3-5-第五步、定义类的属性——slots"><a href="#3-5-第五步、定义类的属性——slots" class="headerlink" title="3.5 第五步、定义类的属性——slots"></a>3.5 第五步、定义类的属性——<em>slots</em></h2><p>单纯的类别不足以为回答能力问题提供足够的信息，一旦我们用以上的方法定义了类别之后，需要为这些类别提供额外的信息，比如酒的颜色、口感、含糖量、产地等。这些信息就是类别的属性。</p><p>通常，一下集中类型的对象可以成为本体的属性：</p><ul><li>内秉属性，比如：颜色、口感、含糖量等；</li><li>外部属性，比如：产地、酒名等；</li><li>子结构，如果一个对象是结构化的，那么它的实体结构和抽象结构都可以成为它的属性；</li><li>与其他对象的关系。比如，酒的厂家、酒的原料等。</li></ul><p>所有子类都要继承父类的属性。比如“酒”的属性包括“厂家”、“颜色”、“口感”、“含糖量”、“产地”等，那么“酒”的子类“白酒”也要继承这些属性。因此，定义 <em>slots</em> 的时候通常是附加在具有该属性的最顶级类别上。</p><h2 id="3-6-第六步、定义-slots-的刻面"><a href="#3-6-第六步、定义-slots-的刻面" class="headerlink" title="3.6 第六步、定义 slots 的刻面"></a>3.6 第六步、定义 <em>slots</em> 的刻面</h2><p><em>slots</em> 的刻面包括 <em>slots</em> 基数、数据类型、定义域和值域等。比如酒的名称（<em>name</em>）是一个字符串类型的数据，酒厂生产（<em>produces</em>）酒，这些酒是具体的酒的实例，因此其对应的数据类型应该是实例（<em>instance</em>）。</p><ul><li><p>基数（<em>cardinality</em>）</p><p><em>slot</em> 基数定义了一个 <em>slot</em> 可以有多少个值。有些 <em>slot</em> 只能至多有一个值，而有些则可以有多个值。比如一种酒只能有一种颜色，却可以有多个产地。</p><p>有些系统会规定 <em>slot</em> 基数的最大值和最小值。最小值 <em>N</em> 表明该 <em>slot</em> 至少有 <em>N</em> 个值，比如葡萄酒的原料葡萄 <em>slot</em> 最小值为 1，表明该种葡萄酒的原料中至少包含一种葡萄。最大值 <em>M</em> 表明该 <em>slot</em> 最多有 <em>M</em> 个值。比如葡萄酒的原料葡萄 <em>slot</em> 最大值为 2，表明该种葡萄酒最多有两种不同品种的葡萄酿制而成。如果最大值最小值都是 1，说明这种葡萄酒就是 1 种葡萄酿制而成的。所有时候将最大值设置成 0 也是非常有用的，表明对于某些特定的子类没有任何值满足条件。</p></li><li><p>数据类型</p><p>数据类型定义了 <em>slot</em> 的数据类型。</p><ul><li>字符串（<em>string</em>）：最简单，最常用的数据类型。</li><li>数字（<em>number</em>）：数值类型的 <em>slot</em>，比如年龄、价格等。</li><li>布尔型（<em>boolean</em>）：<em>yes</em> 或者 <em>no</em>，<em>true</em> 或者 <em>false</em> 等</li><li>可枚举（<em>enumerated</em>）：给定 <em>slot</em> 可取到的值的列表，比如酒的口味可以是 [“重”，“中等”，“清淡”] 中的任意一种，而不能超过这三种的范围。</li><li>实例类型（<em>instance</em>）：<em>slot</em> 允许定义两个单独实体的关系，但是必须定义清楚哪些类别的实体是可以作为 <em>slot</em> 的值。比如“酒”这个类别，可以作为 “<em>produces</em>” 的值。</li></ul></li><li><p>定义域和值域</p><p>允许使用实例类型作为 <em>slot</em> 的类别称之为值域，比如 “酒” 作为 “<em>produces</em>” 的 <em>slot</em> 值，“酒” 就是  “<em>produces</em>” 的值域。简单来说，就是 $x \rightarrow y$，其中 $y$ 的所有实例数据类型的取值就是值域。</p><p>而定义域就是 <em>slot</em> 描述的对象。比如 “酿酒厂”，“<em>produces</em>”，“酒”，其中 “酿酒厂” 就是定义域。是简单来说，就是 $x \rightarrow y$，其中 $x$​ 的取值范围就是定义域。</p><p>确定定义域和值域的规则是相似的：</p><blockquote><ul><li>找到最通用的类或者最具代表性的类别；</li><li>另一方面，不要将定义域和值域定义得范围太大，定义域中所有的值都可以被 <em>slot</em> 描述，值域中的所有值应该是 <em>slot</em> 的潜在填充值。不要选择过于笼统的类别，而是应该选择涵盖所有填充值的类别。</li></ul></blockquote><p>比如，“酿酒厂”，“<em>produces</em>” 的值域不应该是所有“酒” 的子类（“白酒”，“啤酒”，“红酒”等），而直接就是“酒”。同时，也不应该将“酒”进一步泛化到 “<em>THING</em>”。</p><p>具体来讲：</p><blockquote><p>  <em>如果 slot 的值域或者定义域包括一个类别以及该类别下的子类，那么将其子类全部删掉</em></p></blockquote><p>比如，一个 <em>slot</em> 的值域包括“酒”和“红酒”，那么应该将“红酒”删掉，因为“红酒”是“酒”的子类。</p><blockquote><p>  <em>如果 slot 的值域或者定义域包含了 A 类的所有子类，但不包含 A 类本身，那么值域应该只包含 A 类本身而不包括其子类。</em></p></blockquote><p>比如，一个 <em>slot</em> 的值域是“白酒”、“红酒”、“啤酒”等，我们可以将值域设为“酒”本身。</p><blockquote><p>  <em>如果 slot 的值域或者定义域包含类别 A 中除少数子类以外的所有子类，那么我们应该考虑将类别 A 本身进行重新定义。</em></p></blockquote><p>将一个 <em>slot</em> 挂在到一个类别上，和将该类别设为 <em>slot</em> 的定义域是完全等价的。一方面，我们应该尽可能泛化，另一方面我们应该保证 <em>slot</em> 对应的类别确实有相应的属性。总之一句话，我们既要一个都不差，也要避免张冠李戴。</p></li></ul><h2 id="3-7-第七步、构建实例"><a href="#3-7-第七步、构建实例" class="headerlink" title="3.7 第七步、构建实例"></a>3.7 第七步、构建实例</h2><p>最后一步就是根据我们建立的类别层级结构构建实例。定义一个实例需要：</p><ol><li>选择一个类别；</li><li>创建该类的单一实例</li><li>填充 <em>slot</em> 值</li></ol><p>比如，我们创建一个 <em>飞天茅台</em> 的实体用来表示 “白酒” 的实例。它的属性如下：</p><blockquote><p>  酒精度：53%</p><p>  颜色：无色透明</p><p>  香气：幽雅细腻</p><p>  口味：回味悠长</p><p>  产地：贵州省仁怀市</p><p>  生产商：贵州茅台酒股份有限公司</p></blockquote><h1 id="4-定义类别和类别层级结构"><a href="#4-定义类别和类别层级结构" class="headerlink" title="4. 定义类别和类别层级结构"></a>4. 定义类别和类别层级结构</h1><p>本节讨论定义类别和类别层级结构时需要注意的点和容易出现的错误。对于任意领域来说都没有一个唯一正确的层级结构。我们所定义的层级结构依赖于我们要怎样使用本体，应用中必要的细节，个人喜好以及有时候可能还需要与其他模型进行兼容。但是我们还是要讨论一些开发层级结构时的一些指南，当我们开发完新的层级结构以后，回过头重新审视我们的定义是否满足这些指南，可以帮助我们避免很多错误。</p><h2 id="4-1-保证类别层级结构的正确性"><a href="#4-1-保证类别层级结构的正确性" class="headerlink" title="4.1 保证类别层级结构的正确性"></a>4.1 保证类别层级结构的正确性</h2><ul><li><p>“is-a” 关系</p><p>如果类别 A 中的所有实例同时也是类别 B 的实例，此时我们就说类别 A 是类别 B 的子类，我们就可以定义关系（A，is-a，B）。比如，（“茅台酒”，“is-a”，“白酒”）。另一个也可以表示这种关系的是 “kind-of”，（“茅台酒” ，“kind-of”，“白酒”），（“肉”，“kind-of”，“食物”）等等。</p></li><li><p>单一的酒不是所有酒的子类</p><p>一个常见的错误是，在层级结构中包含同一个概念的单数版本和复数版本，然后令单数版本是复数版本的子类。比如，定义 “<em>wine</em>” 是 “<em>wines</em>” 的子类。然而这种关系是错误的。为了避免这种情况发生，在给类别命名的时候最好都采用单数形式或者都采用复数形式（第六节中讨论类别命名）。</p></li><li><p>层级关系的可传递性</p><p>满足以下条件的关系是可传递的：</p><blockquote><p>  <em>如果 B 是 A 的子类，C 是 B 的子类，那么 C 也是 A 的子类。</em></p></blockquote><p>比如，我们定义一个类别是 “酒”，然后定义 “白酒” 是 “酒” 的子类。然后再定义 “茅台酒” 是 “白酒” 的子类。那么可传递性表示 “茅台酒” 也是 “酒” 的一个子类。有时候我们会区分直接子类和间接子类。直接子类表示在层级结构中两个类别之间没有其他子类，即某一类别与其父类直接相连。而间接子类就是需要一个中间子类再与父类相连。实际上该子类也是中间父类的直接子类。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/image-20210803104950521.png" alt></p></li><li><p>类别层级结构的演化</p><p>随着定义域的变化，要维护一个不变的层级结构可能会是一件很困难的事。比如通常我们见到的 “茅台酒” 是无色透明的，所以我们将 “茅台酒” 定义为 “白酒” 的子类。但是有可能在未来的某一天，酒厂发明了一种新的酿酒技术使得酒变成了黄色或者红色。此时，我们再将 “茅台酒” 归类到 “白酒” 里面可能就不太合适了。（这个例子实际上并不是很典型，一个比较典型的例子是组织结构的本体。组织结构的变动是很频繁的，一些部门今天还在，明天可能就取消了。）</p></li><li><p>类别及其名字的区别</p><p>区分类别和它的名字是至关重要的，通常也很难被注意到。</p><blockquote><p>  类别代表的是某一领域内的概念本身，而不是代表这个概念的几个单词。</p></blockquote><p>我们选择的术语不同，其类别名就会发生变化，但实际上不同的术语表示的是同一个概念。比如 “克劳修斯表述”，“开尔文表述”，“熵增定律” 等等虽然名字不同，但都表示 “热力学第二定律” 这一概念。只是各自的名字不同罢了。再比如 “飞人乔丹”，“乔帮主” 都可以表示 “迈克尔·乔丹” 这个概念。</p><p>现实情况下，我们应该遵循以下规则：</p><blockquote><p>  <em>表示相同概念的同义词不可代表不同的类别</em></p></blockquote><p>同义词仅仅是相同概念的不同术语而已，因此，我们不能使用同义词来命名不同的类别。很多本体系统允许将同义词与表示类别名称相关联。比如，我们可以定义 “<em>same_as</em>” 关系，将“熵增定律”、“克劳修斯表述”和“开尔文表述”都关联到“热力学第二定律”上。如果不允许这种关联，则应该在类别文档中列出同义词。</p></li><li><p>避免类别套娃</p><p>类别层级结构中的循环指的是，类别 A 是类别 B 的子类的同时，类别 B 又是 类别 A 的子类，即两个类别互为子类和父类。是在构建层级结构的时候，我们应该避免出现这种情况。一旦出现这种情况就说明 A 和 B 是等价的：A 的所有实例也是 B 的实例，同时 B 的所有实例也是 A 的实例。</p></li></ul><h2 id="4-2-分析同级类别"><a href="#4-2-分析同级类别" class="headerlink" title="4.2 分析同级类别"></a>4.2 分析同级类别</h2><ul><li><strong>层次结构中的同级类别</strong></li></ul><p>同级类别（<em>siblings</em>）指的是具有相同直接父类的类别。</p><blockquote><p>  除了根节点，所有同级类别必须处于同一层。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/image-20210803143900031.png" alt></p><p>如图所示，“红酒”、“白酒”、“玫瑰酒” 同级别，“五粮液”、“鸭溪窖酒”、“贵阳大曲” 是同级别。</p><ul><li><strong>多少是太多？多少是太少？</strong></li></ul><p>并没有一个硬性指标规定一个类别至少应该有多少个直接子类。但是很多结构比较规范的本体中的类别通常有 2 个到 12 个直接子类。因此，我们可以有以下经验：</p><blockquote><p>  <em>如果一个给定类别只有一个直接子类，可能是本体建模有问题，或者本体不完全</em>；</p><p>  <em>如果一个给定类别的子类过多（超过 12 个），可能需要一些中间类别重新归类</em></p></blockquote><p>比如上图，如果我们构建的本体，“酒” 只有 “白酒” 一个直接子类，说明我们丢了 “红酒”、“玫瑰酒”等其他酒品。而如果把所有白酒都挂到 “白酒” 下面可能说明，我们对 “白酒” 的分类过于粗糙。因此，我们可以对 “白酒” 再进一步细分成 “酱香型”、“浓香型”、“清香型”等等。然后，再将对应的白酒挂上去。</p><h2 id="4-3-多继承"><a href="#4-3-多继承" class="headerlink" title="4.3 多继承"></a>4.3 多继承</h2><p>大多数知识表示系统都允许多继承：一个类别可以同时是多个类别的子类。比如，“啤酒”既可以是 “酒” 的直接子类，也可以是 “食物” 本体中 “调味料” 的直接子类。因此，“啤酒” 可以有两个父类：“酒” 和 “调味料”。“啤酒” 的所有实例同时也是 “酒” 和 “调味料” 的实例。当然，“啤酒” 也会同时继承 “酒” 和 “调味料” 的一些属性。</p><h2 id="4-4-什么时候应该（不应该）引入新的类别？"><a href="#4-4-什么时候应该（不应该）引入新的类别？" class="headerlink" title="4.4 什么时候应该（不应该）引入新的类别？"></a>4.4 什么时候应该（不应该）引入新的类别？</h2><p>本体构建最难的部分应该就是什么时候引入新的类别，或者什么时候通过不同的属性值加以区分。也就是说，对于一个新的对象，我们是把它归到已有的类别然后给予不同的属性，还是新建一个类别？如果新建过多类别，会造成类别过多，甚至会出现彼此嵌套。而如果是新加属性加以区分的话 ，又会造成属性过于复杂。如何找到一个平衡点并不容易。</p><p>为了寻找这样一个平衡点，我们可以设定一些规则：</p><blockquote><p>  一个子类通常需要满足以下条件之一：</p><ol><li>有一些父类不具备的属性；</li><li>与父类的限制条件不同；</li><li>与父类参与的关系类型不同</li></ol></blockquote><p>比如，“烤肉” 有一个属性 “几分熟”，但是其父类 “肉” 通常不会有这个属性。或者 “白酒” 的颜色限制为 “无色透明”（或者 “微黄透明”），而 “酒” 没有这个限制。换句话说，当我们想要描述的对象无法通过父类来描述的时候，就需要定义新的子类。</p><p>实际情况下，每个子类都应该有新的 <em>slot</em>，或者有新的 <em>slot</em> 值，又或者要覆盖原有的继承自父类的刻面。</p><p>有时候，没有新的属性的时候也可以引入新子类：</p><blockquote><p>  <em>术语层级结构不需要引入新的属性</em></p></blockquote><p>比如，电子病历系统基础的本体可以包括对各种疾病的分类。这个分类可能只是没有属性（或者有相同属性集）的术语层级结构。比如，“糖尿病”、“心脏病”、“高血压” 都是不带属性的，但是我们还是应该将这些术语分成不同的类，而不应该看成属性。</p><p>另一个无新增属性而要新建类别的情况是——约定俗成。某些领域内的对象，在领域专家眼中通常是区分对待的，我们构建的本体系统应该反映出领域专家对该领域的看法。因此，这种情况下，还是需要新增子类。</p><p>最后，我们不应该为每个附加的限制创建一个子类。比如，我们可以构建 “红酒”、“白酒”、“玫瑰酒” 的分类，但是不能构建 “扬州酒”、“贵州酒”、“法国酒” 等，单独根据产地属性的类别。</p><h2 id="4-5-新的类别或者属性值？"><a href="#4-5-新的类别或者属性值？" class="headerlink" title="4.5 新的类别或者属性值？"></a>4.5 新的类别或者属性值？</h2><p>当我们对一个领域进行建模的时候，通常需要考虑将某些对象定义为属性还是类别的问题。</p><p>我们是要定义 “白酒”、“红酒” 作为 “酒” 的子类，还是要将 “白酒”、“红酒” 作为 “酒” 的 “颜色” 属性值？这通常取决于我们构建的本体的范围。“白酒” 在你的领域内重要性如何？如果 “白酒” 只是为我们提供一些边缘信息，或者与其他对象没有很重要的关系，那么我们就不应该将 “白酒” 作为一个类别来对待。</p><blockquote><p>  <em>如果有不同 slot 的概念会变成其他类别的不同 slot 的限制，那么我们应该新建一个类别。否则我们就在属性中加以区分即可。</em></p><p>  换句话说就是，如果 slot 的值发生了变，会使得类别也发生变化的话，那么我们应该新建一个类别。 </p></blockquote><p>比如，“红啤”、“白啤”、“黑啤” 等，这几种酒确实不是同一种酒。</p><blockquote><p>  <em>如果领域内对对象的区分非常重要，并且我们将具有不同值的对象视为不同种类的对象，那么我们应该创建一个新类</em></p></blockquote><p>同时我们还应该注意：</p><blockquote><p>  <em>一个实例所属的类别不应该时常发生变动</em></p></blockquote><p>通常情况下，当我们用外部属性而不是内秉属性来划分类别的时候，实例所属的类别经常会发生变化。比如，“热牛奶” 和 “常温牛奶” 并不应该分成两个类别，而是应该把温度设置成 “牛奶” 的属性。</p><p>另外，数字、颜色、地点通常应该是属性而不是类别。但是，对于 “酒” 来说，颜色应该是一个很重要的分类标准，所以在 “酒” 的分类中颜色应该属于类别而不是属性。</p><p>另一个人体解剖学本体的例子。当我们表示 “肋骨” 的时候，是否应该将肋骨分成 “左侧第一根肋骨”、“左侧第二根肋骨” 等？或者我们将肋骨的顺序和位置当成属性？如果在我们的本体中每根肋骨承载的信息非常不同的话，我们么确实应该为每根肋骨构建一个类别。比如，如果我们想要对肋骨不同位置的邻接信息建模，以及运动过程中每根肋骨所起的作用，或者不同肋骨保护的不同器官等等，这时候我们就需要对每根肋骨新建一个类别。如果我们只是对人体解剖学进行大致建模，那么我们只需要构建一个 “肋骨” 的类别，然后把 “位置” 和 “顺序” 作为属性即可。</p><h2 id="4-6-实例还是类别？"><a href="#4-6-实例还是类别？" class="headerlink" title="4.6 实例还是类别？"></a>4.6 实例还是类别？</h2><p>决定一个对象是类别还是实例，还是取决于我们构建的本体的潜在应用场景。类别结束实例开始的位置决定了本体的细节粒度。比如，“酸奶” 应该算是一个类别，还是实例？如果作为类别，它下面还有 “成都老酸奶”、“青海老酸奶” 等更细粒度的类别，如果 “酸奶” 作为实例，那么就不需要区分 “成都老酸奶” 和 “青海老酸奶”了。</p><p>要确定本体的细节粒度，可以回到本体构建步骤的第一步——我们想要利用这个本体回答什么问题？</p><blockquote><p>  知识库中，单实例是粒度最细的概念。</p></blockquote><p>比如，如果我们关心的是是否易消化，那么 “酸奶”、“纯牛奶” 就可以作为实例，而如果还要考察 “酸奶” 的制作工艺，口味特点等。那么 “酸奶” 就需要成为一个类别。</p><p>另外，如果满足以下条件，则可以将实例转化成类别：</p><blockquote><p>  <em>如果一个概念天然有层级结构，那么我们应该把它当成类别。</em></p></blockquote><p>比如，“地球有七个大洲”，我们可以把 “七大洲”（“亚洲”，“欧洲”，…） 当成 “地球” 的实例，但是 “七大洲” 是由不同国家组成的。因此，通常我们把每个大洲作为类别，而不是实例。</p><p>需要注意的是，只有类别有值域。在知识表示系统中，不存在 “subinstance” 的概念。因此，如果我们还想对一个概念进行细分，即使概念本身没有任何实例，也要把它当成类别，而不是实例。</p><h2 id="4-7-限制本体范围"><a href="#4-7-限制本体范围" class="headerlink" title="4.7 限制本体范围"></a>4.7 限制本体范围</h2><blockquote><p>  <em>本体系统不需要包括领域内所有的信息：我们不需要细化或者泛化超过实际需求的本体。</em></p></blockquote><p>比如，如果我们的用处是酒和食物的单配，那么我们就不需要知道如何酿酒和如何烹饪。</p><blockquote><p>  <em>本体系统不需要包含所有的属性以及不同类别之间的区别。</em></p><p>  <em>本体系统不需要包含所有的关系</em></p></blockquote><h2 id="4-8-无交集子类"><a href="#4-8-无交集子类" class="headerlink" title="4.8 无交集子类"></a>4.8 无交集子类</h2><p>如果两个类别的实例中没有公共实例，我们就认为这两个类别是无交集类别。比如 “红酒” 和 “白酒” 就是无交集类别：没有一种酒即是红酒又是白酒。在构建本体系统的时候，我们可以指定两个类别是无交集类别。指定无交集类别的好处是可以使本体更好进行验证——如果不指定的话，我们要看两个类别是否有交集还要将两个类别的实例全部读取出来，然后求交集看是否为空。不仅浪费空间还浪费时间。</p><p>同时，如果我们指定 “红酒” 和 “白酒” 是无交集类别的话，在进行建模的时候，如果创建了一个多继承子类，父类中包括了 “红酒” 和 “白酒”，那么系统可以很快识别出建模错误。</p><h1 id="5-定义属性——更多细节"><a href="#5-定义属性——更多细节" class="headerlink" title="5. 定义属性——更多细节"></a>5. 定义属性——更多细节</h1><p>本节主要讨论可逆属性和属性默认值。</p><h2 id="5-1-互逆属性（slot）"><a href="#5-1-互逆属性（slot）" class="headerlink" title="5.1 互逆属性（slot）"></a>5.1 互逆属性（slot）</h2><p>一个 <em>slot</em> 的值可能依赖于另一个 <em>slot</em> 的值。比如，（<em>wine</em>，<em>produced_by</em>，<em>winery</em>）和（<em>winery</em>，<em>produces</em>，<em>wine</em>）这两个关系就是互逆关系。如果在本体系统中将两个关系都存下来，会显得整个本体冗杂。当我们知道某种酒的生产厂家是某某某的时候，我们就可以推断出某某某厂家生产了某种酒。从知识获取的角度来说，明确这种互逆关系对知识获取来说是很方便的。知识获取系统可以自动填写互逆关系的值，以确保知识库的一致性。</p><p>比如，当我们明确了 “<em>produced_by</em>” 和 “<em>produces</em>” 是互逆关系之后，当我们填写了 “茅台酒 <em>produced_by</em> 贵州茅台酒股份有限公司” 以后，系统可以自动填写 “贵州茅台酒股份有限公司 <em>produces</em> 茅台酒”。</p><h2 id="5-2-默认属性值（slot-value）"><a href="#5-2-默认属性值（slot-value）" class="headerlink" title="5.2 默认属性值（slot value）"></a>5.2 默认属性值（slot value）</h2><p>许多基于框架的系统允许指定默认属性值。如果多数实例的特定属性是相同的，那么我们可以给这个属性指定一个默认值。然后，每次往该类别下添加有该属性的实例的时候，系统可以自动填充属性值。如果我们默认的属性值与该实例的实际属性值不符，我们还可以手动修改。</p><p>比如，如果多数白酒都是 53° 的，那么我们在 “酒精度数” 中可以默认为 53°。如果有些白酒不是 53°，还可以手动改成其他度数。</p><p>需要注意的是，默认属性值与属性值是不同的，默认属性值是可以修改的，而属性值是不可修改的。即如果我们定义了 “白酒” 的酒精度是 53°，那么所有 “白酒” 的子类和实例的酒精度都是 53°，这个度数在任意子类和实例中都不可修改。</p><h1 id="6-名字包含什么？"><a href="#6-名字包含什么？" class="headerlink" title="6. 名字包含什么？"></a>6. 名字包含什么？</h1><blockquote><p>  本节讨论对概念命名规则，主要集中在英文名称中会出现的一些问题，比如大小写、分隔符、单复数等。这些问题在中文中都基本不会出现。但是就个人而言，还是建议使用英文进行知识建模。众所周知，现在很多系统对中文的支持并不是十分友好，使用中文建模的话很可能出现各种意想不到的问题，因此，能用英文建模的就尽量使用英文建模吧。</p></blockquote><p>为本体中的概念设定一些命名规则，不仅可以使本体更容易理解，还能够帮助我们避免一些常见的建模错误。命名方法有很多，实际应用的时候可以选择合适的方法。但是，我们要：</p><blockquote><p>  <em>定义一种类别和属性的命名规范，然后遵守它。</em></p></blockquote><p>在知识表示系统中，我们可以考虑以下特征用于对概念进行命名：</p><ul><li>本体中是否存在同名的类别、属性、实例？比如 “酿酒厂” 既是类别又是属性？</li><li>本体系统大小写敏感吗？比如，系统是否人为 “<em>Wine</em>” 和 “<em>wine</em>” 是同一个概念？</li><li>名称中允许出现什么样的分隔符？空格、逗号、星号等等？</li></ul><h2 id="6-1-大小写与分隔符"><a href="#6-1-大小写与分隔符" class="headerlink" title="6.1 大小写与分隔符"></a>6.1 大小写与分隔符</h2><p>首先，如果我们在本体中保证概念名称的大小写一致性能够大幅提升本体的可读性。比如，通常的做法是大写类别名称，小写属性名称（假设大小写敏感）。</p><p>当概念名称中有不止一个词的时候，我们需要在词与词之间添加分隔符。通常分隔符有以下几种选择：</p><ul><li>空格</li><li>词与词之间没有分隔符，而是将每个词的首字母大写，比如 “MealCourse”</li><li>使用下划线或者连接符，比如 “meal_course”，“meal-course” 等</li></ul><p>在使用空格的时候，需要考虑你所使用的本体建模工具是否支持空格，以及你构建出来的本体是否会与其他本体系统交互使用，如果有交互，需要交互的本体系统是否支持空格。因此，虽然用空格作为分隔符更符合人类的习惯，但是需要考虑的因素比较多，更建议使用后两种方案。</p><h2 id="6-2-单数还是复数？"><a href="#6-2-单数还是复数？" class="headerlink" title="6.2 单数还是复数？"></a>6.2 单数还是复数？</h2><p>一个类别的名称代表的是一些列对象的集合。所以，建议使用复数作为类别的名称。但是无论使用单数还是复数，都要在整个本体中保持一致，不要出现在这里是单数，淡了另一处就变成了复数。甚至有些本体建模工具会要求用户指定概念名称的单复数。</p><h2 id="6-3-前缀和后缀的规则"><a href="#6-3-前缀和后缀的规则" class="headerlink" title="6.3 前缀和后缀的规则"></a>6.3 前缀和后缀的规则</h2><p>有些知识库会建议使用前缀或者后缀还区分类别名和属性名。属性名中常用的两种前缀或者后缀：“has-” 或者 “-of”。比如 “<em>has-maker</em>” 或者 “<em>*maker-of</em>”。通过这种方式区分类别名和属性名可以提高可读性。</p><h2 id="6-4-命名中的一些其他考量"><a href="#6-4-命名中的一些其他考量" class="headerlink" title="6.4 命名中的一些其他考量"></a>6.4 命名中的一些其他考量</h2><ul><li>不要在概念名称中出现 “<em>class</em>”、“<em>property</em>”、“<em>slot</em>” 等词汇</li><li>避免使用缩写</li><li>对直接子类进行命名的时候，要么所有子类都包含父类的名称，要么都不包含父类的名称。不要出现有些子类包含父类有些不包含的情况。比如 “<em>red wine</em>” 和 “<em>*white</em>”</li></ul><h1 id="7-其他可参考资料"><a href="#7-其他可参考资料" class="headerlink" title="7. 其他可参考资料"></a>7. 其他可参考资料</h1><ol><li><a href="https://dl.acm.org/doi/10.1006/ijhc.1999.0366" target="_blank" rel="noopener">WonderTools? A comparative study of ontological engineering tools</a>, <em>Duineveld, A.J., Stoter, R., Weiden, M.R., Kenepa, B. and Benjamins, V.R. (2000).</em></li><li>Knowledge sharing and reuse. <em>Gómez-Pérez, A. (1998).</em></li><li><a href="https://www.cambridge.org/core/journals/knowledge-engineering-review/article/abs/ontologies-principles-methods-and-applications/2443E0A8E5D81A144D8C611EF20043E6" target="_blank" rel="noopener">Ontologies: Principles, Methods and Applications</a>, <em>Uschold, M. and Gruninger, M. (1996).</em></li><li><a href="http://www.ksl.stanford.edu/software/ontolingua/tutorial.pdf" target="_blank" rel="noopener">Ontolingua tutorial</a>. <em>Farquhar, A. (1997).</em></li><li><a href="https://www.researchgate.net/publication/profile/Richard_Fikes/publication/221393548_An_Environment_for_Merging_and_Testing_Large_Ontologies/links/564d604f08ae4988a7a44137/An-Environment-for-Merging-and-Testing-Large-Ontologies.pdf" target="_blank" rel="noopener">An Environment for Merging and Testing Large Ontologies</a>. <em>McGuinness, D.L., Fikes, R., Rice, J. and Wilder, S. (2000).</em></li></ol><h1 id="8-总结"><a href="#8-总结" class="headerlink" title="8. 总结"></a>8. 总结</h1><p>本文描述了构建本体的方法和步骤。讨论了构建过程中需要注意的问题。但是我们需要记住一点：</p><p><strong>对于任意领域来说都没有一个唯一正确的方法</strong></p><p>本体的构建是一个创造的过程，即使是以相同的目的和应用场景构建相同领域的本体，不同的人都会得到不同的本体。主要能满足我们的需求，就是好的本体。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://protege.stanford.edu/publications/ontology_development/ontology101.pdf" target="_blank" rel="noopener">Ontology Development 101: A Guide to Creating Your First Ontology</a>. <em>Natalya F. Noy and Deborah L. McGuinness</em> </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/blog-imgs/kgicon.png&quot; alt&gt;&lt;/p&gt;
&lt;h1 id=&quot;1-什么是本体？&quot;&gt;&lt;a href=&quot;#1-什么是本体？&quot; class=&quot;headerlink&quot; title=&quot;1. 什么是本体？&quot;&gt;&lt;/a&gt;1. 什么是本体？&lt;/h1&gt;&lt;p&gt;“本体”（&lt;em&gt;ontology&lt;/em&gt;）的概念来源于哲学对本体论的研究。随着人工智能（AI）的发展，科学家们将“本体”这一概念引入到计算机领域。不同的文献对本体有着不同的定义，甚至有些定义是相互矛盾的。为了方便起见，我们将本体定义为：&lt;strong&gt;本体是一系列词汇，这些词汇包括机器可读的概念定义和概念之间的关系&lt;/strong&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="知识图谱" scheme="https://rogerspy.gitee.io/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
    
      <category term="ontology" scheme="https://rogerspy.gitee.io/tags/ontology/"/>
    
      <category term="KG" scheme="https://rogerspy.gitee.io/tags/kg/"/>
    
  </entry>
  
  <entry>
    <title>双数组前缀树</title>
    <link href="https://rogerspy.gitee.io/2021/08/16/double_array_trie/"/>
    <id>https://rogerspy.gitee.io/2021/08/16/double_array_trie/</id>
    <published>2021-08-16T12:34:43.000Z</published>
    <updated>2022-01-12T08:37:38.329Z</updated>
    
    <content type="html"><![CDATA[<p>前缀树（trie）又叫字典树，顾名思义通过字符串的前缀进行查找、匹配的数据结构。Trie 树的应用场景主要包括：分词、词频统计、字符串查询和模糊匹配、字符串排序等。Trie 树大幅降低重复字符串的比较，所以执行效率非常高。</p><a id="more"></a><h1 id="1-Trie-树简介"><a href="#1-Trie-树简介" class="headerlink" title="1. Trie 树简介"></a>1. Trie 树简介</h1><p>前缀树是将字符串存储在一棵树结构内，该树是将字符串的公共前缀作为父节点。以下图为例：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210816213207.png" alt></p><p>假设有三个词，分别是“茶叶”、“茶树”、“走廊”。将这三个词存储在 Trie 树里如上图所示。“茶树”和“茶叶”有公共前缀“茶”，所以“茶”作为“叶”和“树”的父节点。而“走廊”和前两个词无公共前缀，所以独立成一个分支。另外 <code>root</code> 节点表示根节点，所有词的匹配、查找都是从根节点开始的。而 <code>null</code> 为叶节点表示从根节点 <code>root</code> 到 叶子节点 <code>null</code> 的路径组成一个完整的词。Trie 树具有以下几个特点：</p><ul><li>具有相同前缀的词必须位于同一个路径下。“叶”和“树”要共用一个父节点“茶”。</li><li>Trie 树中的词只可共用前缀，不可共用词的其他部分。比如，现在有一个新词“卖茶”，虽然都有“茶”，但是它并不是“卖茶”的前缀，所以“卖茶”要与“茶叶”和“茶树”在不同的分支上。</li><li>Trie 树中任何一个完整的词，都必须是从根节点开始至叶子节点结束，这意味着对一个词进行检索也必须从根节点开始，至叶子节点才算结束。</li></ul><h1 id="2-搜索-Trie-树的时间复杂度"><a href="#2-搜索-Trie-树的时间复杂度" class="headerlink" title="2. 搜索 Trie 树的时间复杂度"></a>2. 搜索 Trie 树的时间复杂度</h1><p>在 Trie 树中搜索一个字符串，会从根节点出发，沿着某条路径向下逐字比对字符串的每个字符，直到抵达底部的叶子节点才能确认字符串为该词，这种检索方式具有以下两个优点：</p><ol><li><p>公共前缀的词都位于同一个串内，查词范围因此被大幅缩小（比如首字不同的字符串，都会被排除）。</p></li><li><p>Trie 树实质是一个有限状态自动机（Deterministic Finite Automaton, DFA），这就意味着从 Trie 树 的一个节点（状态）转移到另一个节点（状态）的行为完全由状态转移函数控制，而 <strong>状态转移函数本质上是一种映射</strong>，这意味着：<strong>逐字搜索 Trie 树时，从一个字符到下一个字符比对是不需要遍历该节点的所有子节点的。</strong></p><blockquote><p>确定的有限自动机 M 是一个五元组：</p><script type="math/tex; mode=display">M = (\Sigma, Q, \delta, q_0, F)</script><p>其中，</p><ul><li><p>$\Sigma$ 是输入符号的有穷集合；</p></li><li><p>$Q$ 是状态的有限集合；</p></li><li><p>$\delta$ 是 $Q$ 与 $\Sigma$ 的直积 $Q × \Sigma$ 到 $Q$ (下一个状态) 的映射。它支配着有限状态控制的行为，有时也称为状态转移函数。</p></li><li><p>$q_0 \in Q$ 是初始状态；</p></li><li><p>$F$ 是终止状态集合，$F \subseteq Q$；</p></li></ul><p>可以把 DFA 想象成一个单放机，插入一盘磁带，随着磁带的转动，DFA 读取一个符号，依靠状态转移函数改变自己的状态，同时磁带转到下一个字符。</p></blockquote></li></ol><p>这两个优点相结合可以最大限度地减少无谓的字符比较，使得搜索的时间复杂度理论上仅与检索词的长度有关：$O(m)$，其中 $m$ 为检索词的长度。</p><h1 id="3-Trie-树的缺点"><a href="#3-Trie-树的缺点" class="headerlink" title="3. Trie 树的缺点"></a>3. Trie 树的缺点</h1><p>综上可知， Trie 树主要是利用词的公共前缀缩小查词范围、通过状态间的映射关系避免了字符的遍历，从而达到高效检索的目的。这一思想有赖于字符在词中的前后位置能够得到表达，因此其设计哲学是典型的“<strong>以信息换时间</strong>”，当然，这种优势同样是需要付出代价的：</p><ol><li>由于结构需要记录更多的信息，因此 Trie 树的实现稍显复杂。好在这点在大多数情况下并非不可接受。</li><li>Trie 型词典不仅需要记录词，还需要记录字符之间、词之间的相关信息，因此字典构建时必须对每个词和字逐一进行处理，而这无疑会减慢词典的构建速度。对于强调实时更新的词典而言，这点可能是致命的，尤其是采用双数组实现的 Trie 树，更新词典很大概率会造成词典的全部重构，词典构建过程中还需处理各种冲突，因此重构的时间非常长，这导致其大多用于离线；不过也有一些 Trie 可以实现实时更新，但也需付出一定的代价，这个缺点一定程度上影响了 Trie 树的应用范围。</li><li>公共前缀虽然可以减少一定的存储空间，但 Trie 树相比普通字典还需表达词、字之间的各种关系，其实现也更加复杂，因此实际空间消耗相对更大（大多少，得根据具体实现而定）。尤其是早期的“Array Trie”，属于典型的以空间换时间的实现，（其实 Trie 本身的实现思想是是以信息换时间，而非以空间换时间，这就给 Trie 树的改进提供了可能），然而 Trie 树现今已经得到了很好的改进，总体来说，对于类似词典这样的应用，Trie 是一个优秀的数据结构。</li></ol><h1 id="4-Trie-树的几种实现"><a href="#4-Trie-树的几种实现" class="headerlink" title="4. Trie 树的几种实现"></a>4. Trie 树的几种实现</h1><p>Trie 树实现:一般的链表指针方式，三数组实现，双数组实现，HAT，burst trie 等。</p><ul><li><p><strong>链表指针方式</strong></p><p>即每个节点对应一个字符，并有多个指针指向子节点，查找和插入从根节点按照指针的指向向下查询。这种方案，实现较为简单，但指针较多，较为浪费空间；树形结构，指针跳转，对缓存不够友好，节点数目上去之后，效率不够高。</p></li><li><p><strong>Hash Trie 树以及 Burst trie</strong></p><p>是将 trie 树和其他数据结构，比如 HashMap，结合起来，提高效率。但主要用于键值查找，对于给定一个字符串匹配其前缀这种场景不适用。</p></li><li><p><strong>三数组实现</strong></p><p>利用三个数组（分别叫做 base, next, check）来实现状态的转移，将前缀树压缩到三个数据里，能够较好的节省内存；数组的方式也能较好的利用缓存。</p></li><li><p><strong>双数组实现</strong></p><p>是在三数组的基础上，将 base 数组重用为 next 数组，节省了一个数组，并没有增加其他开销。与三数组相比，内存使用和效率进一步提升。</p></li></ul><p>综上，双数组 trie（Double Array trie，简称为 DATrie）的实现有明显的优势，以下讨论 DATrie 的细节（只介绍构造和查询，删除节点不常用，而且比较复杂，暂时略过）。</p><h1 id="5-Double-Array-Trie-树"><a href="#5-Double-Array-Trie-树" class="headerlink" title="5. Double-Array Trie 树"></a>5. Double-Array Trie 树</h1><h2 id="5-1-DATrie-构造方法"><a href="#5-1-DATrie-构造方法" class="headerlink" title="5.1 DATrie 构造方法"></a>5.1 DATrie 构造方法</h2><ol><li><p>数组表示 trie 树的状态转移，父节点跳转到子节点转化为父状态跳转到子状态。</p></li><li><p>利用两个数组 <code>base</code>, <code>check</code>表示状态的转移：</p><ul><li><code>base</code> 数组的索引用来表示状态</li><li><code>base</code> 数组里存的数据称为 offset</li><li><code>check</code> 数组里存的数据是父状态的索引</li><li><code>check</code> 与<code>base</code> 大小相同，一一对应，用于保存父状态，以及解决冲突</li></ul></li><li><p>状态 S 接收到字符 c 后转移到状态 T:</p><script type="math/tex; mode=display">S \overset{c}{\to} T</script><p>满足：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">check[base[S] + c] = S</span><br><span class="line">base[S] + c = T</span><br><span class="line">base[T] = base[S]</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/trie1.png" alt></p><ul><li><code>base</code> 数组的索引为 0，1，…, base[s], …, S, …, T，均表示 trie 树的状态</li><li>从 S 状态接收到 c 跳转到 T, 则表示为 base 数组索引为 S 的内容 base[S] 为基地址，加上跳转偏移 c，得到下一个 T 状态在 base 的索引 <code>T=base[S] + C</code></li><li><code>check</code> 数组对应 T 的内容 check[T] 为跳转过来的父状态，即 S。</li></ul></li></ol><h2 id="5-2-DATrie-查询"><a href="#5-2-DATrie-查询" class="headerlink" title="5.2 DATrie 查询"></a>5.2 DATrie 查询</h2><ol><li>从 <code>base</code> 数组索引 0 开始，初始状态为 S=base[0]，其中偏移的基地址为 base[S]</li><li>接受到 c，则跳转到 <code>base</code> 数组索引 T=base[S] + c，检查此时 <code>check</code> 数组的 check[T] == S，为真跳转到 3，否则匹配失败。</li><li>如果 <code>base[T] == LEAF_VALUE</code> （这里 <code>LEAF_VALUE</code> 用来表示叶子节点的特殊值），则匹配完成；否则，令 S = T, 跳转到 2。</li></ol><p>状态更新的伪码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">T := base[S] + c</span><br><span class="line"></span><br><span class="line">if check[T] = S then</span><br><span class="line">    next state := T</span><br><span class="line">else</span><br><span class="line">    fail</span><br><span class="line">endif</span><br></pre></td></tr></table></figure><h2 id="5-3-举个例子"><a href="#5-3-举个例子" class="headerlink" title="5.3 举个例子"></a>5.3 举个例子</h2><ul><li><p>假定输入两个前缀为 ‘ab’ ,  ‘ad’ ，将字母 a-z 映射为数字 1，2，3,…, 26.</p></li><li><p>这里用 -1 代表数组元素为空，-2 代表叶子节点，-3 代表根节点</p></li><li><p>状态如下：</p><ol><li><p>初始状态</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/dat_example1.png" alt></p></li><li><p>输入 ‘a’ （’ab’）</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/dat_example2.png" alt></p><p><code>base[0]+a</code>，由状态 0 跳转到状态 2。<code>check[2]</code> 为 -1，说明为空，更新为父状态 0；<code>base[2]</code>更新为跳转过来的 <code>base</code>, 即 <code>base[0]</code> 的值 1。</p></li><li><p>输入 ‘b’ （’ab’）</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/dat_example3.png" alt></p><p><code>base[2]+b</code>，由状态 2 跳转到状态 3，<code>check[3]</code>为 -1，说明为空，更新为父状态 2；由于字符串结束，将 <code>base[3]</code> 更新为 -2，代表叶节点。</p></li><li><p>输入 ‘a’（’ad’）</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/dat_example4.png" alt></p><p>图中 <code>base</code> 和 <code>check</code> 的状态不会变化。 根据 <code>base[0]+a</code>，从状态 0 跳转到 2。<code>check[2]</code> 不为空，但<code>check[2]</code> 的值 0 与其父状态 S=0 相等，则无需更新，进入状态 2，等待输入下一个字符。这个过程相当于一个查询过程。</p></li><li><p>输入 ‘d’ （’ad’）</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/dat_example5.png" alt></p><p><code>base[2]+d</code>，由状态 2 跳转到状态 5，<code>check[5]</code> 为 -1，说明为空，更新为父状态 2；由于字符串结束，将 <code>base[5]</code> 更新为 -2，代表叶节点。</p></li></ol></li></ul><h2 id="5-4-解决冲突"><a href="#5-4-解决冲突" class="headerlink" title="5.4 解决冲突"></a>5.4 解决冲突</h2><p>DATrie 不可避免会出现冲突。仍以上面的例子说明，继续插入 ‘ca’：</p><ul><li><p>输入 ‘c’（’ca’）</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/dat_example6.png" alt></p><p>状态由 0 跳转到状态 4，<code>check[4]</code> 空闲，将 <code>check[4]</code> 赋值为 0，<code>base[4]</code> 赋值为1。</p></li><li><p>输入 ‘a’ （’ca’）</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/dat_example7.png" alt></p><p>根据 <code>base[4]+4</code> 状态从4跳转到2， 但是 <code>check[2]</code> 非空，并且 <code>check[2]=0</code> 不等于父状态 4，此时发生冲突。</p></li><li><p>解决冲突</p><ol><li><p>挪动以 4 为父状态状态转移，查找对应 <code>base</code>，<code>check</code> 的连续的空闲空间以放入状态。这里只有最新的输入 ‘a’ 带来的状态转移以 4 为父状态。<code>base[6]</code>, <code>check[6]</code> 有空闲。</p></li><li><p>修改 <code>base[4]</code>, 使其能够根据输入跳转到空闲空间，即 <code>base[4] = 6 - a = 5</code>。</p></li><li><p>重新插入 ‘a’，如下图</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/dat_example8.png" alt></p></li></ol></li></ul><h1 id="6-Trie-树的压缩"><a href="#6-Trie-树的压缩" class="headerlink" title="6. Trie 树的压缩"></a>6. Trie 树的压缩</h1><p>双数组 Trie 树虽然大幅改善了经典 Trie 树的空间浪费，但是由于冲突发生时，程序总是向后寻找空地址，导致数组不可避免的出现空置，因此空间上还是会有些浪费。另外， 随着节点的增加，冲突的产生几率也会越来越大，字典构建的时间因此越来越长，为了改善这些问题，有人想到对双数组 Trie 进行尾缀压缩，具体做法是：将非公共前缀的词尾合并为一个节点（tail 节点），以此大幅减少节点总数，从而改善树的构建速度；同时将合并的词尾单独存储在另一个数组之中（Tail array）， 并通过 tail 节点的 base 值指向该数组的相应位置，以 <code>{baby, bachelor, badge, jar}</code> 四词为例，其实现示意图如下:</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/dat_example9.png" alt></p><ul><li>速度：减少了 <code>base</code>， <code>check</code> 的状态数，以及冲突的概率，提高了插入的速度。</li><li>内存：状态数的减少的开销大于存储 tail 的开销，节省了内存。</li><li>删除：能很方便的实现删除，只需将 tail 删除即可。</li></ul><h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h2><ol><li>Trie 树是一种以信息换时间的数据结构，其查询的复杂度为 $O(m)$。</li><li>Trie 的单数组实现能够达到最佳的性能，但是其空间利用率极低，是典型的以空间换时间的实现。</li><li>Trie 树的哈希实现可以很好的平衡性能需求和空间开销，同时能够实现词典的实时更新。</li><li>Trie 树的双数组实现基本可以达到单数组实现的性能，同时能够大幅降低空间开销；但是其难以做到词典的实时更新。</li><li>对双数组 Trie 进行 tail 改进可以明显改善词典的构建速度，同时进一步减少空间开销。</li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://segmentfault.com/a/1190000008877595" target="_blank" rel="noopener">小白详解 Trie 树</a>, <em>xu_zhoufeng</em></li><li><a href="https://www.iteye.com/blog/huangwei1024-813697" target="_blank" rel="noopener">Double-Array Trie（双数组字典树）</a>, <em>huangwei1024</em></li><li><a href="https://turbopeter.github.io/2013/09/02/prefix-match/" target="_blank" rel="noopener">前缀树匹配(Double Array Trie)</a>, <em>minzhan’s blog</em></li><li><a href="https://zhuanlan.zhihu.com/p/35193582" target="_blank" rel="noopener">双数组前缀树（Double-Array Trie）</a>, <em>两片</em> </li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前缀树（trie）又叫字典树，顾名思义通过字符串的前缀进行查找、匹配的数据结构。Trie 树的应用场景主要包括：分词、词频统计、字符串查询和模糊匹配、字符串排序等。Trie 树大幅降低重复字符串的比较，所以执行效率非常高。&lt;/p&gt;
    
    </summary>
    
      <category term="博客转载" scheme="https://rogerspy.gitee.io/categories/%E5%8D%9A%E5%AE%A2%E8%BD%AC%E8%BD%BD/"/>
    
    
      <category term="双数组前缀树" scheme="https://rogerspy.gitee.io/tags/%E5%8F%8C%E6%95%B0%E7%BB%84%E5%89%8D%E7%BC%80%E6%A0%91/"/>
    
      <category term="数据结构" scheme="https://rogerspy.gitee.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>预训练语言模型：Word Embedding</title>
    <link href="https://rogerspy.gitee.io/2021/08/11/ptm-word-embedding/"/>
    <id>https://rogerspy.gitee.io/2021/08/11/ptm-word-embedding/</id>
    <published>2021-08-11T13:27:18.000Z</published>
    <updated>2022-01-12T08:37:38.404Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://aylien.com/images/uploads/general/tumblr_inline_o8tinsmw081u37g00_540.png" alt></p><p>词嵌入（word embedding）是一种用稠密向量来表示词义的方法，其中每个词对应的向量叫做词向量（word vector）。词嵌入通常是从语言模型中学习得来的，其中蕴含着词与词之间的语义关系，比如 “猫” 和 “狗” 的语义相似性大于 “猫” 和 “计算机” 。这种语义相似性就是通过向量距离来计算的。</p><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><h2 id="1-1-词表示法简史"><a href="#1-1-词表示法简史" class="headerlink" title="1.1 词表示法简史"></a>1.1 词表示法简史</h2><p>自然语言文本在很长时间里并没有一个统一的表示法，用于计算机进行计算。通常人们给每个词分配一个 id，将词作为离散符号输入计算机系统。</p><ul><li><p><strong>查字典</strong></p><p>最直接的方法是创建一个词表，每个词分配一个唯一的 ID，比如：</p><blockquote><p>  我， 0</p><p>  是， 1</p><p>  谁， 2</p><p>  …</p></blockquote></li><li><p><strong>One-hot 编码</strong></p><p>同样是先建立一个词表，然后给词表中的每个词分配一个大小为词表大小的向量来表示词。每个词对应的向量中，只有一个位置的数字为 1，其他位置上的数字全部是 0。词与词的 one-hot 向量两两正交。整个词表就是一个 $1\times (N+1)$ 的矩阵，其中 $N$ 表示词表大小，额外的 1 表示 <em>UNK</em> ，即不在词表中的词的统一标识。比如：</p><blockquote><p>  我，[1, 0, 0, 0, …]</p><p>  是，[0, 1, 0, 0, …]</p><p>  谁，[0, 0, 1, 0, …]</p><p>  …</p></blockquote></li><li><p><strong>Distributional 表示法</strong></p><p>以上两种方法存在着一下几个问题：</p><ol><li>正交。词与词之间的语义丢失，我们没有办法从向量表示中得到词与词之间的关联性，</li><li>维度爆炸。通常一个词表会有几万个词，如果用 one-hot 表示，那么整个词表的 one-hot 就是一个几万乘几万的矩阵，极大地消耗了计算机资源。</li><li>矩阵稀疏。one-hot 矩阵中，除了特定位置上的数字是 1， 其余位置全部是 0，造成整个矩阵极端稀疏化，运算过程中极大地浪费了算力，</li></ol><p>因此，人们提出了分布式表示法，希望通过稠密向量来获得词嵌入矩阵。而得到稠密向量的方法就是我们下面要介绍的。</p></li></ul><h2 id="1-2-发展里程碑"><a href="#1-2-发展里程碑" class="headerlink" title="1.2 发展里程碑"></a>1.2 发展里程碑</h2><div class="timeline"><div class="timenode"><div class="meta"><p></p><p>2003 年 —— 前馈神经网络语言模型</p><p></p></div><div class="body"><p>2003 年 <em>Bengio</em> 等人提出前馈神经网络语言模型（FFNNLM），该模型的一个重要副产物就是词向量。相当于提出了一种利用语言模型训练词向量的方法，同样为后来的 Word2vec 打下了基础。</p></div></div><div class="timenode"><div class="meta"><p></p><p>2005 年 —— 层级 Softmax</p><p></p></div><div class="body"><p><em>Morin &amp; Bengio</em> 提出层级 softmax 思想。给定大小为 $V$ 的词表，通过一棵二叉树计算输出词的概率分布，将计算复杂度从 $O(V)$ 降到 $O(\log(V))$。这一思想成为后来 word2vec 模型的重要组成部分。</p></div></div><div class="timenode"><div class="meta"><p></p><p>2010 年 —— Noise Contrastive Estimation</p><p></p></div><div class="body"><p><em>Gutmann &amp; Hyvarinen</em> 提出噪声对比估计（NCE）方法。其基本思想是：一个好的模型可以利用<strong>逻辑回归</strong>从噪声中识别有用数据。后来 NCE 被 <em>Mnih &amp;Teh</em> 用于语言模型。后来 Word2vec 中的负采样技术就是 NCE 的简化版。</p></div></div><div class="timenode"><div class="meta"><p></p><p>2013 年 —— word2vec</p><p></p></div><div class="body"><p><em>Mikolov</em> 等人提出 word2vec 模型，使得大规模训练词向量成为现实。Word2vec 包含两个模型：<em>skip-gram</em> 和 <em>CBOW</em>。为了加速计算，word2vec 将 softmax 替换成层级 softmax，二叉树用的是哈夫曼树（Huffman tree）。</p></div></div><div class="timenode"><div class="meta"><p></p><p>2013 年 —— 负采样</p><p></p></div><div class="body"><p><em>Mikolov</em> 等人对原来的 word2vec 模型进行了优化，提出负采样的方法。负采样是噪声对比估计的简化版，比层级 softmax 更简单、更快。</p></div></div><div class="timenode"><div class="meta"><p></p><p>2014 年 —— GloVe</p><p></p></div><div class="body"><p><em>Pennington</em> 等人基于词共现的方法，提出另一种训练词向量的方法：Glove。与 word2vec 相比，两个模型表现相差不大，而 GloVe 更容易并行化训练。</p></div></div></div><p>接下来我们介绍两种主要的训练词嵌入的方法：</p><ul><li><strong>Context-based</strong>：给定上下文，设计模型预测中心词。</li><li><strong>Count-based</strong>：统计文本中词的共现矩阵，然后利用矩阵分解的方法对矩阵进行降维。</li></ul><h1 id="2-Context-based-Word2Vec"><a href="#2-Context-based-Word2Vec" class="headerlink" title="2. Context-based: Word2Vec"></a>2. Context-based: Word2Vec</h1><p>2013 年 <a href="http://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">Mikolov</a> 等人提出一种模型 —— Word2Vec。该模型包含两种架构：<em>Continuous Bag-of-Words（CBOW）</em> 和  <em>Continuous Skip-gram（Skip-gram）</em>，然后在随后的文章中提出了两种模型训练的优化方法：<em>hierarchical softmax（层级 softmax）</em> 和 <em>negative sampling（负采样）</em>。Mikolov 等人不是第一个提出连续向量表示词的人，但是他们提出的 word2vec 模型是第一个能应用在大规模语料上的模型，具有非常重要的意义。</p><p>假设有一个固定大小的滑动窗口，沿着句子从头到尾滑动取片段，每个窗口中中心词即为目标词（target），其他的词为上下文（context）。举个例子（假设已经分词）：</p><blockquote><p>  天才 就是 百分之一 的 灵感 加 百分之九十九 的 汗水。</p></blockquote><div class="table-container"><table><thead><tr><th>滑动窗口（size=5）</th><th>target</th><th>context</th></tr></thead><tbody><tr><td>[<font color="red">天才</font>, 就是, 百分之一]</td><td>天才</td><td>就是, 百分之一</td></tr><tr><td>[天才, <font color="red">就是</font>, 百分之一, 的]</td><td>就是</td><td>天才, 百分之一,的</td></tr><tr><td>[天才, 就是, <font color="red">百分之一</font>, 的, 灵感]</td><td>百分之一</td><td>天才, 就是, 的, 灵感</td></tr><tr><td>…</td><td>…</td><td>…</td></tr><tr><td>[灵感, 加, <font color="red">百分之九十九</font>, 的, 汗水]</td><td>百分之九十九</td><td>灵感, 加, 的, 汗水</td></tr><tr><td>[加, 百分之九十九, <font color="red">的</font>, 汗水]</td><td>的</td><td>加, 百分之九十九, 汗水</td></tr><tr><td>[百分之九十九, 的, <font color="red">汗水</font>]</td><td>汗水</td><td>百分之九十九, 的</td></tr></tbody></table></div><h2 id="2-1-Skip-Gram-Model"><a href="#2-1-Skip-Gram-Model" class="headerlink" title="2.1 Skip-Gram Model"></a>2.1 Skip-Gram Model</h2><p>Skip-gram 模型的核心思想是通过<strong>中心词</strong>预测<strong>上下文</strong>，即：</p><script type="math/tex; mode=display">p(w_{i-2}, w_{i-1}, w_{i+1}, w_{i+2}|w_i)</script><p>Skip-gram 模型采用的是一个浅层神经网络来计算这个概率分布。该一共只有三层：输入层、投影层（隐藏层）、输出层。模型结构如下图：</p><p><img width="500" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210811213240.jpg"></p><p>假设上下文中的词相互独立，则：</p><script type="math/tex; mode=display">p(w_{i-2}, w_{i-1}, w_{i+1}, w_{i+2}|w_i) = p(w_{i-2}|w_i)\cdot p(w_{i-1}|w_i) \cdot p(w_{i+1}|w_i)\cdot p(w_{i+2}|w_i)</script><p>相当于训练样本的（target，context）对拆解成 $2m$个（target，context word）对，其中 $m$ 表示滑动窗口除中心词外一半大小（很多地方会直接把 $m$ 定义为窗口大小），context word 表示上下文中每个词。例如，中心词为 “百分之九十九”，那么训练样本就是：</p><blockquote><p>  （百分之九十九，灵感）</p><p>  （百分之九十九，加）</p><p>  （百分之九十九， 的）</p><p>  （百分之九十九， 汗水）</p></blockquote><p>此时，上面的模型结构则等效于下图：</p><p><img width="500" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210811213223.jpg"></p><p>模型的输入是中心词，输出是上下文词之一。</p><p>假设词表 $\mathcal{V}$​​​​​ 的大小为 $V=|\mathcal{V}|$​​​​​，中心词在词典中的索引为 $i$​​​​​，上下文对应的词在词表中的索引为 $j$， $N$ 表示词向量 $\boldsymbol{v}_i$ 的维度，即 $\boldsymbol{v}_i \in \mathbb{R}^N$​​​​​。</p><p>关于模型的一些细节：</p><ul><li><p>$\boldsymbol{x}$​​​ 和 $\boldsymbol{y}$​​​ 都是 one-hot 编码，编码中 $i$​ 和 $j$​ 对应的位置为 1，其余位置全部为 0，$\boldsymbol{x},\boldsymbol{y} \in \mathbb{R}^{1\times V}$​。​​​</p></li><li><p>首先，将输入 $\boldsymbol{x}$​​ 与一个 $\boldsymbol{W}$​​ 矩阵相乘得到隐藏层 $\boldsymbol{h}$​​，其中 $\boldsymbol{W}\in \mathbb{R}^{V\times N}$​​​​​​​ ，则 $\boldsymbol{h}\in \mathbb{R}^{1\times N}$​​。实际上 $\boldsymbol{h}$​ 相当于 $\boldsymbol{W}$​ 的第 $i$​ 行：</p><script type="math/tex; mode=display">[0, ..., 1, ..., 0] \times \left[\begin{matrix}w_{00}, w_{01}, ..., w_{0N} \\\\\vdots \\\\w_{i0}, w_{i1}, ..., w_{iN} \\\\\vdots \\\\w_{V0}, w_{V1}, ..., w_{VN}\end{matrix}\right] = \left[w_{i0}, ...,w_{ii}, ..., w_{iN}\right]</script></li><li><p>用 $\boldsymbol{h}$​ 与另一个矩阵 $\boldsymbol{W’}\in \mathbb{R}^{N\times V}$​​ 相乘得到一个 $1\times V$ 的向量 $\boldsymbol{h’}$。</p></li><li><p>将 $\boldsymbol{h’}$ 进行归一化即可得到 $\boldsymbol{y}$​ 的 one-hot 概率分布：</p><script type="math/tex; mode=display">\boldsymbol{y} = \mathrm{softmax}(\boldsymbol{h'})</script></li><li><p>$\boldsymbol{y}$ 中概率最大的位置 $j$ 即对应词表第 $j$ 个词：</p><script type="math/tex; mode=display">w_j = \mathcal{V}_{j=\arg \max (\boldsymbol{y})}</script><p>比如：</p><blockquote><p>  假设 $\mathcal{V}=[我，的，灵感，天才，…]$</p><p>  $\boldsymbol{y} = [0.1, 0.2, 0.3, 0.2, 0.15, 0.05]$</p><p>  $\boldsymbol{y}$​ 中最大概率为 0.3，对应的索引是 2，即 $j=2$​​，</p><p>  则 $w_j = \mathcal{V}_2 = 灵感$​。</p></blockquote></li><li><p>模型中有两个矩阵 $\boldsymbol{W}$ 和 $\boldsymbol{W’}$​，非别对应着中心词的向量编码和上下文的向量编码。在自然语言处理应用中，一般使用中心词向量作为词的表征向量，即 $\boldsymbol{W}$ 就是我们最终得到的 word embedding。</p></li></ul><h2 id="2-2-CBOW-Model"><a href="#2-2-CBOW-Model" class="headerlink" title="2.2 CBOW Model"></a>2.2 CBOW Model</h2><p>连续词袋模型（CBOW）模型与 skip-gram 模型正相反，CBOW 是利用<strong>上下文</strong>来预测<strong>中心词</strong>，即：</p><script type="math/tex; mode=display">p(w_i|w_{i-2},w_{i-1},w_{i_1},w_{i+1})</script><p>模型结构如下图所示：</p><p><img width="500" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210811213150.png"></p><p>由于 CBOW 模型的输入有多个，所以我们将得到的 context 向量取平均，然后使用和 skip-gram 一样的方法来计算中心词的概率分布。</p><script type="math/tex; mode=display">\boldsymbol{h} = \frac{1}{2m}\sum \boldsymbol{x}_i \cdot \boldsymbol{W}</script><h2 id="2-3-Loss-object-Functions"><a href="#2-3-Loss-object-Functions" class="headerlink" title="2.3 Loss/object Functions"></a>2.3 Loss/object Functions</h2><p>无论是 skip-gram 模型还是 CBOW 模型，模型参数就是中心词向量和上下文词向量对应的嵌入矩阵 $\boldsymbol{W}$​​​​ 和 $\boldsymbol{W’}$​​​。给定输入词 $w_I$​​​ ，其在 $\boldsymbol{W}$​​​ 中对应的向量为 $\boldsymbol{v}_I$​​​​（即 $\boldsymbol{h}$​​）。$\boldsymbol{W’}$​​ 中每一列对应的词向量为 $\boldsymbol{v’}_j$​​​​​​。输出词 $w_O$​​ 对应的词向量为 $\boldsymbol{v’}_o$​​。</p><p>通过最小化损失函数对模型进行训练，下面以 skip-gram 为例介绍一些常用的损失/目标函数。</p><h3 id="2-3-1-标准-Softmax（Full-Softmax）"><a href="#2-3-1-标准-Softmax（Full-Softmax）" class="headerlink" title="2.3.1 标准 Softmax（Full Softmax）"></a>2.3.1 标准 Softmax（Full Softmax）</h3><p>用数学语言来描述上面的模型，即对于单个样本我们的目标函数为：</p><script type="math/tex; mode=display">p(w_O|w_I) = \frac{\exp(\boldsymbol{v'}_O^\mathsf{T} \cdot \boldsymbol{v}_I)}{\sum_{j=1}^V\exp(\boldsymbol{v'}_j^\mathsf{T} \cdot \boldsymbol{v}_I)}</script><p>从上式可以看出，对于任意单一样本，我们都需要对全词表进行指数求和，然而当 $V$ 非常大的时候（实际情况下 $V$​ 通常会有几万到几十万），计算将会变得非常复杂，根据 2.3.3 节关于交叉熵损失函数的介绍中，我们也可以看出进行后向传播的时候，计算过程同样是需要计算完整词表。因此，<a href="https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf" target="_blank" rel="noopener">Morin and Bengio</a> 等人在 2005 年的时候，提出了层级 Softmax，采用二叉树来加速计算。</p><h3 id="2-3-2-层级-Softmax（Hierarchical-Softmax）"><a href="#2-3-2-层级-Softmax（Hierarchical-Softmax）" class="headerlink" title="2.3.2 层级 Softmax（Hierarchical Softmax）"></a>2.3.2 层级 Softmax（Hierarchical Softmax）</h3><p><img width="500" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210811213210.png"></p><p>由于标准的 softmax 的计算复杂度较高，所以人们就不断思考对其进行优化。2001 年 <a href="https://arxiv.org/abs/cs/0108006" target="_blank" rel="noopener"><em>Goodman</em></a> 提出基于分类思想的加速方案。简单来说，假设我们词表中有 10000 个词，在传统的方法是在这 10000 个词上做 <em>softmax</em> 获得每个词的概率分布，然后取出概率最大的词，这样我们需要计算 10000 次。如果我们将这 10000 个词进行分类，假设分成 100 个类别，每个类别 100 个词。这个时候我们的计算过程是，先用一个 <em>softmax</em> 计算下一个词是属于什么类别，然后再用一个 <em>softmax</em> 计算概率最大的类别中的词的概率分布，这样我们只需要两个 100 次的计算量，计算速度直接提升 50 倍。</p><p>基于这个思想，<a href="https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf" target="_blank" rel="noopener"><em>Morin &amp; Bengio</em></a> 于 2005 年提出层级 softmax 的方法：使用平衡二叉树来构建这种分类关系，能够将计算复杂度降到 $O(\log_2(|\mathcal{V}|))$。由于他们利用的是先验知识（wordnet 中的 is-a 关系）来构建二叉树，最终的而效果并不理想。随后 <em>Mnih &amp; Hinton</em> 采用 boostrapping 的方法，从一个随机树开始自动学习一棵平衡二叉树。</p><p>直到 2013 年 <em>Mikolov</em> 等人提出使用 Huffman 树来代替平衡二叉树，使得层级 softmax 在效果和效率上都达到了新的高度。</p><h4 id="2-3-2-1-Huffman-树"><a href="#2-3-2-1-Huffman-树" class="headerlink" title="2.3.2.1 Huffman 树"></a>2.3.2.1 Huffman 树</h4><p>Huffman 树是一个用于数据压缩的算法。计算机中所有的数据都是以 0 和 1 进行存储的，最简单的数据编码方式是 <strong>等长编码</strong>。假设我们的数据中有 6 个字母，那么我们要将这些字母区分开，就至少需要三位二进制数来表示，$2^3=8&gt;6$，如果数据中的字符数更多，那就需要更长的二进制数进行编码。然而我们希望用尽可能少的二进制数对数据进行编码，尤其是实际生活中，有些字符使用频率非常高，另一些字符很少使用。我们希望使用频率高的字符编码长度更短，这样就可以节省存储空间了。所以这里就涉及到 <strong>变长编码</strong>。</p><p>比如，给定一个字符串 <code>aabacdab</code>，包含了 8 个字符，我们发现这个这个字符串中包含了 4 个不同的字符 <code>a</code>、<code>b</code>、<code>c</code>、<code>d</code>，分别对应的频率为 4、2、1、1。由于 <code>a</code> 的频率大于 <code>b</code>，<code>b</code> 的频率大于 <code>c</code> 和 <code>d</code>。所以，我们可以给 <code>a</code> 分配一个 1 位的编码长度，<code>b</code> 分配 2 位的编码长度，<code>c</code> 和 <code>d</code> 分配 3 位的编码长度：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a: 0</span><br><span class="line">b: 11</span><br><span class="line">c: 100</span><br><span class="line">d: 011</span><br></pre></td></tr></table></figure><p>所以，<code>aabacdab</code> 就被编码成了 <code>00110100011011</code>（<code>0|0|11|0|100|011|0|11</code>）。但是这个编码会有问题，那就是歧义性。因为我们不仅需要编码，还需要解码。当我们把数据存储到计算机以后，还需要从计算机中将数据读取出来。读取数据的过程就是解码的过程。如果我们用上面的编码进行存储解码的时候，会出现不同的解码方式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">0|011|0|100|011|0|11    adacdab</span><br><span class="line">0|0|11|0|100|0|11|011   aabacabd</span><br><span class="line">0|011|0|100|0|11|0|11   adacabab</span><br><span class="line">…</span><br></pre></td></tr></table></figure><p>为了避免解码歧义，我们需要保证编码满足 “<strong>前缀规则</strong>”：任意编码不能是其他编码的前缀。在上例中，<code>0</code> 是 <code>011</code> 的前缀，所以才会出现解码歧义性问题。</p><p>Huffman 树就是用来做这种变长编码的数据结构，构造过程如下：</p><ol><li><p>计算字符频率</p><p><img width="250" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/hf-character-frequency.png"></p></li><li><p>根据词频对字符进行排序，并按升序进行排列，得到序列 <code>Q</code>：</p><p><img width="250" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/hf-character-frequency-sorted.png"></p></li><li><p>创建一个空节点 <code>z</code>。节点 <code>z</code> 的左子节点是频率最低的字符，右子节点是频率第二低的字符。节点 <code>z</code> 的频率为左右子节点字符频率之和</p><p><img width="200" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/hf-encoding-1.png"></p></li><li><p>从 <code>Q</code> 中删除两个上一步中两个频率最低的字符，然后将两者频率之和添加到 <code>Q</code> 中。</p></li><li><p>重复 3-4 两步</p><table><tr>    <td><center><img width="250" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/hf-encoding-2.png"></center></td>    <td><center><img width="250" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/hf-encoding-3.png"></center></td></tr></table>            </li><li><p>将左侧的边赋值为 0，右侧的边为 1。</p><p><img width="250" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/hf-encoding-4.png"></p></li></ol><p>这样就构建好了一棵 Huffman 树。Huffman 编码就是找到从根节点到对应的字符之间的路径，然后将路径上的边对应的值拼接在一起。比如，上例中的 <code>A</code>、<code>B</code>、<code>C</code>、<code>D</code> 的编码分别为：<code>11</code>、<code>100</code>、<code>0</code>、<code>101</code>。</p><p>解码过程就是按照编码找到相应的路径：</p><p><img width="250" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/hf-decoding.png"></p><h4 id="2-3-2-2-基于-Huffman-树的层级-softmax"><a href="#2-3-2-2-基于-Huffman-树的层级-softmax" class="headerlink" title="2.3.2.2 基于 Huffman 树的层级 softmax"></a>2.3.2.2 基于 Huffman 树的层级 softmax</h4><p>Word2vec 中是预先统计语料中的词频，根据词频构建起一棵 Huffman 树。</p><blockquote><p>Huffman 树的每个叶子节点是词表中的一个词，每个除叶子节点和根节点以外的节点都表示一个二分类的概率，这个概率用来决定去往左右子节点的路径。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210811213210.png" alt></p><p>如上图所示，每个叶子结点（白圈）表示一个词表中的词 $w_i$，每个非叶子节点（灰圈）表示该路径上的概率。每个词都有一条唯一可达的路径，$n(w_i, j)$ 表示 $w_i$ 的路径上 第 $j$ 个节点。比如 $w_2$ 的路径就是 $n(w_2,1)n(w_2,2)n(w_2,3)w_2$。这条路径就对应 Huffman 编码。$w_2$ 的概率就是这条路径上每个节点的概率累积：</p><script type="math/tex; mode=display">p(w_O \vert w_I) = \prod_{j-1}^{L(w_O)-1} p(n(w_O,j))</script><p>其中 $L(w_O)$  表示 $w_O$ 的路径长度（Huffman 编码长度）。由于这是一个二叉树，相当于 $p(n(w_O,j))$ 是一个二分类，所以可以使用 $\sigma$ 函数进行计算：</p><script type="math/tex; mode=display">p(w_O \vert w_I) = \prod_{j=1}^{L(w_O)-1} \sigma({\mathbb{I}_{\text{turn}} \cdot\boldsymbol{v'}_{n(w_O, j)}}^{\top} \cdot \boldsymbol{v}_{w_I})</script><p>其中 $v’_{n(w_O,j)}$ 表示 $n(w_,j)$ 节点对应的向量，$\mathbb{I}_{\text{turn}}$ 表示特殊的标识函数：如果 $n(w_O,j+1)$ 是 $n(w_O,j)$ 的左子节点，则 $\mathbb{I}_{\text{turn}}=1$ ，否则为 $\mathbb{I}_{\text{turn}}=-1$。比如，上图中，我们要计算 $w_2$ 的概率：</p><script type="math/tex; mode=display">P(w_2 \mid w_I) = \sigma(\boldsymbol{v'}_{n(w_2,1)}^\top \boldsymbol{v}_I) \cdot \sigma(\boldsymbol{v'}_{n(w_2,2)}^\top \boldsymbol{v}_I) \cdot \sigma(-\boldsymbol{v'}_{n(w_2,3)}^\top \boldsymbol{v}_I)</script><p>内部节点的向量 $\boldsymbol{v’}_{n(w_i, j)}$ 可以通过训练得到。由 $\sigma(\cdot)$ 的定义：</p><script type="math/tex; mode=display">\sigma(z) = \frac{1}{1+\exp(-z)}</script><p>可知，整个概率的计算都无需遍历整个词表，只需计算 $\log_2(V)$ 次 $\sigma(\cdot)$ 即可，相当于将计算复杂度降低到了 $\log_2(V)$，大幅提升了计算效率。</p><p>由于 $\sigma(x)+\sigma(-x)=1$，给定中心词 $w_I$，生成词典 $\mathcal{V}$ 中任意词的体哦阿健概率之和也满足：</p><script type="math/tex; mode=display">\sum_{w\in \mathcal{V}} p(w|w_I)=1</script><p>由于 Huffman 树是用来对数据进行压缩编码的，其主要思想是高频的词距离根节点越近，那么它的路径就会越短，所需要计算的 $\sigma(\cdot)$ 函数的次数也会越少。所以相比平衡二叉树，Huffman 树的计算更有效率。</p><p>需要注意的是，我们在训练过程中，由于已知我们需要预测的词是哪一个，所以只需要计算对应的词的概率，然后进行优化即可。但是在推理过程中，我们并不知道哪个词是最优解，所以还是需要遍历整个词表。所以基于 Huffman 树的 word2vec 加速了训练过程而没有加速推理过程。</p><h3 id="2-3-3-交叉熵（Cross-Entropy）"><a href="#2-3-3-交叉熵（Cross-Entropy）" class="headerlink" title="2.3.3 交叉熵（Cross Entropy）"></a>2.3.3 交叉熵（Cross Entropy）</h3><p>交叉熵用于度量两个概率（$p$ 和 $q$​​）分布间的差异性信息的一个指标。计算公式如下：</p><script type="math/tex; mode=display">H(p, q) = -\sum_xp(x)\log q(x)</script><p>当交叉熵用于损失函数的时候，我们需要度量的是真实标签概率分布（$\boldsymbol{y}_{true}$）和模型输出标签概率分布（$\boldsymbol{y}_{pred}$）之间的差异，即：</p><script type="math/tex; mode=display">H(\boldsymbol{y}_{true}, \boldsymbol{y}_{pred}) = -\sum \boldsymbol{y}_{true}\cdot \log(\boldsymbol{y}_{pred})</script><p>在我们的情况下，$\boldsymbol{y}_{true}$​ 中只有 $y_{i=O}=1$​，其余位置 $y_j$​ 全部是 0，$\boldsymbol{y}_{pred} = p(w_i|w_I)$​。也就是说，我们只需要计算 $w_i=w_O$​ 位置的交叉熵即可，如下图所示。 </p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/20210813001314.jpg" alt></p><script type="math/tex; mode=display">\mathcal{L}_{\theta} = H(y_i, w_i) = -\sum_{i=1}^{V}y_i\log p(w_i|w_I) \overset{i=O}{=} -\log p(w_O|w_I)</script><p>式中 $\theta$ 表示我们需要训练的参数。如上面介绍的，交叉熵是用来度量两个分布的差异性的指标。对于我们的模型来说，当然是 $\boldsymbol{y}_{ture}$ 和 $\boldsymbol{y}_{pred}$​ 的差异越小越好。所以我们模型训练最终的目的是<strong>最小化交叉熵</strong>。</p><p>将 $p(w_O|w_I)$ 的 full softmax 公式代入交叉熵损失函数中得到：</p><script type="math/tex; mode=display">\mathcal{L}_{\theta} = -\log \frac{\exp(\boldsymbol{v'}_{O}^\mathsf{T} \cdot \boldsymbol{v}_I)}{\sum_{j=1}^V\exp(\boldsymbol{v'}_{j}^\mathsf{T} \cdot \boldsymbol{v}_I)}=-\boldsymbol{v'}_{O}^\mathsf{T} \cdot \boldsymbol{v}_I + \log \sum_{j=1}^V \exp(\boldsymbol{v'}_{j}^\mathsf{T} \cdot \boldsymbol{v}_I)</script><p>使用随机梯度下降算法对模型开始训练，需要计算损失函数的梯度。为了简化，我们令 $z_{IO}=\boldsymbol{v’}_{O}^\mathsf{T} \cdot \boldsymbol{v}_I$​ 及 $z_{Ij}=\boldsymbol{v’}_{j}^\mathsf{T} \cdot \boldsymbol{v}_I$​。</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}\nabla_\theta \mathcal{L}_\theta &= \nabla_\theta(-z_{IO}+\log\sum_{j=1}^V\exp(z_{Ij}))\\\\                                 &= -\nabla_\theta z_{IO} + \nabla_\theta(\log \sum_{j=1}^V \exp(z_{Ij})) \\\\                                 &= -\nabla_\theta z_{IO} + \frac{1}{\sum_{j=1}^V\exp(z_{Ij})} \sum_{j=1}^V \exp(z_{Ij}) \cdot \nabla_\theta z_{Ij} \\\\                                 &= -\nabla_\theta z_{IO} + \sum_{j=1}^V \frac{\exp(z_{Ij})}{\sum_{j=1}^V\exp(z_{Ij})} \cdot \nabla_\theta z_{Ij} \\\\                                 &= -\nabla_\theta z_{IO} + \sum_{j=1}^V p(w_j|w_I) \cdot \nabla_\theta z_{Ij} \\\\                                 &= -\nabla_\theta z_{IO} + \mathbb{E}_{w_j \sim Q(\bar{w})} \cdot \nabla_\theta z_{Ij}\end{aligned}\end{equation}</script><p>将 $z_{IO}$ 和 $z_{Ij}$ 代回原式，根据下面两式：</p><script type="math/tex; mode=display">\nabla_\theta z_{IO} =  \frac{\partial (\boldsymbol{v'}_{O}^\mathsf{T} \cdot \boldsymbol{v}_I)}{\partial \boldsymbol{v}_I} = \boldsymbol{v'}_{O} ,\quad\nabla_\theta z_{Ij} = \frac{\partial (\boldsymbol{v'}_{j}^\mathsf{T} \cdot \boldsymbol{v}_I)}{\partial \boldsymbol{v}_I} = \boldsymbol{v'}_{j} \\\\</script><p>可得：</p><script type="math/tex; mode=display">\nabla_\theta \mathcal{L}_\theta = -\boldsymbol{v'}_{O} + \mathbb{E}_{w_j \sim Q(\tilde{w})} \cdot \boldsymbol{v'}_{j}</script><p>上式中 $Q(\tilde{w})$​ 表示噪声概率分布。根据上式，输出词的词向量越大，损失越小；而其他词的词向量越小，则损失越小。因此，交叉熵损失函数会使模型将正确的输出更加凸显，而对错误的输出进行压制，从而使参数达到最优。</p><h3 id="2-3-4-Noise-Contrastive-Estimation"><a href="#2-3-4-Noise-Contrastive-Estimation" class="headerlink" title="2.3.4 Noise Contrastive Estimation"></a>2.3.4 Noise Contrastive Estimation</h3><p>噪声对比估计（NCE）是通过简单的逻辑回归来区分目标词和非目标词的。</p><p>给定输入词 $w_I$，正确的输出词是 $w_O$。同时，我们可以从噪声词分布 $Q(\tilde{w})$ 中进行采样得到 $N$ 个负样本词：</p><script type="math/tex; mode=display">\tilde{w}_1,\tilde{w}_2,\dots,\tilde{w}_N \sim Q(\tilde{w})</script><p>此时，我们的样本就成了 $w_O$  为正样本，$\tilde{w}_1,\tilde{w}_2,\dots,\tilde{w}_N$ 为负样本，然后再用一个二分类器进行分类：</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - \left[ \log p(d=1 \vert w_O, w_I) + \sum_{i=1, \tilde{w}_i \sim Q}^N \log p(d=0|\tilde{w}_i, w_I) \right]</script><p>$d$ 表示二分类器的输出标签。</p><p>当 $N$ 足够大时，根据<a href="https://en.wikipedia.org/wiki/Law_of_large_numbers" target="_blank" rel="noopener">大数定理</a>可得:</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - \left[ \log p(d=1 \vert w_O, w_I) + N\mathbb{E}_{\tilde{w}_i \sim Q} \log p(d=0|\tilde{w}_i, w_I) \right]</script><p>为了计算概率分布 $p(d=1 \vert w_O, w_I)$，我们可以从联合概率 $p(d, w_j \vert w_I), w_j \in [w_O, \tilde{w}_1, \tilde{w}_2, \dots, \tilde{w}_N]$。我们有 $1/(N+1)$ 的概率得到 $w_j=w_O$，这个概率是一个条件概率 $p(w_j=w_O\vert w_I)$，同时我们有 $N/(N+1)$ 的概率得到噪声词 $q(\tilde{w}_{1:N})$。</p><script type="math/tex; mode=display">p(d, w_j | w_I) = \begin{cases} \frac{1}{N+1} p(w_O \vert w_I) & \text{if } d=1 \\\\ \frac{N}{N+1} q(\tilde{w}_{1:N}) & \text{if } d=0 \end{cases}</script><p>然后我们可以计算 $p(d=1 \vert w, w_I)$ 和 $p(d=0 \vert w, w_I)$：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned} p(d=1 \vert w, w_I) &= \frac{p(d=1, w \vert w_I)}{p(d=1, w \vert w_I) + p(d=0, w \vert w_I)} \\\\                     &\overset{贝叶斯公式}{=} \frac{p(w \vert w_I)}{p(w \vert w_I) + Nq(\tilde{w})}\end{aligned}\end{equation}</script><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}p(d=0 \vert w, w_I) &= \frac{p(d=0, w \vert w_I)}{p(d=1, w \vert w_I) + p(d=0, w \vert w_I)}\\\\ &\overset{贝叶斯公式}{=} \frac{Nq(\tilde{w})}{p(w \vert w_I) + Nq(\tilde{w})} \end{aligned}\end{equation}</script><p>最后，NCE 二分类器的损失函数为：</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned} \mathcal{L}_\theta & = - \left[ \log p(d=1 \vert w, w_I) + \sum_{\substack{i=1 \\\\ \tilde{w}_i \sim Q}}^N \log p(d=0|\tilde{w}_i, w_I) \right] \\\\                    & = - \left[ \log \frac{p(w \vert w_I)}{p(w \vert w_I) + Nq(\tilde{w})} + \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^N \log \frac{Nq(\tilde{w}_i)}{p(w \vert w_I) + Nq(\tilde{w}_i)} \right] \end{aligned}\end{equation}</script><p>然而，我们会发现公式中仍然有 $p(w \vert w_I)$ ，即仍然要对整个词表进行求和。为了方便，令 $Z(w_I)$ 为 $p(w\vert w_I)$ 的分母。NCE 对于 $Z(w_I)$ 的处理有两种假设：</p><ol><li><p>将 $Z(w_I)$ 视作常数。<a href="https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf" target="_blank" rel="noopener">Mnih &amp; Teh, 2012</a> 证明对于参数量很大的神经网络模型来说，将 $Z(w_I)$ 固定为 1 对每个 $w_I$ 仍是成立的。此时，上面的损失函数可以简化成：</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - \left[ \log \frac{\exp({v'_w}^{\top}{v_{w_I}})}{\exp({v'_w}^{\top}{v_{w_I}}) + Nq(\tilde{w})} + \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^N \log \frac{Nq(\tilde{w}_i)}{\exp({v'_w}^{\top}{v_{w_I}}) + Nq(\tilde{w}_i)}\right]</script><ul><li><p>这种情况下，我们可以证明，当 $N \to \infty$ 时，$\nabla_\theta \mathcal{L}_{NCE}=\nabla_\theta\mathcal{L}_{entrpy}$。证明过程可参看 <a href="https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf" target="_blank" rel="noopener">Mnih &amp; Teh, 2012</a>。所以 NCE 的优化目标和交叉熵是一样的。作者还发现，当 $N=25$ 时，效果就已经与标准 softmax 效果差不多了，但是速度提升了 45 倍。</p></li><li><p>实际上 $Z(w_I)$ 到底取值是多少，不同作者都有过不同的尝试。但是从表现来看，不同点只是开始的时候收敛速度不同，最终的结果相差不大。</p></li><li><p>噪声分布 $Q(\tilde{w})$ 是一个可调参数，在选择 $Q$ 的分布的时候应该考虑两点：</p><p>① 接近真实数据分布；</p><p>② 容易采样</p></li></ul></li><li><p>将 $Z(w_I)$ 看作一个可训练的参数。</p></li></ol><p>从实践来看，当训练语料比较小的时候，$Z(w_I)$ 直接设置为常数效果更好。当有足够语料的时候，$Z(w_I)$ 作为可训练的一个参数效果更好。</p><p>NCE 处理似乎是故意绕开了标准 Softmax 计算量最大的分母，但其背后有充分的理论推导和证明。如果直接在最大似然估计上用这两种假设（之一）是否可行？</p><p>答案还真是不行。两种情况：</p><ol><li>如果最大似然估计中的 $Z(w_I)$ 为常数，那么 $\mathcal{L}_\theta$ 的第二项 $\log Z(w_I)$ 就是常数，这就意味着 $\mathcal{L}_\theta$ 的导数的第二项就为 0。也就是噪声词的词向量缺少约束，模型只需要让目标词的概率变大即可，最坏情况下预测所有词的概率为 1 即可。</li><li>如果 $Z(w_I)$ 为可训练的一个参数，这个参数没有和数据产生任何联系，只需要简单的变小，就可以让似然概率变大，得到一个完全与数据无关的结果，所以也不可行。</li></ol><h3 id="2-3-5-Negative-Sampling"><a href="#2-3-5-Negative-Sampling" class="headerlink" title="2.3.5 Negative Sampling"></a>2.3.5 Negative Sampling</h3><p><em><a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener">Mikolov</a></em> 等人 2013 年提出的负采样方法是 NCE 的一个简化版变种。因为 word2vec 的目标是训练高质量的词向量，而不是对自然语言中的词进行建模。所以，<em>Mikolov</em> 等人在 NCE 的基础上进一步简化。</p><p>在 NCE 假设 $Z(w_I)=1$ 的基础上，进一步令 $N q(\tilde{w})=1$，则</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}p(d=1\vert w, w_I) &= \frac{p(w \vert w_I)}{p(w \vert w_I)+1} \\\\                   &= \sigma({v'_{w}}^\top v_{w_I}) \\\\p(d=0\vert w, w_I) &= \frac{1}{p(w \vert w_I) + 1} \\\\                    &= 1 - \sigma({v'_{w}}^\top v_{w_I}) \\\\                   &= \sigma(-{v'_{w}}^\top v_{w_I})\end{aligned}\end{equation}</script><p>那么负采样的损失函数为：</p><script type="math/tex; mode=display">\mathcal{L}_\theta =  - \left[ \log \sigma({v'_{w}}^\top v_{w_I}) + \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^N \log \sigma(-{v'_{\tilde{w}_i}}^\top v_{w_I}) \right]</script><p>因为 $Nq(\tilde{w})=1$，所以 $q(\tilde{w})=1/N$ 是一个均匀分布。这里的均匀采样并不是每个词采样概率相同，而是在总的语料中进行均匀采样。这就意味着，它实际上是按照每个词本身的词频来进行采样的，词频越高，采样的概率就越高。这种情况下，模型最终拟合的实际是词的互信息。详细解答看这里：<a href="https://spaces.ac.cn/archives/5617" target="_blank" rel="noopener">“噪声对比估计”杂谈：曲径通幽之妙</a>。互信息与条件概率的区别就类似：条件概率反映“我认识周杰伦，周杰伦却不认识我”，而互信息反映的是“你认识我，我也认识你”。所以，通常负采样的效果比层次 softmax 要好一些。</p><h2 id="2-4-一些小技巧"><a href="#2-4-一些小技巧" class="headerlink" title="2.4 一些小技巧"></a>2.4 一些小技巧</h2><ul><li><p><strong>Soft slide window</strong>。利用滑动窗口构建输入词和输出词样本对的时候，我们可以给距离较远的词更低的权重。比如，设置窗口就最大值 $s_{\text{max}}$，然后每次训练时的真实窗口大小是从 $[1, s_{\text{max}}]$ 中进行随机采样。因此，每个上下文词都有 $1/(d)$ 的概率被取到，其中 $d$ 表示到中心词的距离。</p></li><li><p><strong>下采样高频词</strong>。极端高端的词可能由于太常见而无法得以区分（比如停用词）。而低频词可能会带有很重要的信息。为了平衡高频词和低频词，<em>Mikolov</em> 等人提出采样时对每个词施加一个采样概率 $1-\sqrt{t/f(w)}$。其中 $f(w)$ 表示词频，$t$ 表示相关性阈值，通常取值为 $10^{-5}$。</p></li><li><p><strong>先学词组</strong>。词组表示一个有意义的概念单元，而非简单的独立单词的组合。先学习这些词组将他们作为一个词单元来处理可以提升词向量的质量。比如基于 unigram 和 bigram 统计：</p><script type="math/tex; mode=display">s_{\text{phrase}} = \frac{C(w_i w_j) - \delta}{ C(w_i)C(w_j)}</script><p>其中 $C(\cdot)$ 表示 unigram $w_i$ 或 bigram $w_iw_j$ 的数量，$\delta$ 表示衰减阈值，防止过高频的词或词组。$s_{\text{phrase}}$ 得分越高则采样几率越高。为了形成长于两个单词的短语，我们可以随着分数截止值的降低多次扫描词汇表。</p></li></ul><h1 id="3-Count-based-GloVe"><a href="#3-Count-based-GloVe" class="headerlink" title="3. Count-based: GloVe"></a>3. Count-based: GloVe</h1><p>GloVe（<em>The Global Vector</em>）是 <a href="http://www.aclweb.org/anthology/D14-1162" target="_blank" rel="noopener">Pennington</a> 等人于 2014 年提出的模型。 GloVe 结合了 矩阵分解和 skip-gram 模型。</p><p>众所周知，统计数量和共现可以表示词义。为了区分上下文的词嵌入 $p(w_O \vert w_I)$，我们定义共现概率：</p><script type="math/tex; mode=display">p_{\text{co}}(w_k \vert w_i) = \frac{C(w_i, w_k)}{C(w_i)}</script><p>$C(w_i, w_k)$ 表示 $w_i$ 和 $w_k$ 的共现频率。假设有两个词 $w_i=”ice”$ 和 $w_j=”steam”$，第三个词 $\tilde{w}_k=”solid”$ 与 $”ice”$ 相关，但是与 $”steam”$ 无关，我们希望：</p><script type="math/tex; mode=display">p_{\text{co}}(\tilde{w}_k \vert w_i) > p_{\text{co}}(\tilde{w}_k \vert w_j)</script><p>因此 $\frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert w_j)}$ 会非常大。而如果 $\tilde{w}_k=”water”$ 与 $”ice”$ 和 $”steam”$ 都有关系，或者 $\tilde{w}_k=”fashion”$ 与两者都没有关系，$\frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert w_j)}$ 会接近 1。</p><p>以上描述给我们的直观感受就是，词义是通过共现概率分布的比例得到的，而非共现概率本身。所以，GloVe 模型是将第三个词的向量取决于另两个词之间的关系：</p><script type="math/tex; mode=display">F(w_i, w_j, \tilde{w}_k) = \frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert w_j)}</script><p>确定 $F$ 的函数形式过程如下：</p><ol><li><p>$F(w_i, w_j, \tilde{w}_k)$ 是考察 $i, j, k$ 三个词的相似关系，不妨单独考察 $i, j$ 两个词。在线性空间中，两个向量的相似性最简单的就是欧氏距离 $v_i, v_j$，所以 $F$ 可以是</p><script type="math/tex; mode=display">F(w_i-w_j, \tilde{w}_k) = \frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert w_j)}</script></li><li><p>$\frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert w_j)}$ 是一个标量，而 $F$ 是作用在两个向量上的，向量与矢量之间的关系自然就可以想到内积，所以进一步确定 $F$ 的形式：</p><script type="math/tex; mode=display">F((w_i-w_j) \top \tilde{w}_k) = F(w_i\top \tilde{w}_k-w_j \top \tilde{w}_k) = \frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert w_j)}</script></li><li><p>上式中，左边是差，右边是商。可以通过 $\exp(\cdot)$ 函数将两者结合在一起：</p><script type="math/tex; mode=display">\exp(w_i\top \tilde{w}_k-w_j \top \tilde{w}_k) = \frac{\exp(w_i \top \tilde{w}_k)}{\exp(w_j \top \tilde{w}_k)} = \frac{p_{\text{co}}(\tilde{w}_k \vert w_i)}{p_{\text{co}}(\tilde{w}_k \vert w_j)}</script></li><li><p>现在只要让分子分母分别相等，上式就可以成立：</p><script type="math/tex; mode=display">\exp(w_i \top \tilde{w}_k) = p_{co}(\tilde{w}_k \vert w_i) \\\\\exp(w_j \top \tilde{w}_k) = p_{co}(\tilde{w}_k \vert w_j)</script></li><li><p>只需要满足：</p><script type="math/tex; mode=display">{w_i}^\top \tilde{w}_k = \log p_{\text{co}}(\tilde{w}_k \vert w_i) = \log \frac{C(w_i, \tilde{w}_k)}{C(w_i)} = \log C(w_i, \tilde{w}_k) - \log C(w_i)</script></li><li><p>由于 $w_i$ 和 $\tilde{w}_k$ 是向量，所以 $\tilde{w}_k \top w_i = w_i \top \tilde{w}_k$ ，这就意味着上式中 $i, k$ 是顺序不敏感的，但是右边交换 $i,k$ 的顺序结果就会不同。为了解决这个对称性问题，模型引入两个偏置项 $b_i, b_k$，则模型变成：</p><script type="math/tex; mode=display">\log C(w_i, \tilde{w}_k) = w_i \top \tilde{w}_k + b_i +\tilde{b}_k</script></li><li><p>上面的公式只是理想状态下，实际上左右只能无限接近，所以损失函数定义为：</p><script type="math/tex; mode=display">\mathcal{L}_\theta = \sum_{i=1, k=1}^V ({w_i}^\top \tilde{w}_k + b_i + \tilde{b}_k - \log C(w_i, \tilde{w}_k))^2</script></li><li><p>根据经验，如果两个词共现次数越多，那么两个词在损失函数中的影响就应该越大，所以可以根据两个词共现的次数设计一个权重来对损失函数进行加权：</p><script type="math/tex; mode=display">\mathcal{L}_\theta = \sum_{i=1, j=1}^V f(C(w_i,\tilde{w}_k)) ({w_i}^\top \tilde{w}_k + b_i + \tilde{b}_k - \log C(w_i, \tilde{w}_k))^2</script><p>权重函数 $f(\cdot)$ 应该有以下性质：</p><p>① $f(0)=0$，即如果两个词没有共现过，那么权重为 0；</p><p>② $f(x)$ 必须是一个单调递增的函数。两个词共现次数越多，反而权重越小违反了设置权重项的初衷；</p><p>③ $f(x)$ 对于共现次数过多的词对，不能有太大的值，比如停用词。</p><p>有了这三个性质，可以将 $f(x)$ 定义为：</p><script type="math/tex; mode=display">f(x) = \begin{cases}(\frac{x}{x_{\text{max}}})^\alpha,\quad & \text{if}\quad x<x_{\text{max}}\\\\1, \quad & \text{otherwise}\end{cases}</script><p>根据经验 GloVe 作者认为 $x_\text{max}=100, \alpha=3/4$ 是一个比较好的选择。</p></li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p><a href="https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/" target="_blank" rel="noopener">The amazing power of word vectors</a>, <em>Adrian Colyer</em></p></li><li><p><a href="https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#glove-global-vectors" target="_blank" rel="noopener">Learning Word Embedding</a>, <em>Lilian Weng</em></p></li><li><p><a href="https://jalammar.github.io/illustrated-word2vec/" target="_blank" rel="noopener">Illustrated word2vec</a>, <em>Jay Alammar</em></p></li><li><p><a href="https://zh.d2l.ai/chapter_natural-language-processing/word2vec.html" target="_blank" rel="noopener">Dive into deep learning: word2vec</a></p></li><li><p><a href="https://zh.d2l.ai/chapter_natural-language-processing/glove.html" target="_blank" rel="noopener">Dive into deep learning: GloVe</a></p></li><li><p><a href="http://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">Efficient Estimation of Word Representations in Vector Space</a> <em>Mikolov et al. 2013</em></p></li><li><p><a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener">Distributed Representations of Words and Phrases and their Compositionality</a> <em>Mikolov et al. 2013</em></p></li><li><p><a href="http://www.aclweb.org/anthology/N13-1090" target="_blank" rel="noopener">Linguistic Regularities in Continuous Space Word Representations</a> <em>Mikolov et al. 2013</em></p></li><li><p><a href="http://arxiv.org/pdf/1411.2738v3.pdf" target="_blank" rel="noopener">word2vec Parameter Learning Explained</a> <em>Rong 2014</em></p></li><li><p><a href="http://arxiv.org/pdf/1402.3722v1.pdf" target="_blank" rel="noopener">word2vec Explained: Deriving Mikolov et al’s Negative Sampling Word-Embedding Method</a> <em>Goldberg and Levy 2014</em></p></li><li><p><a href="https://nlp.stanford.edu/pubs/glove.pdf" target="_blank" rel="noopener">GloVe: Global Vectors for Word Representation</a>, <em>Jeffrey Pennington et al. 2014</em></p></li><li><p><a href="https://www.gavagai.io/text-analytics/a-brief-history-of-word-embeddings/" target="_blank" rel="noopener">A Brief History of Word Embeddings</a>, <em>gavagai</em></p></li><li><p><a href="https://licor.me/post/word-representation/" target="_blank" rel="noopener">Word Representation</a>, <em>Chuanrong Li</em></p></li><li><p>Devopedia. 2020. “Word2vec.” Version 4, September 5. Accessed 2021-03-28. <a href="https://devopedia.org/word2vec" target="_blank" rel="noopener">https://devopedia.org/word2vec</a></p></li><li><p><a href="https://www.cnblogs.com/peghoty/p/3857839.html" target="_blank" rel="noopener">word2vec 中的数学原理详解</a>, <em>peghoty</em></p></li><li><p><a href="https://www.techiedelight.com/huffman-coding/" target="_blank" rel="noopener">Huffman Coding Compression Algorithm</a> </p></li><li><p><a href="https://www.programiz.com/dsa/huffman-coding" target="_blank" rel="noopener">Huffman Coding</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/368939108" target="_blank" rel="noopener">噪声对比估计 Noise Contrastive Estimation</a>, <em>码农要术</em></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/42073620" target="_blank" rel="noopener">(十五）通俗易懂理解——Glove算法原理</a>, <em>梦里寻梦</em> </p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://aylien.com/images/uploads/general/tumblr_inline_o8tinsmw081u37g00_540.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;词嵌入（word embedding）是一种用稠密向量来表示词义的方法，其中每个词对应的向量叫做词向量（word vector）。词嵌入通常是从语言模型中学习得来的，其中蕴含着词与词之间的语义关系，比如 “猫” 和 “狗” 的语义相似性大于 “猫” 和 “计算机” 。这种语义相似性就是通过向量距离来计算的。&lt;/p&gt;
    
    </summary>
    
      <category term="语言模型" scheme="https://rogerspy.gitee.io/categories/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="词向量" scheme="https://rogerspy.gitee.io/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>算法与数据结构（Python）：array</title>
    <link href="https://rogerspy.gitee.io/2021/08/09/ds-array/"/>
    <id>https://rogerspy.gitee.io/2021/08/09/ds-array/</id>
    <published>2021-08-09T14:21:24.000Z</published>
    <updated>2022-01-12T08:37:38.345Z</updated>
    
    <content type="html"><![CDATA[<p>数组是一个容器，它容纳的元素应该是相同的数据类型。数组有两个重要概念：</p><ul><li><strong>元素</strong> —— 存储的数组中的数据称为元素。</li><li><strong>索引</strong> —— 数组中每个元素所在的位置。</li></ul><a id="more"></a><h1 id="1-数组的表示"><a href="#1-数组的表示" class="headerlink" title="1. 数组的表示"></a>1. 数组的表示</h1><p><img src="https://codingdict.com/static/assets/tutorials/python/ds/array_declaration.jpg" alt></p><ul><li><code>int</code>  表示数组中数字的类型为整型</li><li><code>array</code> 表示数组的名字</li><li><code>[10]</code> 表示数组的尺寸，即数组中有多少个元素</li><li><code>{35, 33, 42, ...}</code> 表示数组存储的数据</li></ul><p><img src="https://codingdict.com/static/assets/tutorials/python/ds/array_representation.jpg" alt></p><ul><li>索引从 0 开始</li><li>数组的尺寸是 10，表示它可以存储 10 个元素</li><li>每个元素可以通过索引访问</li></ul><h1 id="2-基本操作"><a href="#2-基本操作" class="headerlink" title="2. 基本操作"></a>2. 基本操作</h1><p>数组的基本操作包括：</p><ul><li><strong>遍历</strong> —— 逐个获得数组中的元素</li><li><strong>插入</strong> —— 在指定的位置（索引）处添加一个元素</li><li><strong>删除</strong> —— 删除指定位置（索引）处的元素</li><li><strong>搜索</strong> —— 搜索指定位置（索引）处的元素</li><li><strong>更新</strong> —— 更新指定位置（索引）处的元素</li></ul><p><code>python</code> 内置的 <code>array</code> 模块可以用来创建数组：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> array <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">arrayName = array(typecode, [Initializerss])</span><br></pre></td></tr></table></figure><p>其中 <code>typecode</code> 用于定义数组中元素的数据类型，一些常用的 <code>typecode</code> 如下：</p><div class="table-container"><table><thead><tr><th style="text-align:center">typecode</th><th style="text-align:left">表示</th></tr></thead><tbody><tr><td style="text-align:center">b</td><td style="text-align:left">大小为1字节/ td&gt;的有符号整数</td></tr><tr><td style="text-align:center">B</td><td style="text-align:left">大小为1字节的无符号整数</td></tr><tr><td style="text-align:center">C</td><td style="text-align:left">大小为1字节的字符</td></tr><tr><td style="text-align:center">i</td><td style="text-align:left">大小为2个字节的带符号整数</td></tr><tr><td style="text-align:center">I</td><td style="text-align:left">大小为2个字节的无符号整数</td></tr><tr><td style="text-align:center">F</td><td style="text-align:left">大小为4字节的浮点</td></tr><tr><td style="text-align:center">d</td><td style="text-align:left">大小为8个字节的浮点</td></tr></tbody></table></div><p>举个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> array <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">array1 = array(<span class="string">'i'</span>, [<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>,<span class="number">50</span>])</span><br></pre></td></tr></table></figure><h2 id="2-1-遍历"><a href="#2-1-遍历" class="headerlink" title="2.1 遍历"></a>2.1 遍历</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> array1:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10</span></span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="number">30</span></span><br><span class="line"><span class="number">40</span></span><br><span class="line"><span class="number">50</span></span><br></pre></td></tr></table></figure><h2 id="2-2-搜索"><a href="#2-2-搜索" class="headerlink" title="2.2 搜索"></a>2.2 搜索</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法一</span></span><br><span class="line">print(array1[<span class="number">0</span>])</span><br><span class="line">print(array1[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法二</span></span><br><span class="line">print(array1.index(<span class="number">40</span>))</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10</span></span><br><span class="line"><span class="number">30</span></span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure><h2 id="2-3-插入"><a href="#2-3-插入" class="headerlink" title="2.3 插入"></a>2.3 插入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array1.insert(<span class="number">1</span>,<span class="number">60</span>)</span><br><span class="line">print(array1)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array(<span class="string">'i'</span>, [<span class="number">10</span>, <span class="number">60</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>])</span><br></pre></td></tr></table></figure><h2 id="2-4-删除"><a href="#2-4-删除" class="headerlink" title="2.4 删除"></a>2.4 删除</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array1.remove(<span class="number">40</span>)</span><br><span class="line">print(array1)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array(<span class="string">'i'</span>, [<span class="number">10</span>, <span class="number">60</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">50</span>])</span><br></pre></td></tr></table></figure><h2 id="2-5-更新"><a href="#2-5-更新" class="headerlink" title="2.5 更新"></a>2.5 更新</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array1[<span class="number">2</span>] = <span class="number">80</span></span><br><span class="line">print(array1)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array(<span class="string">'i'</span>, [<span class="number">10</span>, <span class="number">60</span>, <span class="number">80</span>, <span class="number">30</span>, <span class="number">50</span>])</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://codingdict.com/article/4830" target="_blank" rel="noopener">Python-数组</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数组是一个容器，它容纳的元素应该是相同的数据类型。数组有两个重要概念：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;元素&lt;/strong&gt; —— 存储的数组中的数据称为元素。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;索引&lt;/strong&gt; —— 数组中每个元素所在的位置。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="数据结构与算法" scheme="https://rogerspy.gitee.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数组" scheme="https://rogerspy.gitee.io/tags/%E6%95%B0%E7%BB%84/"/>
    
  </entry>
  
  <entry>
    <title>ROC-AUC原理及计算方法</title>
    <link href="https://rogerspy.gitee.io/2021/07/29/roc-auc/"/>
    <id>https://rogerspy.gitee.io/2021/07/29/roc-auc/</id>
    <published>2021-07-29T15:26:19.000Z</published>
    <updated>2022-01-12T08:37:38.407Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文转载自知乎用户<strong>码农要术</strong>的文章 <a href="https://zhuanlan.zhihu.com/p/141266017" target="_blank" rel="noopener">衡量指标篇：ROC-AUC</a>。</p></blockquote><h1 id="1-历史起源"><a href="#1-历史起源" class="headerlink" title="1. 历史起源"></a>1. 历史起源</h1><p>1941年，日军偷袭珍珠港，太平洋战争由此爆发。美军的雷达操作员（Radar operator）开始忙碌了起来，他们要识别出雷达屏幕上的光点是不是日本的战机。</p><a id="more"></a><p><img width="300" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/radar.jpg"></p><p>因为光点也有可能是我方军舰，也有可能是噪声。为了衡量识别的性能，研究者们设计了ROC曲线（Receiver operating characteristic curve）。所谓 Receiver 就是雷达接收器，operating characteristic 则是表明雷达操作员（Radar operator）的识别能力。</p><p>后来，ROC曲线被应用到了医学领域，还有机器学习领域。虽然名字比较奇怪，但是从诞生之初，ROC 曲线的目的就是衡量分类的性能。AUC 是 ROC 曲线下的面积（ <strong>A</strong>rea <strong>U</strong>nder the ROC <strong>C</strong>urve），有一些优雅的性质，我们后面再说。</p><p>想讲清楚 ROC曲线，先要讲一下混淆矩阵。</p><h1 id="2-混淆矩阵"><a href="#2-混淆矩阵" class="headerlink" title="2. 混淆矩阵"></a>2. 混淆矩阵</h1><p>先从两类开始说起，Positive 和 Negative，医学上叫阳性和阴性，机器学习称之为正例和负例。经过分类器的决策后，一般情况下，正例预测的有对有错，负例预测的也有对有错。这样数据会被划分成4部分：<strong>正例预测对（True Positive），正例预测错（False Negtative），负例预测对（True Negative），负例预测错（False Positive）。</strong></p><p><img width="600" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/confusion_matrix1.jpg"></p><h1 id="3-如何衡量分类器的好坏？"><a href="#3-如何衡量分类器的好坏？" class="headerlink" title="3. 如何衡量分类器的好坏？"></a>3. 如何衡量分类器的好坏？</h1><p>如何衡量一个分类器是有效的，而不是随机结果？还是以雷达识别敌舰这个场景来说明。</p><h2 id="3-1-两个假设"><a href="#3-1-两个假设" class="headerlink" title="3.1 两个假设"></a>3.1 两个假设</h2><ul><li><p>正负例等比例分布 </p></li><li><p>分类器输出是离散值, 也就是 label 的集合</p></li></ul><p>此时预测为正的结果可以划分成两部分：$TP$ 和 $FP$。比较两者关系，有如下结论：</p><ol><li>如果分类器是随机抽样，那么模型的输出和正负例比例一致。也就是 $TP=FP$。这个时候向识别出来的敌舰(预测为正的样本)开炮就是无差别攻击。</li><li>如果 $TP&gt;FP$, 可以说分类器有一定的甄别能力，战争中可以做到伤敌一千，自损八百。</li><li>如果是 $TP&lt;FP$ ,则说明分类器太差了，都不如随机抽样。用在战争中可以做到伤敌八百，自损一千。</li></ol><h2 id="3-2-一个假设"><a href="#3-2-一个假设" class="headerlink" title="3.2 一个假设"></a>3.2 一个假设</h2><blockquote><p>分类器输出是离散值, 也就是 label 的集合</p></blockquote><p>这个时候在用TP和FP的绝对值做对比就显得不公平, 举个例子，我方军舰10艘，敌方军舰100艘。预测并且击沉我方军舰 8 艘，敌方军舰 9 艘.绝对数量上确实是占优势，但是我方基本全军覆没，敌方绝大多数战力仍然保留。样本不均衡时，就得做归一化，看相对值。</p><p>这里引入两个概念：$TPR$ （True Positive Rate），$FPR$（False Positive Rate）</p><script type="math/tex; mode=display">TPR = \frac{TP}{TP+FN} \\\\FPR = \frac{\mathrm{FP}}{FP+TN}</script><p>$TPR$ 就是正例中预测正确的比率。$FPR$ 就是负例预测错的比例。</p><p>$TPR$ 和 $FPR$，比较两者关系，有如下结论：</p><ol><li>如果分类器是随机抽样，那么模型的输出和正负例比例一致。也就是 $TPR=FPR$。这个时候向识别出来的敌舰(预测为正的样本)开炮就是无差别攻击。</li><li>如果 $TPR&gt;FPR$, 可以说分类器有一定的甄别能力，战争中伤敌的比率高于自损的比率。</li><li>如果是 $TPR&lt;FPR$ ,则说明分类器太差，不如随机抽样。战争中伤敌的比率低于自损的比率。</li></ol><p>把 $TPR$ 和 $FPR$ 可视化，在 “<em>分类器输出是离散值, 也就是 label</em>“ 的假设下，$TPR$ 和 $FPR$ 是确定的，在二维坐标系上就是一个点。这个点就是 ROC 曲线的雏形。如下图：</p><p><img width="300" src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/roc-auc8.jpg"></p><p>图中，E 点就是随机抽样 （$TPR=FPR$）。A，B，D点表示分类器有一定的甄别能力（$TPR&gt;FPR$）。其中 A 点对应的是一个完美的分类器，所有的正例被识别正确（$TPR=1$），所有的负例没有识别错误（$FPR=0$）。F 点就是分类器太差（$TPR&lt;FPR$），不如随机抽样。</p><h2 id="3-3-另一个假设"><a href="#3-3-另一个假设" class="headerlink" title="3.3 另一个假设"></a>3.3 另一个假设</h2><blockquote><p> 分类器输出连续值</p></blockquote><p>此时需要确定一个阈值来决定混淆矩阵和 $TPR$，$FPR$。</p><p>$TPR$ 的计算如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/roc-auc1.gif" alt></p><p>$FPR$ 的计算如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/roc-auc2.gif" alt></p><p>对于同一个分类器，不同的阈值对应不同的 $TPR$ 和 $FPR$，遍历阈值，即可得到 ROC 曲线。如下图：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/roc-auc3.gif" alt></p><p>对于一个分类器，固定阈值，则得到一条 ROC 曲线。不同分类器会使预测的数据分布不同，在固定阈值的情况下，ROC 曲线变化如下图：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/roc-auc4.gif" alt></p><p>直观来看，分类器的区分度越好，ROC 曲线则越往左上角靠拢。AUC 就越大。怎么解释？</p><h1 id="4-AUC-的概率解释"><a href="#4-AUC-的概率解释" class="headerlink" title="4. AUC 的概率解释"></a>4. AUC 的概率解释</h1><p>如果把 ROC 曲线看成是 $TPR$ 对 $FPR$ 的函数，$TPR=F(x)$ 我们对这个函数进行积分。如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/roc-auc5.jpg" alt></p><script type="math/tex; mode=display">AUC = \int_{0}^1F(x)dx</script><p>假设样本标签为 $y$，模型预测得分为 $s$，阈值为 $t$，正例的概率密度函数为 $f_1(s)$，负例的概率密度函数为 $f_0(s)$，则有</p><script type="math/tex; mode=display">TPR = F(x) = \int_t^\infty f_1(s)ds = P(s>t|y=1) \\\\FPR = x = \int_t^\infty f_0(s)ds = 1-\int_{-\infty}^t f_0(s)ds</script><p>$x$ 是 $t$ 的积分上限函数，根据积分上限函数的性质，得到</p><script type="math/tex; mode=display">\frac{dx}{dt} = \frac{d}{dt}(1-\int_{-\infty}^t f_0(s)ds) = -f_0(t) \\\\dx = -f_0(t)dt = -P(s'=t|y'=0)dt</script><p>则有</p><script type="math/tex; mode=display">\begin{equation} \nonumber\begin{aligned}AUC &= \int_0^1F(x)dx \\\\    &= \int_{\infty}^{-\infty} F(x)(-f_0(t))dt \\\\    &= \int_{-\infty}^{\infty} F(x)f_0(t)dt \\\\    &= \int_{-\infty}^{\infty} P(s>t|y=1)f_0(t)dt \\\\    &= \int_{-\infty}^{\infty} P(s>t|y=1)\times P(s/=t|y'=0)dt \\\\    &= \int_{-\infty}^{\infty} P(s>s'\ \&\ s'=t|y=1\ \&\ y'=0)dt \\\\    &= P(s>s'|y=1\ \&\ y'=1)\end{aligned}\end{equation}</script><p>上面推导需要解释一下：</p><ol><li>第二行，因为 $FPR$ 的取值范围从 0 到 1，对应着阈值是从大到小的变化。可以从动图中看出，只不过动图中阈值是从小到大，$FPR$ 是从 1 到 0。</li><li>第五行，$f_0(t)$ 的含义就是该样本为负例，得分为 $t$ 的概率。加引号是为了和正例区分。</li><li>第七行，该积分相当于是遍历阈值 $t$，同时负例得分和 $t$ 相同，也就是负例遍历所有可能的得分情况。</li></ol><p>最终得到这么一个结论：</p><blockquote><p><strong><em>AUC 的值，就是从样本中任意取一个正例和一个负例，正例得分大于负例得分的概率。</em></strong></p></blockquote><h1 id="5-AUC-的一些性质"><a href="#5-AUC-的一些性质" class="headerlink" title="5. AUC 的一些性质"></a>5. AUC 的一些性质</h1><p>从公式可以看出，$TPR$ 的计算只局限在正例中，$FPR$ 的计算只局限在负例中。正例（或负例）如果同分布的增加或者减小，对于 ROC 曲线来说没有区别，因为在正例（或负例）内部已经做了归一化。如下图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/roc-auc6.gif" alt></p><p>但如果正例（或负例）的比例在变化的同时，分布也发生了变化，那么 ROC 和 AUC 也会随之变化。如下图</p><p><img src="https://cdn.jsdelivr.net/gh/rogerspy/blog-imgs/roc-auc7.gif" alt></p><p>AUC 使用时，有几点需要注意：</p><ol><li>AUC 只关注预测的正负例的顺序关系，对于和 label 的拟合情况则不关注。比如正例得分都是 0.2，负例得分都是 0.1，AUC 很完美，但是 loss 就会比较大。</li><li>在非常不均衡的样本上，AUC 表现可能很好，但 precision 可能比较差。比如 $TP=80$，$FN=20$，$FP=200$，$TN=8000$，此时从 ROC 空间上看，效果还不错，但是 precision 低的可怜。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文转载自知乎用户&lt;strong&gt;码农要术&lt;/strong&gt;的文章 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/141266017&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;衡量指标篇：ROC-AUC&lt;/a&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;1-历史起源&quot;&gt;&lt;a href=&quot;#1-历史起源&quot; class=&quot;headerlink&quot; title=&quot;1. 历史起源&quot;&gt;&lt;/a&gt;1. 历史起源&lt;/h1&gt;&lt;p&gt;1941年，日军偷袭珍珠港，太平洋战争由此爆发。美军的雷达操作员（Radar operator）开始忙碌了起来，他们要识别出雷达屏幕上的光点是不是日本的战机。&lt;/p&gt;
    
    </summary>
    
      <category term="博客转载" scheme="https://rogerspy.gitee.io/categories/%E5%8D%9A%E5%AE%A2%E8%BD%AC%E8%BD%BD/"/>
    
    
      <category term="ROC-AUC" scheme="https://rogerspy.gitee.io/tags/roc-auc/"/>
    
  </entry>
  
</feed>
